# RL Agents for LLMs

The typical LLM training pipeline is as follows
1. SfM fine-tuning - Depends heavily on human expert demo data, and requires a lot of effort. This step can be compared to behavior cloning in RL
2. Some pipelines also talk about pre-training that can be thought of as weight initialization. This step is not used so much anymore.
3. After this stage, *learning based on feedback*, has become a standard step. It comes in many forms, RLHF, RLAIF, etc. The primary approach, Reinforcement Learning with Human Feedback essentially rates different texts generated by the AI, and uses this as a reward model and improves the model considering it as a policy.

    This step is cheaper than the other steps, so companies are pushing towards improving this. However, in practice it does not seem to work without pre-training. 
    
    This step also involves training a human-proxy reward function. In whole, this is known as post-training development.
    
## RLHF
As we mentioned, the goal is to collect preference feedback and train a reward model. It is a kind of a  personalized learning to correct the text generated by the model. One thing to note is that the rewards are given towards the end and there are no intermediate rewards. 

For the policy training itself, we have studied value and policy based algorithms. If the value of the MDP is estimated, the policy can be determined implicitly (argmax or epsilon-greedy). However, values and rewards of partial sentences are more difficult to estimate. 

Due to these limitations, researchers have gravitated towards policy-based RL. It has better convergence properties and is effective in high-dimensional or continuous actions spaces. However, they can converge to a local optimal rather than a global optima. 

“”” I was super sleepy after this, please update after watching the recording “””

Now, for each step, we make a decision within a one-step MDP
- Expectation equation
    - SFT step is important

    
**Policy Gradient Theorem** - For any differential blue policy $$\pi_\theta (s, a)
