<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://sudhansh6.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sudhansh6.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-17T00:02:10+00:00</updated><id>https://sudhansh6.github.io/feed.xml</id><title type="html">Sudhansh Peddabomma</title><entry><title type="html">Brains and AI</title><link href="https://sudhansh6.github.io/blog/brains-and-ai/" rel="alternate" type="text/html" title="Brains and AI"/><published>2025-01-17T00:00:00+00:00</published><updated>2025-01-17T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/brains-and-ai</id><content type="html" xml:base="https://sudhansh6.github.io/blog/brains-and-ai/"><![CDATA[<h1 id="distributed-representations-of-words-and-phrases-and-their-compositionality"><a href="https://arxiv.org/abs/1310.4546">Distributed Representations of Words and Phrases and their Compositionality</a></h1> <p>Presented by <a href="https://www.linkedin.com/in/joycelyn-yiu">Joycelyn Yiu</a></p> <p>What are distribution representations of words? Since symbols have no internal structure, researchers tries to represent words with vectors such that words that are similar to each other are closer in the vector space and vice versa.</p> <p>The inspiration for this idea comes from 1980’s, to convey semantics of language to machines. These representations can capture patterns and relationships between words like synonyms and grammar rules. How do we capture these patterns in practice?</p> <h3 id="skip-gram-model">Skip-Gram model</h3> <p>The skip gram model is a method that teaches a computer to understand the meaningful of words by reading large pieces of text. It basically learns the word relationships by predicting the surrounding words for a given word.</p> <p>In essence, from the training text, we compute the probability of <em>context</em> words occurring after a <em>center</em> word. How do you learn a distribution over words?</p> <p>The authors of the paper replaced softmax with a simpler one called <strong>Noise Contrastive Estimation NCE)</strong> that improved training and quality of representations.</p> <p>This model typically does not work well for rare words.</p> <p>In addition to the vanilla model, they added extension to represent phrases. Let us delve into each of these contributions.</p> <h2 id="method">Method</h2> <h3 id="hierarchical-softmax">Hierarchical softmax</h3> <p>The authors introduced hierarchical softmax that brought down the number of evaluations to \(\log_2(W)\) nodes rather than the typical \(W\) output nodes (where \(w\) is the number of words in the vocabulary).</p> <blockquote> <p>Is this used in LLMs? LLMs have become ubiquitous because of their parallelizability, and this takes it away to some extent.</p> </blockquote> <h3 id="negative-sampling">Negative Sampling</h3> <p>Introduced by Guzman and Hyvarinen, NCE tries to differentiate data from noise using logistic regression.</p> <h3 id="subsampling-of-frequent-words">Subsampling of Frequent Words</h3> <p>Common words like “the”, “a”, etc. occur frequently in text and do not provide a lot of information as compared to rare words. To counter this imbalance, the authors introduced a frequency based discard probability for the words to subsample the words. This improves the training speed and the accuracy of rare words.</p> <h3 id="learning-phrases">Learning phrases</h3> <p>The simple addition the authors did was to create new tokens for phrases like “The New York Times” - increasing the vocabulary size potentially making it unscalable. However, iteratively training the model considering longer phrases seemed to obtain a decent performance according to the authors.</p> <p>These techniques can be added to any underlying neural network model.</p> <h2 id="experiments">Experiments</h2> <p>The authors noticed that the representations posses a linear structure that allowed vector arithmetic. This potentially is a consequence of the training objective - the word vectors are in a linear relationship with the inputs to softmax non-linearity.</p> <h1 id="attention-is-all-you-need"><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></h1> <p>Presented by <a href="https://www.linkedin.com/in/hansenlillemark">Hansen Jin Lillemark</a></p> <p>What is intelligence? We represented words with high-dimensional vectors, and we seemed to have cracked natural language? How does the brain understand and represent concepts?</p> <p>Language is one of the easiest avenues to test our algorithms of artificial intelligence. We have stored a lot of data in the past few decades, and this set up a good foundation to train models here.</p> <p>Transformers, inspired from some brain mechanisms, have become the go-to mechanism for most of the ML applications. It starts off with the language modeling task where the task is to predict the next word \(x_t\) give the previous words in the sentence. This very general task was first introduced by Shannon to define information and entropy.</p> <p>Domains like mathematics have deterministic distributions whereas reasoning usually has a flatter distribution. Another issue with language is that it keeps changing with time, and the models need to be dynamic enough to maintain this.</p> <h2 id="method-1">Method</h2> <p>Continuing from the Word2Vec paper, we represent words with vector embeddings.</p> <blockquote> <p>The embedding size is essentially like PCA - decomposing meanings into a low-dimensional space. Although there are over hundred thousand words, we are representing them in 500 dimensions?</p> </blockquote> <blockquote> <p>Can we create different similarity kernels to create different meanings? This effect is achieved through multi-head attention.</p> </blockquote> <blockquote> <p>Different language to language translation is possible with LLMs. Is there an inherent universal language?</p> </blockquote> <blockquote> <p>LLMs are able to reason well in some scenarios. Try asking it a novel puzzle and see how it does. However, it struggles with math. Is there something we are missing?</p> </blockquote> <blockquote> <p>Decoder inference is still sequential… Training seems more efficient but inference…</p> </blockquote> <h1 id="bias-variance-tradeoff">Bias-Variance Tradeoff</h1> <p>This topic seems like a thing of the past. We have built very large models around large amounts of data, and everything seems to work. So, why worry about bias or variance? Bias is solved by large models and variance is solved by large data. If all’s good in the world, then why don’t we have zero loss?</p> <blockquote> <p>Recap: The bias-variance trade-off. When low complexity models cannot inherently capture the complexity of the data, they have a <em>bias</em> or they <em>underfits</em> the training data. It can arise from a poor architecture or features engineered from the data. On the other end, complex models with large amount of parameters may fit the training data really well, but they tend to perform poorly on the test loss. This error, called <em>variance error</em>, occurs due to <em>overfitting</em>. in these cases, models are said to not generalize well across different data distributions. This trade-off between bias and variance is still present at the core of modern models.</p> </blockquote> <p>To counter these problems, early approaches involved <strong>regularizing</strong> models to prefer solutions with “simple” patterns - ones that have low-norm weights even in over-parameterized models. There are modern theoretical frameworks such as the <a href="https://arxiv.org/abs/1806.07572">Neural Tangent Kernels (NTK)</a> that show that over-parameterized networks behave like kernel machines and converge to smooth solutions due to regularization. That is, interpolating regularized solutions seems to generalize well.</p> <p>I also mentioned <em>poor model architecture</em> as a source of these errors. Over-parameterized models can often be replaced with models designed with a good <strong>inductive bias</strong> - models that leverage the structures in the data to generate structured solutions. For example, CNNs leveraged that images have spatial-features, and replaced fully-connected layers with convolutional kernels that greatly reduced the number of parameters.</p> <blockquote> <p><em>Model architectures</em> designed with inductive bias are the best kind of models. Attention is also a product of such design philosophies.</p> </blockquote> <p>So far, we have</p> <ul> <li>Maybe over-parameterization works but we need to have appropriate regularization tricks such as dropout and weight decay</li> <li>Adaptive optimizers, early stopping, large batch training all seem to make sense</li> </ul> <p>In 2018, <a href="https://arxiv.org/abs/1812.11118">Belkin et al.</a>, showed that test loss follows a “double descent” curve - it peaks at a critical model complexity, then decreases. This phenomenon has been seen to occur in CNNs, Transformers, and linear models with high dimensional features. The take away message is that more complexity does not mean worse generalization in the modern architectures.</p> <p><img src="/assets/img/BrainsAI/17377675747647.jpg" alt=""/></p> <p>How does data fit in all this? More the amount of data, the simpler the model becomes - we have seen that the inner layers of LLMs require sparse changes in the weights to fine-tune to different datasets (LoRA). It maybe seen as if large datasets prevent overfitting since larger models are able to absorb the noise in the data without harming the underlying signal.</p> <p>In 2019, <a href="https://arxiv.org/abs/1906.11300">Bartlett et al.</a>, showed that models can memorize noisy data but still generalize if noise is structured or data has low intrinsic dimensions. High-dimensional mode old can separate signal from noise via implicit regularization. At the core of some of the large models we have built, we made a rather huge assumption - the noise in the data is Gaussian. The MSE loss is nothing but a negative likelihood over Gaussian noise. These assumptions must be carefully considered while building models for different applications.</p> <p>So what do we make of all this? It’s new information that we didn’t have before while designing models. Maybe it’s because of this the model scaling laws are working.</p> <h1 id="benign-overfitting-in-linear-regression"><a href="https://arxiv.org/abs/1906.11300">Benign overfitting in linear regression</a></h1> <p>The ultimate goal of machine learning is how to train a model that fits to a given data. We are approaching this by reducing the empirical training risk/error through a loss function. There is a mis-match between what are doing and the goal we are trying to achieve - reducing the test loss or generalize well to new data. Let us understand this better.</p> <p>A model’s ability to fit a wide variety of functions or patterns in the data is a known as its <em>capacity</em>. As we have increased the models’ capacity, we seemed to have the cross the peak in the double descent curve - they are over-fitting but it seems to be benign. That is, they seem to have zero training risk and the test risk approaches the best possible value. Why do we think this is benign? As the model capacity increases, the test loss seems to be decreasing even more. So how do we reach this benign overfitting region?</p> <p>The authors tested this with linear regression and significant over-parameterization. For a linear regression model the minimum norm solution is given by</p> \[\hat \theta = X^T = (XX^T)^{-1}y \; X\theta = y\] <p>The authors define the excess risk of the estimator as</p> \[T(\theta) := \mathbb E_{x, t} [(y - x^T \theta)^2 - (y - X^T \theta^*)^2]\] <p>How do you over-parameterize a linear regression model? The authors consider the number of eigenvectors of the covariance of the data. They consider quantities from the PCA theory. They concluded that if the decay of the eigenvalues of the covariance is sharp, then we can reach the benign overfitting region.</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[The primary motivation for AI stems from our brains. How has our research in cognitive science shaped the modern AI systems?]]></summary></entry><entry><title type="html">Machine Learning Systems</title><link href="https://sudhansh6.github.io/blog/machine-learning-systems/" rel="alternate" type="text/html" title="Machine Learning Systems"/><published>2025-01-16T00:00:00+00:00</published><updated>2025-01-16T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/machine-learning-systems</id><content type="html" xml:base="https://sudhansh6.github.io/blog/machine-learning-systems/"><![CDATA[<blockquote> <p>Acknowledging use of Perplexity, ChatGPT and the reference books and papers to aid with the content.</p> </blockquote> <h1 id="introduction">Introduction</h1> <p>As with every other article, this one starts with the same remark too - We are at an inflection point in human history. Artificial Intelligence (AI) is everywhere, and its growth has been unparalleled. We are in the AI revolution.</p> <p>At times like these, it is important to think about the future - how do we create systems that work alongside humanity to tackle the biggest problems life faces? We must master a new field of engineering to maintain this unprecedented pace.</p> <h2 id="history-of-ai">History of AI</h2> <p><img src="assets/reading01/2025-01-16-17-56-14-image.png" alt=""/></p> <p>Starting with symbolic AI, one of the early approaches, STUDENT system developed by Daniel Bobrow in 1964, demonstrated natural language understanding by converting English text into algebraic equations. However, it was a rule-based system that was very brittle to the grammatical structure - although the solution may appear intelligent, it did not have a genuine understanding of language or the actions it was taking. This issue is the primary problem behind rule-based approaches.</p> <p>Then followed expert systems in 1970s, which focused on domain specific problems over generalized AI. For example, MYCIN developed by Stanford showed a major leap in medical AI with it’s 600 rules-based system. Nevertheless, scaling these rules with newer discoveries is infeasible.</p> <h3 id="statistical-learning">Statistical Learning</h3> <p>In the 1990s, something radical happened - the field moved away from rules to data-driven approaches. With the digital revolution, new capabilities got unlocked. From regression to neural networks, we allowed machines to discover patterns in the data leading to diverse applications. This new paradigm changed the way we approached machine learning. Quality of data, metrics to evaluate performance and trade-offs in design choices - all became relevant research questions.</p> <p>During the 2000s, algorithms like decision trees and KNNs made their way into practical applications. SVMs with their “kernel trick” became very popular. Human engineered features combined with statistical learning was the characteristic in the algorithms, and they had strong mathematical foundations as well. The models performed well with limited data, and produced reliable results. Even modalities like face detection with Viola-Jones algorithm (2001) became feasible.</p> <h3 id="deep-learning">Deep Learning</h3> <p>A double-edged sword, deep learning became the new kid in the block since the 2010s. Deep learning networks automatically discovered features in the data, doing away with feature engineering. Starting with AlexNet in 2012, the successes with deep learning models have never been shy. Researchers realized that bigger models equals better performance. Now, in the 2020s, we have entered the age of large models which have parameters reaching into few hundreds of billions! The datasets are well into the Petabytes stage.</p> <p>The three pillars required for these models to be successful, big data, better compute, and algorithmic breakthroughs, have successfully been put in place over the past few decades.</p> <p>These new depths have raised important questions about the future: how do we store and serve such models and datasets?!</p> <h2 id="ml-systems-engineering">ML Systems Engineering</h2> <p>It is the discipline of designing, implementing, and operating AI systems across computing scales. A machine learning system is an integrated computing system with three main parts - data, models and the compute infrastructure.</p> <p>Software Engineering as a field has been well established over the past decade. Even though the field is not yet mature, it has practices in place to enable reliable development, testing and deployment of software systems. However, these ideas are not entirely applicable to Machine Learning systems. Changes in the data distribution can alter the system behavior - this is a new paradigm we have not addressed before.</p> <p>More than often, the performance requirements guide the design decisions in the architecture. The complexities arise due to the broad spectrum across which ML is deployed today - from edge-devices to massive GPU-cloud clusters, each presents unique challenges and constraints. Operational complexities increase with system distribution. Some applications may have privacy requirements. The budget of the system acts as an important constraint. All these tradeoffs are rarely simple choices.</p> <p>Modern ML systems must seamlessly connect with existing software, process diverse data sources, and operate across organizational boundaries, driving new approaches to system design. FarmBeats by Microsoft, Alphafold by Deepmind and Autonomous vehicles are excellent examples of how proper systems in place can really push the extent of ML applicability.</p> <h2 id="challenges">Challenges</h2> <ol> <li> <p>Data Challenges - How do we store and process different kinds of data, and how to accommodate patterns with time?</p> </li> <li> <p>Model Challenges - How do we create efficient systems for different forms of learning, and test their performance across a wide range of scenarios?</p> </li> <li> <p>Systems Challenges - How do we set up pipelines in place to combine all of this in place? Systems that have monitoring and stats, that allow model updates and handle operational challenges.</p> </li> <li> <p>Ethical and Social Considerations - How do we address bias in such large-scale models? Can we do something about the “black-box” nature? Can we handle data privacy and handle inference attacks?</p> </li> </ol> <p>A major solution to address all these challenges has been to <em>democratize AI technology</em>. Similar to an “all hands-on deck” solution, with the involvement of a large amount of people in this evolution, we are tackling one of the most innovative problem’s humanity has ever faced - how do we achieve AGI?</p> <h1 id="dnn-architectures">DNN Architectures</h1> <p>Assuming the reader’s know enough about different model architectures, we shall now discuss the core computations involved in these models to design the systems around them.</p> <h2 id="architectural-building-blocks">Architectural Building Blocks</h2> <p>So far, we have the following major architectures summarized below -</p> <h3 id="multi-layer-perceptrons-mlps">Multi-Layer Perceptrons (MLPs)</h3> <ul> <li> <p>Purpose: Dense pattern processing</p> </li> <li> <p>Structure: Fully connected layers</p> </li> <li> <p>Key operation: Matrix multiplication</p> </li> <li> <p>System implications:</p> <ul> <li>High memory bandwidth requirements</li> <li>Intensive computation needs</li> <li>Significant data movement</li> </ul> </li> </ul> <h3 id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</h3> <ul> <li> <p>Purpose: Spatial pattern processing</p> </li> <li> <p>Key innovations: Parameter sharing, local connectivity</p> </li> <li> <p>Core operation: Convolution (implemented as matrix multiplication)</p> </li> <li> <p>System implications:</p> <ul> <li>Efficient memory access patterns</li> <li>Specialized hardware support (e.g., tensor cores)</li> <li>Opportunity for data reuse</li> </ul> </li> </ul> <h3 id="recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</h3> <ul> <li> <p>Purpose: Sequential pattern processing</p> </li> <li> <p>Key feature: Maintenance of internal state</p> </li> <li> <p>Core operation: Matrix multiplication with state update</p> </li> <li> <p>System implications:</p> <ul> <li>Sequential processing challenges</li> <li>Memory hierarchy optimization for state management</li> <li>Variable-length input handling</li> </ul> </li> </ul> <h3 id="transformers-and-attention-mechanisms">Transformers and Attention Mechanisms</h3> <ul> <li> <p>Purpose: Dynamic pattern processing</p> </li> <li> <p>Key innovation: Self-attention mechanism</p> </li> <li> <p>Core operations: Matrix multiplication and attention computation</p> </li> <li> <p>System implications:</p> <ul> <li>High memory requirements</li> <li>Intensive computation for attention</li> <li>Complex data movement patterns</li> </ul> </li> </ul> <p>Some innovations like skip connections, normalization techniques, and gating mechanisms are highlighted as important building blocks that require specific architectures. With these in mind, we require the following primitives -</p> <ul> <li> <p><strong>Core Computational Primitives</strong> -Matrix multiplication, Sliding window operations, Dynamic computation</p> </li> <li> <p><strong>Memory Access Primitives</strong> - Sequential access, Strided access, Random access</p> </li> <li> <p><strong>Data Movement Primitives</strong> - Broadcast, Scatter, Gather and Reduction</p> </li> </ul> <p>We address these primitives, by designing efficient systems as well as hardware.</p> <ul> <li> <p><strong>Hardware</strong> - Development of specialized processing units (e.g., tensor cores), Complex memory hierarchies and high-bandwidth memory, Flexible interconnects for diverse data movement patterns</p> </li> <li> <p><strong>Software</strong> - Efficient matrix multiplication libraries, Data layout optimizations, Dynamic graph execution support</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <p>In summary, understanding the relationship between high-level architectural concepts is important for their implementation in computing systems. The future advancements in deep learning will likely stem from both novel architectural designs and innovative approaches to implementing and optimizing fundamental computational patterns.</p> <p>Now that we have understood the basics of Machine Learning Systems, let us delve into the two biggest frameworks that support the ML systems today.</p> <h1 id="tensorflow-a-system-for-large-scale-machine-learning"><a href="https://arxiv.org/pdf/1605.08695">TensorFlow: A system for large-scale machine learning</a></h1> <p>A product of Google brain built to tackle large scale systems in heterogeneous environments. TensorFlow uses data-flow graphs to represent computations, shared states and operations. These can be mapped across many machines giving a flexibility to the developers.</p> <p>TensorFlow is based on a previous product of Google Brain, DistChild, that was used for a large number of research and commercial tasks. The team recognized the recent advances in ML - CNNs that broke records reaching up to millions of parameters and GPUs that accelerated the training processes. They developed a framework that is well-optimized for general scenarios for both training and inference. They also meant for it to be <em>extensible</em> - to allow ability to experiment and scale in production with the same code.</p> <p>Prior to this work, Caffe, Theano and Torch were the major frameworks. Caffe was known for its high-performance, Theano for its data flow model and Torch for its imperative programming model. Along with these works, TensorFlow also draws inspiration from the map-reduce paradigm that improved the performance of data systems significantly.</p> <h2 id="tensorflow-execution-model">TensorFlow execution model</h2> <p>The computation and state of the algorithm, including the mathematical operations are represented in a single dataflow graph. The communications between the subcomputations are made explicitly to execute independent computations in parallel and partition the computation across distributed devices. The key point to note here is that the graph has mutable states for the individual vertices, meaning that data can be shared between different executions of the graph. Although this point may seem trivial now, architectures prior to TensorFlow worked with static computations graphs that did not allow changes to the weights - algorithms such as mini-batch gradient descent were not scalable to large parameter models. Due to this change in the architecture, developers can experiment with different optimization algorithms, consistency schemes and parallelization strategies.</p> <h3 id="dataflow-graph-elements">Dataflow graph elements</h3> <p>Each vertex in the dataflow graph is an atomic unit of computation and each edge is the output or input to a vertex - values flowing through tensors.</p> <p><strong>Tensors</strong> - Notably, <em>tensors</em> as a computational quantity were introduced in this paper. Their work assumes all tensors are dense - this allows simple implementations for memory allocation and serialization.</p> <blockquote> <p>Modern neural networks on the contrary have sparse tensors in many scenarios - can be optimized greatly</p> </blockquote> <p>TensorFlow does allow representing sparse tensors but at the cost of more sophisticated shape inference.</p> <p><strong>Operations</strong> - An operation can simply be thought of as a function that takes \(m \geq 0\) tensors as input and returns \(n \geq 0\) tensors as output. The number of arguments to these operators can be constant or variable (<em>variadic</em> operators). <strong>Stateful operations</strong> (variables and queues) contain mutable states that can be updated every time the operator executes. Variables are mutable buffers storing shared parameters and queues support advanced forms of coordination.</p> <h3 id="partial-and-concurrent-execution">Partial and concurrent execution</h3> <p>The key advantage of storing the dataflow as a graph is the ability to execute independent operations in parallel. Once a graph is defined by the user, the API allows for executing any sub-graph the user queries. Each invocation is a <em>step</em> and TensorFlow supports multiple <em>concurrent steps</em>. This ability shines for the batch-processing workflows in neural networks. Furthermore, TensorFlow has a checkpointing subgraph that runs periodically for fault tolerance.</p> <p>This functionality of running graphs partially and concurrently contribute much to TensorFlow’s flexibility.</p> <h3 id="distributed-execution">Distributed execution</h3> <p>Since the communication between subcomputations is explicit, the distribution of the dataflow becomes simpler. Each operation resides on a particular <em>device</em> (note that this feature has also been adapted in PyTorch), and a device is responsible for executing a <em>kernel</em> for each operation assigned to it. TensorFlow allows multiple kernels to be registered for a single operation.</p> <p>The placement algorithm computes a feasible set of devices for each operation, calculates the sets of operations that must be colocated and selects a satisfying device for each colocation group. In addition, the users can also specify their device preferences for a particular task. <em>Yes, TensorFlow was advanced since the beginning</em>.</p> <p>Once the operations are placed, the partial subgraphs are computed for a step, and are pruned and partitioned across the devices. The communication mechanisms between devices is also put in place with specialized implementations to optimize for latency with large-subgraphs running repeatedly. On the user-end, a <em>client session</em> maintains a mapping from step definitions to cached subgraphs. This computation model performs the best with static, reusable graphs but also supports dynamic computations with control flows.</p> <h3 id="dynamic-control-flow">Dynamic control flow</h3> <p>Although most evaluation in TensorFlow is <em>strict</em>, wherein it requires for all inputs to an operation to be computed before the operation executes, advanced algorithms (such as RNNs), require dynamic control flow, requiring non-strict evaluation. To aid with this, TensorFlow also supports primitive <strong>Switch</strong> (demultiplexer) and <strong>Merge</strong> (multiplexer) operations for dynamic dataflow architectures! These primitives can be used to build a non-strict conditional sub-graph that executes one of two branches based on the runtime values of a tensor. These primitives also support loops with additional structural constraints based on the dataflow!</p> <h2 id="extensibility">Extensibility</h2> <ul> <li> <p><strong>Differentiation</strong> - TensorFlow has a user-level library that <strong>automatically differentiates</strong> expressions. It performs a breadth-first search to identify all of the backwards paths from the target operation (loss function) to a set of parameters in the computation graph, and sums the partial gradients that each path contributes. With optimizations such as batch normalization and gradient clipping, the algorithm also supports backpropagation through conditional and iterative subcomputations! All of this done with memory management on GPU.</p> </li> <li> <p><strong>Optimization</strong> - SGD is a simple algorithm encoded in the framework. However, for more advanced optimization schemes like Momentum, TensorFlow relies on community driven implementations that are easily pluggable without modifying the underlying system. Such a modular framework was not available before.</p> </li> <li> <p><strong>Handling Large models</strong> - Even back then, the language models were too large to store in RAM of a single host. For the language specific case, TensorFlow has <em>sparse embedding layers</em> that is a primitive operation that abstracts storing and reading a tensor across different memory spaces. They are implemented with operators such as <code class="language-plaintext highlighter-rouge">gather</code>, <code class="language-plaintext highlighter-rouge">part</code> and <code class="language-plaintext highlighter-rouge">stitch</code>, and their gradients are also implemented. Along with innovations such as Project Adam, TensorFlow’s training was ultimately made efficient through community driven improvements.</p> </li> <li> <p><strong>Fault Tolerance</strong> - Since many learning algorithms do not require consistency and writing at every step is compute intensive, TensorFlow implements user-level checkpointing for fault tolerance. This design decision leaves it to the user to build their own best fault handling mechanisms. <em>I wish they had an automatic version as well</em>.</p> </li> <li> <p><strong>Synchronous replica coordination</strong> - SGD is robust to asynchrony, and TensorFlow is designed for asynchronous training. In the asynchronous case, each worker reads the current value when the step begins, applies its gradient to the different current value at the end. The synchronous cases use queues to coordinate execution, allowing multiple gradient updates together. Although the throughput is reduced, this way of training is more efficient. TensorFlow implements <em>backup workers</em> to improve the throughput of this synchronous case by 15%.</p> </li> </ul> <h2 id="implementation">Implementation</h2> <p>The core TensorFlow library is implemented in C++ for portability and performance with its implementation being open-source. It consists of</p> <ul> <li> <p>Distributed master - given a graph and a step definition, it prunes and partitions the graphs to each devices, and caches these subgraphs so that they may be reused in subsequent steps.</p> </li> <li> <p>Dataflow executor - Schedules the execution of the kernels that comprise a local subgraph - optimized for running large fine-grained graphs with low overhead.</p> </li> </ul> <p>The API can be accessed both via C++ and Python.</p> <p>The library does not provide huge gains for single-system training but has higher throughput for large models across multiple devices.</p> <h2 id="conclusion-1">Conclusion</h2> <p>When this paper was published, TensorFlow was still a work in progress. Later, the library transformed significantly with the introduction of v2, and other optimizations.</p> <h1 id="pytorch-an-imperative-style-high-performance-deep-learning-library"><a href="https://arxiv.org/abs/1912.01703">PyTorch: An Imperative Style, High-Performance Deep Learning Library</a></h1> <p>A product of Facebook research, PyTorch is the most popular deep-learning library. They addressed the biggest limitation in previous frameworks - usability. They targeted both performance and usability by designing an imperative-style ML framework.</p> <p>In contrast to PyTorch, the previous approaches created a static dataflow graph that represents the computation. Since the whole computation is visible ahead of time, it can be leveraged to improve performance and scalability. However, due to this, the usability is reduced and cannot be used to iteratively build experiments. PyTorch, a python library, performs immediate execution of dynamic tensor computations with automatic differentiation and GPU acceleration while maintaining comparable performance to the static libraries.</p> <h2 id="background">Background</h2> <p>There were four major trends in scientific computing that have become important for deep learning:</p> <ol> <li>Development of domain-specific languages and libraries for tensor manipulation (e.g., APL, MATLAB, NumPy made array-based productive productive)</li> <li>Automatic differentiation, making it easier to experiment with different machine learning approaches</li> <li>Shift towards open-source Python ecosystem from proprietary software. The network effects of Python contributed to its exponential growth.</li> <li>Availability of general-purpose parallel hardware like GPUs</li> </ol> <p>PyTorch builds on these trends by providing an array-based programming model accelerated by GPUs and differentiable via automatic differentiation integrated in the Python ecosystem.</p> <h2 id="design-principles">Design Principles</h2> <p>PyTorch’s design is based on four main principles:</p> <ol> <li>Be Pythonic: Integrate naturally with Python ecosystem, keep interfaces simple and consistent</li> <li>Put researchers first: Make writing models, data loaders, and optimizers easy and productive</li> <li>Provide pragmatic performance: Deliver compelling performance without sacrificing simplicity</li> <li>“Worse is better”: Prefer simple, slightly incomplete solutions over complex, comprehensive designs</li> </ol> <h2 id="usability-centric-design">Usability-centric Design</h2> <p>The approach to PyTorch starts by considering deep-learning models as just another Python program. By considering so, PyTorch maintains the imperative programming model inspired from Chainer and Dynet. Defining layers, composing models, loading data, running optimizers and parallelizing the training process can all be expressed using familiar Python syntax. It allows any new potential neural network architecture to be easily implemented with composability.</p> <p>Mainly, since PyTorch programs execute eagerly, all features of Python like print, debugging and visualization work as expected. In addition, PyTorch has -</p> <ul> <li> <p><strong>Interoperability</strong> - Integrated well with other libraries to allow bidirectional exchange of data (NumPy, Matplotlib, etc).</p> </li> <li> <p><strong>Extensibility</strong> - Supports custom differentiable functions and datasets. The abstractions take care of shuffling, batching, parallelization, and management of pinned CUDA memory to improve the throughput and performance. In general, PyTorch components are completely interchangeable!</p> </li> </ul> <h3 id="automatic-differentiation">Automatic differentiation</h3> <p>Python is a dynamic programming language that has most of the behavior defined at run-time, making it difficult to pre-calculate the differentiation. Instead, PyTorch uses operator overloading approach to build up a representation of the computed function every time it is executed. It notes the difference between forward mode and backward mode automatic differentiation and adopts the latter which is better suited for ML applications.</p> <p>Their system can differentiate through code with mutation on tensors as well. They also have a versioning system for tensors as a failsafe to track the modifications.</p> <h2 id="performance-focused-implementation">Performance-focused Implementation</h2> <p>With all these considerations to usability, the developers implemented many tricks to maintain the performance of the library. Since the models have to run on Python interpreter, which has its own limitations such as the global interpreter lock (ensures only one of any concurrent threads is running at a given time), PyTorch optimized every aspect of execution and also enabled users to add their own optimization strategies. Prior frameworks avoided these constraints by deferring the evaluation to their custom interpreters.</p> <p>Most of PyTorch is implemented C++ for high performance. The core <code class="language-plaintext highlighter-rouge">libtorch</code> library implements the tensor data structure, GPU and CPU operators, automatic differentiation system with gradient formulas for most built-in functions and basic parallel primitives. The pre-computed gradient functions allow computation of gradients of core PyTorch operators in a multithreaded evaluation evading the Python global interpreter lock. The bindings are generated using YAML meta-data files, that allowed the community to create bindings to other languages.</p> <p>PyTorch separates the control (program branches, loops) and the data flow (tensors and the operations). The Control flow handled by Python and optimized C++ code on CPU and Data flow can be executed on CPU or GPU. PyTorch is designed to run asynchronously leveraging the CUDA stream mechanism. This allows overlap of Python code execution on CPU with tensor operations on the GPU, effectively saturating GPU with large tensor operations. The main performance cover-up comes from this design.</p> <p>Since every operator needs to allocate an output tensor to hold the results, the speed of <em>dynamic memory allocators</em> needs to be optimized. CPU has efficient libraries to handle this. However, to avoid the bottleneck of <code class="language-plaintext highlighter-rouge">cudaFree</code> routine that blocks code until all previously queued work on GPU completes, PyTorch implements its custom allocator that incrementally builds up a cache of CUDA memory. It reassigns it to later allocations without CUD APIs for better interoperability allowing users to use other GPU enabled Python packages. This allocator is further optimized with memory usage patterns and its implementation is simplified with the <em>one-pool-per-stream</em> assumption.</p> <p>The <code class="language-plaintext highlighter-rouge">multiprocessing</code> library was developed to evade the global interpreter lock on Python. However, this is inefficient for large arrays, so it is extended as <code class="language-plaintext highlighter-rouge">torch.multiprocessing</code> to allow automatic movement of data to shared memory improving the performance significantly. It also transparently handles sharing of CUDA tensors to build analytical systems on top of this.</p> <p>Since users write models to consume all the memory resources, PyTorch treats memory as a scarce resource and handles it carefully. The overheads of garbage collection are too large. To solve this, PyTorch relies on a reference counting scheme to track the number of uses of each tensors, and frees the underlying memory <em>immediately</em> when this count reaches zero. This ensures immediate freeing of memory when tensors become unneeded.</p> <p>With all these optimizations, PyTorch achieves</p> <ul> <li> <p>ability to asynchronously execute dataflow on GPU with almost perfect device utilization</p> </li> <li> <p>custom memory allocator showing improved performance</p> </li> <li> <p>performance within 17% of the fastest framework across all benchmarks</p> </li> </ul> <h2 id="conclusion-2">Conclusion</h2> <p>The paper concludes by highlighting PyTorch’s success in combining usability with performance. Future work includes:</p> <ul> <li>Developing PyTorch JIT for optimized execution outside the Python interpreter</li> <li>Improving support for distributed computation</li> <li>Providing efficient primitives for data parallelism</li> <li>Developing a Pythonic library for model parallelism based on remote procedure calls</li> </ul> <p>##</p> <h1 id="deep-learning-performance-on-gpus">Deep Learning Performance on GPUs</h1> <blockquote> <p>Source: https://docs.nvidia.com/deeplearning/performance/index.html#performance-background</p> </blockquote> <p>GPUs accelerate computing by performing independent calculations in parallel. Since deep learning operations involve many such operations, they can leverage GPUs very well.</p> <p>It is useful to understand how deep learning operations are utilizing the GPU. For example, operations that are not matrix multiplies, such as activation functions and batch-normalization, often are <em>memory-bound</em>. In these cases, tweaking parameters to more efficiently use the GPU can be ineffective. In these cases, the data movement between the host and the device can limit the performance. By understanding such intricacies, we can design better code to leverage GPUs to the fullest.</p> <blockquote> <p>We shall discuss the concepts in context of NVIDIA GPUs because of the existing monopoly.</p> </blockquote> <h2 id="architecture-fundamentals">Architecture Fundamentals</h2> <p>The core paradigm of GPU is parallel compute. It has processing elements and a memory hierarchy. At a high level, GPUs consist of Streaming Multiprocessors (SMs), on-chip L2 cache, and high-bandwidth RAM.</p> <p>Each SM has its own instruction scheduler and execution pipelines. In a neural network, <em>multiply-add</em> is the most frequent operation. A typical spec sheet shows FLOPS rate of GPU operations - #SMs*SM clock rate*#FLOPs.</p> <h2 id="execution-model">Execution Model</h2> <p>GPUs execute functions in a 2-level hierarchy of threads - a given function’s threads are grouped into equally-sized <em>thread blocks</em>, and a set of thread blocks are launched to execute the function. The latency caused by dependent instructions is hidden by switching to the execution of other threads. As a result, the number of threads needed to effectively use a GPU is much higher than the number of cores or instruction pipelines.</p> <p>At runtime, each thread block is placed on an SM for execution. All the threads in the block communicate via a shared memory in the blocks and synchronize efficiently. To use the GPU to the maximum, we need to execute many SMs (since each threadblock only activates one SM). Furthermore, each SM can execute multiple threadblocks simultaneously. Therefore, we need to launch many threadblocks, a number several times higher than the number of SMs.</p> <p>These thread blocks running concurrently are termed as a <em>wave</em>. In effect, we need to launch functions that execute several waves of thread blocks to minimize the <em>tail effect</em> (towards the end of a function, only a few active thread blocks remain).</p> <h2 id="understanding-performance">Understanding performance</h2> <p>There are three factors determining performance - <strong>memory bandwidth, math bandwidth and latency</strong>. To analyze these better, let \(T_{mem}\) be the time spent in accessing memory and \(T_{math}\) is the time spent performing math operations.</p> \[\begin{align*} T_{mem} &amp;= \text{\# bytes accessed}/ \text{memory bandwidth} \\ T_{math} &amp;= \#\text{ math ops}/\text{math bandwidth} \end{align*}\] <p>If these functions can be overlapped, then the total time is approximately \(\max(T_{mem}, T_{math})\). And based on the dominance, we categorize the functions as <em>math-limited</em> and <em>memory-limited</em>. So if a function is math-limited, we have</p> \[\#\text{ops}/\#\text{bytes} &gt; BW_{math}/BW_{mem}\] <p>The left-hand side of this inequality is known as <strong>arithmetic intensity</strong>, and the right-hand side is called as <em>ops:byte</em> ratio. Based on this description, we have the following data for NVIDIA V100 GPU for FP16 data -</p> <table> <thead> <tr> <th>Operation</th> <th>Arithmetic Intensity</th> <th>Usually limited by…</th> </tr> </thead> <tbody> <tr> <td>Linear layer (4096 outputs, 1024 inputs, batch size 512)</td> <td>315 FLOPS/B</td> <td>arithmetic</td> </tr> <tr> <td>Linear layer (4096 outputs, 1024 inputs, batch size 1)</td> <td>1 FLOPS/B</td> <td>memory</td> </tr> <tr> <td>Max pooling with 3x3 window and unit stride</td> <td>2.25 FLOPS/B</td> <td>memory</td> </tr> <tr> <td>ReLU activation</td> <td>0.25 FLOPS/B</td> <td>memory</td> </tr> <tr> <td>Layer normalization</td> <td>&lt; 10 FLOPS/B</td> <td>memory</td> </tr> </tbody> </table> <p>Many common operations have low arithmetic intensities (memory-limited). <em>Note.</em> This analysis assumes that the workload is large enough to saturate the device and not be limited by latency.</p> <h2 id="operation-categories">Operation Categories</h2> <p>There are three main types of operations in a neural network</p> <ol> <li> <p>Element-wise operations - Unary/binary operations such as ReLU and element-wise addition. These layers tend to be memory-limited as they perform few operations per byte accessed.</p> </li> <li> <p>Reduction operations - Operations that produce values over a range of input tensor values, such as pooling, softmax and normalization layers. These layers usually have low arithmetic intensity (memory limited).</p> </li> <li> <p>Dot-product operations - Operations can be expressed as dot-product of two tensors, such as attention and fully-connected layers. These operations tend to be math-limited if the matrices are large. For smaller operations, these are memory limited.</p> </li> </ol> <h2 id="summary">Summary</h2> <p>The number of SMs in the GPU determines the ops:bytes ratio. To maximize the performance, ensure sufficient parallelism by oversubscribing the SMs. The most likely performance limiter is</p> <ul> <li>Latency if there is not sufficient parallelism</li> <li>Math if there is sufficient parallelism and algorithm arithmetic intensity is higher than the GPU ops:byte ratio</li> <li>Memory if there is sufficient parallelism and algorithm arithmetic intensity is lower than the GPU ops:byte ratio</li> </ul> <h1 id="matrix-multiplication">Matrix Multiplication</h1> <p>As we’ve observed previously, General Matrix Multiplications (GEMMs) is the most frequent operation in neural network layers. For the sake of the discussion, based on the conventions, let \(A \in \mathbb R^{m \times k}, B \in \mathbb R^{k \times n}\) and \(C = A \times \in \mathbb R^{m \times n}\). \(C\) would require a total of \(M*N*K\) fused multiply-adds (FMAs) (twice for the FLOPs). In general, for smaller multiplications, the operation is memory limited, and otherwise it is math-limited. As a consequence, vector products \(M = 1\) or \(N = 1\) are always memory limited and their arithmetic intensity is less than 1.</p> <h2 id="implementation-1">Implementation</h2> <p>Most of the GPUs implement matrix multiplication as a tiling operation - each thread block computes its output tile by stepping through the \(K\) dimension in tiles.</p> <p>The latest NVIDIA GPUs have Tensor Cores to maximize multiplications in tensors. The performance is better when \(M, n&lt; k\) are aligned to multiples of 16 bytes.</p> <p>To aid with the code design, NVIDIA has provided the cuBLAS library that contains its optimized GEMM implementations. The tradeoff arises between parallelism and memory reuse - Larger tiles have fewer memory I/O but reduced parallelism. This is, in particular, a concern for smaller matrices. The library contains a variety of tiling to techniques to best utilize the GPU.</p> <h2 id="dimension-quantization-effects">Dimension Quantization effects</h2> <p>A GPU function is executed by launching a number of thread blocks, each with the same number of threads. This introduces two potential effects on execution efficiency - tile and wave quantization.</p> <ul> <li><strong>Tile Quantization</strong> - Tile quantization occurs when matrix dimensions are not divisible by the thread block tile size.</li> </ul> <p><img src="/assets/img/Machine Learning Systems/2025-01-25-23-37-44-image.png" alt=""/></p> <ul> <li><strong>Wave quantization</strong> - The total number of tiles is quantized to the number of multiprocessors on the GPU.</li> </ul> <h1 id="mi300x-vs-h100-vs-h200">MI300X vs H100 vs H200</h1> <blockquote> <p>Source: <a href="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/">MI300X vs H100 vs H200 Benchmark Part 1: Training; CUDA Moat Still Alive; SemiAnalysis</a></p> </blockquote> <p>A case-study to examine why numbers on paper do not translate to real-life performance.</p> <p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/10-H100-vs-H200-vs-MI300X-Basic-Specs-initial-1.jpg?resize=2184%2C1088&amp;ssl=1" alt=""/></p> <p>Comparing numbers on sheets is similar to comparing the megapixel count of cameras - the software is very important. NVIDIAs real world performance is also quite low compared to the papers. The analysis boils down to this - The potential on paper of AMD’s MI300X is not realized due to lack of AMDs software stack and testing. The gap between AMDs and NVIDIAs software is large - AMD is still catching up while NVIDIA is racing ahead. AMDs PyTorch is unfortunately broken.</p> <h2 id="matrix-multiplication-1">Matrix Multiplication</h2> <p>OpenAI provides a <code class="language-plaintext highlighter-rouge">do_bench</code> function to benchmark matrix multiplications - it provides cache clearing between runs, ways to warmup and execute the benchmark multiple times and takes the median result. The results are as follows -</p> <p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/71-bf16-gemm-perf-for-real-world-shapes-w-amd-images.png?resize=1489%2C1084&amp;ssl=1" alt=""/></p> <p>Notice how the marketed value is much higher than the real-world performance! The main finding from this study is that AMDs software stack is riddled with bugs that lowered its performance quite a lot.</p> <p>It is also important to ensure that the underlying benchmark has no issues. For example, a popular benchmark for GEMM showed comparable performance for AMD and NVIDIA. However, on closer look, it had issues - it did not clear out L2 Cache and displayed the max performance rather than mean/median.</p> <h2 id="hbm-memory-bandwidth-performance">HBM Memory Bandwidth Performance</h2> <p>Higher HBM bandwidth leads to better inference performance. It is also helpful during training, specially with higher batch sizes.</p> <p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/115-HBM-Copy-Bandwidth-Chart.png?resize=1485%2C1047&amp;ssl=1" alt=""/></p> <h2 id="training-large-language-models">Training Large Language Models</h2> <p>MLPerf GPT3 175B training is a good metric to measure the time it takes to train a model. However, this benchmark turned out to be very difficult for the AMD GPUs. Instead, the authors designed a benchmark better representing a user workload. They noted that many AI practitioners do not use Megatron, NeMo and 3D parallelism (advances in modern networks architectures for faster inference) due to their rigidity and complexity.</p> <p>Overall for this test, on a single node, the authors obtained the following results -</p> <p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/121-bf16-single-node-8gpu-training-perf-with-new-AMD-images.png?resize=1491%2C1180&amp;ssl=1" alt=""/></p> <p>The public releases of NVIDIA perform much higher than AMDs bugs-resolved builds. Apart from these single node builds, these providers also have a server cluster availability. NVIDIAs GPUs are merged with the NVLink fabric whereas AMD has xGMI. They connect up to 8GPUs with up to 450GB/s of bandwidth per GPU. This network of GPUs synchronize via the map-reduce command, and NVIDIAs topology performs better than that of AMDs.</p> <p>Ironically, AMDs RCCI team has only 32 GPUs to experiment with the algorithms. Looking at this dismal conditions, TensorWave and SemiAnalysis sponsored some clusters to the team to aid the team in fixing their software.</p> <h2 id="conclusion-3">Conclusion</h2> <p>Due to poor internal testing and a lack of good testing suite internally at AMD, MI300 is not usable out of the box. There have been advanced in attention mechanisms such as the <strong>FlexAttention</strong> that can improve the speed of window attention by 10-20x. However, due to the lacking nature of AMDs software, they are 6 months behind NVIDIA which is a long time in the rapid AI age.</p> <p>In fact, many of AMDs libraries are forked off NVIDIA’s open-source libraries! The authors suggest AMD should hire more software talent to reduce the gap and reshape their user interface.</p> <h1 id="tvm-end-to-end-compiler-for-deep-learning"><a href="https://arxiv.org/pdf/1802.04799">TVM: End-to-end compiler for Deep Learning</a></h1> <p>Current ML frameworks require significant manual effort to deploy on different hardware. TVM exposes the graph-level and operator-level optimizations to provide performance portability. In the current frameworks, techniques such as computational graph representations, auto-differentiation and dynamic memory management have been well established. Optimizing these structures on different hardware is however often infeasible, and developers have resorted to highly engineered and vendor-specific operator libraries. These libraries require significant amount of manual tuning and cannot be ported across devices.</p> <p>How does the hardware matter? The input to hardware instructions are multi-dimensional, with fixed or variable lengths; they dictate different data layout and they have special requirements for memory hierarchy. These different constraints (memory access, threading pattern, hardware primitives - loop tiles and ordering, caching, unrolling) form a large search space that needs to be optimized to get efficient implementation.</p> <p>TVM, an end-to-end ML compiler, has been designed to take a high-level specification of a deep-learning program from existing frameworks and generate a low-level optimized code for a diverse set of hardware backends. To do so, they made the following contributions</p> <ul> <li> <p>A <em>tensor expression language</em> to build operators and provide program transformation primitives to generate equivalent programs</p> </li> <li> <p>An <em>automated program optimization framework</em> to find optimized tensor operators based on an ML cost model that adapts and improves based on the data received from the hardware backend.</p> </li> <li> <p>A <em>graph-rewriter</em> compiler together the high-level and operator-level optimizations.</p> </li> </ul> <p><img src="/assets/img/Machine Learning Systems/2025-02-04-17-41-07-image.png" alt=""/></p> <h2 id="graph-optimization">Graph Optimization</h2> <p>As mentioned previously, DL frameworks represent the data flow with graphs. This is different from the low-level compiler intermediate representation due to the presence of large, multi-dimensional tensors. Similar to compiler graph optimization, TVM also performs similar optimizations on this graph -</p> <ol> <li> <p><strong>Operator Fusion</strong> - Fuse multiple small operations together, reducing the activation energy for executing each operator. The commonly used operators can be classified as injective (one-to-one, e.g., add), reduction (many-to-one, e.g., sum), complex-out-fusable (element-wise map based output, e.g., convolution), and opaque (cannot be fused, e.g., sort).</p> <p>These classifications help us identify operators that can be combined. For example, in general</p> <ul> <li> <p>Multiple injective operators can be fused into one another</p> </li> <li> <p>A reduction operator can be fused with the input injective operators</p> </li> <li> <p>Complex-out-fusable operators can fuse element-wise operators to its output</p> </li> </ul> <p>These fusions generate <strong>up to 1.2x to 2x speedup</strong>!</p> </li> <li> <p><strong>Constant folding</strong> - Pre-compute graph parts that can be determined statically, saving execution costs</p> </li> <li> <p><strong>Static memory planning pass</strong> - Pre-allocate memory to hold each intermediate tensor</p> </li> <li> <p><strong>Data Layout transformations</strong> - Transform internal data layouts into back-end friendly forms. For example, a DL accelerator might exploit \(4\times4\) matrix operations, requiring data to be tiled into \(4\times4\) chunks. The memory hierarchy constraints are also taken into consideration.</p> </li> </ol> <p>These optimizations is where the search-space complexity arises. With more operators being introduced on a regular basis, the number of possible fuses can grow significantly considering the increasing number of various hardware back-ends as well. Doing these optimizations manually is infeasible and we need to automate the process.</p> <h2 id="tensor-operations">Tensor Operations</h2> <p>TVM produces efficient code for each operator by generating many valid implementations for each hardware back-end and choosing an optimized implementation. This process is built on Halide’s idea of decoupling descriptions from computation rules and extends it to support new optimizations (nested parallelism, tensorization and latency hiding).</p> <h3 id="tensor-expression-and-schedule-space">Tensor Expression and Schedule Space</h3> <p>Unlike high-level computation graph representations, where the implementation of tensor operations is opaque, each operation is described in an index formula expression language. For example, matrix multiplication is written as</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="sh">'</span><span class="s">m</span><span class="sh">'</span><span class="p">),</span> <span class="n">t</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="sh">'</span><span class="s">n</span><span class="sh">'</span><span class="p">),</span> <span class="n">t</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="sh">'</span><span class="s">h</span><span class="sh">'</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> 
                <span class="n">t</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">))</span>
</code></pre></div></div> <p>Each compute operation specifies both shape and expression for output. Since the language does not specify the loop structure and other execution details, it provides the flexibility for adding hardware-aware optimizations. <em>Schedules</em> are built incrementally with basic transformations to preserve the program’s logical equivalence. To achieve high performance on many back-ends, the schedule primitives must be diverse enough to cover various hardware backends.</p> <h3 id="nested-parallelism">Nested Parallelism</h3> <p>A common paradigm based on fork-join concept, nested parallelism is present in many existing solutions. Based on memory hierarchies, solutions also consider <em>shared memory spaces</em> and <em>fetching data cooperatively</em>. TVM builds on this idea with the concept of <strong>memory scopes</strong> to the schedule space so that a compute stage can be marked as shared. These shared tasks have <em>memory synchronization barriers</em> to ensure proper read/writes.</p> <h3 id="tensorization">Tensorization</h3> <p>To ensure high arithmetic intensity, hardware developers have started implementing <em>tensor compute primitives</em> that consider the common operations and design specific hardware for their execution. They can improve the performance (similar to vectorization for SIMD architectures), but with more and more accelerators with their own variants of tensor instructions, there is a need for an extensible solution that can seamlessly integrate with all these.</p> <p>To do so, TVM extends the tensor expression language to include hardware intrinsics. Doing so decouples the schedule from specific hardware primitives. The generated schedules are broken into a sequence of micro-kernel calls which can also use the <em>tensorize</em> primitive to make use of accelerators.</p> <h3 id="explicit-memory-latency-hiding">Explicit Memory Latency Hiding</h3> <p>Latency hiding refers to the process of overlapping memory operations with computation to maximize utilization of memory and compute resources. These strategies depend on the underlying hardware -</p> <ul> <li> <p>On CPUs, memory latency hiding is achieved implicitly with simultaneous multithreading or hardware prefetching.</p> </li> <li> <p>GPUs rely on rapid context switching of many warps of threads</p> </li> <li> <p>Specialized DL accelerators such as the TPU usually favor leaner control with a decoupled access-execute (DAE) architecture and offload the problem of fine-grained synchronization to software. This is difficult to program because it requires explicit low-level synchronization. To solve this, TVM introduces a virtual threading scheduling primitive that lets programmers specify a high-level data parallel program. TVM automatically lowers the program to a single instruction stream with low-level explicit synchronization.</p> </li> </ul> <p>Empirically, they observed that peak utilization increased by 25% for ResNet with latency hiding.</p> <h2 id="automating-optimization">Automating Optimization</h2> <p>Now that we have a set of schedule primitives, TVM needs to find the optimal operator implementations for each layer of the DL model. An <em>automated schedule optimizer</em> automates optimizing the hardware parameters through a high-dimensional search space. It consists of</p> <ol> <li> <p>A schedule explorer that proposes new configurations. A <em>schedule template specification</em> API is used to let a developer define the changeable parameters in a code for a given hardware. TVM also has a <em>generic master template</em> to automatically extract possible knobs based on the tensor expression language. More the number of configurations, better optimization is possible.</p> </li> <li> <p>A machine learning cost model that predicts the performance of a given configuration. Defining a perfect cost model is difficult - we need to consider memory access patterns, data reuse, pipeline dependencies, threading patterns, etc for every hardware. So, TVM uses a statistical approach by training an ML model that keeps getting better with newer data. The model has been chosen for <em>quality</em> and <em>speed</em>. The model uses a rank objective to predict the relative order of runtime costs. The model itself is a <em>gradient tree boosting model</em> (based on XGBoost) that makes predictions based on features extracted from the loop program.</p> </li> </ol> <p>The explorer starts with random configurations, and, at each step, randomly walks to a nearby configuration. This exploration method is more efficient than random exploration.</p> <p>A <em>distributed device pool</em> scales up the running of on-hardware trials and enables fine-grained resource sharing among multiple optimization jobs.</p> <h2 id="summary-1">Summary</h2> <p>The core of TVM is implemented in C++ with approximately 50k lines of code. It achieves significantly higher performance as compared to earlier works. However, even with this extensive approach, the models can be designed carefully to achieve much better performance. For example, as seen in Flash Attention, the optimization is a result of human intellectual effort rather than a manual exploration of a partially defined search space by an automated compiler.</p> <h1 id="triton-an-intermediate-language-and-compiler-for-neural-network-computations"><a href="https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf">Triton: An intermediate language and compiler for Neural Network computations</a></h1> <p>As with the previous motivation, Triton was developed to provide a way for users to test new models efficiently without needing to manually optimize it for the hardware. Triton is a language and a compiler centered around the concept of <em>tile</em>, statically shaped multi-dimensional sub-arrays. It is a C-based language for expressing tensor programs in terms of operations on parametric tile variables and provides novel tile-level optimization passes for compiling programs into efficient GPU code.</p> <p>Previous approaches to tackle the wide-variety of deep-learning models include <strong>Domain-Specific Languages (DSLs)</strong> based on polyhedral machinery (tenor comprehensions) and/or loop synthesis techniques (Halide, TVM, PlaidML, etc). These systems perform well for models such as depthwise-separable convolutions, but they are much slower than vendor-based libraries like cuBLAS and cuDNN. The problem with vendor based libraries is that they support only a restricted set of tensor operations.</p> <p>These issues have been addressed by the use of micro-kernels - hand-written tile-level intrinsics, but it requires significant manual labour and cannot be generalized. Compilers do not support these tile-level operators and optimizations. The prior approaches can be summarized as</p> <ol> <li> <p>Tensor level IRs - XLA, Flow to transform tensor programs into predefined LLVM-IR and CUDA-C operation templates (e.g., tensor contractions, element-wise operations, etc) using pattern matching. Triton provides more flexibility.</p> </li> <li> <p>The polyhedral model - Tensor Comprehensions (TC), Diesel to parameterize and automate the compilation of one of many DNN layers into LLVM-IR and CUDA-C programs. Triton supports non-affine tensor indices.</p> </li> <li> <p>Loop synthesizers - Halide, TVM to transform tensor computations into loop nests that can be manually optimized using user-defined schedules. Automatic inference of possible execution schedule in Triton.</p> </li> </ol> <h2 id="triton-c">Triton-C</h2> <p>A C-like language for expressing tensor programs in terms of parametric tile variables. It provides a stable interface for existing DNN trans-compilers and programmers familiar with CUDA. Here is an example of matrix multiplication</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Tile shapes are parametric and can be optimized</span>
<span class="c1">// by compilation backends</span>
<span class="k">const</span> <span class="n">tunable</span> <span class="kt">int</span> <span class="n">TM</span> <span class="o">=</span> <span class="p">{</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">};</span>
<span class="k">const</span> <span class="n">tunable</span> <span class="kt">int</span> <span class="n">TN</span> <span class="o">=</span> <span class="p">{</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">};</span>
<span class="k">const</span> <span class="n">tunable</span> <span class="kt">int</span> <span class="n">TK</span> <span class="o">=</span> <span class="p">{</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">};</span>

<span class="c1">// C = A * B.T</span>
<span class="n">kernel</span> <span class="kt">void</span> <span class="nf">matmul_nt</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">b</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// 1D tile of indices</span>
    <span class="kt">int</span> <span class="n">rm</span><span class="p">[</span><span class="n">TM</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_global_range</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">rn</span><span class="p">[</span><span class="n">TN</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_global_range</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">rk</span><span class="p">[</span><span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">...</span><span class="n">TK</span><span class="p">;</span>
    
    <span class="c1">// 2D tile of accumulators</span>
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">TM</span><span class="p">,</span> <span class="n">TN</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>

    <span class="c1">// 2D tile of pointers</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">pa</span><span class="p">[</span><span class="n">TM</span><span class="p">,</span> <span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">rm</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">]</span> <span class="o">+</span> <span class="n">rk</span> <span class="o">*</span> <span class="n">M</span><span class="p">;</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">pb</span><span class="p">[</span><span class="n">TN</span><span class="p">,</span> <span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span> <span class="o">+</span> <span class="n">rn</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">]</span> <span class="o">+</span> <span class="n">rk</span> <span class="o">*</span> <span class="n">K</span><span class="p">;</span>
    
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="n">K</span><span class="p">;</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">-=</span> <span class="n">TK</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">bool</span> <span class="n">check_k</span><span class="p">[</span><span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="n">rk</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">;</span>
        <span class="n">bool</span> <span class="n">check_a</span><span class="p">[</span><span class="n">TM</span><span class="p">,</span> <span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">rm</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">)[</span><span class="o">:</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">]</span> <span class="o">&amp;&amp;</span> <span class="n">check_k</span><span class="p">;</span>
        <span class="n">bool</span> <span class="n">check_b</span><span class="p">[</span><span class="n">TN</span><span class="p">,</span> <span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">rn</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)[</span><span class="o">:</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">]</span> <span class="o">&amp;&amp;</span> <span class="n">check_k</span><span class="p">;</span>
        
        <span class="c1">// Load tile operands</span>
        <span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">TM</span><span class="p">,</span> <span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="n">check_a</span> <span class="o">?</span> <span class="o">*</span><span class="n">pa</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
        <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">TN</span><span class="p">,</span> <span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="n">check_b</span> <span class="o">?</span> <span class="o">*</span><span class="n">pb</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
        
        <span class="c1">// Accumulate</span>
        <span class="n">C</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">trans</span><span class="p">(</span><span class="n">B</span><span class="p">));</span>
        
        <span class="c1">// Update pointers</span>
        <span class="n">pa</span> <span class="o">=</span> <span class="n">pa</span> <span class="o">+</span> <span class="n">TK</span> <span class="o">*</span> <span class="n">M</span><span class="p">;</span>
        <span class="n">pb</span> <span class="o">=</span> <span class="n">pb</span> <span class="o">+</span> <span class="n">TK</span> <span class="o">*</span> <span class="n">N</span><span class="p">;</span>
    <span class="p">}</span>
    
    <span class="c1">// Write-back accumulators</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">pc</span><span class="p">[</span><span class="n">TM</span><span class="p">,</span> <span class="n">TN</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span> <span class="o">+</span> <span class="n">rm</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">]</span> <span class="o">+</span> <span class="n">rn</span> <span class="o">*</span> <span class="n">M</span><span class="p">;</span>
    <span class="n">bool</span> <span class="n">check_c</span><span class="p">[</span><span class="n">TM</span><span class="p">,</span> <span class="n">TN</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">rm</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">)[</span><span class="o">:</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">]</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">rn</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">);</span>
    <span class="err">@</span><span class="n">check_c</span> <span class="o">*</span> <span class="n">pc</span> <span class="o">=</span> <span class="n">C</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>It is a front-end to describe CUDA like syntax with Numpy like semantics. The syntax is based on ANSI C, and has the following changes</p> <ul> <li> <p>Tile declarations: Syntax for multi-dimensional arrays that can be made parametric with <code class="language-plaintext highlighter-rouge">tunable</code> keyword. Ranges with ellipses.</p> </li> <li> <p>Broadcasting using <code class="language-plaintext highlighter-rouge">newaxis</code> keyword</p> </li> <li> <p>Predicated statements in tiles with <code class="language-plaintext highlighter-rouge">@</code></p> </li> </ul> <p>A tile is an abstraction to hide details involving intra-tile memory coalescing, cache management and specialized hardware utilization.</p> <p>The triton programming model is similar to that of CUDA - each kernel corresponds to a single thread execution.</p> <h2 id="triton-ir">Triton-IR</h2> <p>An LLVM-based Intermediate Representation (IR) that provides an environment suitable for tile-level program analysis, transformation and optimization. Triton-IR programs are constructed directly from Triton-C during parsing, but automatic generation from embedded DSLs is unimplemented. Here is an example for the <code class="language-plaintext highlighter-rouge">max</code> operation</p> <pre><code class="language-C">define kernel void @relu ( float * %A , i32 %M , i32 % N ) {
prologue :
% rm = call i32 &lt;8 &gt; get_global_range (0) ;
% rn = call i32 &lt;8 &gt; get_global_range (1) ;
; broadcast shapes
%1 = reshape i32 &lt;8 , 8 &gt; % M;
% M0 = broadcast i32 &lt;8 , 8 &gt; %1;
%2 = reshape i32 &lt;8 , 8 &gt; % N;
% N0 = broadcast i32 &lt;8 , 8 &gt; %2;
; broadcast global ranges
%3 = reshape i32 &lt;8 , 1 &gt; % rm;
% rm_bc = broadcast i32 &lt;8 , 8 &gt; %3;
%4 = reshape i32 &lt;1 , 8 &gt; % rn;
% rn_bc = broadcast i32 &lt;8 , 8 &gt; %4;
; compute mask
% pm = icmp slt % rm_bc , % M0;
% pn = icmp slt % rn_bc , % N0;
% msk = and % pm , % pn;
; compute pointer
% A0 = splat float * &lt;8 , 8 &gt; % A;
%5 = getelementptr % A0 , % rm_bc ;
%6 = mul % rn_bc , % M0;
% pa = getelementptr %5 , %6;
; compute result
% a = load % pa;
% _0 = splat float &lt;8 , 8 &gt; 0;
% result = max % float %a , % _0;
; write back
store fp32 &lt;8 , 8 &gt; % pa , % result
}
</code></pre> <p>It is similar to LLVM-IR, but it includes the necessary extensions for tile-level data-flow and control-flow.</p> <p>It constructs a data-flow graph including nodes for multi-dimensional tiles and instructions made from basic blocks of code. The control-flow is handled with the use of <em>Predicated SSA</em> and \(\psi\)<em>-functions</em> (compare and merge).</p> <h2 id="triton-jit-compiler">Triton-JIT compiler</h2> <p>A Just-In-Time (JIT) compiler and code generation backend for compiling Triton-IR programs into efficient LLVM bitcode. It includes</p> <ul> <li> <p>A set of tile-level, machine-independent passes aimed at simplifying input compute kernels independently of any compilation target. It involves operations such as pre-fetching to reduce cache misses and tile-level peephole optimization that implement algebraic tricks with tensors.</p> </li> <li> <p>A set of tile-level, machine dependent passes for generating efficient GPU-ready LLVM-IR. It involves</p> <ul> <li> <p>Hierarchical Tiling - The tiles defined by the user are further broken down to the machine’s constraints.</p> </li> <li> <p>Memory coalescing - Making sure that adjacent threads simultaneously access nearby memory locations.</p> </li> <li> <p>Shared memory allocation - To improve memory reuse.</p> </li> <li> <p>Shared memory synchronization - Barriers to preserve program correctness in parallel execution.</p> </li> </ul> </li> <li> <p>An auto-tuner that optimizes any meta-parameters associated with the above passes. Traditional auto-tuners rely on hand-written templates. In contrast, Triton-JIT extracts optimization spaces from Triton-IR (hierarchical tiling parameters only - up to 3 per dimension per tile) and optimizes using an exhaustive search. <strong>This needs to be improved in the future</strong></p> </li> </ul> <h2 id="summary-2">Summary</h2> <p>Triton defeats the other prior solutions achieving performance close to DSLs. However, the authors have highlighted many areas where this framework can be improved in the future - support for tensor cores (the ones TVM talked about), implementation of quantized kernels and integration into higher-level DSLs.</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Few keypoints from books other important papers in the field.]]></summary></entry><entry><title type="html">Data Systems for Machine Learning</title><link href="https://sudhansh6.github.io/blog/data-systems-for-ml/" rel="alternate" type="text/html" title="Data Systems for Machine Learning"/><published>2025-01-06T00:00:00+00:00</published><updated>2025-01-06T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/data-systems-for-ml</id><content type="html" xml:base="https://sudhansh6.github.io/blog/data-systems-for-ml/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Machine Learning systems have played a pivotal role in the rapid adaptation of Ai in the world today. This domain is essential for solving future problems and also making the current architectures more efficient. This is crucial considering that companies are reactivating nuclear power plants to power AI in the real-world.</p> <p>Along with the progress in AI from small neural networks to large language models, there has been a development in the size of datasets as well. Big data arrived, and AI today relies on these internet-scale datasets. After all, doesn’t ChatGPT just do pattern-matching in the internet?</p> <p>Moreover, the compute capabilities have been scaling exponentially. Just last year (2024), NVIDIA released a new super-chip architecture <a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing">Blackwell</a> that has 97 billion transistors that can reach up to 1.4 exa-flops! The largest super-computer was barely able to reach 1 exa-flop. All this power in the palm of your hand…</p> <p>Richard Sutton once said, search and learning can scale unparalleled with growing computation power.</p> <p>Consider the year 2012 — AlexNet made waves showing SOTA capabilities with images. They use Stochastic Gradient Descent, dropout, convolution networks and initialization techniques. Without ML systems (CUDA, etc), the code would have been 44,000 lines with days of training! With these systems (Jax, PyTorch, TensorFlow) in place, you can achieve the same result in 100 lines within hours of training.</p> <h3 id="in-practice">In Practice</h3> <p>In industry, problems are typically of the form - improve the self-driving car’s pedestrian detection to be X-percent accurate at Y-ms latency budget. For an ML engineer is, the general approach is to design a better model with better learning efficiency followed by hyper-parameter running, pruning, distillation. An ML systems engineer would take the best model by ML researchers, specialize the implementation to target the H/W platform to reduce latency. <em>Streamlining the entire process from development to deployment</em>.</p> <h2 id="overview">Overview</h2> <p>From ad-hoc methods having diverse models and optimization algorithms with various data pre-processing techniques - we have arrived at an optimal algorithm that is <em>iterative and convergent</em>. As our models have become more and more specialized, the computation resources scaled exponentially.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17363053041704.jpg" alt=""/></p> <p>Through the course of this article, we will cover deep learning basics, computational graphs, Autodiff, ML frameworks, GPUs, CUDA and collective communication.</p> <p>There are more related topics to the ones discussed here</p> <ul> <li>ML for systems</li> <li>ML Hardware design</li> </ul> <p>Unfortunately, the textbook for this content is just miscellaneous research papers.</p> <h1 id="background">Background</h1> <h2 id="dl-computation">DL Computation</h2> <p>The idea is to concatenate composable layers</p> \[\theta^{(t + 1)} = f(\theta^{(t)}, \nabla_L(\theta^{(t)}, D^{(t)})\] <p>A <strong>model</strong> is a parameterized function that describes how we map inputs to predictions. The parameters are optimized using optimization methods like <strong>SGD</strong>, Newton methods, etc. A <strong>loss function</strong> guides the model to give feedback on how well the model is performing.</p> <p>Having these basic definitions, we will build abstractions to map all the models being used today. It is not possible to build systems to support all models. A quick refresher of important models</p> <ul> <li><strong>CNNs</strong> - Learnable filters to convolute across images to learn spatial features. The top 3 breakthrough architectures were - AlexNet, ResNet, U-Net. What are the important components in CNNs? <ul> <li>Convolution (1D, 2D, 3D)</li> <li>Matmul</li> <li>Softmax</li> <li>Element-wise operations - ReLU, add, sub, pooling, normalization, etc.</li> </ul> </li> <li><strong>Recurrent Neural Networks</strong> - Many problems in nature are many-to-many. RNNs maintain an internal state that is updated as a sequence is processed. Arbitrary inputs and outputs can be generated, and any neural network can be used in the RNN architecture. The top 3 breakthrough architectures were - Bidirectional RNNs, LSTMs, GRU. What are the important components in RNNs? <ul> <li>Matmul</li> <li>Element-wise non-linear - ReLU, Sigmoid, Tanh</li> <li>Underlying MLP RNNs have a problem of forgetting (\(0.9*0.9*… \approx 0\)). Additionally, they lack <strong>parallelizability</strong> - both forward and backward passes have \(O(sequence length)\).</li> </ul> </li> <li><strong>Transformers</strong> (Attention + MLP) - Treat representations of each element in the sequences as queries to access and incorporate information from a set of values. Transformers have an encoder part (BERT most famous) and a decoder part (GPT most famous). Along with these, DiT is one of the top 3 models. What are the important components in Transformers? <ul> <li>Attention - Matmul, softmax, Normalization</li> <li>MLP</li> <li>Layernorm, GeLU, etc.</li> </ul> </li> <li><strong>Mixture of Experts</strong> - Voting from many experts is better than one expert. Latest LLMs are mostly MoEs - Grok, Mixtral, Deepseek-v3. A router (Matmul, softmax) is the novel component in MoE - it makes system design difficult.</li> </ul> <h2 id="machine-learning-systems">Machine Learning Systems</h2> <p>As mentioned before, the three pillars for the systems are data, model and compute. The foal is to express as manny as models as possible using one set of programming interface by connecting math primitives.</p> <h3 id="computational-dataflow-graph">Computational Dataflow Graph</h3> <p>A representation to show data flow in programs. A <strong>node</strong> represents the computation (operator) and an <strong>edge</strong> represents the data dependency (data flowing direction). A node can also represent the input/output tensor of the operator.</p> <h3 id="example-deep-learning-with-tensorflow-v1">Example: Deep learning with TensorFlow v1</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tinyflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span> <span class="c1"># Forward declaration 
</span><span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span><span class="n">tf</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">reduction_indices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># Loss function declaration 
</span><span class="n">W_grad</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">,</span> <span class="p">[</span><span class="n">W</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Automatic differentiation
</span><span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">assign</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">W_grad</span><span class="p">)</span> <span class="c1"># SGD update rule 
</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">})</span> <span class="c1"># Real-execution happens here
</span></code></pre></div></div> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17364786998993.jpg" alt=""/></p> <p>This DAG representation opens up all possibilities of optimizations. However, creating such a graph doesn’t allow flexibility - once a graph is defined, it cannot be changed based on the input.</p> <h3 id="example-pytorch">Example: PyTorch</h3> <p>PyTorch also uses computational graphs, but it creates it on the fly. Previously, we had defined the graph and then executed it. Symbolic declaration vs imperative programming. Define-then-run vs Define-and-run. C++ vs Python.</p> <p>What are the pros and cons?</p> <table> <thead> <tr> <th> </th> <th>Good</th> <th>Bad</th> </tr> </thead> <tbody> <tr> <td>Symbolic</td> <td>Easy to optimize, much more efficient (can be 10x faster)</td> <td>The way of programming can be counter-intuitive, hard to debug and less flexible</td> </tr> <tr> <td>Imperative</td> <td>More flexible, easy to program and debug</td> <td>Less efficient and more difficult to optimize</td> </tr> </tbody> </table> <p>How does TensorFlow work in Python then? Tensorflow has Python as the interface language.</p> <p>Apart from these two famous frameworks, there were more like Caffe, DyNet, mxnet (has ability to switch between both), etc. Recently, Jax (derived from Tensorflow) has been getting more popular.</p> <h3 id="just-in-time-jit-compilation">Just-in-time (JIT) compilation</h3> <p>Ideally, we want define-and-run during development and define-then-run during deployment. However do we combine both? PyTorch introduced a deploy mode through a decorator <code class="language-plaintext highlighter-rouge">torch.compile()</code>. So is there an issue with JIT? It creates only static graphs, and cannot work with conditionals or loops in the code.</p> <h3 id="static-vs-dynamic-models">Static vs Dynamic models</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17364797895738.jpg" alt=""/> Static graphs are defined and optimized only once. The execution follows a defined computation. On the other hand, dynamic graphs depend on the input. It is difficult to express complex flow-control logic and debug. The implementation is also difficult.</p> <p>As seen above, LSTMs are trying to replace the dynamics in the natural language problem.</p> <p><strong>How to handle dynamics?</strong></p> <ul> <li>Just do Define-and-run and forget about JIT - most popular unforunately :(</li> <li>Introduce Control Flow Ops - <ul> <li>Example: Switch and Merge. This can be added a computational primitive in the graph and introduce dynamics in the graph.</li> <li>These ideas are natural across all programming languages - conditionals and loops. However, the problem with this approach is that graphs becomes complex, and more importantly, how does we do back propagation? What is the gradient of “switch”? TensorFlow team has been working on this.</li> </ul> </li> <li>Piecewise compilation and guards - This approach is better adopted than control flow. <ul> <li>Case 1: A graph accepting input shapes of \([x, c1, c2]\) where \(x\) is variable. The solution is to compile for different values of \(x\) (powers of 2).</li> </ul> </li> </ul> <p>So far, we have seen representations that express the forward computations using primitives. But, how do we represent backward computations?</p> <h1 id="autodiff-ad">Autodiff (AD)</h1> <p>Derivative can be taken using the first order principles. However, this approach can be slow since we have to evaluate the function twice \(f(\theta + \epsilon) , f(\theta)\) and it is also error prone \(\theta(\epsilon^2)\).</p> <p>To optimize the derivative calculation, we pre store the gradients of primitives and map the derivative chain rules in the computational graph. There are two ways of doing this as well</p> <ol> <li>Calculating the derivative from left (inside) to right (outside) in a network - from inputs to outputs</li> <li>Calculating it from right to left - from outputs to inputs</li> </ol> <p>Both are valid approaches and we will discuss them in detail.</p> <h2 id="forward-mode-autodiff">Forward Mode Autodiff</h2> <p>We start from the input nodes, and derive the gradients all the way to the output nodes. <strong>Cons</strong> - - For \(f: R^n \to R^k\), we need \(n\) forward passes to get the gradients with respect to each input. - However, it is usually the case that \(k = 1\) (loss) and \(n\) is very large.</p> <blockquote> <p>If this is confusing, think of it this way - we want the gradient of output with respect to all parameters to update weights. However, forward mode calculates the gradient of inputs with respect to all parameters.</p> </blockquote> <h2 id="reverse-mode-autodiff">Reverse Mode Autodiff</h2> <p>We define the quantity <em>adjoint</em> \(\bar v_i = \frac{\partial y}{\partial v_i}\). We then compute each \(\bar v_i\) in the reverse topological order of the graph. This way, we can simply do one backward pass to get the necessary gradients.</p> <p>In some scientific scenarios, we can have \(k &gt;&gt; n\) where the forward mode can be more efficient.</p> <blockquote> <p>What are the size bounds of the backward graph as compared to the neural network?</p> </blockquote> <p>We construct backward graphs in a symbolic way to reuse it multiple times.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369099776396.jpg" alt=""/></p> <h2 id="backpropagation-vs-reverse-mode-ad">Backpropagation vs. Reverse-mode AD</h2> <p>In old frameworks like Caffe/cuda-convnet, the backward computations were done through the forward graph itself. Newer frameworks like Tensorflow and PyTorch construct the backward graph explicitly. The reasons to do so are -</p> <ol> <li>Explicit graphs allow backward computation with any input values. They have flexibility to even calculate gradient of gradients.</li> <li>Having an explicit backward graph can help optimization!</li> <li>Gradient update rules can be efficiently implemented.</li> </ol> <h2 id="gradient-update-rules">Gradient update rules</h2> <p>Typically done via gradient descent, the weights are updated with the gradients with the following simplified rule</p> \[f(\theta, \nabla_l) = \theta - \eta \nabla_L\] <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369103443367.jpg" alt=""/></p> <h1 id="architecture-overview-of-ml-systems">Architecture Overview of ML systems</h1> <p>The aim is to make the systems fast, scalable, memory-efficient, run on diverse hardware, energy efficient and easy to program/debug/deploy. Phew.</p> <p>We have discussed dataflow and Autodiff graphs. However, there are numerous things that can be added to these - graph optimization, parallelization, runtime memory, operator optimizations and compilation.</p> <h2 id="graph-optimization">Graph Optimization</h2> <p>The goal is to rewrite the original graph \(G\) as \(G’\) that is faster.</p> <p>Consider the following motivating example - Typically, convolution is followed by batch normalization. Instead of performing batch normalization, just update the weights in convolution to do everything in one step!</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369110087495.jpg" alt=""/></p> <p>Note that some steps can become slower based on the hardware, but you get the general idea.</p> <p>Similarly, in attention calculations, the code is typically written with a concatenated vector of queries, keys and values. This version is optimal - it can be understood with <em>Arithmetic Intensity</em> which the ratio of #operations and #bytes. For example, an addition operation has intensity of \(1/3\) (2 loads and one store). However, fusing multiple arithmetic operations reduces the loads and stores by bringing all variables into memory once, improving the arithmetic intensity.</p> <h3 id="so-how-do-we-optimize-graphs">So how do we optimize graphs?</h3> <p>We write rules or templates for opportunities to simplify graphs. There is also implementation of <em>auto-discovering</em> optimizations in the latest libraries, we shall study these.</p> <h2 id="parallelization">Parallelization</h2> <p>The goal is to parallelize the graph computation over multiple devices. Note that devices can be connected with fast (memory communication NVLink) and slow connections (across GPUs), with up to 10x performance difference. Ideally, we do not want to describe partitioning rules for every new model that comes up. Based on these communication patterns, distributing the tasks is not an easy problem. So, we shall discuss how we partition the computational graph on a device cluster.</p> <h2 id="runtime-and-scheduling">Runtime and Scheduling</h2> <p>How do we schedule the compute, communication and memory in a way that execution is as fast as possible, communication is overlapped with compute and is subject to memory constraints?</p> <h2 id="operator-implementations">Operator Implementations</h2> <p>The goal is this layer is to get the fastest possible implementation of <code class="language-plaintext highlighter-rouge">matmul</code>s, for different hardware, different precision and different shapes.</p> <p>NVIDIA releases a GPU every 2 years, and they have rewrite all operations every time! Notably, previously, models were trained using 32-bit floating points, but now researchers are emphasizing on lower and lower precisions.</p> <p>Now, we shall delve into each of these architectures.</p> <h1 id="operator-optimization-and-compilation">Operator Optimization and Compilation</h1> <p>The goal is maximize arithmetic intensity. In general there are three ways to speed up operators</p> <h3 id="vectorization">Vectorization</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369120762344.jpg" alt=""/></p> <p>The right version is faster because of the hardware - cache sizes, etc. Tensorflow and PyTorch have this built-in.</p> <h3 id="refactoring-data-layout">Refactoring data layout</h3> <p>This is again related to how data is stored in memory. For example, C++ stores matrices in row-major order. Accessing columns of a matrix can be 10x slower! Remember this while writing code to lower cache misses and reduce pointer movements.</p> <p>ML systems don’t store tensors in row or column major but in a new format called <strong>strides format</strong> - <code class="language-plaintext highlighter-rouge">A[i, j, …] = A.data[offset + i*A.strides[0] + j*A.strides[1] + …</code>. It is a generalization of row and column major storage, and it offers more flexibility - so based on the batch-sizes or other parameters in a neural network.</p> <p>Strides can separate the underlying storage and the view of the tensor. Consider the following operations</p> <ol> <li><code class="language-plaintext highlighter-rouge">slice</code> - simply changing the offsets and shape will output the slice without any copying involved.</li> <li><code class="language-plaintext highlighter-rouge">transpose</code> - modifying strides will transpose the tensor without any copying! For example, consider the following example <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     <span class="n">M</span><span class="p">.</span><span class="nf">strides</span><span class="p">()</span> <span class="c1"># (24, 12, 4, 1)
</span>     <span class="n">M</span><span class="p">.</span><span class="nf">permute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
     <span class="n">M</span><span class="p">.</span><span class="n">t</span><span class="p">.</span><span class="nf">strides</span><span class="p">()</span> <span class="c1"># (12, 4, 1, 24)
</span></code></pre></div> </div> </li> <li><code class="language-plaintext highlighter-rouge">broadcast</code> - Suppose we have to extend a tensor’s data across a dimension for performing operations with another tensor, then by simply adding <code class="language-plaintext highlighter-rouge">0</code> stride in the appropriate dimensions would be enough! Again, no copying</li> </ol> <p>Many more operations can be done without copying the data and simply modifying the strides. For example, consider the following example -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17370816868291.jpg" alt=""/></p> <p>However, strides also has an issue - Memory access may become non-contiguous, and many vectorized ops require continuous storage.</p> <h3 id="summary">Summary</h3> <p>To make operators efficient, we have seen the following tactics -</p> <ol> <li>Vectorization - leverage platform-specific vectorized functions that reduce seek time</li> <li>Data layout - strides format that allow zero-copies enabling fast array-manipulations</li> <li>Parallelization on CPUs</li> </ol> <p>These were techniques for general operations. However, we can optimize certain operators with their special properties.</p> <h3 id="matmul-optimization">Matmul optimization</h3> <p>The brute-force approach takes \(\mathcal O(n^3)\). The best approach humans know is \(\mathcal O(n^{2.371552})\)!</p> <p>How to improve the speed in practice then? Recall that we are trying to increase AI = #ops/#bytes.</p> <blockquote> <p><strong>Memory Hierarchy</strong> If everything ran on registers, things would be super-fast. But, that is expensive. Remember that L1-Cache has 0.5ns latency, L2-Cache has 7ns and DRAM has 200ns (400x slower!)</p> </blockquote> <p>Let us analyze the AI of <code class="language-plaintext highlighter-rouge">matmul</code> considering the different layers of memory</p> <ol> <li>We can directly move data to registers in every iteration in inner loop</li> </ol> <h2 id="gpus-and-accelerators">GPUs and accelerators</h2> <p>Recall that parallelizing operations across threads is super useful! CPUs have some level of parallelism through SIMD operations (vectorization) but they are limited. Building on the same idea, GPUs were born.</p> <p>When we started out, the ALU units were limited by the physical space on the chips. As technology improved, we moved from 70nm process all the way 3nm process! That is, we can fit up to 20x more cores in the same area! The majority of the area on CPUs is consumed by Control and Cache, and Jensen thought, ditch those and put cores.</p> <p>Graphical Processing Unit (GPU) are tailored for matrix or tensor operations. The basic idea is to use tons of ALUs (weak but specialized) with massive parallelism (SIMD on steroids).</p> <p>There are other hardware accelerators like Tensor Processing Unit (TPU) or Application specific integrated circuit (ASIC), etc. The common theme across all these is the same - there are specialized cores. What are specialized cores? They can only compute certain computations. Specialized cores can be super powerful - <img src="/assets/img/2025-01-06-data-systems-for-ml/17370849151083.jpg" alt=""/></p> <p>Companies also tried reducing precision and maintain the same performance. Additionally, they also tune the distribution of different components for specific workloads.</p> <blockquote> <p>Why does quantization work in ML systems?</p> </blockquote> <h2 id="recap">Recap</h2> <p>Consider the following question - What is the arithmetic intensity of multiplying two matrices?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Load A
Load B
C = matmul(A, B)
</code></pre></div></div> <p>Given \(A \in \mathbb{R}^{mxn}, B \in \mathbb{R}^{nxp}, C \in \mathbb{R}^{mxp}\), the number of I/O operations is \(mn + np + mp\), and the number of compute operations is \(2mnp\) since there are approximately \(mnp\) addition and multiplication operations. The arithmetic intensity is then \(\frac{\text{\#compute operations}}{\text{\#I/O operations}} = \frac{2mnp}{mn + np + mp}\). Setting \(m=n=p=2\), results in \(\frac{2x2x2x2}{2x2 + 2x2 + 2x2} = \frac{4}{3}\).</p> <p>\textit{Note.} The addition operation discussed in the previous lecture also has the same I/O operations. However, \texttt{matmul} is a denser operation that results in a higher arithmetic intensity. In practice, it takes the same time to execute matrix addition and multiplication on GPUs, which is why they are so powerful.</p> <p><code class="language-plaintext highlighter-rouge">matmul</code> is an important operation. <Check></Check></p> <p>Now, consider the following operations</p> <ul> <li><code class="language-plaintext highlighter-rouge">broadcast_to</code></li> <li><code class="language-plaintext highlighter-rouge">slice</code></li> <li><code class="language-plaintext highlighter-rouge">reshape</code></li> <li>Permute dimensions</li> <li><code class="language-plaintext highlighter-rouge">transpose</code></li> <li>indexing like <code class="language-plaintext highlighter-rouge">t[:, 1:5]</code></li> </ul> <p>All these operations are optimized due to strided access in tensors. On the other hand <code class="language-plaintext highlighter-rouge">contiguous()</code> cannot take advantage of this.</p> <p>Just to recap, the strides of a tensor of shape <code class="language-plaintext highlighter-rouge">[2, 9, 1]</code> stored in row major order are <code class="language-plaintext highlighter-rouge">[9, 1, 1]</code></p> <p>Consider the cache tiling operation -</p> <ul> <li>It increases the memory allocated on Cache and memory transfers between cache and register</li> <li>It reuses the memory movement between Dram and Cache</li> <li>The arithmetic intensity <em>decreases</em> since there is more load and store</li> </ul> <h1 id="gpu-and-cuda">GPU and CUDA</h1> <p>We have seen that specialized cores offer much better performance over traditional CPUs. Consider the following basic architecture of a GPU</p> <p>Let us see the basic terminology for understanding the architecture -</p> <ul> <li><strong>Threads</strong> - Smallest units to process a chunk of data.</li> <li><strong>Blocks</strong> - A group of threads that share memory. Each block has many threads mapped to a <em>streaming multiprocessor</em> (SM/SMP).</li> <li><strong>Grid</strong> - A collection of blocks that execute the same kernel.</li> <li><strong>Kernel</strong> - CUDA program executed by many CUDA cores in parallel.</li> </ul> <p>A GPU can be made more powerful by</p> <ul> <li>Adding SMs</li> <li>Adding more cores per SM</li> <li>Making the cores more powerful - at a point of <em>diminishing rewards</em>.</li> </ul> <p>NVIDIA, the largest GPU company, has released P100, V100, A100, H100 and B100 (Blackwell) for ML development. K80, P4, T4 and L4 were a lower tier of GPUs. Let us analyze how the compute has changed across these versions</p> <ol> <li>V100 (2019 -) - 80SMs, 2048 threads/SM - $3/hour</li> <li>A100 (2020 -) - 108SMs, 2048 threads/SM - $4/hour</li> <li>H100 (2022 -) - 144SMs, 2048 threads/SM - $12/hour</li> <li>B100 and B200 (2025 -)-</li> </ol> <p>The numbers are not doubling, then how has the performance doubled? They decreased the precisions.. :(</p> <h2 id="cuda">CUDA</h2> <p><strong>What is CUDA?</strong> It is a C-like language to program GPUs, first introduced in 2007 with NVIDIA Tesla architecture. It is designed after the grid/block/thread concepts.</p> <p>CUDA programs contain a hierarchy of threads. Consider the following host code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">const</span> <span class="kt">int</span> <span class="n">Nx</span> <span class="o">=</span> <span class="mi">12</span><span class="p">;</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">Ny</span> <span class="o">=</span> <span class="mi">6</span><span class="p">;</span>

<span class="kt">dim3</span> <span class="nf">threadsPerBlock</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// 12</span>
<span class="kt">dim3</span> <span class="nf">numBlocks</span><span class="p">(</span><span class="n">Ns</span><span class="o">/</span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">x</span> <span class="p">,</span> <span class="n">Ny</span><span class="o">/</span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">y</span> <span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// (3, 2, 1) = 6</span>

<span class="c1">// the following call triggers execution of 72 CUDA threads</span>
<span class="n">matrixAddDoubleB</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">);</span>
</code></pre></div></div> <p>The GPUs are associated with constants such as</p> <ul> <li><code class="language-plaintext highlighter-rouge">GridDim</code> - dimensions of the grid</li> <li><code class="language-plaintext highlighter-rouge">blocking</code> - the block inter within the grid</li> <li><code class="language-plaintext highlighter-rouge">blockDim</code> - the dimensions of a block</li> <li><code class="language-plaintext highlighter-rouge">threadIdx</code> - the thread index within a block With these in mind, the CUDA kernel for the above code is designed as</li> </ul> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__device__</span> <span class="kt">float</span> <span class="nf">doubleValue</span><span class="p">(</span><span class="kt">float</span> <span class="n">x</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// kernel definition </span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">matrixAddDoubleB</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">])</span>
    <span class="p">{</span>
        <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="n">C</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">doubleValue</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
</code></pre></div></div> <p>The host code launched a grid of CUDA blocks, which then call the <code class="language-plaintext highlighter-rouge">matrixAdd</code> kernel. The function definition starts with <code class="language-plaintext highlighter-rouge">__global__</code> which denotes a CUDA kernel function that runs of the GPU. Each thread indexes its data using <code class="language-plaintext highlighter-rouge">blockIdx</code>, <code class="language-plaintext highlighter-rouge">blockDim</code>, <code class="language-plaintext highlighter-rouge">threadIdx</code> and execute the compute. It is the user’s responsibility to ensure that the job is correctly partitioned and the memory is handled correctly.</p> <p>The host code has a serial execution. However, the device code has SIMD parallel execution on the GPUs. When the kernel is launched, the CPU program <em>continues executing</em> without <em>halting</em> while the device code runs on the GPU. Due to this design, it is important that the device code does not have any return values - causes erroneous behavior. To get results from the GPU, <code class="language-plaintext highlighter-rouge">CUDA.synchronize</code> is used (an example will be shown later).</p> <p>It is the developers responsibility to map the data to blocks and threads. The blockDim, shapes etc should be statically declared. This is the reason why compilers like <code class="language-plaintext highlighter-rouge">torch.compile</code> requires static shapes. The CUDA interface provides a CPU/GPU code separation to the users.</p> <p>The SIMD implementation has a constraint for the control flow execution - it requires all ALUs/cores to process in the same pace. In a control flow, not all ALUs may do useful work and it can lead to up to 8 times lower peak performance.</p> <h3 id="coherent-and-divergent-execution">Coherent and Divergent execution</h3> <p>A coherent execution applied the same instructions to all data. Divergent executions do the opposite and they need to be minimized in CUDA programs. This distinction is important to note - even the latest models like the LLMs have this behavior. Concepts such as attention masking and sliding window attention are examples of divergent behavior and they need to be specially implemented to extract the most compute from the GPU.</p> <h2 id="cuda-memory-model">CUDA Memory model</h2> <p>CUDA device (SIMD execution on GPU) has its own memory called the <em>HBM</em>.</p> <p>Unlike host (CPU) memory that is stored as pages in the RAM, GPU memory does not use pages but has memory pools (bulk data) that are accessed all at once.</p> <p>Memory can be allocated <code class="language-plaintext highlighter-rouge">cudaMalloc</code> and populated with <code class="language-plaintext highlighter-rouge">cudaMemcpy</code> like usual. CUDA has a concept called <strong>pinned memory</strong> that is part of the host memory which is optimized for data transfer between CPU/GPU. Ig is not pagable by the OS and is locked, and only certain APIs can access it.</p> <p>Every thread has its own private memory space, and every block has a shared memory that all its threads can access. The HBM is the global device memory in the GPU that can be accessed by all threads. The memory complexity is to balance between speed and shared memory parallelism.</p> <p>For example, consider the program for window averaging -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mf">3.0</span>
</code></pre></div></div> <p>How can this be parallelized? Since every 3-element tuple reduction is independent, each reduction can be mapped to a CUDA core. So, each thread can compute the result for one element in the output array.</p> <p>The host code -</p> <pre><code class="language-C">int N = 1024*1024;
cudaMalloc(&amp;devInput, sizeof(float)*(N+2)); // To account for edge conditions
cudaMalloc(&amp;devOutput, sizeof(float)*N);

convolve&lt;&lt;&lt;N/THREADS_PER_BLK, THREADS_PER_BLK&gt;&gt;&gt;(N, devInput, devOutput); 
</code></pre> <p>The device code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="cp">#define THREADS_PER_BLK = 128
</span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolve</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
        <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">//thread-local variable</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span>
        <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span><span class="mf">3.</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div> <p>This program can be optimized - each element is read thrice!<br/> Notice that the number of blocks assigned is much more than what a typical GPU has. This is a general practice in CUDA programming where the blocks are <em>oversubscribed</em>.</p> <p>How to optimize? The memory hierarchy can be utilized -</p> <p>The new device code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="cp">#define THREADS_PER_BLK = 128
</span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolve</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
        
        <span class="k">__shared__</span> <span class="kt">float</span> <span class="n">support</span><span class="p">[</span><span class="n">THREADS_PER_BLK</span><span class="p">];</span>
        <span class="n">support</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
        <span class="k">if</span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">){</span>
            <span class="n">support</span><span class="p">[</span><span class="n">THREADS_PER_BLK</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="n">THREADS_PER_BLK</span><span class="p">];</span>
        <span class="p">}</span>

        <span class="n">__syncthreads</span><span class="p">();</span>

        <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">//thread-local variable</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">3</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">+=</span> <span class="n">support</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">];</span>
            
        <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span><span class="mf">3.</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div> <p>We introduced a synchronization primitive here. <code class="language-plaintext highlighter-rouge">_syncthreads()</code> waits for all threads in a block to arrive at this point. Another primitive <code class="language-plaintext highlighter-rouge">cudasynchronize()</code> that syncs between host and the device.</p> <h2 id="compilation">Compilation</h2> <p>A CUDA program also needs to be converted to low-level instructions to be executed. A compiled CUDA device binary includes -</p> <ul> <li>Program text (instructions)</li> <li>Information about required resources - 128 threads per block, 8 types of local data per thread and 130 floats (520 bytes) of shared space per thread block.</li> </ul> <p>The issue is that different GPUs have different SMs. If the user asks for a static (large) number of blocks, how to handle this? The first solution is that GPUs have varying (limited) number of blocks.</p> <p>Furthermore, CUDA schedules the threadblocks to many cores using a dynamic scheduling policy that respects the resource requirements. It assumes that the thread blocks can be executed in any order. The blocks are assigned based on the available resources and the remaining ones are <em>queued</em>.</p> <h2 id="understanding-a-gpu">Understanding a GPU</h2> <p>Consider a NVIDIA GTX 980 (2014) that has the following specs -</p> <ul> <li>96KB of shared memory</li> <li>16 SMs</li> <li>2048 threads/SM</li> <li>128 CUDA cores/SM Note that the number of CUDA cores is not equal to the number of CUDA threads.</li> </ul> <p>As the GPUs became better, NVIDIA tried to increase the shared memory per SMM. This is similar to the SRAM which is very important for LLM inference.</p> <h1 id="matmul---case-study"><code class="language-plaintext highlighter-rouge">matmul</code> - Case Study</h1> <p>Remember that over subscribing in GPUs is allowed, and identify that work can be performed in parallel. Developing the thought-process while working with CUDA is important</p> <ul> <li>Oversubscribe to keep the machine busy</li> <li>Balance workload with convergent workflows</li> <li>Minimize communication to reduce I/O</li> </ul> <p>Now, let us consider matrix multiplication. What can be parallelized? In our previous CUDA implementation, we let each thread compute one element in the result matrix. So, each thread has \(2N\) reads, and there are \(N^2\) threads, resulting in \(2N^3\) global memory access.</p> <p>We are not leveraging the fact that one element can be used to calculate many values in the result matrix. The trick is to use the shared memory space - thread tiling (similar to what we did in CPUs).</p> <p>We let each thread compute \(V \times V\) submatrix. The kernel is as follows</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mm</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">float</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">){</span>
        <span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">x</span><span class="p">,</span> <span class="o">:</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">y</span><span class="p">){</span>
            <span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">y</span><span class="p">];</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
                <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">k</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">C</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">ybase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="o">:</span><span class="p">];</span>
<span class="p">}</span>

</code></pre></div></div> <p>For this version, we have reduced the read per threads to \(NV + NV^2\) and number of threads to \(N^2/V^2\) - total reads reduce to \(N^3/V + N^3\) with \(V^2 + 2N\) float storage per thread.</p> <p>We can improve this using partial sum computations. With a small change, we get</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mm</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">float</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
        <span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">k</span><span class="p">];</span> <span class="c1">// Grabbing an area</span>
        <span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span><span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">y</span><span class="p">){</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">){</span>
                <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">y</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">C</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">ybase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="o">:</span><span class="p">];</span>
<span class="p">}</span>

</code></pre></div></div> <p>With memory read per thread reduced to \(NV^2\), total memory to \(2N^3/V\) and memory per thread to \(V^2 + 2V\). This version is pretty good for systems with a single layer of memory hierarchy. However, if we have shared memory, it can be made more efficient!</p> <p>Suppose we have an SRAM layer, we can tile hierarchically. Consider the following GPU <code class="language-plaintext highlighter-rouge">matmul</code> v3: SRAM Tiling:</p> <p>The idea is to use block shared memory to let a block compute a \(L \times L\) submatrix and each thread computes a \(V \times V\) submatrix reusing the matrices in the shared block memory.</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mm</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span> <span class="p">{</span>
    <span class="k">__shared__</span> <span class="kt">float</span> <span class="n">sA</span><span class="p">[</span><span class="n">S</span><span class="p">][</span><span class="n">L</span><span class="p">],</span> <span class="n">sB</span><span class="p">[</span><span class="n">S</span><span class="p">][</span><span class="n">L</span><span class="p">];</span>
    <span class="n">Float</span> <span class="n">c</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="n">Float</span> <span class="n">a</span><span class="p">[</span><span class="n">V</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">V</span><span class="p">];</span>
    <span class="n">Int</span> <span class="n">y</span> <span class="n">block</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="n">Iint</span> <span class="n">X</span> <span class="n">block</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">float</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
        <span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">k</span><span class="p">];</span> <span class="c1">// Grabbing an area</span>
        <span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span><span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">y</span><span class="p">){</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">){</span>
                <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">y</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">C</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">ybase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="o">:</span><span class="p">];</span>
<span class="p">}</span>

</code></pre></div></div> <blockquote> <p>Think about it this way. Initially, we performed tiling across one layer of memory balancing the tradeoffs between I/O reads and memory constraints of the threads. Now, we are adding one more layer of such tiling in a similar manner. The key is to understand the partial sums idea.</p> </blockquote> <p>Note that it is highly unlikely that the threads have a large range of execution times, but we have the <code class="language-plaintext highlighter-rouge">__syncthreads()</code> as a failsafe. The statistics of this algorithm are -</p> <ul> <li>\(2LN\) global memory access per thread block</li> <li>\(N^2/L^2\) threadblocks</li> <li>\(2N^3/L\) global memory access</li> <li>\(2VN\) shared memory access per thread</li> </ul> <p>The key addition here is was the shared memory space. For this algorithm to be efficient, the fetching from the memory has to be implemented <em>cooperatively</em> -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="kt">int</span> <span class="n">nthreads</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    
</code></pre></div></div> <p>These summarize the matrix multiplication codes implemented in the GPUs. Simple, isn’t it? Although, we have not addressed the optimal values for \(L, V\). It depends on the number of threads, registers and amount of SRAM available on the GPU - this process is called <strong>kernel tuning</strong>. There are profilers that optimize these values. It is a difficult problem since large ML models have various operations that need to be optimized together based on their processing and memory requirements. Furthermore, this is different for every GPU and there are hundreds of GPUs!</p> <p>One solution is to do brute-force - hire people and throw money at it. On the other side of things, ML researchers are building <strong>operator compilers</strong> that figure these out automatically.</p> <p>There are other GPU optimizations that are utilized in practice</p> <ul> <li>Global memory continuous read</li> <li>Shared memory bank conflict</li> <li>Pipelining - While some threads are reading, let other threads compute.</li> <li>Tensor core - Dedicated hardware component to accelerate matrix multiplications</li> <li>Lower precisions</li> </ul> <h1 id="ml-compilation">ML compilation</h1> <p>A super-hot topic during the late 2010s since there was a lot of inefficient code that was being identified. The goal is to automatically generate optimal configurations and code given users code and <em>target hardware</em>.</p> <p>Traditional compilers have to simply convert high-level code to binary instructions. The stack for ML compilers is</p> <ol> <li>Dataflow graph generation</li> <li>Optimize graphs - pruning, partitioning, distribution, etc</li> <li>Build efficient kernel code - parameter optimization</li> <li>Machine code (This step is fairly easy and already well implemented)</li> </ol> <p>The big problems in this big process are</p> <ol> <li>Programming level - Automatically transforming arbitrary (usually imperative) code into a compilable code (static dataflow graphs)</li> <li>Graph level - Automatic graph transformations to make it faster (recall how convolution and batch norm can be fused). Graph theory researchers are working on this.</li> <li>Operator level - How to use hardware and optimize standard operators like <code class="language-plaintext highlighter-rouge">matmul</code>.</li> </ol> <p>The big players in this big field are</p> <ol> <li>XLA - First compiler for ML, released along with TensorFlow in 2016 (those researchers aimed big). This turned out to be so good, that the current TensorFlow stack still uses this. Also works for PyTorch, and it is useful to deploy on TPUs.</li> <li>tvm (Tensor Virtual Machine) - It is one of the most successful open-source compiler in academia. They founded OctoML with 200M (got acquired by NVIDIA). There is no backward pass.</li> <li>2.0 - Torch based compiler, that isn’t that great in terms of optimization.</li> <li>Modular - They raised 300M, founded by the same person who created LLVM. The co-founders started swift at Apple! They had big claims - 20x faster than 2.0, not sure how true they are.</li> </ol> <p>You can think of TensorFlow and PyTorch as the front end, and the above mentioned compilers as the backend.</p> <h2 id="operator-compilation">Operator Compilation</h2> <p>Each user-level written code (for standard operations) has a library of low-level program variants, and the compiler chooses the fastest one for the given hardware.</p> <p>For example, consider a loop -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
        <span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
</code></pre></div></div> <p>Get converted to</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for xi in range(4):
    for xo in range(32):   
        C[xo * 4 + xi] A[xo * 4 + xi] + B[xo * 4 + xi]
</code></pre></div></div> <p>Which is then efficiently implemented in the GPU kernels.</p> <p>So, how do we make this happens?</p> <ul> <li>Enumerate all possibilities</li> <li>Enumerate all the (close-to-) optimal values for the hardware - register/cache</li> <li>Apply to all operators and devices</li> </ul> <p>How to search or reduce the search space and generalize?</p> <p>Note that for a certain kind of code and hardware, finding these optimal value <em>once</em> is enough.</p> <h3 id="search-via-learned-cost-model">Search via Learned Cost Model</h3> <p>The famous example is Autotvm -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17376893447233.jpg" alt=""/> The code generator here is done with templates (not LLMs).We need a lot of experts to write the template to define the search space.</p> <p>To search in this parameters space, the compiler does beam search with early pruning. The cost model can be trained on the historical data.</p> <h3 id="high-level-ideas">High-level ideas</h3> <p>We represent the programs in an abstract way and build a search space with a set of transformations (that represent a good coverage of common optimizations like tiling). Then effective search with accurate cost models and transferability have to be deployed.</p> <p>So, how well are we doing in this field? If the compilers were that good, they would’ve discovered flash-attention. Okay, it’s not that bad, compilers have found good optimizations and it just goes to show how difficult this problem is.</p> <h1 id="high-level-dsl-for-cuda-triton">High-level DSL for CUDA: Triton</h1> <p>We have seen a device-specific DSL (domain-specific language). Programmers are able to squeeze the last bits of performance through this. However, it requires deep expertise and the performance optimization is very time-consuming. Maintaining codebases is complex.</p> <p>On the other hand, we have ML compilers. They prototype ideas very quickly (automatically) and the programmer does not have to worry about the low-level details. The problem is representing and searching through the search-space is difficult. Compilers were not able to find Flash-attention because the search-space wasn’t able to represent this possibility. Furthermore, code generation is a difficult problem that relies on heavy use of templates - lots of performance cliffs.</p> <p>So compared to these two extremes, Triton is in between - it is simpler than CUDA and more expressive than graph compilers. It was developed by OpenAI as a solution to the problems with CUDA and compilers.</p> <h3 id="triton-programming-model">Triton Programming Model</h3> <p>The users define tensors in SRAM directly and modify them using torch-like primitives.</p> <ul> <li>Embedded in Python - Kernels are defined in Python using triton.jit</li> <li>Supports pointer arithmetics - Users construct tensors of pointers and can (de)reference them element wise.</li> <li>However, it has shape constraints - must have power-of-two number of elements along each direction</li> </ul> <p>Consider the following example</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="kn">import</span> <span class="n">triton.language</span> <span class="k">as</span> <span class="n">tl</span>
    <span class="kn">import</span> <span class="n">triton</span>
    
    <span class="o">@</span> <span class="n">triton</span><span class="p">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">__add</span><span class="p">(</span><span class="n">z_ptr</span><span class="p">,</span> <span class="n">x_ptr</span><span class="p">,</span> <span class="n">y_ptr</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
</code></pre></div></div> <p>The triton kernel will be mapped to a single block (SM) of threads. The users are responsible for mapping to multiple blocks. Basically, the language is automating some parts (like compilers), and making the design process simpler for users (as compared to CUDA). These design philosophies are important because they help build newer mental models for users - because they offload some of the cognitive load for optimization, they can think of newer ways of optimizing with these restricted set of parameters. Consider the example of softmax calculation. This function would be slow if implemented using primitives. PyTorch implements an end-to-end kernel for softmax to increase its performance. With triton, we can construct such an end-to-end operation in a simpler manner while achieving slightly higher performance.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">triton.language</span> <span class="k">as</span> <span class="n">tl</span>
<span class="kn">import</span> <span class="n">triton</span>
<span class="nd">@triton.jit</span>
<span class="k">def</span> <span class="nf">_softmax</span><span class="p">(</span><span class="n">z_ptr</span><span class="p">,</span> <span class="n">x_ptr</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">BLOCK</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">):</span>
    <span class="c1"># Each program instance normalizes a row
</span>    <span class="n">row</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK</span><span class="p">)</span>
    <span class="c1"># Load a row of row-major X to SRAM
</span>    <span class="n">x_ptrs</span> <span class="o">=</span> <span class="n">x_ptr</span> <span class="o">+</span> <span class="n">row</span><span class="o">*</span><span class="n">stride</span> <span class="o">+</span> <span class="n">cols</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">x_ptrs</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">,</span> <span class="n">other</span> <span class="o">=</span>    <span class="nf">float</span><span class="p">(</span><span class="err">‘</span><span class="o">-</span><span class="n">inf</span><span class="err">’</span><span class="p">))</span>
    <span class="c1"># Normalization in SRAM, in FP32    
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">tl</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># This is to avoid vert large and small values
</span>    <span class="n">num</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">den</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">num</span> <span class="o">/</span> <span class="n">den</span><span class="p">;</span> 
    <span class="c1"># Write-back to HBM
</span>    <span class="n">tl</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">z_ptr</span> <span class="o">+</span> <span class="n">row</span><span class="o">*</span><span class="n">stride</span> <span class="o">+</span> <span class="n">cols</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">&lt;</span> <span class="n">N</span>
</code></pre></div></div> <p>Note that Triton achieves good performance with low time investment. However, since it is not as flexible as CUDA, achieving very high-performance is not possible with Triton.</p> <h2 id="recap-1">Recap</h2> <p>A <em>kernel</em> in the GPU is a function that is executed simultaneously by tens of thousands of threads on GPU cores. The shared memory during the GPU execution can be used as a <em>cache</em> that is used by more than one thread, avoiding multiple accesses to the global memory. <em>Over-subscribing</em> the GPU ensures that there are more blocks than SMPs present on the device, helping to hide (tail) latencies by ensuring high occupancy of the GPU.</p> <p>&lt;iNsert a point about GPU memory)</p> <p>Operations such as ReLU, batch normalization and max pooling are not arithmetically dense operations. So typically, operations such as linear layers (and layer normalization with large batches) are limited by arithmetic operations. To calculate which linear layer would have more operations, consider the FLOPs calculation for GEMM.</p> <h1 id="3-graph-optimization">(3) Graph Optimization</h1> <p>Our goal is to rewrite \(G\) as \(G’\) such that \(G’\) runs faster than \(G\) while outputting equivalent results. The straightforward solution is to use a template, wherein human experts write (sub-)graph transformation templates and an algorithm replaces these in the data flow graphs for reduction.</p> <h3 id="graph-optimization-templates-fusion">Graph Optimization Templates: Fusion</h3> <p>Fusing operators reduces I/O and kernel launching (CPU to GPU overhead, all the operations that the SM has to run). The disadvantages of this method is that creating various fused operations is difficult making the codebase unmanageable (e.g., TensorFlow).</p> <p>This also includes <strong>folding constants</strong> in a graph to replace expressions such as <code class="language-plaintext highlighter-rouge">(X + 3) + 4)</code> with <code class="language-plaintext highlighter-rouge">(X + 7)</code>.</p> <h3 id="cuda-graph">CUDA graph</h3> <p>NVIDIA allows users to capture the graph at the CUDA level.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381192844173.jpg" alt=""/></p> <blockquote> <p>Is this define-then-run?</p> </blockquote> <h3 id="standard-compiler-techniques">Standard compiler techniques</h3> <ul> <li>Common subexpression elimination (CSE). The high-level idea is replacing expressions such as <code class="language-plaintext highlighter-rouge">a = b; b = c</code> with <code class="language-plaintext highlighter-rouge">a = c</code></li> <li>Dead Code elimination (DCE). After the CSE hit, we eliminate the dead-code with unused variables.</li> </ul> <p>These both are run iteratively to reach an optimal code. These operations are every useful to eliminate parts of graph based on, say default arguments -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381197256019.jpg" alt=""/></p> <h2 id="how-to-ensure-performance-gain">How to ensure performance gain?</h2> <p>When we greedily apply graph optimizations, we may miss some options that initially decrease the performance but massively increase it later. Furthermore, the same optimizations could lead to an improvement in one hardware and reduction in other. Due to the existence of hundreds of operators (200-300), thousands of graph architectures and tens of hardware backends, it is infeasible to manually design graph optimizations for all cases.</p> <p>There are other issues with template based optimizations</p> <ol> <li>Robustness - Heuristics are not generalizable across architectures and hardware</li> <li>Scalability - New operators and graph structures require newer rules</li> <li>Performance - Misses subtle optimizations specific to DNNs/hardware.</li> </ol> <p>What’s the solution?</p> <h2 id="automate-graph-transformation">Automate Graph Transformation</h2> <p>The main idea is to replace manually-designed graph optimizations with automated generation and verification of graph substitutions for tensor algebra. Basically, generate all possible substitutions and verify if they generate the same output.</p> <p>We start by enumerating all possible graphs up to a fixed size using available operators.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381201066754.jpg" alt=""/> There are up to 66M graphs with 4 operators!</p> <p>Then, with a graph substitution generator, we compute the output with random input tensors. For 4 operators, we can still generate up to 28744 substitutions!</p> <p>These are further pruned based on <em>variable renaming</em> and <em>common subgraphs</em>.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381203016208.jpg" alt=""/></p> <p>These substitutions are formally verified to ensure that they are equivalent mathematically for all inputs. This verification is done by using the properties of operators. For example, convolution with concatenated kernels is same as concatenation of convolutions of the same kernels.</p> <p>So this <em>automated theorem prover</em> can be used to generate valid substitutions scaling up. It takes up to 5 minutes to verify 750 substitutions and there are about 45 rules for the operators which takes about 10 minutes. Adding a new operator is easy - just provide its specifications!</p> <h3 id="incorporating-substitutions">Incorporating substitutions</h3> <p>How do we apply verified substitutions to obtain an optimized graph? The cost is based on the sum of individual operator’s cost and the cost on the target hardware. We greedily apply the substitutions to improve the performance.</p> <p>This approach can be further improved to train a model to learn which kind of substitutions optimize the graph. This was successfully implemented by TASO and it showed good results for real-life models -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381207164574.jpg" alt=""/></p> <h2 id="summary-1">Summary</h2> <p>In summary for graph optimization,</p> <ol> <li>We first construct a search space</li> <li>Enumerate all possibilities for substitutions</li> <li>Prune the candidates, and select the top ones based on profile/cost model</li> <li>Apply the transformations to iteratively improve the performance.</li> </ol> <p>What could go wrong with this? The search may be slow and the evaluation of various graphs can be expensive.</p> <p>Sometimes the substitutions may only be partially equivalent, but can be orders of magnitude faster. In such cases, we can trade off accuracy for performance. E.g., Convolution vs Diluted convolution.</p> <p>Consider the following example. Suppose we have to use the same kernel to perform convolution on two different tensors. Then, we could concatenate these tensors, apply the convolution, and then apply a correction to achieve the correct result. These transformations use partial equivalent transformations yielding some speed up. These are not explorable in the previous case with fully equivalent operators.</p> <h2 id="partially-equivalent-transformations">Partially Equivalent Transformations</h2> <p>Like the previous example, we <em>mutate</em> the programs and correct them to get an optimized graph.</p> <p>The steps to do this automatically, we do something similar to before</p> <ol> <li>Enumerate all possible programs up to a fixed size using available operators</li> <li>Only consider transformations with equal shapes (in contrast with equal results as compared to before)</li> </ol> <p>With this, all the crux of the algorithm comes to the correction of the mutant programs - how do we detect which part is not equivalent and how to correct it?</p> <p>By enumeration - For each possible input and position, check if the values match. For complete correctness, this search would be \(m \times n\) for \(m\) possible inputs and \(n\) output shape. We reduce the effort by reducing \(m, n\)</p> <ul> <li> <p>Reducing \(n\) - Since neural networks are mostly multi-linear, we can make such assumptions.</p> <p>Theorem: For two multi-linear functions \(f\) and \(g\), if \(f = g\) for \(O(1)\) positions in a region, then \(f = g\) for all positions in the region.</p> <p>As a consequence, the search reduces from \(\mathcal O(mn)\) to \(\mathcal O(mr)\)</p> </li> <li> <p>Reducing \(m\) - Theorem - If \(\exists l, f(l)[p] \neq g(l)[p]\), then the probability that \(f\) and \(g\) give identical results on \(t\) random inputs is \(2^{-31t}\).</p> <p>Using this, we can run \(t\) random tests with random inputs, and if all \(t\) pass then it is very likely that \(f\) and \(g\) are equivalent.</p> </li> </ul> <p>The search space reduces to $\mathcal O(tr)$$. How does this relate to correct?</p> <h1 id="ml-compiler-retrospective">ML Compiler Retrospective</h1> <p>This field started in 2013 with Halide. It was a compiler for rendering, but since the workflow is very similar to neural networks, the later compilers draw motivation from here.</p> <p>Then came XLA in 2016-17, that has good performance but had very difficult to understand code. Companies tried other operations such as TensorRT, cuDNN and ONNX for template based graph substitution. CuDNN is still popularly used but no one understands the code since it was written in a very low level language.</p> <p>Then came <code class="language-plaintext highlighter-rouge">tvm</code> in 2018 that we’ve discussed before. In 2019-20, MLIR and Flexflow were introduced - these are layers in the compiler that provided specific optimizations. Then came 2.0 and Torch Dynamo.</p> <p>However, the community is shifting away from compilers. Why? One part is that many optimizations have been found. The main reason is that we’ve seen a certain class of neural networks architectures that work really well. For example, transformers are all the rage. So instead of focusing on compilers, people can focus on just building fused kernels for the attention mechanisms. That’s how we got flash-attention that no compiler is able to beat.</p> <h1 id="runtime">Runtime</h1> <h2 id="memory-and-scheduling">Memory and Scheduling</h2> <p>Our goal is to fit the workload on limited memory and ensure that the peak memory usage is less than the available memory.</p> <blockquote> <p>Need to add some stuff here.</p> </blockquote> <p>Consider the GPT-3 architecture -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17382918001192.jpg" alt=""/></p> <p>For every model, we check the precision and multiply the number of parameters by 2 or 4 to calculate the total memory consumption.</p> <p>For the main model with 175B parameters, if each parameter is 2 bytes, then we require 350Gb of memory! How did we make this work?</p> <p>Why does this rule of thumb work? Let us check the activation sizes for different layers</p> <ol> <li>For 2D convolution: The input has the size \((bs, nc, wi, hi)\), and the output has \((bs, nc, wo, ho)\). The activation size is \(bs*nc*wo*ho*\text{sizeof(element)}\)</li> <li>For an MLP with input size \((bc, m, n)\) and output size \((bs, m, p)\), the activation size is \(bs*m*p*\text{sizeof(element)}\)</li> <li>For a transformer (ignoring activation layers and other FFLs) - the input size is \((bs, h, seq\_len)\) and the output size is \((bs, h, seq\_len). The activation size is\)bs<em>h</em>seq_len*\text{sizeof(element)}$$</li> </ol> <p>So for GPT-3, the per-layer activation assuming sequence length 1 comes to 78 or 156 Gb. Let us add some more elements to this calculation.</p> <p>The Adam Optimizer estimates the first and second moment vectors with parameters for exponential decays. It also has a step-size or learning rate. The algorithm is given by</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17382924545817.jpg" alt=""/></p> <p>Along with the learning rate, since it also stores the moments, it has to store two more values for each parameter! The memory usage becomes thrice of what it should be!</p> <h3 id="lifetime-of-activations-at-training">Lifetime of activations at training</h3> <p>Because we need to store the intermediate values for the gradient steps, training an \(N\)-layer neural network would require \(O(N)\) memory. This is the main difference between training and inference. In inference, we wouldn’t need to store the parameters at all layers, so we would just need \(O(1)\) memory.</p> <p>So we’ve seen for GPT-3, we require 350 or 700 Gb. So for a sequence length of 96, we would require 7488 or 14976 Gb! These numbers are just crazy! We haven’t even considered the composite layers.</p> <p>Therefore, it is important to take care of memory.</p> <h3 id="single-device-execution">Single Device execution</h3> <p>How do we reduce the memory usage?</p> <p>Idea 1 - the input or the activation is not needed until the backward pass reaches the layer. So, we can discard some of them and recompute the missing intermediate nodes in small segments. This technique is called <em>recomputation, rematerialization, checkpoint activation, etc</em>. It’s essentially the time-space tradeoff.</p> <p>For an \(N\) layer neural network, if we checkpoint every \(K\) layers, then the memory cost reduces to</p> \[\text{Memory cost} = \mathcal O\left(\frac{N}{K}\right) + \mathcal O(K)\] <p>To minimize this, we can pick \(K = \sqrt{N}\). The total recomputation increases by \(N\) - essentially another forward pass. In PyTorch, this feature can be activated using <code class="language-plaintext highlighter-rouge">torch.utils.checkpoint</code>.</p> <p>So when do we use this? When memory is a constraint and time of training is not a concern. The memory usage also depends on the layer being checkpointed - the layers can have different out sizes. In transformers, the layer boundary is typically checkpointed. The disadvantage is that this only works for activations.</p> <blockquote> <p>why?</p> </blockquote> <p>The second idea is <strong>gradient accummulation</strong>. The activation memory is linear to batch size. The idea is to compute the gradient for the batch but will limited memory. We split the original batch into micro-batches and accumulate the gradients at each layer. We then update the weights for the complete batch.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17382936900407.jpg" alt=""/></p> <p>The disadvantage of this strategy is that over-subscribing of GPUs is difficult since we have smaller matrices.</p> <p>An alternative method to save on GPU memory is to use the memory hierarchy. We have <code class="language-plaintext highlighter-rouge">SwapIn</code> (swap from CPU DRAM to HBM) and <code class="language-plaintext highlighter-rouge">SwapOut</code> (swap from HBM to CPU DRAM) that can be applied to both weights and activation. As we do a forward pass, we swap in the next layers and swap out the passed layers. You can be a bit more intelligent about it and pre-fetch the layers based on the computation and swap latencies. This strategy is becoming more practical as more companies are adopting the unified memory architecture. The memory hierarchy seems to be breaking.</p> <p>All these strategies can be used together to probably train GPT-3 on a single device but it would take forever.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387232533206.jpg" alt=""/> Why do we start with gradient accumulation instead of gradient checkpointing? Checkpointing greatly increases the computation time, so we try the other alternatives first.</p> <h2 id="memory-and-compute">Memory and Compute</h2> <h3 id="quantization">Quantization</h3> <p>All our memory usage is a multiple of \(\text{sizeof(element)}\). What if we reduce that parameter?</p> <p>Quantization is the process of constraining an input from a continuous or otherwise large set of value to a discrete set. We use a lower-precision representation for data while preserving ML performance (accuracy), speeding up compute, reducing memory, saving energy, etc. Most of the edge models use quantization.</p> <p>To understand this better, let’s understand the representation of data in memory -</p> <ul> <li>An unsigned integer has the range \([0, 2^n - 1]\)</li> <li>A signed integer with \(n\)-bit has the range \([-2^{n-1} - 1, 2^{n - 1} - 1]\). To avoid saving 0 twice by storing a sign bit, computer architects decided to use <em>Two’s complement representation</em>.</li> <li>Fixed point number - An arbitrary bit is chosen as the boundary for the integer and the decimal. This representation is mainly used in security applications now.</li> <li> <p>Floating point representation - We use a sign bit, 8-bit exponent and 23 bit fraction. That is the value is, \((-1)^{sign} \times (1 + \text{ fraction}) \times 2^{\text{exponent} - 127}\).</p> <p>How do we represent 0 then? Representation-wise, we technically cannot represent 0, so we make a special representation - <em>normal vs subnormal values</em>. Whenever the exponent bits are zero, we remove the bias term \(1\) that is added to the fraction, and represent the value as \((-1)^{sign} \times \text{ fraction} \times 2^{\text{exponent} - 127}\). This expressions is only used with the exponent is \(0\). This way, we also extend the range of the representation and the smallest positive number we can represent is \(2^{-149}\).</p> <p>How about special values? Exponent with all set bits is infinity and sign is decided by the sign bit. NaN is represented in the subnormal range with exponent bits set to 1. In summary, we have</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387239819544.jpg" alt=""/></p> </li> </ul> <blockquote> <p>Calculate model sizes from parameters table.</p> </blockquote> <p>Notice that the precision of floating point numbers is much higher when the values themselves are small. This is a tradeoff we make based on the applications. Here is a summary of other floating point representations -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387241256605.jpg" alt=""/></p> <p>The BF16 system has been observed to provide much better performance over FP16 for neural network training. The representation lowers the precision for a higher range. It could be that the higher range can stabilize the training during exploding or vanishing gradients. During inference, the precision matters more so FP16 might perform better in some cases.</p> <p>For practice, consider the following examples</p> <ul> <li>The FP16 bit set <code class="language-plaintext highlighter-rouge">1 10001 1100000000</code> represents -7.1. Why? The bias in the exponent is always the median value subtracted by 1. Here it is \(2^4 - 1 = 15\). The exponent is then \(17 - 15 = 2\), and the fraction is \(0.5 + 0.25 = 0.75\)</li> <li>The decimal 2.5 is represented as <code class="language-plaintext highlighter-rouge">0 10000000 0100000</code>. The bias is \(2^7 - 1 = 127\)</li> </ul> <p>After these representations, newer ones came up to improve the performance in deep-learning. <img src="/assets/img/2025-01-06-data-systems-for-ml/17387248528126.jpg" alt=""/> The rule of thumb is that we require higher range for training and higher precision for inference.</p> <p>As if this were not enough, for even lower compute, NVIDIA has been pushing for INT4 and FP4 to keep up with the Moore’s law.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387250679255.jpg" alt=""/></p> <p>There has been no successful model with these representations, making it a very promising avenue for research.</p> <p>Alright, with these things in mind, let us come back to quantization. There have been two approaches for quantization</p> <ol> <li> <p><strong>K-Means based quantization</strong> - widely used, almost all mobile devices have it.</p> <p>Consider we have a weight matrix that we are trying to quantize. The idea is to cluster close values together into an integer or a lower-precision representation. The number of clusters is chosen based on the chosen requirements - for 2-bit quantization, number of clusters is 4.<br/> To do so, we simply apply K-means. The centroids are stored in a code book whereas the matrix is simply stored in the lowest possible representation to further reduce storage. <img src="/assets/img/2025-01-06-data-systems-for-ml/17387254097673.jpg" alt=""/> How do we perform a backward pass on these matrices? The gradients are accumulated based on the classes and are applied to the centroids to fine-tune them. In practice, quantization starts affecting the performance sharply only after a certain threshold. Therefore, quantization becomes a hyper-parameter tuning problem, and we can achieve significantly lower memory consumption. The number of bits used can vary with layers as well!</p> <blockquote> <p>Try coding a library with variable quantization layers. Shared code books across layers</p> </blockquote> <p>How is the run-time affected? The computations are still FP arithmetic. There is an added computation cost for weight compression/decompression and code book lookup. K-means has been quite effective with convolution networks.</p> </li> <li> <p><strong>Linear quantization</strong> - The idea is to determine a linear mapping of integers to real numbers. It can be seen as a linear optimization problem.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387262709675.jpg" alt=""/></p> <p>So, we need to determine zero-point and scale parameters. These parameters are often also publicly shared on platforms such as HuggingFace. The parameters can be chosen for the whole model or for each layer based on our performance tradeoff appetite. For many popular models like Llama, the quantization is done tensor-wise.</p> <p>The parameters can easily be determined as follows</p> \[\begin{align*} S &amp;= \frac{r_{max} - r_{min}}{q_{max} - q_{min}} \\ Z &amp;= \text{round}(q_{min} - \frac{r_{min}}{S}) \end{align*}\] <p>The bit-width \(n\) determines \(q_{min} = -2^{n -1}\) and \(q_{max} = 2^{n - 1} - 1\).</p> <p>Suppose we apply linear quantization to <code class="language-plaintext highlighter-rouge">matmul</code> -</p> \[\begin{align*} Y &amp;= WX \\ S_Y(q_Y - Z_Y) &amp;= S_W(q_W - Z_W)S_X(q_X - Z_X) \\ Q_Y = \frac{S_W S_X}{S_Y} \left( q_W q_X - Z_W q_X - \underset{Z_X q_W + Z_W Z_X) + Z_Y}{\text{precomputed for inference}} \right) \end{align*}\] <p>Empirically, the factor \(\frac{S_W S_X}{S_Y}\) is between 0 and 1. Instead of using floating point multiplications, it is represented as fixed point multiplication and bit shift. Also, empirically \(Z_W\) follows normal distribution and can be approximated as 0. Thus, the heavy lifting operation is \(q_Wq_X\) which is simply integer multiplication.</p> <p>Therefore, we reduced both storage and computation time (integer arithmetic is much faster and cheaper). This can also be used to reduce FP16 to FP8 rather than integers.</p> </li> </ol> <p>In summary, we have <img src="/assets/img/2025-01-06-data-systems-for-ml/17387273312564.jpg" alt=""/></p> <h1 id="large-language-models-on-cloud-and-edge">Large Language Models on Cloud and Edge</h1> <p>The first wave of revolution in ML systems was sparked by Big Data in 2010s (competitions like Netflix recommendation price), which resulted in systems such as XGBoost, Spark and GraphLab. Then in mid 2010s, deep-learning started gaining traction, and TensorFlow, TVM and Torch were created. Now, we’re in the third revolution with Generative AI. ML Systems is playing a much bigger role.</p> <p>The challenges involved are -</p> <ol> <li> <p>Memory - Llama-70B consumes 320GB VRAM just to store parameters in FP32</p> </li> <li> <p>Compute - The post-Moore era brings great demand for diverse specialized compute, system support becomes bottleneck</p> </li> </ol> <p>The design of systems depends on the paradigm that the industry is moving towards. Currently, we have cloud based models with a client-server architectures. This is how computers initially started before the age of personal computers. So, would we move towards personal AI in consumer devices?</p> <p>There are many engineering challenges involved in this. As we covered previously, there are specialized libraries and systems for each backend involving manually created optimizations. The area is very labor intensive with huge market-opportunities.</p> <p>Our approach to this has been in terms of composable optimizations to rewrite kernel codes. Furthermore, we added techniques such as parameter sharding, memory planning, operator fusion, etc to add to these optimizations. What have we learned from this journey?</p> <p>There are four abstractions we use</p> <ul> <li> <p>Computational Graphs - Graph and its extensions enable higher level program rewriting and optimization</p> </li> <li> <p>Tensor Programs - These abstractions focus on loop and layout transformation for fused operators</p> </li> <li> <p>Libraries and Runtimes - Optimizing libraries are built by vendors and engineers to accelerate key operators of interests</p> </li> <li> <p>Hardware Primitives - The hardware builders exposes novel primitives to provide native hardware acceleration</p> </li> </ul> <p>It has not been about a <strong>silver bullet system but continuous improvement and innovations</strong>. ML Engineering is going hand-in-hand with ML modeling.</p> <p>The developers of TVM are expanding it to TVMUnity to bring the compiler flow to the user. As we’ve studied, IRModule is the central abstraction in TVM. Once the user-level code is written in this form, TVM takes care of the hardware backend making it easy to the models on various architectures.</p> <p>TVM generates an optimized models, and features are continuously added with data to make it better over time.</p> <h2 id="tvm-unity">TVM Unity</h2> <h3 id="first-class-symbolic-shape-support">First-class symbolic shape support</h3> <p>Traditional models like ResNet have some key-characteristics. The compilers are being built under the assumption of these fixed parameters, but with the age of generative AI, these parameters keep changing continuously. TVMUnity leverages symbolic shift to incorporate variable sizes in the model. In essence, traditional compilers are unable to handle dynamic shapes, whereas TVMUnity allows it with symbolic support.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-47-07-image.png" alt=""/></p> <p>Knowing the dependencies between different variable parameters allows for better optimizations during runtime - static memory planning for dynamic shapes</p> <h3 id="composable-tensor-optimization">Composable Tensor Optimization</h3> <p>In the early ages, we had scalar computing. Then came the age of vector computing with SIMD. Now, NVIDIA has TensorCore and TPUs became a thing for tensor computing. How do we leverage these hardware developments in our programs? We want both loop based optimization and tensor-based programs.</p> <p>The first step is to isolate the internal computation tensorized computation from external loops to create a <code class="language-plaintext highlighter-rouge">Block</code>.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-52-50-image.png" alt=""/></p> <p>With this, TVMUnity does <em>Imperative Schedule Transformation</em> to search different variants of programs by changing blocks to create an optimized IR.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-53-13-image.png" alt=""/></p> <p>By providing this interface to the user, where the user can mention the parameters involved, TVMUnity can perform tensorization along with graph-optimization problem.</p> <h3 id="bringing-compilation-and-libraries-together">Bringing compilation and Libraries Together</h3> <p>We have seen this tradeoff with compilers and libraries.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-57-28-image.png" alt=""/></p> <p>TVMUnity provides interfaces like <code class="language-plaintext highlighter-rouge">Relax-BYOC</code> to offload the computational load to libraries such as TensoIR to leverage the library-optimized kernels. The native compilation works on top of the library offloading to increase the performance even more.</p> <p>By adding this flexibility, the compiler can update (live with library updates) to squeeze the best performance.</p> <p>Since libraries come out with data layout requirements, the compiler can use this information to optimize the remaining parts of the code.</p> <h2 id="ml-compilation-in-action">ML Compilation in Action</h2> <p>How does this architecture help with incremental developments? The users can try new optimizations in coordination with the compiler to scale to newer models.</p> <p>In language models, the compiler does targeted optimizations such a low-bit quantizations, dynamic shape planning, fusion and hardware aware optimizations, etc. Along with these, it works with KVCache etc to improve the performance.</p> <p>The <strong>MLCEngine</strong> is a universal LLM deployment engine being developed to deploy LLMs on any device. The key contribution here is <em>universal</em>. This development also revealed some key insights - iOS devices require custom Metal language kernels, Snapdragon devices require OpenCL - and this project is trying to take care of all of that.</p> <p>It also helps with structured generation with near zero overhead. This feature would be super useful for agent use cases. <a href="https://huggingface.co/spaces/mlc-ai/WebLLM-Structured-Generation-Playground">Surprisingly, MLCEngine does it with near zero overhead with something known as XGrammar!</a></p> <p>They also created <a href="https://webllm.mlc.ai/">WebLLM</a> with the new WebGPU standard to use the local compute to the browser. Yes, it uses your local GPUs and doesn’t send your data to any server! You can use this to build stuff on top of it with the typescript API and a Node package. It is an open source project too!</p> <p>Let us continue our discussion on Quantization. We discussed about quantization granularity (per-tensor, per-channel, group quantizations).</p> <p>Per-channel quantization is preferred to tensor quantization in some cases because the channels can have very different ranges, and using a single \(S\) can result in lopsided representations. Even then, some applications can lose too much on performance for per-channel quantization. In these cases, group quantization is useful.</p> <p>Group quantization is more fine-grained. E.g., per vector. It has more accuracy, less quantization error trading off some of the savings.</p> <blockquote> <p>Can we do some sort of statistical grouping here?</p> </blockquote> <p>The sweet-spot that works in practice is a two-level quantization that quantizes hierarchically. Instead of using \(r = S(q - Z)\) we use \(r = \gamma S_q (q = Z)\). \(\gamma\) is a gloating-point coarse grained scale factor, and \(S_Q\) is an integer per-vector scale factor. It can be further generalized into multi-level quantization with scale factors for each levels.</p> <p>So far, we have done quantization of weights. They are static during inference, and so everything works. What if we want to quantize the activations? The stats change with every input. This problem is solved in two ways:</p> <ol> <li>Moving average - Observed ranges are smoothed across thousands of training steps</li> <li>Calibration dataset - Figure out the range from subset of training set</li> </ol> <h2 id="mixed-precision">Mixed Precision</h2> <p>In the previous methods, we have done <em>uniform quantization</em> wherein every weight parameter is quantized to the same number of bits. However, what if we use different precision for different weights? Does this improve the performance a lot? The implementation is however very complicated since we have to deal with different data formats. Also, what formats give the best savings and high performance?</p> <p>These are hard questions. So we throw ML at the problem. We define a cost function to let a model discover the combination of formats that give the best parameters. It was a good research field, and the accuracy of models improved by 10% or so.</p> <p>After a while, <a href="https://arxiv.org/pdf/1710.03740">NVIDIA released a paper</a> (it’s a good read) that became the standard for mixed precision training. The intuition for this approach is as follows. Some layers are more sensitive to dynamic range and precision. For example, softmax and normalization layers have a large range of values. We identify such operations and assign them higher precision.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393289702270.jpg" alt=""/></p> <p>Since Adam calculates two moments and normalizes the gradients, we use FP32 for the weight updates.</p> <p><em>Note.</em> Deepseek changed the standard with new precisions.</p> <p>Let us see how the memory of models changes with this new precision system. Again, for the largest GPT, there are 175B parameters. So it occupies 350G with FP16 for all the weights. The activations occupy 7488G assuming checkpointing at each layer boundary.</p> <p>So in this system, the master model with FP32 weights occupies 4<em>175 = 700G. The gradients occupy 2</em>175 = 350G. The running copy of the model used for inference is 2<em>175 = 350G. Finally, we need Adam mean and variance (FP32) that is 2</em>4*175 = 1400G. The rule of thumb in general is \((4 + 2 + 2 + 4 + 4)N = 16N\) memory for LLMs.</p> <h3 id="scaling-down-ml">Scaling down ML</h3> <p>Running ML on edge devices is always strongly demanded, and the market is very fragmented. It is easy to build a startup and get acquired in this space. The possible research directions are quantization, pruning, ML energy efficiency, federated ML, etc.</p> <h1 id="parallelization-1">Parallelization</h1> <p>Moore’s law came to an end. However, ML models were scaling 3x every 18 months! Why? Bigger model gives better accuracy. People have also started seeing emergent capabilities in larger models.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393299103392.gif" alt=""/></p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393300313845.jpg" alt=""/></p> <p>So, the models are going to get bigger. Along with this, the memory demand is increasing too. It was back in 2019 when we last fit an entire model in one GPU. Now we require 100s of GPUs just to store the model parameters. The only way out of this is parallelization. Wait! Aren’t GPUs already doing that?</p> <blockquote> <p>GPUs did data parallelization. We are talking about having multiple GPUs and using them together. All the things we considered so far assumed the entire model is on one GPU. Now, we need to distribute the model training. We just seem to be creating problems for ourselves…</p> </blockquote> <p>Intuitively there are multiple ways we can go about this</p> <ol> <li>Parallelize along the layers (cutting through depth)</li> <li>Parallelize each layer (cutting through breadth) - this one is rather complicated with more data traveling between clusters.</li> </ol> <p>Apart from these considerations, a GPU cluster also has its own constraints. There are different latency communication channels (kind of like memory hierarchy).</p> <p>Let us look at the problem from a computational lens. A model involves parameters, weight updates, model spec and the data.</p> <ul> <li>Computing - The forward pass and backward pass require compute</li> <li>Memory - The data and parameters require memory. Between these, we require communication (typically done with interconnects or network, e.g., NVLink) which is the main bottleneck in this whole setup. How do we communicate parameters and activations?</li> </ul> <p>In <strong>data parallelism</strong>, we partition the data across GPUs and give each GPU a copy of the parameters and gradients. Then, we need to synchronize the updates together across the GPUs before the next iteration.</p> <p>In <strong>model parallelism</strong>, we partition the model across GPUs and use the data to update parts of the model. This method is more complicated. Let us delve deeper.</p> <p>How do we partition a computation graph on a device cluster?</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393313839159.jpg" alt=""/></p> <p>There are more strategies that consider hybrid variants - some parts of the model are inter-op, intra-op and some parameters are replicated across devices.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393315930822.jpg" alt=""/></p> <p>These are the standard techniques being used for the models today.</p> <p>Let us delve deeper into these. <img src="/assets/img/2025-01-06-data-systems-for-ml/17393318019536.jpg" alt=""/> In the above example, intra-op is similar to what we discussed before for matmul on GPUs - each GPU computes a partial sum. Surprisingly, we can show mathematically that this is the best we can do with inter-op.</p> <p>What are the pros and cons?</p> <ul> <li>Inter-op parallelism requires point-to-point communication but results in devices being idle.</li> <li>Intra-op parallelism keeps the devices busy but requires collective communication.</li> </ul> <p>The all-reduce operation is computationally intensive.</p> <p>Since intra-op requires more communication, this is an important aspect to consider. On the other hand, inter-op results in hardware bubble wherein some of the GPUs are idle (can be prevented to some extent with pipelining).</p> <p>In summary, inter-op parallelism requires point to point communication but results in idle devices. The devices are busy in intra-op parallelism but requires collective communication.</p> <blockquote> <p><strong>A note on terminology</strong> Previously, the literature used to talk about data and model parallelism. Data parallelism is general and precise but the term “model parallelism” is vague. That is why, we stick to the terms inter-op and intra-op parallelism which rely on the pillars of computational graph and device cluster. Furthermore, we are mainly discussing about model parallelism because of the regime in which models do not fit on a single GPU anymore.</p> </blockquote> <p>With that, let us formulate our goal - “What is the most efficient way to execute a graph using combinations of inter-op and intra-op parallelism subject to memory and communication constraints?”</p> <p>Let us equip ourselves with a metric to quantify this goal. Previously we had, Arithmetic Intensity. Now, we consider <strong>Model FLOPs Utilization (MFU)</strong></p> \[MFU = \#FLOPs/t/\text{peak FLOPs}\] <p>Where #FLOPs is the total FLOPs in the ML program, and \(t\) is the time required to finish the program. The goal is to maximize this quantity.</p> <p>Why don’t we achieve the peak FLOPs? The Peak FLOPs values are typically achieved by matrix multiplications but a neural network model like a transformer involves many layer operations (memory bounded) that lower the performance. Moreover, we have seen how optimization can greatly reduce the time of execution and increase the operations within the same time. Unoptimized code can lower the value further. Adding to these communication like we have discussed before can reduce the quantity a lot too. Finally, all these parameters also depend on the precision, code and the GPU type being used.</p> <p>How do we calculate the MFU?</p> <ol> <li>Count the number of operations (FLOPs) considering the shape of the data in the model. For example, we have seen the formula \(2mnp\) for <code class="language-plaintext highlighter-rouge">matmul</code>.</li> <li>Run the model for one forward and backward pass on the GPU to get benchmark for the running time \(t\)</li> <li>Check the GPU spec, type of cores, and their peak FLOPs</li> <li>Calculate the MFU</li> </ol> <p>What is the history of these numbers?</p> <ol> <li>The best ML system on V100 a couple years ago got 40-50% MFU - only half the peak utilization! The peak FLOPs on V100 is 112 TFLOPs, so we were only able to use 50-60 FLOPs</li> <li>With A100, we were still in the same range until FlashAttention came up, which took the MFU value to 60%! A100 has 312 FLOPs, so we are using ~160FLOPs at this stage!</li> <li>With H100, we are able to use only 30-50% depending on the model size (Larger <code class="language-plaintext highlighter-rouge">matmul</code> is better for MFU over smaller). Why did it decrease? The peak value of H100 is very high (990 TFLOPs), and our software did not catch up to this. Remember that communication also plays an important role</li> <li>This year, with B100 the peak FLOPs is 1.8 PFLOPs!</li> </ol> <p>Besides MFU, we also define <strong>Hardware FLOPs Utilization (HFU)</strong>. This quantity is to consider operations that do not contribute to the model For example, we can treat gradient checkpointing as 2 forward passes and 1 backward pass (each backward pass can be approximated as 2 forward passes due to gradient updates)</p> <h2 id="collective-communication">Collective Communication</h2> <p>In Machine Learning systems, there are usually two types of connections</p> <ol> <li>Point-to-point communication - Comprises of a sender and a receiver. Very simple.</li> <li>Collective Communication - It’s a common concept in HPC (high performance computing). There are multiple forms of this <ol> <li>Broadcast - One worker shares data with all the other workers</li> <li>Reduce(-to-one) - All the data among the workers is combined into one rank (one worker). Essentially reverse of broadcast in a way.</li> <li>Scatter - The data from one rank is split and each of the splits is <em>scattered</em> across other ranks. That is, every worker gets on part of the data.</li> <li>Gather - The reverse operation of Scatter, where different parts of the data are <em>gathered</em> into one rank.</li> <li>All-gather - Essentially gather followed by broadcast.</li> <li>Reduce-scatter - Essentially reduce followed by scatter.</li> <li>All-reduce - Reduce followed by scatter.</li> </ol> <p>Collective communication is more expensive than P2P since it can be thought of as a combination of many P2Ps. However, collective communication has been highly optimized in the past 2 decades (<code class="language-plaintext highlighter-rouge">(x)ccl</code> libraries - NVIDIA has <code class="language-plaintext highlighter-rouge">NCCL</code> the best, Microsoft has <code class="language-plaintext highlighter-rouge">MCCL</code> and Intel has <code class="language-plaintext highlighter-rouge">OneCCL</code>). The important thing to note is that collective communication is not fault tolerant (if one device fails, then everything fails).</p> </li> </ol> <h3 id="basics">Basics</h3> <p>Let us understand some more terminology for communication models. There is a terminology called as \(\alpha\beta\) model that talks about latency and bandwidth. For example, if the model is \(\alpha + n \beta\) then \(\alpha\) refers to the latency and \(\beta = 1/B\) is the reciprocal of bandwidth determined by the hardware. For smaller messages, latency (\(\alpha\)) is the dominant factor, and for larger messages (larger \(n\)), the bandwidth utilization (\(n\beta\)) dominates.</p> <p>Based on these distinctions, the community works on two mainstream algorithms/implementations</p> <ol> <li>For smaller messages, an MST based algorithm that emphasizes low latency</li> <li>For larger messages, a <strong>Ring algorithm</strong> to emphasize on bandwidth utilization.</li> </ol> <p>There are over 50+ algorithms in this area and the HPC community even got the 2023 Turing award!</p> <p>The core principle for lower latency is to minimize the number of rounds needed for communication. For example, consider the Broadcast operation.</p> <ol> <li>We first split the ranks into half, and send the message to the half without the message</li> <li>Then repeat broadcast recursively in each half</li> </ol> <p><em>Beautiful.</em></p> <p>For the core operations, we have the following communication models</p> <ul> <li>Reduce-to-one - \(\log(p) (\alpha + n \beta + n \gamma)\)</li> <li>Scatter and Gather- \(\log(p)\alpha + \frac{p - 1}{p} n\beta\)</li> <li>Broadcast - \(\log(p)(\alpha + n \beta)\)</li> </ul> <p>The remaining composite operations can simply use these primitives.</p> <p>What are the problems with this approach? Since latency is prioritized over bandwidth, some links are idle. Building on the same idea, the core principle for high-bandwidth is to use all links between every two nodes.</p> <p>The <strong>ring algorithm</strong> essentially is a logical ring that can be embedded in a physical linear array with worm-hole routing such that the “wrap-around” message does not conflict. Look at the diagram below for clarity -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17395038648341.jpg" alt=""/> At every instant, all the links are used ensuring full-bandwidth. For example, reduce-scatter can be implemented as consecutive reduce and propagate ring passes.</p> <p>For this regime of communication, the core primitives of communication are</p> <ul> <li>Reduce-scatter</li> <li>Scatter</li> <li>Gather</li> <li>All-gather and the other operations can be implemented as the composites of these</li> <li>Reduce(-to-one) is reduce-scatter followed by gather</li> <li>All-reduce is reduce-scatter followed by all-gather</li> <li>Broadcast is scatter followed by all-gather.</li> </ul> <p>So, how does this all come back to ML? ML systems are usually composed of large data communications. Inter-op systems always result in P2P communications and intra-op result in collective communication.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17395041727515.jpg" alt=""/></p> <p>The key point to note is that communication always happens between nodes that are partitioned differently. Remember that with tensors there are more dimensions, and the above diagram is a simplified version.</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Prompting ChatGPT is not enough. To build large-scale AI systems, it is imperative to understand how to design the proper systems to optimize all the computations. The following blog is a deep-dive into system/data design for Machine Learning frameworks.]]></summary></entry><entry><title type="html">Reinforcement Learning Theory</title><link href="https://sudhansh6.github.io/blog/rl-theory/" rel="alternate" type="text/html" title="Reinforcement Learning Theory"/><published>2025-01-06T00:00:00+00:00</published><updated>2025-01-06T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/rl-theory</id><content type="html" xml:base="https://sudhansh6.github.io/blog/rl-theory/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Through this article, we aim to understand the theory of reinforcement learning in the context of Large Language Models.</p> <p>Reinforcement learning addresses the domain of sequential decision problems, wherein an <em>agent</em> takes <em>actions</em> in an <em>environment</em>. These configurations are generally represented using <em>Markov Decision Processes</em> (MDP). An MDP is characterized by -</p> <ul> <li>Time - T - Discrete, infinite</li> <li>States - S - Discrete</li> <li>Actions - A - Discrete</li> <li> <p>Transitions - \(\tau : S \times A \to \Delta(S)\) - a probability distributions of states</p> \[P(s’ \vert s, a) = P_R[ \tau (s, a) = s’]\] <p>In more complicated setups, the transition function could be a function of the time \(T\) (stationarity and non-stationarity). In finite-time horizons, the time can be embedded inside the state itself, converting non-stationarity scenarios to stationarity ones.</p> </li> <li> <p>Reward - \(R: S \times A \times S \to \mathbb R: [-M, M]\). There is no need to make this non-deterministic since that is already generalized by \(\tau\). The rewards are usually bounded. The expected reward for an action is given by</p> \[R’(s, a) = \sum_{s’ \in S} R(s’, a, s) p(s’ \vert s, a)\] </li> <li>Initial state \(S_0\) - Can be a single state or a distribution over states.</li> <li>Discount Factor \(\gamma\) - A factor \(&lt;1\) to bound the total expected reward from the future. This will be better understandable from the text later.</li> </ul> <p>A <strong>policy</strong> \(\pi\) is a probability distributions over actions based on the current state. Consider a distribution \(\beta: S \to \delta(A)\), then a policy formally is</p> \[\pi(s, a) = P_R[\beta(s) = a]\] <p>A <strong>trajectory</strong> describes the sequence of states, actions of an agent in a <em>run</em> - \(s_0, a_0, r_0, s_1, a_1, r_1, \dots\).</p> <p>A policy is associated with a <strong>value function</strong> \(V_{\pi, t} (s) = \sum_{i = t}^\infty \gamma^{i - t} r_i\) Note that the dependence of the value of function on time is redundant in stationarity situations.</p> <p>The goal is to maximize this value function over all possible policies in an environment</p> \[\pi^* = \arg \max_{\pi} V_\pi (S_0)\] <p>This is known as the <em>Markov Decision Problem</em>. In a strict setting, the supremum may not exist (be a valid policy).</p> <p>How is Reinforcement Learning different from Supervised learning? The key differences are</p> <ul> <li>There is no sequentiality</li> <li>Every decision in SL has an associated reward, and there is no stochasticity</li> </ul> <p>Policies are optimized in multiple methods</p> <ul> <li>There are hill-climb methods - policy iteration, value iteration</li> <li>Learning based methods - think of a policy as a matrix that has to optimized to satisfy certain constraints.</li> </ul> <h2 id="bellman-equations">Bellman Equations</h2> <p>These are a recursive formulations of policies and value functions.</p> \[V_\pi(s) =3.142 s \pi(s, a)[ p(s’ \vert s, a) [R(s’, a, s) + \gamma V_\pi (s’)]\] <h2 id="the-markov-assumption">The Markov Assumption</h2> <p>Many processes in nature are <em>Markov</em> in nature - the action at the current state only depends on the current state, and not the history of states.</p> <p>A general policy is a function from trajectories \(H_t\) to a distribution over actions. However, such general representation is difficult to compute. Additionally, due to the Markov assumption, a stationary policy \(\pi: S \to \delta(A)\) is general enough for optimality. On surface, this may seem like we are limiting the power of an agent, but we will see that this is not the case. In fact, even a deterministic policy will do.</p> <p>How about reward? Are we limiting the reward function with a Markovian assumption? Since we consider expected rewards - the expectation value can be embedded within the state-based rewards as well. So in fact, both representations are the same.</p> <p>With a general policy, the probability of a trajectory is given by</p> \[P[S_0 = s_0, A_0 = a_0, \dots, S_t = s_t] = \mu(s_0) \pi(a_0 \vert s_0) P(s’ \vert s_0, a_0) \pi(A_1 \vert S_0, a_0, s_1) \dots\] <p>Continuing with the Bellman equations, our goal was to find a policy that maximizes the value function</p> \[\begin{align*} V_\pi(s) &amp;= \mathbb E_{P, \pi} \sum_{t = 0}^\infty \gamma^t [R(S_t, A_t) \vert S_0 = s] \max_{\pi} V_\pi(s) \\ &amp;= V^*(s) \\ \end{align*}\] <p>Firstly, does there exist a policy such that \(V_\pi(s) = V^*(s)\) for all \(s\)? An optimal policy for a particular state is guaranteed because of compactness, but is there such a policy for all states?</p> <p>To aid our analysis, we also define a <strong>Q-function</strong> \(Q: S \times A \to R\), that describes the expected reward \(Q(s, a)\) of taking an action \(a\) at state \(s\). This function is very similar to the value function but has an additional condition on the action as well. Why have both these functions? Convenience. Analogously, we also define \(Q^*(s, a) = \max_\pi Q_\pi (a, a)\).</p> <p><strong>Theorem (Existence theorem).</strong> There exists a stationary and deterministic policy that has the optimal value for all states.</p> <p>Therefore, Bellman equations are possible due to assumptions of Markovian nature of the policy and state transition functions. We have the following equations</p> \[\begin{align*} V_\pi(s) &amp;= R(s, \pi(s)) + \gamma \sum_{s’} P(s’ \vert a, \pi(s)) V_\pi (s’) \\ Q_\pi(s, a) &amp;= R(s, a) + \gamma \sum_{s’} P(s’ \vert s, a) \sum_{a’} \pi(a’ \vert s’) Q_\pi(s’, a’) \\ V_\pi(s) &amp;= \sum_a \pi(s) Q_\pi(s, a) \\ Q_\pi(s, a) &amp;= R(s, a) + \gamma \sum_{s’} P(s’ \vert s, a) V_\pi(s’) \end{align*}\] <h2 id="summary">Summary</h2> <p>The equation we’ve been working with for value iteration is</p> \[Q_{t + 1}(s, a) = L(Q_t) = R(s, a) + \mathbb E_{s’ \sim P(. \vert s, a)} \max_{a’} Q_{t}(s’, a’)\] <p>The key step here is to extract a policy from the current \(Q\) function. We noted that the optimal function \(Q^*\) satisfies the optimality equation.</p> <p>Aside from the optimality equation, we have the expectation equation that every \(Q\) function satisfies</p> \[Q_\pi(s, a) = R(s, a) + \mathbb E_{s’} \mathbb E_{a’ \sim \pi(\cdot \vert s’)} Q(s’, a’)\] <p>The advantage of looking at the optimality equation as an operation \(Q_{t + 1} = L(Q_{t})\) is that we can apply the contraction concepts to arrive at \(Q^*\) with Banach’s fixed point theorem. This way, we prove that there is a unique optimal \(Q\) function.</p> <p>Now we show that, after enough number of iterations, we can also get the value function \(V\) arbitrarily close the optimal value. All these subtleties together show that the value iteration algorithm works!</p> <p>How do obtain these bounds based on iterations? We need to find an upper bound for \(\| Q_{t} - Q^* \|_\infty\). We can show that this value is \(leq \|Q^* - Q_0 \|_\infty\). Assuming we start with a \(Q\) with all zeroes, the maximum value of \(Q^*\) is simply \(R_{\max}/(1 - \gamma) = (\max_{s, a} \vert R(s, a)\vert)/(1 - \gamma)\).</p> <p><em>Lemma.</em> \(\|V_m(s) - V^*(s)\| \leq \frac{2}{1 - \gamma}\| Q_m - Q^*\|_\infty\).</p> <p><em>Proof.</em></p> \[\begin{align*} V^*(s) - V_m(s) &amp;= Q^*(s, \pi^*(s)) - Q_m(s, a = \pi_m(s)) \\ &amp;= Q^*(s, \pi^*(s)) - Q^*(s, a) + Q^*(s, a) - Q_m(s, a) \\ &amp;= Q^*(s, \pi^*(s)) - Q^*(s, a) + \gamma \mathbb E_{s’} (V^*(s’) - V_m(s’)) &amp;\leq Q^*(s, \pi^*(s)) - Q^*(s, a) + \gamma \|V^* - V_m\|_\infty \\ &amp;\leq (Q^*(s, \pi^*(s)) - Q_m(s, \pi^*(s))) + (Q_m(s, \pi^*(s)) - Q^*(s, a)) + \gamma \|V^* - V_m \|_\infty \\ &amp;\leq (Q^*(s, \pi^*(s)) - Q_m(s, \pi^*(s))) + (Q_m(s, a) - Q^*(s, a)) + \gamma \|V^* - V_m \|_\infty \\ &amp;\leq 2\|Q^* - Q_m\|_\infty + \gamma \|V^* - V_m\|_\infty \\ \|V^* - V_m \|_\infty \leq \frac{2}{1 - \gamma} \|Q^* - Q_m \|_\infty \end{align*}\] <h1 id="policy-iteration">Policy Iteration</h1> <p>Instead of modifying the policy based on the current value, why not do it the other way round? Iterate over the policy, get its value and improve it again? There is a subtle different as compared to the previous algorithm, and it turns out that this method is much more efficient!</p> \[\pi_0 \underset{Q_0}{\longrightarrow} \to \pi_1 \to \cdots \to \pi_k \underset{Q_k}{\longrightarrow} \pi_{k + 1}\] <p>Policy iteration takes \(\mathcal O(\vert S\vert^3 + \vert S \vert^2 \vert A\vert)\) whereas value iteration is \(\mathcal O(\vert S \vert^2 \vert A\vert)\).</p> <h1 id="model-free-methods">Model-free methods</h1> <p>Policy iteration and Value iteration are closely related to each other. For both the algorithms, we need to evaluate a policy to find the corresponding value function. However, in many cases, we do not know the exact transition and reward functions. In other cases, the environment can have a large number of states, making it impossible to model it.</p> <p>For such situations, we rely on <strong>Monte-Carlo methods</strong>. Any method that solves a problem by generating suitable random numbers and observing that a fraction of numbers obey some property or properties, can be classified as a Monte Carlo method. The key ideas here are using a <em>sampling technique</em> for a heuristic <em>estimator</em>. These methods do not make use of the Markov assumption much, making them much more generalizable.</p> <p>The idea is to learn directly from episodes of experience without a prior knowledge of MDP transitions (rewards). Since the idea relies on episodes, one caveat is that it can only be applied to <em>episodic MDPs</em> - episodes have to terminate.</p> <h2 id="prediction-problem">Prediction Problem</h2> <p>Let us consider the first problem - estimating \(V_\pi(s)\) for a state. Instead of updating after every action, we update after each episode by taking the mean reward across all sample trajectories sampled from this stage. The <em>first-visit</em> algorithm is given by</p> \[\tilde V_\pi(s) = \frac{1}{m} \sum G_i^s\] <p>Where \(G_i^s\) is the total reward after \(s\) first appears in the episode. Even though the number of states is large, it’s nonetheless finite. Using this fact, we can show theoretically that the value above can be bounded. The convergence time is also associated with the underlying transition probabilities (rare states require more episodes to appear in the trajectory).</p> <p>There are more questions to answer. For example, is this a biased or unbiased estimator?</p> \[\begin{align*} \mathbb E(\tilde V_\pi(s)) &amp;= \frac{1}{m} \sum_i \mathbb E(G_i^s) \\ &amp;= V_\pi(s) \quad \because \text{ Markovian assumption} \end{align*}\] <p>How about doing a second-visit algorithm or every-visit algorithm? They are valid approaches too, the theoretical analysis slightly varies. The estimators may not be unbiased but they use the data more efficiently. That is useful in cases where sampling is expensive. The <em>every-visit algorithm</em> typically has a higher bias (due to dependencies in an episode for the occurrences) but lower variance and higher efficiency.</p> <p>These differences are important to understand. For example, in the latest <a href="https://openai.com/index/openai-o3-mini/">o3-mini</a> model, they observed that the every-visit variant of an RL algorithm obtained much better performance than the first-visit variant.</p> <p>In contrast, consider estimating the \(Q_\pi(s, a)\) function. A policy might choose a certain action for a given state. However, to get all the Q-values, the policy must account for exploring all actions at different states to get a good estimate. The exploration probability is captured by the <strong>epsilon-greedy class of algorithms</strong>.</p> <h2 id="temporal-difference-tdlambda-algorithms">Temporal Difference TD(\(\lambda\)) Algorithms</h2> <p>The idea of this class of algorithms is to improve the policy as we keep exploring the environment more.</p> <p>The first variant of these algorithms is TD(0) - We sample state transition from the trajectory (one from each) and update the existing value function based on the action and reward obtained. Formally, given \(V_t(.)\) and a sample from the trajectory \((s_t, a_t, r_t, s_{t + 1})\), how do we obtain \(V_{t + 1}(s_t)\)?</p> <p>From the Bellman’s equations, we have</p> \[V_\pi(s) = \mathbb E_{a \sim \pi(s)} [R(s, a) + \gamma \mathbb E_{s’ \in P(s, a)} V_\pi(s’)]\] <p>So, can we do</p> \[V_{t + 1}(s_t) \gets r_t + \gamma V_t(s_{t + 1})\] <p>This equation omits our previous estimate \(V_t(s_t)\). How do we use it? We can do a sort of an averaging or gradient descent</p> \[\begin{align*} V_{t + 1}(s_t) &amp;\gets V_t(s_t) + \alpha_{s_t} \delta_t \\ &amp;\delta_t = r_t + \gamma V_t(s_{t + 1}) - V_t(s_t) \end{align*}\] <p>In essence, we are averaging over the previous visits in the trajectory but with slightly different update rules.</p> <p>How do we choose the \(\alpha\)’s? For convergence purposes, we require \(\sum_{t = 0}^{\infty} \alpha(t) \to \infty\) and \(\sum_{t = 0}^\infty \alpha^2(t) &lt; \infty\). These parameters are important for controlling the bias and variance of the estimators as we will discuss later. Possible learning schedules include \(\alpha(t) = \frac{1}{t^{0.5 + \epsilon}}\) with \(0 &lt; \epsilon \leq 0.5\) and $$\alpha(t) = \frac{1}{\sqrt{t}\log t}.</p> <p>With these learning schedules, you can prove the convergence thinking of the process as a contraction. It converges to the Bellman’s equation for some new value \(\gamma\).</p> <p>To compare various statistical approaches (Monte Carlo methods), we need to compare the bias, variance, convergence and sampling complexity into account. To improve on some of these criteria, we have the general TD(\(\lambda\)) algorithms.</p> <p>Instead of considering a single transition, the general algorithm considers more transitions to update the value function.</p> \[V^k_t = r_t + \gamma r_{t + 1} + \dots + \gamma^{k} r_{t + k} +\gamma^{k + 1} V_t{s_t + k}\] <p>What is the advantage of this approach? We slowed the updates, which seemingly increases the bias but may decrease the variance. Another way to understand TD(\(\lambda\)) is to think of it as a combination of Temporal Difference and Monte Carlo learning. It is an average of \(k\)-step returns. Kind of <em>truncated Monte Carlo method</em>.</p> <p>What’s more? We can generalize the above mention equation a bit more. For different states, we can consider different \(k\)’s, and take the average of them.</p> \[V_t^\lambda(s_t) = (1 - \lambda)\sum_{i = 0}^\infty \lambda^i V_t^k(s_t)\] <p>where the update algorithm is</p> \[V_{t + 1} \gets V_t(s_t) + \alpha_t (V_t^{\lambda} (s_t) - V_t(s_t))\] <p>It reduces the variance because we are considering a a geometric weighted mean. Since longer windows \(k\) have higher variance, we reduce their weight in the average with \(lambda\).</p> <p>In practice, when we approach a new state, we take our current estimate of \(V_t\), update all our previous calculations to recompute the new value function. The convergence proofs for these algorithms is not mathematically rigorous. It is a good area of research to find better proofs or better yet, more efficient algorithms.</p> <p>How do we execute these algorithms in practice? Theoretically, setting the upper limit of \(j\) to \(\infty\) is the best estimate. However, \(\lambda\) acts like a discount factor, and if the rewards are sparse (towards the end of the episode), then the convergence would take a long time - <em>episodic</em> algorithms.</p> <p>In essence, this version of the algorithm requires many forward executions of the simulation. In many cases, since this is infeasible in practice, developers have started using a <em>backwards version</em> of the algorithm.</p> <p>We define the eligibility trace of a state \(e_t(s) = \gamma \lambda e_{t - 1}(s) + 1\). The update equation becomes</p> \[V_{t + 1}(s) \to V_t(s) + \alpha \delta_t e_t(s)\] <p>where \(\delta_T = R_t + \gamma V_{t + 1} (S_{t + 1}) - V_{t - 1}(S_t)\).</p> <p>These equations are almost same as the previous algorithm but with better practice implementation. (When you expand the formulae, and interchange the summations, this is what you get).</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[A deep dive of the basic RL theory and how we used them in modern ML systems.]]></summary></entry><entry><title type="html">AI Agents</title><link href="https://sudhansh6.github.io/blog/ai-agents/" rel="alternate" type="text/html" title="AI Agents"/><published>2025-01-06T00:00:00+00:00</published><updated>2025-01-06T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/ai-agents</id><content type="html" xml:base="https://sudhansh6.github.io/blog/ai-agents/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>The content on this article is based on the <a href="https://github.com/pearls-lab/ai-agents-course">course</a> by <a href="https://prithvirajva.com">Prof. Prithviraj</a> at UC San Diego. This <a href="https://www.kaggle.com/whitepaper-agents">whitepaper</a> about AI Agents by Google is also a good read.</p> <h2 id="what-is-an-agent">What is an agent?</h2> <p>Agent is an entity with <em>agency</em>. <a href="https://minecraft.wiki/w/Agent">A Minecraft agent?</a>. Agents see applications within the workspaces in the form of workflow automations, household or commercial robotics, software development and personal assistants. Generally, the theme is that <em>agents</em> take actions.</p> <p>Historically, the use of agents started in the early 1900s in the field of control theory. They were used for dynamic control of flight systems, and in 1940s it expanded to flight guidance, etc. By the 1950s the concepts of MDPs and dynamic programming were being expanded to many use cases. Surprisingly, one of the first natural language chatbots, Eliza, was created as a psychotherapist simulator in the 1960s! Finally, reinforcement learning became a field of study in the 1990s for sequential decision making.</p> <h2 id="sequential-decision-making">Sequential Decision Making</h2> <p>These tasks are different from other ML problems like classification. A model that has an accuracy of 99% at each step, has a cumulative accuracy of ~30% after 120 steps!</p> <p>These problems are formalized as a Markov Decision Process - an <strong>agent</strong> performs <strong>actions</strong> in an <strong>environment</strong>, and in turn receives <strong>rewards</strong> as feedback. These configurations are distinguished as <strong>states</strong>, and the whole process can be seen as sequential decision making.</p> <p>The core components of an agent, often agreed on, are</p> <ul> <li><strong>Grounding</strong> - Language is anchored to <em>concepts</em> in the world. Language can be grounded to different forms of information systems - images, actions and cultural norms. <ul> <li>Agency (ability to act) - At each state, an agent needs to have multiple choices to act. <em>If an agent has to select what tools to use but there’s always only one tool, is that agency?</em> The action space has to be well-defined to look for agency. Although there is a single tool call, different parameters for the tool call can probably be considered as different actions. Actions can be defined as something the agent does and changes the environment. The distinction between an agent and environment is not very clear in many cases. Although, our approximations mostly serve us well.</li> <li>Planning (Long horizon)</li> <li>Memory - <ul> <li>Short-term - What is the relevant information around the agent that it needs to use to act now</li> <li>Long term - What information has the agent already gathered that it can retrieve to take an action</li> </ul> </li> <li>Learning (from feedback) - Doesn’t necessarily always mean <em>backpropagation</em>.</li> </ul> </li> <li><strong>Additional</strong> - <ul> <li>Embodiment (physically acting in the real-world). <em>Embodied hypothesis</em> - embodiment is necessary for AGI.</li> <li>Communication - Can the agent communicate its intentions to other agents. Very necessary pre-requisite for multi-agent scenarios.</li> <li>World Modeling - Given the state of the world and an actions, predict the next state of the world. Is <a href="https://deepmind.google/technologies/veo/veo-2/">Veo</a>/<a href="https://sora.com">Sora</a> a world model? It is an attempt for world model since they have no verifiability. <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie</a> is another such attempt. So is <a href="https://genesis-embodied-ai.github.io">Genesis</a> - this is much better if it works.</li> <li>Multi-modality - The clean text on the internet is only a few terabytes, and our models have consumed it (took use 2 decades though). YouTube has 4.3 Petabytes of new videos a day. CERN generates 1 Petabyte a day (modalities outside vision and language). Some people believe this form of scaling is the way to go. There are more distinctions -</li> </ul> </li> </ul> <table> <thead> <tr> <th>Model</th> <th>AI System</th> <th>Agent</th> </tr> </thead> <tbody> <tr> <td>GPT-4</td> <td>ChatGPT</td> <td>ChatGPT computer use</td> </tr> <tr> <td>Forward passes of a neural net</td> <td>Mixing models together</td> <td>Has agency</td> </tr> </tbody> </table> <p>It is important to remember that not every use case needs an agent and most use cases just need models or AI systems. <em>Occam’s razor</em>.</p> <h1 id="simulated-environments-and-reality">Simulated Environments and Reality</h1> <p>Why do we need simulations? Most tasks have many ways of completing them. There is no notion of <em>global</em> optimal solutions ahead of time but usually known once the task is complete.</p> <p>The agent needs to explore to find many solutions to compare and see what is the most efficient. However, exploration in the read world is expensive - wear and tear of robots, excessive compute, danger to humans, etc.</p> <p>Simulations offer an easy solution to these problems. Assign a set of rules, and let a world emerge. One of the early examples of this is <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Conway’s Game of Life</a> which theorized that complicated behaviors can emerge by just a few rules.</p> <p>From an MDP perspective, a simulation contains \(&lt;S, A, T&gt;\) where</p> <ul> <li>\(S\) is the set of all states. It consists of propositions that are true/false. Example: You are in a house, door is open, knife in drawer</li> <li>\(A\) is the set of all actions. Example: Take knife from drawer, walk through door</li> <li>\(T\) is the transition matrix - (You are in the house, you walk out of the door) -&gt; You are outside the house.</li> </ul> <p>A simulation need not have an explicit reward.</p> <h2 id="sim2real-transfer">Sim2Real Transfer</h2> <p>The ability of an agent trained in simulation transfer to reality is dependent on how good the model extrapolates out of distribution. With the current stage of agents, the simulation is made as close to reality as possible to reduce the Sim2Real gap.</p> <p>How do we measure closeness to reality? The tasks in the real world have different types of complexities -</p> <ol> <li>Cognitive complexity - Problems that requires long chains of <em>reasoning</em> - puzzles, math problems or moral dilemmas</li> <li>Perceptive complexity - Requires high levels of vision and/or precise motor skills - bird watching, threading a needle, Where’s Waldo</li> </ol> <p>Examples of simulations -</p> <ol> <li>Grid world - low cognitive and almost zero perceptive. However, this idea can arbitrarily scale to test algorithms for their generalization potential in controllable settings.</li> <li>Atari - low perceptive, medium cognitive. Atari games became very popular in 2013, when Deepmind released their <a href="https://arxiv.org/pdf/1312.5602">Deep Q-Net</a> paper that achieved human level skills on these games.</li> <li><a href="https://en.wikipedia.org/wiki/Zork">Zork</a>, <a href="https://www.nethack.org">NetHack</a> - low perceptive, high cognitive. These are configurations or worlds that you purely interact with text. The worlds are actually so complex that there is no agent that is able to finish the challenge!</li> <li><a href="https://cs.stanford.edu/people/jcjohns/clevr/">Clevr simulation</a> - medium perceptive, low cognitive - This simulation generates images procedurally with certain set of objects and has reasoning questions for each image.</li> <li><a href="https://ai2thor.allenai.org">AI2 THOR</a> - medium perceptive, medium cognitive. Worlds with ego-centric views for robotics manipulation and navigation simulations</li> <li><a href="https://arxiv.org/pdf/2407.18901">AppWorld</a> - medium perceptive, medium cognitive. A bunch of different apps that you would generally use in daily life. The agents can access apps, and the simulation also has human simulators. This simulation is one that is closest to reality in the discussed so far!</li> <li><a href="https://www.minecraft.net/en-us">Minecraft</a> - medium perceptive, high cognitive. A voxel based open-world game that lets players take actions similar to early-age humans.</li> <li><a href="https://mujoco.org">Mujoco</a> - high perceptive, low cognitive. It is a free and open source physics engine to aid the development of robotics.</li> <li><a href="https://ai.meta.com/research/publications/habitat-a-platform-for-embodied-ai-research/">Habitat</a> - high perceptive, medium cognitive. A platform for research in embodied AI that contains indoor-world ego-centric views similar to AI2 THOR, but with much better graphics. They have recently added sound in the environment too!</li> <li> <p>High perceptive, high cognitive - Real world, and whoever gets this simulation right, wins the race to AGI. It requires people to sit down and enumerate all kinds of rules. Game Engines like Unreal and Unity are incredibly complex, and are the closest we’ve gotten.</p> <p>Some researchers try to “learn” the simulations from real-world demonstrations.</p> </li> </ol> <p>In each of these simulators, think of the complexity and reward sparsity in the environment. It is easy to build a simulator that gives rewards at a goal state than the one that gives a reward for each action. There are some open-lines of research in this domain -</p> <ol> <li>Which dimensions of complexity transfer more easily? Curriculum learning</li> <li>Can you train on lower complexity and switch to a higher complexity?</li> <li>Can we learn the world model holy grail?</li> </ol> <h2 id="how-to-make-simulations">How to make simulations?</h2> <p>As we’ve seen, simulations can range from games to real-world replications with physics involved. Most simulations are not designed keeping AI in mind. However, with the current state of AI, this is an important factor to keep in mind.</p> <p>Classical environments like in Zork/AI2 Thor/Mujoco have something known as <strong>PDDLs</strong>. Some simulations are built through AI, like <em>AI Dungeon</em> that spins up worlds for role-play games.</p> <h3 id="planning-domain-definition-language-pddl">Planning Domain Definition Language (PDDL)</h3> <p>Standard encoding for classic planning tasks. Many specific languages for creating simulations have similarities with PDDL.</p> <p>A PDDL Task consists of the following</p> <ul> <li>Objects - things in the world that interest us</li> <li>Predicates - Properties of objects that we are interested in, can be true or false</li> <li>Initial state - The state of the world that we start in</li> <li>Goal specification - Things that we want to be true</li> <li>Actions/Operators - Ways of changing the state of the world.</li> </ul> <p>These are split across two files - domain and problem <code class="language-plaintext highlighter-rouge">.pddl</code> files.</p> <p>Classic symbolic planners read PDDLs and give possible solutions. Checkout the <a href="https://planning.wiki/ref/planners/atoz">Planning.wiki</a>. In many cases these planners are used over reinforcement learning due to lack of algorithmic guarantees.</p> <p>There were other attempts</p> <h1 id="search-for-planning-in-simulations">Search for Planning in Simulations</h1> <h1 id="reinforcement-learning-abridged">Reinforcement Learning (Abridged)</h1> <p>We have been building chronologically, and next in line is basic RL.</p> <h2 id="terminology">Terminology</h2> <ul> <li> <p>A <strong>policy</strong> is a function that defines an agent’s behavior in the environment. Finding the optimal policy is known as the <em>control</em> problem.</p> <p>Formally, a policy is a distribution over actions for a given state.</p> \[\pi(a \vert s) = P(A_t = a \vert S_t = s)\] <p>Due to the Markov property, the policy depends only on the current state and not history. In cases where the history is needed, the state is modified to embed the history to evade this dependence.</p> <p>For a given MDP, an optimal policy always exists that achieves the maximum expected reward at every state. (This is proved using the compactness properties of the state-action space using the Banach’s fixed point theorem - refer to <a href="/blog/cse291g">these notes</a> for more details).</p> </li> <li> <p>The <strong>value function</strong> determines how good each state or actions is. Finding the optimal value functions is known as the <em>prediction</em> problem.</p> <p>There are two functions that capture the value of the current state/action of the agent</p> <ol> <li>\(V_\pi(s) = E_\pi[R_{t + 1} + \gamma V_\pi(S_{t + 1} \vert S_t = s]\) - The expected reward obtained by a policy \(\pi\) starting from a given state \(s\).</li> <li>\(Q_\pi(s, a) = E_\pi[R_{t + 1} + \gamma Q_\pi(S_{t + 1}, A_{t + 1}) \vert S_t = s, A_t = a]\) - The expected reward for a given state \(s\) upon taking a certain action \(a\). These two functions are closely related to each other, and knowing one determines the other. In general, these functions (matrices, in discrete spaces) do not have a closed form solution.</li> </ol> </li> <li> <p>A <strong>model</strong> is the agent’s representation of the environment</p> </li> </ul> <p>RL algorithms are classified under various categories</p> <ul> <li>Model free and Model-based</li> <li>Off-policy and on-policy</li> </ul> <h1 id="its-all-dynamic-programming">It’s all Dynamic Programming?</h1> <p>The core theory of RL, the properties of the Bellman equation (refer to these notes for more details) and the recursive nature of the value functions, ties to dynamic programming. This insight helps us design algorithms to solve the problems in RL (Prediction and Control).</p> <p>We ideally want the solution to the control problem since we want to define the optimal behavior of an agent in the environment. To do so, the prediction problem is a pre-cursor that needs to be solved.</p> <h2 id="policy-evaluation">Policy Evaluation</h2> <p>The prediction problem involves calculating the rewards obtained by a given policy \(\pi\). The expectation can be written as</p> \[\begin{align*} V_\pi(s) &amp;= \sum_{a \in A} \pi(a \vert s) Q_\pi(s, a) \\ &amp;= \sum_{a \in A} \pi(a \vert s) (R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’)V_\pi(s’)) \end{align*}\] <p>where \(T\) is the state-transition function of the MDP.</p> <p>Ideally, we want to find the optimal policy that reaches the maximum value at every state.</p> \[\begin{align*} V*(s) &amp;= \max_\pi V_\pi(s) \\ Q*(s, a) &amp;= \max_\pi Q_\pi(s, a) \\ \pi*(s) &amp;= \arg\max_\pi Q_\pi(s, a) \end{align*}\] <p>These can be determined (iteratively) from the Bellman’s Optimality equations -</p> \[\begin{align*} Q*(s, a) &amp;= R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’) V*(s’) &amp;= R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’) \max_{a’} Q*(s’, a’) \end{align*}\] <blockquote> <p>Note the subtlety here. Although Bellman’s optimality equations aren’t seemingly much different from the Bellman’s equations, there is a very strong claim the optimality equations make - they claim that the existence a policy that gets the maximum possible value at every state. The existence is not a trivial claim and it is the proof I referred to in the terminology. Furthermore, it turns out that a deterministic policy is just as good as a stochastic one.</p> </blockquote> <p>So how do we find these optimal values?</p> <h2 id="policy-iteration">Policy Iteration</h2> <p>Given a policy \(\pi\), we iteratively update its actions at each state to improve its value. Remember that we can <em>evaluate a policy</em> to get its value function.</p> <p>At each state, if there is an action \(a\) such that \(Q_\pi(s, a) &gt; Q_\pi(s, \pi(s))\), then the policy is <em>strictly improved</em> by updating \(\pi(s) \gets a\). In each iteration, we update the actions this way across all the states, and repeat this until the policy does not change.</p> <p>How many iterations should we repeat this for? Because the number of policies is finite (bounded by \(O(\vert A \vert^{\vert S\vert})\), we are guaranteed to reach the optimum. Each iteration costs \(O(\vert S\vert^2 \vert A\vert + \vert S\vert^3)\). Although these numbers seem big, in practice, this algorithm typically takes only a few iterations.</p> <h2 id="value-iteration">Value Iteration</h2> <p>It is similar to the policy iteration algorithm, but focuses on recursively improving the value function instead.</p> <p>We start out with a random value function, and at each state, we choose the action that gives the maximum value (with the currently set values across the states). Once the values are updated across all the states, the process is repeated until the improvement is below a threshold. At the end of the iterations, we can extract the policy from the value function deterministically (the algorithm itself is a hint to this).</p> <p>Although this seems very similar to the policy iteration algorithm, there are some key differences. We do need to reach the optimal value function to get the optimal policy - if it is close enough, we can get the optimal policy. Also, the iterations <em>asymptotically reach</em> the optimal policy and there is no upper bound to this.</p> <h2 id="limitations">Limitations</h2> <p>Although dynamic programming approaches have theoretical guarantees, they are not widely used in practice. Why?</p> <p>The curse of dimensionality. These algorithms are have very limited applicability in practice. Many environments have a very large set of states and actions. In some cases, these could be continuous as well. The iteration algorithms are computationally infeasible in such cases.</p> <h1 id="model-free-rl">Model-free RL</h1> <p>Since we cannot look at every state action combination, we resort to approximations. We explore the world (say, with Monte Carlo sampling) and build experiences to heuristically guide the policy.</p> <p>The goal is to optimize the value of an unknown MDP through experience based learning. Many real world problems are better suited to be solved by RL techniques over dynamic programming based approaches (the iterative algorithms).</p> <h2 id="monte-carlo-control">Monte Carlo Control</h2> <p>It suggests greedy policy improvements over \(V(s)\) requires a model of the MDP. However, improvement over \(Q(s, a)\) is a model-free method! (This was the importance of defining both \(V_\pi(s)\) and \(Q_\pi(s, a)\)).</p> <p>The \(Q\) function can be learned from experiences. These concepts are the foundation concepts for deep RL!</p> <p>This approach can be thought of as a hybrid approach between policy and value iteration. In these exploration/sampling based techniques, it is important to gather data about the model through exploration and not be greedy. This forms the basis of <strong>epsilon-greedy</strong> algorithms.</p> <h2 id="epsilon-greedy-exploration">\(\epsilon\)-greedy exploration</h2> <p>At each state, with a certain probability we choose to exploit (greedily take the action based on the optimal policy we developed so far) or explore (take a random action to sample more outcomes)</p> \[\pi(a \vert s) = \begin{cases} \epsilon/m + 1 - \epsilon &amp; a* = \arg\max_{a \in A} Q(s, a) \\ \epsilon/m \end{cases}\] <p>This class of algorithms also has some theory but it is limited. This core trade-off between exploration/exploitation is still a core element in the modern RL algorithms.</p> <h2 id="temporal-difference">Temporal Difference</h2> <p>In Monte Carlo methods, we update the value function from a complete episode, and so we use the actual accurate discounted return of the episode. However, with TD learning, we update the value function from a step, and we find an estimated return called <strong>TD target</strong> - a bootstrapping method similar to DP.</p> \[\begin{align*} \text{Monte Carlo }&amp;: V(S_t) \gets V(S_t) + \alpha[G_t - V(S_t)] \\ \text{TD Learning }&amp;: V(S_t) \gets V(S_t) + \alpha[R_{t + 1} + \gamma V(S_{t + 1}) - V(S_t)] \end{align*}\] <p>The high-level view of MCTS is <img src="/assets/img/AIAgents/17388852350762.jpg" alt=""/></p> <h2 id="alphago-a-case-study">AlphaGo: A case study</h2> <p>The game has a large number of states. The rewards we use are \(\plusminus 1\) based on the player won. We define policies for both the players and <em>train</em> the policies with self-play.</p> <p>Making use of the symmetry of the game, we can use the episodes of the opponent player seen before to train the policy.</p> <p>AlphaGo used these exact MC methods with neural networks (CNNs, which is super useful for Go) to learn the probabilities and outcome rewards. It was trained with a lot of human games to train initial value networks. The developers also hand-crafted features to represent knowledge in the game.</p> <p>AlphaZero, an extension of this, relaxed the constraint of requiring a lot of human data and scaled.</p> <p>All these algorithms have been model-free. That is, we cannot estimate the consequences of our actions in the environment, and are simply learning based off our experiences. We are not learning anything about the dynamics of the environment.</p> <p>On the flip side, if we know the model of the world, can we do better? So given a <em>world model</em>, how do we use it?</p> <h1 id="model-based-rl">Model-based RL</h1> <p>We learn the model of the world from experiences, and then plan a value function (and/or policy) from the model. What is a model? A representation of an MDP \((S,A,T,R, \gamma)\), and we try to approximate \(T, R\).</p> <p><em>Assumption.</em> A key assumption that developers make is that the state transitions and rewards are conditionally independent.</p> <p>We have the experience \(S_1, A_1, R_2, \dots, S_T\), and we just train a model in a supervised problem setting \(S_i, A_i \to R_{i + 1}, S_{i + 1}\). Learning \(R\) is a regression problem and learning \(P\) is</p> <p>How do we use the learned model? Since the learned model has errors and uncertainty, training a policy would take a long time. It is like we are learning the rules of Go whereas previously we knew the rules, and were just trying to win.</p> <p>The advantages of model-based RL is the it can use all the (self, un) supervised learning tricks to learn from large scale data and can reason about uncertainty. The disadvantage is that we need to first build a model, and then estimate a value from the estimated model - introduced two sources of error.</p> <h2 id="introduction-to-transformers-and-language">Introduction to Transformers and Language</h2> <p>A function approximator (neural networks) needs to be good for the task. For example, CNNs were great for Atari and Go but they did not work well for language.</p> <p>What are the right inductive biases for language then? The <em>attention mechanism</em> was again borrowed from some cognitive functions of the brain. An attention matrix tries to find the <em>alignment</em> of elements within a sequence. Before the scaled dot product attention, there were multiple variants of the formulation -</p> <p><img src="/assets/img/AIAgents/17393174376471.jpg" alt=""/> Self-attention was first proposed by Cheng et al. in 2016 (originally called intra-attention) that brought in a sense of understanding in models - finding relations between elements of the same sequence.</p> <p>In 2015, there was a concept of global vs local attention introduced in the form of windowed-attention. This concept is being used widely in vision and language based systems.</p> <p>Let us discuss the intuition for <em>scaled dot product attention</em> -</p> <ol> <li>Query: What are the things I am looking for?</li> <li>Key: What are the things I have?</li> <li>Value: What are the things I will communicate?</li> </ol> <p>So essentially, the queries attend to the keys to find the aligned ones, and the corresponding values are returned. The multi-head part of the architecture is simply the multiple ways of learning these alignments between the query and key sequences. The alternate way of thinking about it is, using an ensemble representation to provide robustness in our predictions. Furthermore, to add a notion of position in the sequence, a positional encoding is added to the sequence elements.</p> <p>To apply all these mechanisms to language, we need a way to represent language as sequences - this step is known as <em>tokenization</em>. Word-level is too discrete (causes issues for questions like “how many r’s are in ‘strawberry’?”) and character-level is too redundant (often causes issues for questions like “Is 9.9 greater than 9.11?”. The current standard is a sub-word (Tiktoken Byte Pair Encoding BPE) that is learned from a representative subset of data.</p> <p>The other ideas are to use <em>heuristics</em> for numerical values like new tokenizers and right-to-left processing, etc. The amount of research done in tokenization is underwhelming as compared to the rest of the transformer stack. People have also come up with <a href="https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/">byte-level transformers</a> and <a href="https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/">LCMs</a>. Since bytes form too long sequences, people tried hierarchical representations that kind of work. However, there are many training issues with this and byte-encoding issues (PNG and Unicode are weird encodings). They were only able to train a 7B model with the byte-level learning.</p> <p>Language modeling is another long-standing problem that is a fundamental cornerstone to make these networks learn. The two popular mechanisms used are</p> <ul> <li>Infill (like in BERT): The 47th US President is [??] Trump</li> <li>Next Token prediction (like in GPT): The sun rises in the ??</li> </ul> <p>RNNs had an encoder-decoder architecture wherein the initial sequence is <em>encoded</em> into a vector (latent space) and it is then <em>decoded</em> into another sequence. This terminology has been carried through and is still used in transformers. The original paper started out as an encoder-decoder architecture, but these models are not used much anymore.</p> <ol> <li>Encoder only models (auto encoding models) - They are used for Sentence classification, NER, extractive question-answering and masked language modeling. Examples are BERT, RoBERTa, distilBERT.</li> <li>Decoder only models (auto regressive models) - They are used for text generation and causal language modeling. Examples are GPT-2, GPT Nero, GPT-3</li> </ol> <p>Check <a href="https://github.com/karpathy/nanoGPT">this repository by Andrej Karpathy</a> for a clean implementation of GPT.</p> <h2 id="how-does-all-this-relate-to-rl">How does all this relate to RL?</h2> <p>We now have a neural net architecture that works well on language. How about using this for MDPs with language-based state-action spaces.</p> <h2 id="paper-discussion-2xdqn-network">[Paper Discussion] 2xDQN network</h2> <p>As we have seen the Q-learning update</p> \[Q(S_t, a_t) \gets Q(s_t, a_t) + \alpha[r(s_t, a_t) + \gamma \max_{a’} Q(s’, a’) - Q(s, a)]\] <p>The problems with Q-learning are</p> <ol> <li>Curse of dimensionality - High dimensions and continuous state spaces do not work</li> <li>Slow convergence</li> <li>Inefficient generalization</li> <li>Exploration-exploitation trade-off</li> </ol> <p>This led to <strong>Deep Q-learning</strong>. The idea is to replace Q0tables with Neural networks to approximate Q-functions.</p> <ul> <li><em>*Experience replay</em> stores the agent’s past experiences in a replay buffer. The algorithm randomly samples episodes for training, to break the correlation between consecutive experiences.</li> <li><strong>Target Networks</strong> - Uses a separate, slower updating networking to compute target Q-values. Reduces instability caused by changing Q-values too frequently during learning.</li> </ul> <p>The problems with this are</p> <ul> <li>Overestimation of Q-values - due to the mathematical formulation the Q-values are over-estimated (has been proved mathematically)</li> <li>Sample inefficient - uses large amount of e</li> <li>Catastrophic forgetting - may forget previously learned experiences when trained on new experiences</li> </ul> <p>This led to <strong>Double Deep Q-learning</strong>. They decouple the action evaluation and action selection networks. The online network is used for action selection whereas the action value function evaluates the action. Previously, to stabilize the learning in Q-learning, the updates to action selection network were less frequent leading to choosing sub-optimal actions in some cases.</p> <p>The problems with this approach are</p> <ol> <li>Might not completely eliminate the overestimation bias - Since action selection and value estimation are not completely decoupled.</li> <li>Leads to slower convergence in some environments - where overestimation is helpful (high exploration settings)</li> <li>Sensitive to hyper parameters</li> <li>Still susceptible to some core DQN issues - Sparse rewards, discrete action space, sample inefficient.</li> </ol> <p>After this, newer methods were proposed such as <em>Rainbow Deep Q-learning</em> - Combination of previous improvements - Prioritized experience replay, uses a dueling network (two explicit specialized networks) for evaluation and selection, Stabilized learning via distributional RL, and Multi-step updates.</p> <h1 id="rl-agents-for-llms">RL Agents for LLMs</h1> <p>The typical LLM training pipeline is as follows</p> <ol> <li>SfM fine-tuning - Depends heavily on human expert demo data, and requires a lot of effort. This step can be compared to behavior cloning in RL</li> <li>Some pipelines also talk about pre-training that can be thought of as weight initialization. This step is not used so much anymore.</li> <li> <p>After this stage, <em>learning based on feedback</em>, has become a standard step. It comes in many forms, RLHF, RLAIF, etc. The primary approach, Reinforcement Learning with Human Feedback essentially rates different texts generated by the AI, and uses this as a reward model and improves the model considering it as a policy.</p> <p>This step is cheaper than the other steps, so companies are pushing towards improving this. However, in practice it does not seem to work without pre-training.</p> <p>This step also involves training a human-proxy reward function. In whole, this is known as post-training development.</p> </li> </ol> <h2 id="rlhf">RLHF</h2> <p>As we mentioned, the goal is to collect preference feedback and train a reward model. It is a kind of a personalized learning to correct the text generated by the model. One thing to note is that the rewards are given towards the end and there are no intermediate rewards.</p> <p>For the policy training itself, we have studied value and policy based algorithms. If the value of the MDP is estimated, the policy can be determined implicitly (argmax or epsilon-greedy). However, values and rewards of partial sentences are more difficult to estimate.</p> <p>Due to these limitations, researchers have gravitated towards policy-based RL. It has better convergence properties and is effective in high-dimensional or continuous actions spaces. However, they can converge to a local optimal rather than a global optima.</p> <p>“”” I was super sleepy after this, please update after watching the recording “””</p> <p>Now, for each step, we make a decision within a one-step MDP</p> <ul> <li>Expectation equation <ul> <li>SFT step is important</li> </ul> </li> </ul> <p><strong>Policy Gradient Theorem</strong> - For any differential blue policy $$\pi_\theta (s, a)</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Everyone is talking about agents. But what is an agent? Is it just a buzzword being thrown around? This article talks deeply about this issue along with the technical ideas associated.]]></summary></entry><entry><title type="html">Statistical Natural Language Processing</title><link href="https://sudhansh6.github.io/blog/Statistical-NLP/" rel="alternate" type="text/html" title="Statistical Natural Language Processing"/><published>2024-11-03T00:00:00+00:00</published><updated>2024-11-03T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Statistical-NLP</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Statistical-NLP/"><![CDATA[<h1 id="introduction">Introduction</h1> <p><strong>Natural Language Processing (NLP)</strong> is the study of computer systems that take as input or produce as output natural language - languages created by humans. The goal is to give machines the power to understand not just words, but entire sentences, paragraphs and documents.</p> <p>It it worth keeping in mind that the notion of “understanding” is contrived. There is no clear definition - when we claim Large Language Models (LLMs) understand our language, we really don’t know if it is understanding.</p> <p>NLP develops systems for</p> <ol> <li> <p>Analysis of language (NL to some useful output) - text classification, question answering, etc.</p> </li> <li> <p>Generation of language (NL to NL; Image to NL, etc) - summarization, image captioning, machine translation</p> </li> <li> <p>Representation of language (NL to some representation) - learning word embeddings</p> </li> </ol> <p>In the part, systems were task-specific, now we have more general purpose systems capable of all of the above and more.</p> <h3 id="origins">Origins</h3> <p>Back in 1949, Warren Weaver, a wartime US mathematician and scientist, brought the idea of the first computer based application related to natural language - machine translation (MT). He considered the problem of translation as a problem in cryptography. We still use the notation of encoder and decoder in the present techniques. He developed a rule-based program to convert Russian to English.</p> <p>Over time, it became obvious that human language are <strong>ambiguous</strong> (unlike programming and other formal languages) and they are <strong>rich</strong> - any meaning may be expressed in many ways. Human language interpretation depends on the real world, common sense and <strong>contextual knowledge</strong>. Furthermore, there is <strong>linguistic diversity</strong> across genres, styles, and so more.</p> <p>In 1957, Chomsky proposed a <em>generative grammar</em>, a rule based system of syntactic structures, brought insight into how linguistics can help MT. Since the results were not satisfactory, funding was cut-off and then came the winter of AI in 1966.</p> <p>In 1971, Terry Winograd’s MIT thesis has motivated the notion of <strong>grounded language understanding</strong>. In late 80’s, statistical techniques revolutionized NLP. They used early ML algorithms - decision trees with rule based systems.</p> <p>From 90’s to early 2000s, methods like logistic regression, Support Vector Machines (SVM), Hidden Markov Models (HMMs), Conditional Random Fields (CRFs), etc were introduced. Moreover, papers introduced feature engineering for specific tasks - POS tagging, Named Entity Recognition, Parsing, etc.</p> <p>The main language models during this time were n-grams with smoothing.</p> <p><img src="/assets/2024-11-03-Statistical-NLP/2024-11-09-15-15-16-image.png" alt=""/></p> <h3 id="dawn-of-deep-learning-era">Dawn of Deep Learning Era</h3> <p>Bengio et al. in 2003 proposed first neural language models with 1-hidden layer feed-forward neural network. It introduced the notion of <strong>word embeddings</strong> with a real-valued feature vector in \(\mathbb R^d\). In 2008, a new paper proposed training neural network along with a word embedding matrix jointly. There was no need of feature engineering anymore.</p> <p>In 2013, Mikolov et al. introduced arguably the most popular word embedding model - <strong>Word2Vec</strong> - they got rid of hidden layer in the model as well.</p> <p>From 2013 to 2018, Recurrent Neural Networks (RNNs; Elman 1990), Long-Short Term Memory Models (LSTMs), Convolution Neural Networks (CNNs), recursive neural networks (Socher et al.), etc were used for NLP. There were feats of Architectural engineering as well - combining RNNS with CRFs for sequence labeling, CNNs for text classification, summarization with pointer-generators RNNs (2017). <em>In present date, there are little to no changes in the model architecture.</em></p> <p>In 2014, Google introduced <strong>Sequence-to-sequence</strong> learning, a general end-to-end approach for mapping one sequence to another using a single neural network (encoder-decoder architecture). This proved very important for NLP tasks going forward. It was a fundamental shift in paradigm to perform tasks like translation with a single model instead of complicated designed models.</p> <p>Then, in 2015 came the notion of <strong>Attention</strong> - to reduce the bottleneck of sequence-to-sequence models that was compressing the entire content of source sequence into a fixed-size vector. This notion still required sequential processing with RNNs. Finally in 2017, <strong>Transformers</strong> were proposed which eschewed recurrence and relied entirely on attention mechanisms. The parallel nature of the model enabled fast computations.</p> <p>In 2020, people realized instead of just pre-training the word-embedding layer they could just pre-train the whole network and add a layer-head in the end if required for other specialized tasks. Pre-trained LMs then acted as an initialization for fine-tuning on downstream tasks - ELMo (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), GPT (Radford et al., 2018), and BERT (Devlin et al,. 2019). The impact of pre-training all the layers was significant.</p> <p><img src="/assets/2024-11-03-Statistical-NLP/2024-11-09-15-15-02-image.png" alt=""/></p> <h3 id="present-date">Present Date</h3> <p>NLP systems are increasing used in everyday life - in the form of chatbots and other AI assistants. Consider ChatGPT - fastest growing consumer computing applications in history.</p> <p>The key advantage os language models is that there is no need of annotation - nearly unlimited training data*. People also realized that using larger and larger models gives higher performance as data scales. The final ingredient to achieve all this is compute - GPU gave a huge advantage over CPU to train these networks. These three key ingredient - hardware scalability (GPUs), Model scalability (Transformer with many deep layers) and Data scalability (Large datasets with lots of text) enabled the success of GPT models.</p> <p><img src="/assets/2024-11-03-Statistical-NLP/2024-11-09-15-14-43-image.png" alt=""/></p> <p>Realizing the power of scale, GPT1 was trained with a few million parameters and now GPT4 has a few hundred billion parameters. In 2022, researchers at OpenAI realized some tasks were only possible at larger scales - scaling LMs leads to emergent abilities. Another paper (one of the best papers in NeurIPS) questioned this asking whether this finding is just an artifact of how we designed our metrics. The metrics used in the OpenAI paper did not allow continuous rewards which caused the sudden jump in performance after a certain point in scale. With a more continuous metric, the gains due to scale increase continuously without sudden jumps.</p> <p>Then came the question of prompting - how do we talk to these LMs? <strong>Prompt</strong> is a cue given to the pre-trained LM to allow it to better understand people’s questions (Best paper in NeurIPS 2020).</p> <p>GPT3.5 introduced the notion of <strong>Instruction Tuning</strong> - collect examples of (instruction, output) pairs across many tasks and then evaluate on unseen tasks. Furthermore, the output of LMs can be tuned with <strong>Reinforcement Learning with Human Feedback</strong> (RLHF) - explicitly attempt to satisfy human preferences using RL. This was implemented in an ingenious manner -</p> <p><img src="/assets/2024-11-03-Statistical-NLP/2024-11-09-15-13-21-image.png" alt=""/></p> <p>After adding some safety features, GPT3.5 was transformed into ChatGPT.</p> <p><strong>LLM as a reasoning engine</strong> - The knowledge base of LLMs is large but incomplete. To address this limitation, they need to retrieve information from elsewhere, add it to the prompt and then ask the LLM to process it to get an answer. This is the idea behind <strong>Retrieval Augmented Generation (RAG)</strong> for knowledge intensive NLP tasks. The pipeline is more sophisticated, and will be described later.</p> <h3 id="summary">Summary</h3> <p>We have the following big picture -</p> <ol> <li> <p>Feature Engineering in 1990s to 2000s</p> </li> <li> <p>Architecture Engineering - 2010 - 2018 (LSTMs, CNNs, Transformers)</p> </li> <li> <p>Objective Engineering - 2018 (ELMo, BERT, GPT)</p> </li> <li> <p>Prompt Engineering - 2020s - present (instruction-tuning, chain-of-thought, etc)</p> </li> </ol> <p><strong>NLP vs Humans.</strong> General language understanding is still a bit difficult for LLMs</p> <ul> <li> <p>Sample efficiency - LLMs require a lot of data</p> </li> <li> <p>Robustness - LLMs are brittle, can be easily fooled</p> </li> <li> <p>Originality - LLMs lack ability to create truly original content</p> </li> <li> <p>Causality and other forms of reasoning - LLMs have limited understanding of logic</p> </li> </ul> <p>In the following post, we will start from the basis such as text classification with simple neural networks and make our way to the modern sophisticated techniques being used. Although the initial part of the article may seem very straightforward, it is important to understand the motivations and ideas behind the approaches.</p> <h1 id="text-classification">Text Classification</h1> <p>The task is to assign a text document to one or more categories from a predefined set of labels. For example, Sentiment Analysis, Spam Detection, Topic Classification, Authorship Attribution and Language Identification fall under this domain of tasks.</p> <p>Unlike tasks like sorting numbers which have clear rules, text classification does not have a standard algorithm. The issues with rule-based algorithms are as follows -</p> <ol> <li> <p>Semantic Gap - Computer does not know what words mean - their relationships, sentiment, etc.</p> </li> <li> <p>Intra-class variations - There are many ways to be a particular label.</p> </li> <li> <p>Scalability - Have to write rules for every class label</p> </li> </ol> <p>The task rather requires a data-driven approach (in the form of machine learning) -</p> <ol> <li> <p>Collect a dataset of example text inputs and their labels - <strong>training data</strong></p> </li> <li> <p>Use Machine Learning algorithms to train a classifier on the training examples</p> </li> <li> <p>Evaluate the classifier of new text data - <strong>test data</strong> - a good classifier generalizes well.</p> </li> </ol> <p>This is the standard process (used to be) to work with machine learning models. Another important aspect with this approach is modeling of data - i.e., input representation. Machine Learning models require numerical input.</p> <blockquote> <p>At their core, machine learning models are optimization models relying on mathematical operations - cannot be done with text data.</p> </blockquote> <p>Therefore, we need to create a <em>feature vector</em> for the input text for text classification. By creating feature vectors, we are essentially creating another veil of abstraction to reduce the semantic and other complexities in text data.</p> <p><strong>Bag of words</strong> is one such early idea that represents text as an unordered collection of words, disregarding grammar and word order. The vector essentially contains the frequency of each word occurring in the training text. For unseen words in test data, we just treat them as a separate entity under the unknown tag.</p> <p>These feature vectors although being one of the simplest forms of representation have some limitations (owing to the simplicity)</p> <ol> <li> <p>Sparsity - high-dimensional, sparse vectors (size of vocabulary)</p> </li> <li> <p>Loss of sentence structure - This context is very important in some tasks</p> </li> <li> <p>Semantic Gap - Lacks the word meanings</p> </li> </ol> <h3 id="nearest-neighbor-classifier">Nearest Neighbor Classifier</h3> <p>The idea is to represent all training examples in a feature space. Given a test instance, we find the closest training example based on a <strong>distance metric</strong> and assign its label to the test instance. This algorithm does not have any training of sorts and takes constant time. However, the inference time is \(\mathcal O(n)\) since it has to be compared against every test instance. We do not want such behavior with ML models - even if the training is slow, we want the inference to be very fast.</p> <p>Furthermore, the decision boundaries created by this algorithm are not smooth - <em>from experience this is a bad outcome, and we ideally want a smooth decision boundary</em> without too many changes. Motivating from this idea, then came along \(k\)-Nearest Neighbor classification where the nearest \(k\) neighbors are chosen to decide the label. The parameter \(k\) is called as a <strong>hyperparameter</strong> that is tuned based on the dataset, model, etc. There is no clear answer as to which hyper-parameter gives the best performance; they have to be chosen empirically. The distance metric and word representation are other hyperparameters in this algorithm.</p> <p>Suppose we choose the hyperparameter that works best on the <em>test data</em>, it may not perform that well on other test/unseen data. To overcome this, a new split called <strong>validation</strong> is introduced. The hyperparameters are chosen with the validation data and the algorithm is tested on the test data. The golden rule is to run the model on the test set once after everything (training and hyper-parameter tuning) is completed.</p> <p>The advantages of these models are that they are non-parametric - they make no assumptions about the underlying data. Now, we see another form of classifier that does not have this property.</p> <h3 id="linear-classifier">Linear classifier</h3> <p>A linear classifier assumes a specific form of the decision boundary - predefined model complexity. These boundaries are defined by a set of fixed parameters - intercept \(b\) and slope \(m\). This form of a classifier is still used today in practice.</p> <p>The mathematical model can be represented as</p> \[y \text{ (label) } = \underbrace{f(x)}_{n \times 1} = \underbrace{W}_{n \times m} \underbrace{x}_{m \times 1} + b\] <p>Here, \(n\) is the number of classes and \(m\) is the feature dimension. The bias term \(b\) essentially incorporates prior knowledge into the model.</p> <p>Linear layers are the building block of <strong>neural networks</strong>. Neural networks consist of small functions that are stacked together to form a complex function, and linear layers are a common building block in these architectures.</p> <p>How do we find the best \(W, b\)? We define a <strong>loss function</strong> that quantifies our unhappiness with the scores across the training data. This loss function is minimized via <strong>optimization</strong> to get these best values. We typically average the loss function across all the training examples to get the total loss in a training dataset.</p> <p>Finally, the output from a classifier is unnormalized, and we generally want to interpret these raw scores as probabilities. To normalize the outputs, we use a softmax function -</p> \[P(Y = k, X = x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}}\] <p>where \(e\) is the raw output from the model. This interpretation allows us to now define a loss-function for (binary class) classifiers - <strong>negative log likelihood</strong>.</p> \[L_i = - \log P(y = y_i \vert X = x_i)\] <p>Such method of classification is termed as <strong>logistic regression</strong>.</p> <blockquote> <p>For logistic regression, with a small initialization of weights, the expected initial loss is \(\log (# classes)\). A good sanity check to look out for.</p> </blockquote> <p>The above loss function is essentially doing <strong>maximum likelihood estimation</strong> on the training data. Given training samples \((x_i, y_i)\), the maximum likelihood is given by \(W_{ML} = {\arg \max}_{W \in \mathbb R^{\gamma \times C}} \mathcal L(\bf W)\).</p> <p>The likelihood of the data is given as \(\mathcal L(\bf W) = \sum_{i = 1}^n \log p (Y_i \vert x_i; \bf W)\).</p> <h3 id="feed-forward-neural-networks">Feed-forward Neural Networks</h3> <p>Neural networks introduce a powerful regime of data-driven methods. These models are essentially linear layers stacked (without the softmax) against one another to form a huge network. Each linear layer is associated with an <strong>activation function</strong> that introduces non-linearity in the models. For example, the linear output \(Wx + b\) from a linear classifier layer is sent through a sigmoid activation function which essentially maps the input \(x\) to \((1 + e^{-x})^{-1}\). Each weight row in the network is referred to as a <strong>neuron</strong>.</p> <p>Note that we want our models to generalize well on unseen data. Since neural networks are powerful models, they can <em>overfit</em> on the training data to perform really well on the training set but poorly on the test set. Through <strong>regularization</strong>, we prevent overfitting by discouraging the model from fitting the training data too closely. This occurs frequently when in deep and complicated models. With regularization, we are no longer doing Maximum Likelihood Estimation (MLE) with our models. The loss function then becomes</p> \[L(W) = \frac{1}{N} \sum_{i = 1}^N L_i (F(x_i, W), y_i) + \lambda R(W)\] <p>where \(R(W)\) is the regularization function and \(\lambda\) is a <em>regularization parameter</em>, and it represents the regularization strength. There are more complex methods like <strong>dropout</strong> and <strong>batch normalization</strong> to prevent overfitting. Dropout refers to <em>randomly dropping</em> neurons in the network while training to simplify the model complexity during training time.</p> <p>With this model, we have described the basic recipe for supervised machine learning!</p> <p>How do we find the best \(W\) with these building blocks? Start with a random \(W\) and iteratively improve it to lower the loss function - <strong>gradient descent</strong>. The size of iterative improvement is given by the <strong>learning rate</strong> - another important hyperparameter. A small learning rate leads to slower convergence whereas a high learning rate can overshoot the minimum. Typically, the learning rate is changed across training epochs based on a <strong>learning rate schedules</strong> (ex. cosine learning).</p> <p>The gradient function is pre-computed to prevent recomputing the gradient for every example in practice. <strong>Backpropagation</strong> is a method to compute the gradient of the loss function with respect to the weights in the network - it is an application of the chain rule of calculus.</p> <p>The naïve version of gradient descent can be optimized much further using better convergence algorithms like ADAM and using stochasticity to descent over batches rather than the whole training dataset to increase speed.</p> <h1 id="word-embeddings-and-tokenization">Word Embeddings and Tokenization</h1> <p>How do we convert words to numerical values? A simple idea is to consider a <strong>one-hot vector</strong> - maps words into fixed length vectors and they contain only the identity information of the object without any semantic information. <strong>Bag-of-words</strong> is essentially the summation of one-hot vectors across the input text.</p> <p>An interesting result is that word meanings can also be captured through vectors of real numbers - a vector space where similar words (by meaning) have similar vectors (by some distance metric). How do we come up with such vectors that also have a reasonable size?</p> <h3 id="distributional-semantics">Distributional Semantics</h3> <p>Words that appear in similar contexts have similar meanings. The idea is to understand the context around the word and their relative ordering to understand the meaning of the word itself. To do so, we will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts, measuring similarity as the vector dot (scalar) product.</p> <h3 id="word2vec">Word2Vec</h3> <p>A simple and fast model to capture semantic meanings. They use two algorithms - skip-gram and Continuous Bag of Words (CBOW).</p> <p><strong>Skip-gram</strong> - Given a corpus of text as the input, the output is a set of embeddings which is a real valued vector. To generate these embeddings, we set up a fake prediction task - predict a word’s context from that word. For example, in the sentence “the dog bit the man”, for the <em>word</em> “bit”, the <em>context</em> can be “dog” or “the”.</p> <p>For each position $t = 1, \dots, T$ in the corpus, we predict context words within a window of fixed size \(m\) (<em>context window size</em>), given a center word \(w_j\). The joint probability expression is given by</p> \[\text{ Data Likelihood } = \prod_{t = 1}^{T} \prod_{-m \leq j \leq m; j \neq 0} P(w_{t + j} \vert w_t; \theta)\] <p>where \(\theta\) are all the parameters of the model. The loss function can be chosen as</p> \[L(\theta) = -\frac{1}{T} \sum_{t = 1}^T \sum_{-m \leq j \leq m; j \neq 0}\log P(W_{t + j} \vert w_t; \theta)\] <p>The ground truth probability is calculated by looking at all examples of the word occurring in the training corpus. The original paper predicts two vectors per word \(w\) - \(v_w\) when \(w\) is a center word, \(u_w\) when \(w\) is a context word. The probability is then given by</p> \[P(o \vert c) = \frac{\exp (u_o^T v_c)}{\sum_{w \in V} \exp(u^T_w v_c)}\] <p>The gradient of this loss function comes out to be expected context vector subtracted from the observed context vector. The center word is pulled towards words that are observed in its context and away from those that are not</p> \[v_c^{new } = v_c^{old} + \text{observed} - \text{expected}\] <h1 id="low-rank-adaptation-lora">Low-rank Adaptation (LoRA)</h1> \[h = W\] <h1 id="multi-layer-prompt-tuning">Multi-layer Prompt Tuning</h1> <p>Continuous prompts \(\phi_i\) are concatenated with the keys and values in the self-attention layer</p> <h1 id="adapters-in-transformer-models">Adapters in Transformer Models</h1> <p>An adapter in a Transformer layer</p> \[f_{\phi_i}(x) = W^U(\sigma(W^D x))\] <p>where \(W_^D \in \mathbb R^{d\times r},\)W^U$$</p> <h1 id="towards-a-unified-view-for-parameter-efficient-fine-tuning">Towards a Unified View for Parameter Efficient fine-tuning</h1> <p>This works shoes that LoRA, prefix-tuning, and adapters can be expressed with a similar functional form - all methods can be expressed as modifying a model’s hidden representation \(h\)</p> <h3 id="optimizer-state-comparison">Optimizer State comparison</h3> <h2 id="model-compression">Model compression</h2> <h3 id="knowledge-distillation">Knowledge Distillation</h3> <p>A classic approach from Hinton et. al</p> <h3 id="distilbert">DistilBERT</h3> <p>The idea is to use the classification model with output \(P_{\text{teacher}} (y \vert x)\) to minimize \(KL(P_{\text{teacher}}\vert\vert P_{\text{student}})\) to bring student distribution close to the teacher. Note that this approach does not require any labels since the student uses <em>pseudo-labels</em> that the teacher has.</p> <p>For example, we can choose BERT as the teacher model and create a small student model that has half the layers of BERT. The number of parameters reduce by half and so does the inference time. The performance difference is negligible and this is a huge gain for efficiency.</p> <h1 id="knowledge-representation-in-transformer-lms">Knowledge Representation in Transformer LMs</h1> <p>Factual knowledge is captured by the model during the training is stored in the form of model parameters. Which part of the transformer is this information stored? Token embeddings, feedforward laters or attention layers?</p> <h3 id="parameter-distribution-in-transformers">Parameter Distribution in Transformers</h3> <ul> <li> <p>Self-attention layers</p> <ul> <li> <p>Query, Key, Value matrices - \(W_q, W_k, W_v\) each of dimension \(d \times d\)</p> </li> <li> <p>Output matrix is \(W_o\) of dimension \(d \times d\)</p> </li> <li> <p>\(4d^2\) attention parameters per layer.</p> </li> </ul> </li> <li> <p>Feed-forward Network Layers</p> <ul> <li> <p>First linear transformation via \(W_1: 4d^s\) (input to \(4d\))</p> </li> <li> <p>Second linear transformation via \(W_2: 4d^2\) (\(4d\) to output)</p> </li> <li> <p>\(8d^2\) feedforward parameters per layer</p> </li> </ul> </li> <li> <p>The embedding parameters are not usually considered since the vocabulary across all the models is assumed to be the same</p> </li> </ul> <p>Note that feedforward layers have much higher number of parameters than the attention layers - they account for 2/3 of the total parameters.</p> <h2 id="transformer-feed-forward-layers-are-key-values-memories">Transformer Feed-Forward Layers Are Key-Values Memories</h2> <p>An interesting paper that explores the previous ideas. The feedforward layer is represented as \(y = W_2 \sigma(W_1 x)\).</p> <p>\(W_1\) corresponds to keys, and \(W_2\) to values - when the output from the first weight matrix is passed through ReLU - it is similar to <em>selecting</em> some entries of the vector (positive ones) which then choose the corresponding rows in \(W_2\).</p> <p>The authors tested this idea by considering which neurons are selected in the feedforward layers for different tokens - <strong>key trigger analysis</strong>. They analyzed the patterns in the activations to categorize them.</p> <p>Given a key \(k_i^l\) corresponding to \(i\)th row of the \(l\)th feed-forward layer \(W_1\) computer memory efficient for every prefix \(x_1, \dots, x_j\) of every sentence in the training data.</p> <p><strong>Memory coefficient calculation</strong> - calculate \(ReLU(x_j^l \cdot k)\) - incomplete</p> <p>They found the following results -</p> <ul> <li> <p>Shallow layers detect shallow patterns</p> </li> <li> <p>Middle FF layers store knowledge; Upper attention layers “aggregate” relevant knowledge for prediction</p> </li> </ul> <h2 id="editing-knowledge-in-transformer-lms">Editing Knowledge in Transformer LMs</h2> <p>Can we directly edit LM parameters to fix incorrect, obsolete facts? The edits must be deep -</p> <ul> <li> <p>Eiffel tower is located in the city ____ (change from Paris to Rome)</p> </li> <li> <p>Model should understand full implications - The tallest building in Rome is <em>__Eiffel Tower</em>__</p> </li> </ul> <p>The edit must be robust, should not edit all the facts in the model.</p> <p>Can we simply modify the columns of \(W_2\) to change the model’s behavior? This ends up breaking the model. Another work, Meg et al., suggested applying a rank-1 update \(W_2 \to W_1 + uv^T\) to maximize the probability of the edit output. This change minimizes the change in the behavior of \(W_2\) on other inputs.</p> <p>It works well in some scenarios and does not in some other - it is a new research direction!</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Enter the world of Natural Language Processing.]]></summary></entry><entry><title type="html">Large Language Model Reasoning</title><link href="https://sudhansh6.github.io/blog/Large-Language-Models-Research/" rel="alternate" type="text/html" title="Large Language Model Reasoning"/><published>2024-10-14T00:00:00+00:00</published><updated>2024-10-14T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Large-Language-Models-Research</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Large-Language-Models-Research/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>As a part of this article, we delve into the paradigm of Chain of Though reasoning in Large Language Models. The aim is to highlight the importance of this idea and summarize the main research in this area. The blog should provide enough context for the reader in the field of AI to understand the basic concepts and think about the potential research ideas addressing the limitations of the current models.</p> <h1 id="chain-of-thought-reasoning"><a href="https://arxiv.org/pdf/2201.11903">Chain of thought Reasoning</a></h1> <p>Chain of thought (CoT) refers to manifesting the human thought process in large language models by endowing language models with the ability to generate a chain of thought - a coherent series of intermediate reasoning steps.</p> <p>It is hypothesized that CoT prompting helps LLMs to tackle complex arithmetic, commonsense and symbolic reasoning tasks. The following demonstration highlights this improvement.</p> <p><img src="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&amp;w=1920&amp;q=75" alt="COT"/></p> <p>However, there are some limitations with this paradigm of reasoning with the current models.</p> <ul> <li>Small models are unable to improve with CoT prompting. LLMs with more than 100B parameters show performance gains with CoT.</li> <li>The performance improvements are larger with larger models. In other words, the benefits of CoT scale with the size of the models.</li> <li>Sometimes the models arrive at the correct answers with the wrong reasoning. The errors have been classified as <ul> <li><strong>Calculation error</strong> - LLMs are probabilistic models, predicting what token occurs next. So when an LLM tries to do \(3* 25* 8 =\), it does not really calculate the answer but probabilistically guesses the answer which is the next token. This highlights a fundamental limitation in the current architectures of LLMs.</li> <li><strong>Symbol mapping error</strong> - When there are too many variables involved, LLMs sometimes mix up the variables and arrive at the wrong answer. Again, the problem arises from the fundamental architecture flaw highlighted in the previous point.</li> <li>Other than these major errors, the models also have semnatic understanding problems, missing steps, incoherent chain of thought errors</li> </ul> </li> </ul> <h1 id="large-language-models-are-human-level-prompt-engineers"><a href="https://arxiv.org/abs/2211.01910">Large Language Models are Human-level prompt engineers</a></h1> <p>The motivation of this paper is as follows -</p> <ul> <li> <p><strong>Human effort in prompt engineering</strong> - Crafting effective prompts for LLMs is time-consuming and requires significant human expertise.</p> </li> <li> <p><strong>Optimization challenge</strong> - Primpts greatly influence LLM performance, but users often lack insight into how to optimize them for specific tasks.</p> </li> <li> <p><strong>Scalability</strong> - As LLMs grow in size and capabilities, manuallt designing prompts becomes less deasible for a wide range of applications.</p> </li> <li> <p><strong>Automating promtp design</strong> - There is a growing need to automate the prompt engineering process to enhance LLM usability and performance.</p> </li> <li> <p><strong>Real-world impact</strong> - Applications in diverse domains (e.g., AI chatbots, automated content generation) can benefit from optimized and automated prompts.</p> </li> </ul> <p>This work promposes an <strong>Automatic Prompt Engineer (APE)</strong> - asystem that automates prompt generationg and selection for Large Language Models. This task is treated as a program synthesis task wherein the input-output pairs (natural language questions and answers) are given to the APE, and it has to generate the instruction needed to generate these pairs.</p> <p>In essence, the APE is trying to learn the prompts generated by humans. The framework is as follows -</p> <ol> <li> <p>Instruction Generation. An LLM is used as an ingeerence model where the “instruction candidates” are generated based on a small set of input-output demonstrations</p> <p>Example: The input to APE is of the form -</p> <p><em>Input 1</em> - Forward generation technique</p> <p>”””</p> <p>I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs</p> <p>Input: [ ] Output: [ ]</p> <p>Input: [ ] Output: [ ]</p> <p>…</p> <p>The instruction was &lt;COMPLETE&gt;</p> <p>””””</p> <p><em>Input 2</em> - Reverse generation technique</p> <p>”””</p> <p>I instructed my friend to &lt;INSERT&gt;</p> <p>The friend read the instructions and wrote an output for every one of the inptus. Here are the input-output pairs:</p> <p>Input: [ ] Output: [ ]</p> <p>Input: [ ] Output: [ ]</p> <p>…</p> <p>”””</p> </li> <li> <p>Scoring Instructions. Evaluate each instruction by computing a score that reflects how well the instruction guides the target LLM for the task. This is simply the confidence score associated with the log likelihoods of token generation. The authors consider a <em>moving average</em> score considering the probabilities for a window of tokens.</p> <p>They also consider an <strong>execution accuracy</strong> - the success of an instruction by checking if the model produces the correct output (0-1 loss). However, this cannot. be used for all kinds of instructions.</p> <p>The top \(k\)-percentile prompts are selected and the rest are discarded.</p> </li> <li> <p>LLM as Resampling Model. They apply an Iterative Monte search method to resample more prompts. The LLM generates semnatically similar instructions variants to improve the top-performing candidates.</p> <p>Once the prompts are generated, the moving average scores are generated for each of the prompts and the better scoring prompts are selected again.</p> </li> </ol> <p>Can APE be used to guide LLMs?</p> <p><img src="/assets/img/LLMs/2024-10-23-10-41-54-image.png" alt=""/></p> <p>Although this is a very simple example, the work shows potential in taking such framework forward to work with more complex applications.</p> <p>Another interesting approach is to not generate the prompts from scratch, but to help humans design better prompts. Essentially, augment with context from humans to generate better prompts. On the flipside, RLHF can be used to improve these APE.</p> <h1 id="tree-of-thoughts"><a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts</a></h1> <p>The early Language Models were limited by their token-level, left-to-right decision making. However, some tasks require exploration, stratefic lookahead, planning, and backtracking. The vanilla architecture does not support such mechanisms.</p> <h2 id="framework">Framework</h2> <p><img src="/assets/img/LLMs/2024-10-30-09-36-33-image.png" alt=""/></p> <p>Mathematically, these models are depicted as</p> <ul> <li> <p>Input output programming - \(y \sim p_\theta^{IO}(y \vert x)\)</p> </li> <li> <p>Chain</p> </li> </ul> <p>Theoretically, the model seems promising. However, there are some intricate details that need to be figured out -</p> <ul> <li> <p>How to decompose the porcess into steps</p> </li> <li> <p>How to generate the potential thoughts from each state</p> <ul> <li> <p>Need to be small enough so LMs can generate promising and diverse samples</p> </li> <li> <p>Big enough so LMs can evaluate the difference contributing to the results</p> </li> </ul> </li> <li> <p>How to heuristically evaluate each state</p> </li> <li> <p>How to navigate through the generated tree</p> </li> </ul> <h2 id="thought-decomposition">Thought decomposition</h2> <p><img src="/assets/img/LLMs/2024-10-30-09-41-30-image.png" alt=""/></p> <h3 id="method-1---direct-prompting">Method 1 - Direct Prompting</h3> <p>The prompts themselves can ask the LM to segment the problem into multiple problems. Due to the voting mechanism, LM generates multiple possibilities for an answer and chooses the best model. This works better when thought space is rich and i.i.d samples lead to diversity.</p> <h3 id="method-2---backtracking">Method 2 - Backtracking</h3> <p>Propose thoughts sequentially using a “propose prompt”. When the thought space is constrained, this works better - proposing different thoughts in the same context avoids duplication.</p> <h2 id="state-evaluator">State Evaluator</h2> <p>There are two strategies to evaluate each generated state</p> <ul> <li> <p><strong>Value</strong> each state independently, where a value prompt reasons about the state \(s\) to generate a scalar value \(v\). This value is very context dependent.</p> </li> <li> <p><strong>Vote</strong> across states by deliberately comparing different states in \(S\) in a vote prompt.</p> </li> </ul> <h2 id="search-algorithm">Search Algorithm</h2> <ul> <li> <p><strong>BFS</strong> is helpful when the tree depth is limited and the initial thought steps can be evaluated and pruned to a small set</p> </li> <li> <p><strong>DFS</strong> explores longer trees well - subtrees are pruned to trade exploration for exploitation.</p> </li> <li> <p>More advanced approaches such as \(A^*\) and MCTS are left to future work in the paper.</p> </li> </ul> <p>An interesting summary of all thought paradigms - <a href="https://arxiv.org/pdf/2401.14295">Demytifying Chains, Trees, and Graphs of Thoughts</a>.</p> <h1 id="on-second-thought-lets-not-think-step-by-step-bias-and-toxicity-in-zero-shot-reasoning"><a href="https://urldefense.com/v3/__https://arxiv.org/abs/2212.08061__;!!Mih3wA!FUSiREHKHqULp_GaFY0sSmJRsiVZqYBdk9nJf8WWrKLI4UoxKUzc3ir1rIQaWXw6bk6_UVVe0kXW$">On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</a></h1> <p>We have seen how chain of thought improves problem solving capabilities. However, in some cases, CoT actually causes issues -</p> <p><img src="/assets/img/LLMs/2024-10-30-10-17-51-image.png" alt=""/></p> <p>The authors explore such effects in the paper. They consider</p> <h1 id="least-to-most-prompting-enables-complex-reasoning-in-large-language-models"><a href="https://arxiv.org/pdf/2205.10625">Least-to-most prompting enables complex reasoning in Large Language Models</a></h1> <p>The key motivators for the paper are as follows -</p> <ul> <li>Given a new task</li> </ul> <p>In the prior works, CoT reasoning has been effective for many tasks but struggled with “Easy-to-hard generalization”. Inspired from educational philosophies, the model is implemented by few-show ptompting in 2 stages - decomposition stage and subproblem solving stage.</p> <ul> <li> <p>Decomposition stage - The problem is divided into subtasks <em>once</em> before solving</p> </li> <li> <p>Subsequent solving stage - Solve the subsequent problems one by one.</p> </li> </ul> <p>The key difference from CoT prompting is that CoT starts each sub-problem from scratch and is unable to build from previous reasoning. This behaviour is depicted using the symbolic manipulation task in the paper - The performance of CoT progressively decreases as the length of the list increases.</p> <p>This method of prompting achieves significantly better results borrowing the context from the previous subproblems to arrive at the final answer.</p> <p>However, decomposition thoughts don’t generalize across domains. This limitation mainly shows up for math problems where the subproblems need to be correctly decomposed to solve the original problem.</p> <h1 id="chain-of-thoughtlessness"><a href="https://arxiv.org/pdf/2405.04776">Chain of Thoughtlessness</a></h1> <p>CoT prompting sounds too good to be true. The paper aims to test this paradigm rigorously to verify the claims. The paper also tries to identify the difference between complex reasoning and pattern matching - What seems like “complex reasoning” may just be a case of pattern matching?</p> <p>Consider the chain of thought reasoning -</p> <ul> <li> <p>How specific do the prompt examples have to be to the original problem?</p> </li> <li> <p>How generalizable are these prompts or how specific do they need ot be?</p> </li> <li> <p>How much human effort is needed to craft prompts for each problem subclass?</p> </li> </ul> <p>Furthermore, there are issues with the test domains as well. For example, GSM8K</p> <ul> <li> <p>They are non scalable - problem instances cannot be scaled</p> </li> <li> <p>The problems are static and be easily found in the training data</p> </li> </ul> <p>The main point the paper is trying to address the question - “Is it really possible to teach an LLM how to solve a generalizable problem?”. To test this claim, the authors choose “Blocks world” as the problem domain - given an initial and end configuration, output a series of steps to reach the end configuration from the initial configuration.</p> <p>They perform the following experiments</p> <ul> <li> <p><strong>Zero shot CoT</strong> - Simply append “Let’s think step by step” to the prompts.</p> </li> <li> <p><strong>Progression proof</strong> - Specific to planning problems. Each example’s steps describe the init state, action taken, reason of the action and the final step.</p> </li> </ul> <p>They see that zero-shot CoT achieves insignificant performance gains from zero-shot prompting. The progression proof CoT achieves a lower performance - this may be due to overfitting to the training examples. The LLM fails to learn the <em>universal block algorithm</em> (break the tower and put everything back) even with multiple version of CoT prompting. The authors chose a planning domain on purpose because these problems can be scaled up very well.</p> <p>The authors just wanted to highlight that there is a need for more rigorous testing. One might argue that planning problems are way out of domain of LLMs. So, the authors test the findings with commonly tested problems, and they find similar trends.</p> <h1 id="chain-of-thought-without-prompting"><a href="https://arxiv.org/abs/2402.10200">Chain of Thought without prompting</a></h1> <p>Prompting techniques, while effective, often encode task-specific human priors, thereby making it difficult to assess a language model’s intrinsic reasoning abilities. Ideally, a language model should be able to reason independently and provide the optimal response, without requiring humans to tweak the prompts or refine repeatedly if the initial response is unsatisfactory. Model-tuning can be expensive and requires a substantial amount of supervised data. In this work, we explore a different perspective and ask: Can LLMs reason effectively without prompting? And to what extent can they reason? We find that, perhaps surprisingly, there exists a task-agnostic way to elicit CoT reasoning from pre-trained LLMs by simply altering the decoding procedure. Figure 1 illustrates this phenomenon: given a reasoning question, the LLM generates a wrong answer via the standard greedy decoding path, yet alternative top-𝑘 token inspection unveiled inherent CoT paths (e.g., decoding paths 2 and 4), which accurately resolved the query. This decoding modification bypasses prompting and is entirely unsupervised without the need for model tuning.</p> <p><strong>Why can’t LLMs reason if we only consider greedy decoding path?</strong></p> <h1 id="large-language-models-are-zero-shot-reasoners"><a href="https://arxiv.org/abs/2205.11916">Large Language Models are Zero-shot reasoners</a></h1>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[A survey of papers to better understand the workings of Large Language Models.]]></summary></entry><entry><title type="html">Design and Analysis of Algorithms</title><link href="https://sudhansh6.github.io/blog/Design-and-Analysis-of-Algorithms/" rel="alternate" type="text/html" title="Design and Analysis of Algorithms"/><published>2024-09-27T00:00:00+00:00</published><updated>2024-09-27T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Design-and-Analysis-of-Algorithms</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Design-and-Analysis-of-Algorithms/"><![CDATA[<h1 id="greedy-algorithms">Greedy Algorithms</h1> <h2 id="minimum-spanning-tree">Minimum Spanning Tree</h2> <p>Consider a graph \(G\) describe by \(V\) and \(E\) (positive weights). A <strong>spanning tree</strong> of a graph is defined as an edge set \(T \subset E\) such that \((V, T)\) is a tree. A minimum spanning tree is such that \(\sum_{l \in T} l_e\) is minimized.</p> <p>For a graph with \(n\) vertices, there are \(n^{n - 2}\) spanning trees for a complete graph (<strong>Cayley’s formula</strong>).</p> <p>How do we calculate the number of spanning trees for a general graph? <strong>Kirchoff’s theorem</strong> states the following -</p> <ul> <li>Let \(M\) be the adjacency matrix of \(G\)</li> <li>Let \(L - M\) except \(L_{i, i} = -deg(i)\) - This is generally called as <strong>Graph Laplacian</strong></li> <li>Then, #spanning trees is the determinant of any \(m-1\) square sub-matrix (obtained by removing \(i\)th row and column) of \(L\).</li> </ul> <p>Notice how any sub-matrix yields the same value!</p> <h2 id="greedy-idea-1-kruskals-algorithm">Greedy Idea 1: Kruskal’s algorithm</h2> <ul> <li>Sort all the edges with their weights</li> <li>Pick edges as long as they don’t form a cycle</li> </ul> <p>Does this work? If it does, how do we prove it?</p> <p>Firstly, why is it a greedy idea? At each point of the algorithm, we select the current greedy edge (a local minimum) to obtain the minimum spanning tree (a global optimum).</p> <ul> <li> <p><strong>The cut property -</strong> Let \(S \subseteq V\) such that \(S\) and \(V - S\) are non-empty. If \(e\) is an edge across \(S\) and \(V - S\) with the minimum cost, then there always exists a minimum spanning tree with \(e\).</p> <p><strong>Proof.</strong> Exchange argument. If there an MST with \(e’\) across \(S\) and \(V - S\), then replace \(e\) with \(e’\) to obtain another MST.</p> <blockquote> <p>Shouldn’t this argument be more delicate? Why is there a single edge from S to V - S?</p> </blockquote> <p>Essentially, the exchange argument argues replacing a part of the solution improves the solution but does not worsen it.</p> </li> <li> <p>The time complexity of the algorithm comes out to be \(O(m \log m + m \alpha(n))\). The second term in the expression comes from union-find data structures.</p> </li> <li> <p><strong>Correctness of the algorithm</strong> - We shall prove this via induction.</p> <ul> <li>Induction hypothesis - The edges selected in the \(i\)th round of Kruskal’s algorithm can form an MST along with a subset of edges from the remaining edges.</li> <li>Base statement - True for \(i = 0\)</li> <li>Induction step - Cut property</li> </ul> </li> <li> <p><strong>Union-find data structure</strong> - A data structure that supports</p> <ul> <li>Merging elements of two sets into a single set - <code class="language-plaintext highlighter-rouge">union(x, y)</code></li> <li>Checking whether two elements are in the same set - <code class="language-plaintext highlighter-rouge">find(x)</code></li> </ul> <p>efficiently. The amortized time complexity for these operations is \(\alpha(n)\) where \(\alpha(n) \leq 4\) for \(n\) of the order \(2^{2^{2^{2^{16}}}}\). As a result, \(\alpha(n)\) can be regarded as a constant for practical purposes.</p> <p>In our case, the elements are edges and sets represent connected components.</p> </li> </ul> <h2 id="greedy-idea-2-prims-algorithm">Greedy Idea 2: Prim’s Algorithm</h2> <p>Start with any node and expand with the smallest edge connecting to the remaining set of edges. Note that this is different from Kruskal’s algorithm where we sort all the edges and create individual connected components that eventually merge together.</p> <p>The proof for Prim’s algorithm is very similar to that of Kruskal’s. The time complexity is \(O(n^2 + m)\) similar to Djikstra’s algorithm without a data structure. We can maintain a <strong>priority queue</strong> to maintain all edges that come from \(S\) to reduce the time-complexity to \(O((n + m) \log m)\) (without decrease-key). With decrease key and a binary heap, the complexity becomes \(O((n + m) \log n)\). Furthermore, with decrease key and a Fibonacci heap, the complexity reduces to \(O((n\log n + m)\).</p> <h2 id="other-algorithms">Other algorithms</h2> <ul> <li><strong>Reverse deletion</strong> - For every cycle in the original graph and the edge \(e\) with the maximum cost, there always exists an MST without \(e\). Until there are no cycles in the graph, find a cycle and delete the edge with a maximum cost. Note that this algorithm has a higher time complexity since we try and find a cycle for each iteration of the algorithm. How do we implement this?</li> </ul> <h2 id="union-find-data-structure">Union-Find data structure</h2> <p>The idea is to maintain trees with pointers to merge and find elements. The main complication comes while merging the individual sets.</p> <ul> <li> <p>Merging by size (consuming smaller ones by larger sets) - The complexity of merging sets of size \(n\), \(m\) times takes \(O(m \log n)\)</p> </li> <li> <p>To optimize this further, we merge by rank (generalizing the previous approach where rank was simply the size of the set). We add another trick to reduce the amortized time complexity.</p> <ul> <li>Path compression - When <code class="language-plaintext highlighter-rouge">find(x)</code> is called, attach the found elements along the path directly to the root to reduce the path size.</li> </ul> <p>The time complexity then becomes \(O(m \log^* n)\) where \(\log^* n\) is the minimum \(k\) such that \(\log^{(k)} n \leq 1\).</p> </li> </ul> <h1 id="more-greedy-problems-related-to-mst">More Greedy Problems related to MST</h1> <h3 id="k-clustering">\(k\)-clustering</h3> <p>A <strong>maximum spacing</strong> for \(k\)-clustering of \(G =(V, E)\) is defined as</p> <ul> <li>An edge set \(T \subset E\) such that \((V, T)\) has exactly \(k\) connected components</li> <li>The <strong>spacing</strong> is then \(\min d(u, v)\) for \(u, v\) in different connected components</li> <li>The goal is to maximize the spacing</li> </ul> <p>This problem can be solved again with Kruskal’s algotihm to find \(k\)-connected components - perform the <code class="language-plaintext highlighter-rouge">union</code> operation for \(n - k\) times. Why is this correct? WE can show this using a contradiction.</p> <ul> <li>Consider two nodes that lie in the same connected component in the result obtained by Kruskal’s (with spacing \(d’\)). Let them be in different connected components in the optimal solution (with spacing \(d\)). Then,</li> </ul> <h3 id="second-mst">Second MST</h3> <p>A second MST is essentially the spanning tree with the <em>second</em> lowest edge summation cost. How do we find this tree?</p> <ul> <li>Find an MST with weight \(w\)</li> <li>For every edge \(e\) not in the MST, if adding \(e\) yields a cycle in the graph; then remove the largest edge \(e’\) other than \(e\) in the cycle to obtain the second MST</li> <li>The cost of the tree would be \(w + l_e - l_{e’}\)</li> </ul> <p>The time complexity of this algorithm is \(\mathcal O(T_{MST} + mn)\) and can be improved to \(\mathcal O(T_{MST} + m \log n)\) with better data-structures and divide-and-conquer.</p> <p><strong>Lemma.</strong> The second MST only differs by one edge from the MST. Multiple MSTs? <strong>Proof.</strong> Can be shown using contradiction. The idea is that one can move from one spanning tree to another with local changes in the trees. The argument is that you can replace the edges in the second MST with the edges in the MST to obtain a tree with a lower cost. This process can be repeated until there is only one edge that is different from an MST and replacing that would cause the tree to become the MST.</p> <h2 id="more-greedy-problems">More Greedy Problems</h2> <h2 id="needle-in-haystack">Needle in Haystack</h2> <p>Given two strings \(s, t\), decide whether there is a subsequence (need not be contiguous) in \(s\) that matches with \(t\). A naive greedy algorithm is depicted as follows -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">break</span> 
    <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">t</span><span class="p">[</span><span class="n">j</span><span class="p">]:</span>
        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># s[i] is matched with t[j]
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> <p>On first glance, it looks as if this is a very intuitive algorithm. However, there are more intricate details and the proof makes these clearer. The proof relies on the Exchange argument. If there exists a subsequence \(i_1 i_2 \dots i_m\) in \(s\) matching \(t\) (\(\vert t \vert = m\)). If \(i^*_1 &lt; i_1\) is the first index that \(s[i_1] = t[1]\), then \(i^*_1i_2\dots i_m\) also matches \(t\). This way we can find a set of indices \(i^*_1i^*_2 \dots i^*_m\) through the greedy algorithm that gives the correct answer.</p> <p>In general, greedy algorithms can be proven in two general ways</p> <ul> <li>Consider any iteration of the algorithm and show that the decision made by the greedy algorithm is the best and conclude using induction</li> <li>Exchange argument: Pick an optimal solution and gradually change it to the solution produced by the greedy algorithm showing that the optimality is not affected.</li> </ul> <h2 id="matroids">Matroids</h2> <p>A finite matroid \(M\) consists of \((E, I)\) where</p> <ul> <li>\(E\) is a finite set (called the ground set). For example, \(E\) is the set of all edges in the graph \(G = (V, E)\)</li> <li>\(I\) is a collection of subsets of \(E\) (called the independent set). For example, \(I\) consists all subsets of \(S\) of \(E\) such that all the edges in \(s \in S\) form a forest.</li> </ul> <p>\((E, I)\) should satisfy the following properties -</p> <ul> <li>Null set should be in \(I\) - \(\phi \in I\)</li> <li>if \(A \subset B\) and \(B \in I\) then \(A \in I\)</li> <li>If \(A, B \in I\), \(\vert A\vert &gt; \vert B \vert\) then \(\exists e \in A - B\), such that \(B \cup \{e\} \in I\)</li> </ul> <p>Isn’t 2 inclusive of 1 and 3?</p> <p>How does this data structure help us? Suppose we have a graph \(G = (V, E)\) with weights \(c_e \geq 0\). Then, design an algorithm to find an independent set \(S\) that maximizes \(\sum_{e \in S} c_e\). Consider the following algorithm -</p> <ul> <li>Sort \(e\) in decreasing order of weights</li> <li>Let \(S = \phi\). Add \(e\) to \(S\) if \(e\) does not add cycles in \(S\)</li> </ul> <p>This algorithm is very similar to the reverse deletion algorithm and has a time complexity \(\mathcal O(\vert E \vert \log \vert E \vert + T_{check \text{ IS}})\).</p> <p><strong>Lemma.</strong> Let \(e \in E\) have the maximum cost \(c_e\)m then there always exists an IS \(A\) with maximum weight containing \(e\).</p> <p>The proof is very similar to what we have shown with MSTs. This example can be applied to MSTs, and it demonstrates how Matroids can be useful for greedy algorithms.</p> <h2 id="task-scheduling">Task Scheduling</h2> <p>Given \(n\) jobs each with \(t_i\) time to finish and deadlines \(d_i\). Consider that there is a single resource solving tasks sequentially from time \(0\), and a job has to completely finished before moving onto the next one.</p> <p>Suppose in a scheduling algorithm, job \(i\) finishes at time \(f_i\), then the lateness is defined as \(l_i = \max\{0, f_i - d_i\}\). The goal is find an algorithm that minimizes the maximum lateness \(minimize \max_i l_i\).</p> <p>Let us consider a simple case. Suppose there are two jobs with \(d_i \leq d_2\). Note that any scheduling algorithm should not have any idle time between jobs. Why so? If there is idle time, you can always do a job earlier to reduce the lateness. Furthermore, a scheduling algorithm should not have an <em>inversion.</em> That is, if job 1 has deadline earlier than job 2, then it is always optimal to perform job 1 before job 2. This claim can be easily proved using the exchange argument.</p> <h3 id="algorithm">Algorithm</h3> <p>Sort all jobs according to the increasing order of these deadlines \(d_i\), then complete each job without any idle time.</p> <p><strong>Proof.</strong> Generalize the previous two observations to \(n\) jobs.</p> <h2 id="huffman-codes">Huffman Codes</h2> <p>How do we encode an alphabet in binaries to have no ambiguities.</p> <h3 id="prefix-codes">Prefix codes</h3> <p>A prefix code for an alphabet \(T\) is a function \(f:T \to \{0, 1\}^*\), such that for distinct \(x, y \in T\), \(f(x)\) is not a prefix of \(f(y)\).</p> <p>It can be shown that a prefix code gives unique decoding.</p> <p>How do we design an encoding that is most efficient? Let us define efficiency. For every letter \(x\) in \(T\), let its frequency be \(p_x (\sum_{x \in T} p_x = 1)\). Let \(f\) be a prefix code and for every letter \(x \in T\), let \(\vert f(x)\vert\) is the number of bits. The goal is to find a prefix code \(f\) that minimizes the expected number of bits when encoding \(R\) under the frequency \(\{p_x\}\).</p> \[\text{minimize } \sum_{x \in T} p_x \cdot |f(x)|\] <p>It is beneficial to represent prefix codes as a binary tree. Each node has two children: 0 and 1. The paths from the root to other nodes in the tree represent the binary encodings.</p> <ul> <li>For prefix codes, no node of a symbol is an ancestor of node of another symbol (from the alphabet).</li> <li>Another observation is that any optimal prefix code is a full tree (every inner node has two children) - if anode has a single chide, the parent node can itself be used for the symbol deleting the leaf node making the encoding more efficient.</li> <li>There is an optimal tree (prefix code) such that two lower frequent letters are siblings, and are as deep as possible in the tree. This claim can be proved easily with the exchange argument.</li> </ul> <p>With these observations, consider the following algorithm</p> <ul> <li> <p>Initialize each letter \(x\) as a node and label it with \(p_x\)</p> </li> <li> <p>Put all nodes into a min-heap (according to the frequency)</p> </li> <li> <p>While min-heap has atleast two elements</p> <ul> <li> <p>Pop out the two smallest elements \(u, v\) (corresponds to two trees)</p> </li> <li> <p>Combine them to a single tree</p> </li> <li> <p>Push it into the heap, label with \(p_u + p_v\)</p> </li> </ul> </li> </ul> <p>This is the Huffman’s coding algorithm which has a time complexity of \(n \log n\).</p> <h2 id="shannons-source-coding-theorem">Shannon’s source coding theorem</h2> <p>Let \(T\) be an alphabet with frequency \(\{p_x\}\). The entropy of the alphabet is defined as</p> \[H := \sum_{x \in T} p_x \cdot \log \frac{1}{p_x}\] <p>The Shannon’s source coding theorem then states that you cannot send a letter from \(T\) with frequenct \(\{p_x\}\), with expected bits less than \(H\). Huffmman’s encoding gives a solution with expected bits at most \(H + 1\).</p> <p><strong>Important point</strong>. One can suggest to increase the alphabet size with dummy symbols to virtually reduce the value of \(H\) significantly. However, this introduces complexity for encoding algorithms. Therefore, there is a tradeoff with space occupied by encoding and the time for encoding.</p> <blockquote> <p>Even with augmented alphabet, the size of the encoding does not change for the original symbols in the alphabet?</p> </blockquote> <h1 id="binary-search-x-greedy-algorithms">Binary Search X Greedy Algorithms</h1> <p>The basic binary search takes advantage of the monotone structure in arrays to identify elements with certain properties. Any binary search problem can be converted to the following simpler version: For an array \(B\) that has binary elements with all zeros occurring before all ones, find the index of the first occuring \(1\).</p> <p>The binary search algorithm can be simply proved using induction.</p> <p>Let us consider an example - Ternary search. Given an array \(A[1\dots n]\) that is first strictly increasing and then strictly decreasing, dind the largest element. The array \(B[1\dots n - 1]\) is constructed as</p> <ul> <li> \[B[i] = 1 \iff A[i + 1] &gt; A[i]\] </li> <li> \[B[i] = 0 \iff A[i + 1] &lt; A[i]\] </li> </ul> <h2 id="split-array-largest-sum">Split-array largest sum</h2> <h2 id="minimum-fractional-st">Minimum fractional ST</h2> <p>Given an undirected graph \(G = (V, E)\) and each edge has two costs \(a_e, b_e\) both of which are positive, find a spanning tree \(T\) that minimizes</p> \[\frac{\sum_{e \in T} a_e}{\sum_{e \in T} b_e}\] <p>How is this related to binary search? Firstly, we will convert this problem to a decisional version - Given an undirected graph \(G = (V, E)\) and a real number \(U\), decide whether there exists a spanning tree \(T\) such that \(\frac{\sum_{e \in T} a_e}{\sum_{e \in T} b_e} \leq U\).</p> <p>This is equivalent to find a spanning tree such that \(\sum_{e \in T} a_e - U b_e \leq 0\). Construct a new graph with the weights \(a_e - Ub_e\). The reduction is easy to follow.</p> <p>How do we find the monotone structure for binary search? If the decision problem \((G, U)\) is satisfiable, then \((G, U')\) is also satisfiable for any \(U' &gt; U\). Conceptually, assume a function \(B\) (with continuous index) such that \(B[0, S] \to \{0, 1\}\) where \(S\) is an upper bound. \(B(U) = 1\) iuf and only if \((G, U)\) is satisifiable, and \(B\) is monotone.</p> <h1 id="divide-and-conquer">Divide and Conquer</h1> <h2 id="master-theorem">Master Theorem</h2> <p>Consider an algorithm that has the following relationship for running time complexity -</p> \[T(n) = 2T \left(\frac{n}{2}\right) + c n \log^k n \quad (k \geq 0)\] <p>then \(T(n) = \mathcal O(n \log^{k + 1} n)\).</p> <h2 id="closest-point">Closest Point</h2> <h2 id="fast-multiplication">Fast Multiplication</h2> <p>Suppose we have two integers in binary \(a = \sum_{0 \leq 1 \leq n} a_i \cdot 2^i, b = \sum_{0 \leq 1 \leq n} b_i \cdot 2^i\). The goal is to compute \(c = ab = \sum_{0 \leq j &lt; 2n} c_j \cdot 2^j\) where \(c_j = \sum_{0 \leq k \leq j} a_k b_{j - k}\). The naïve brute force approach takes \(\mathcal O(n^2)\) to compute the answer.</p> <p>This question is related to matrix multiplication as well. The naïve algorithm takes \(\mathcal O(n^3)\).</p> <h3 id="algorithm-1">Algorithm 1</h3> <p>We segment \(a, b\) as follows -</p> <ul> <li> <p>\(a = A_1 \cdot 2^{\frac{n}{2}} + A_0\) where \(A_0 = \sum_{0 \leq 1 &lt; n/2} a_i \cdot 2^i\) and \(A_1 = \sum_{n/2 \leq i &lt; n} a_i \cdot 2^{i - n/2}\)</p> </li> <li> <p>\(b = B_1 \cdot 2^{\frac{n}{2}} + B_0\) similarly.</p> </li> </ul> <p>Then, \(ab = (A_1 \cdot 2^{\frac{n}{2}} + A_0)(B_1 \cdot 2^{\frac{n}{2}} + B_0)\). The strategy then is to do a divide and conquer on these halves to get the final answer.</p> \[ab = A_1 B_1 2^n + (A_0 B_1 + A_1 B_0)2^{n/2} + A_0B_0\] <p>The time complexity is then \(T(n) = 4T(\frac{n}{2}) + \mathcal O(n)\). This is essentially \(\mathcal O(n^2)\) that does not give any improvement.</p> <p>This can be optimized further -</p> \[ab = A_1 B_1 2^n + ((A_0 + A_1)(B_0 + B_1) - A_0B_0 - A_1 B_1)2^{n/2} + A_0B_0\] <p>The number of multiplications reduced to 3 - \(T(n) = 3T(\frac{n}{2}) + \mathcal O(n)\). Deriving the final expression, \(T(n) = cn + cn\frac{3}{2} + \dots + cn\left(\frac{3}{2}\right)^{\log n} = \mathcal(3^{\log n})\).</p> <p>This algorithm can be extended to matrix multiplications as well.</p> \[C = AB = \begin{bmatrix} A_{00}B_{00} + A_{01}B_{10} &amp; A_{00}B_{01} + A_{01} B_{11} \\A_{10}B_{00} + A_{11}B_{10} &amp; A_{10}B_{01} + A_{11}B_{11}\end{bmatrix}\] <p>The naïve algorithm shown above is still \(O(n^3)\). Strassen’s algorithm reduces the number of multiplications to \(7\) providing an improvement over the \(\mathcal O(n^3)\) algorithm giving \(\approx \mathcal O(n^{2.81})\).</p> <p>The current state of the art algorithm for matrix multiplication achieves \(\mathcal O(n^{2.371552})\). We do not know if there is an algorithm that achieves \(\mathcal O (n^{2 + o(1)})\).</p> <h3 id="algorithm-2">Algorithm 2</h3> <p>Multiplication can be seen as a special case of convolution and we can use <strong>Fast Fourier Transform (FFT)</strong> to perform this in \(\mathcal O(n \log n)\). The details will be elaborated in the next section.</p> <h2 id="convolution">Convolution</h2> <p>Consider two vectors of the following form -</p> <ul> <li> \[a = (a_{n - 1}, a_{n - 2}, \dots, a_2, a_1, a_0)\] </li> <li> \[b = (b_{n - 1}, b_{n - 2}, \dots, b_2, b_1, b_0)\] </li> </ul> <p>The convolution operation \(\star\) is defined as</p> \[c = a\star b = (c_{n - 1}, \dots, c_0) \quad \text{ where } c_j = \sum_{0 \leq k &lt; n} a_j b_{(j - k)\mod n}\] <p>Convolution is a generalization of integer multiplication (padding + convolution = multiplication). Also, convolution is a central operation in signal processing - used for blurring images and also to learn features from spatial data.</p> <p>The naïve algorithm can be done in \(\mathcal O(n^2)\) time. We can perform convolution using \(\mathcal O(n\log n )\) using <strong>Fourier Transform</strong>.</p> <h1 id="fourier-transform">Fourier Transform</h1> <p>Consider the \(n\) dimensional vector \(a = (a_{n - 1}, a_{n - 2}, \dots, a_2, a_1, a_0)\) and \(b = (b_{n - 1}, b_{n - 2}, \dots, b_2, b_1, b_0)\). Let \(\{e_i\}_i\) form a unit basis of \(\mathbb R^n\) such that \(a = \sum_{0 \leq i &lt; n} a_i e_i, b = \sum_{0 \leq i &lt; n} b_i e_i\).</p> <p>Consider another basis \(\hat e_i(j) = \omega_n^{ij}\) where \(\omega_n = e^{\frac{1\pi \bf{i}}{n}}\) is the \(n\)-th root of unity. Therefore, \(\hat e_i = \frac{1}{\sqrt{n}} \omega_n^{(n - 1)i}, \dots, \omega_n^{2i}, \omega_n^{i}, 1)\).</p> <p>It is easy to check that this is a valid basis. So, again, \(a, b\) can be uniquely represented as</p> <ul> <li> <p>\(a = \sum_{0 \leq i &lt; n} \hat a_i \hat e_i\), \(\hat a_i = \langle a_i, \hat e_i\rangle = \frac{1}{\sqrt{n}} \sum_j a_j \omega_n^{-ij}\)</p> </li> <li> \[b = \sum_{0 \leq i &lt; n} \hat b_i \hat e_i\] </li> </ul> <p>A <strong>Fourier transform</strong> is then defined as - Given \(\{a_i\}_{i \in [n]}\), compute \(F(a) = \{\hat{a_i}\}_{i \in [n]}\).</p> <p>The <strong>inverse problem</strong> is to find \(F^{-1} (\hat a) = \{a_i \}_{i \in [n]}\). It essentially is a change of basis between \(\{e_i\} \iff \{\hat e_i\}\).</p> <h2 id="convolution-theorem">Convolution Theorem</h2> <p>Let \(a, b\) be two vectors in \(\mathbb R^n\); then,</p> \[a \star b = F^{-1} (F(a) \cdot F(b))\] <p>With this claim, convolution can be f=done in \(\mathcal O(2T_{FT} + T_{IFT} + n)\).</p> <h1 id="dynamic-programming">Dynamic Programming</h1> <h2 id="longest-path-on-a-dag">Longest path on a DAG</h2> <p>Given a DAF with \(n\) vertices, \(m\) edges, every edge \(e\) has a weight \(l_e\), compute the longest path on the DAG. The length of a path is defined as the weight sum over all edges in the path.</p> <p>Consider the following algorithm</p> <pre><code class="language-pseudocode">DFS(u):
    if marked[u] = true:
        return DP[u]
    cost &lt;- 0
    for all v that (v, u) in E:
        cost &lt;- max(cost, DFS(v) + l_{v, u})
    marked[u] &lt;- true
    DP[u] &lt;- cost
    return cost
</code></pre> <p>The time complexity of the algorithm is \(\mathcal O(n + m)\). The key point to notice is that instead of recomputing the cost of each path, we have essentially stored the costs in the array <code class="language-plaintext highlighter-rouge">DP</code> to reduce the redundant calculations. This step is known as <strong>memoization</strong>.</p> <h2 id="knapsack-problem">Knapsack Problem</h2> <p>Consider an integer \(U\) representing total capacity and a list of integers \(\{v_i, c_i\}\) that represents the volume and cost of each item respectively. The goal is to pick items such that their total volume is at most \(U\) and their value is maximized.</p> <p><strong>Idea 1</strong>. Sort everything by \(v_i/c_i\) and pick the items until value if \(U\). It is easy to see that this greedy algorithm will not work.</p> <h3 id="a-backtracking-algorithm">A Backtracking algorithm</h3> <p>Consider an iterative algorithm that at step \(i\) has \(C\) volume left and is considering whether to pick or skip the \(i\)-th item. Considering these two possibilities, we can implement a brute force algorithm with memoization for dynamic programming.</p> <p>We set a 2D matrix of size \((U, n)\) where each row \(i\) represents the set of items that need to be picked to maximize the cost within volume \(U\). The time complexity of this algorithm would be \(\mathcal O(2^n)\).</p> <p>The algorithm is as follows -</p> <pre><code class="language-pseudocodedfs(C,">   dfs(C, i):
       if i = 0: return 0
       Cost &lt;- dfs(C, i - 1)
       if C &gt;= c_i:
           Cost &lt;- max(Cost, dfs(C - c_i, i - 1) + v_i)
       return Cost
</code></pre> <h3 id="alternative-view">Alternative view</h3> <p>We can treat every possible \((C, i)\) as a vertex in a graph. Every vertex has at most two outcoming edges - \((C, i) \to (C - c_i, i - 1)\) with cost \(v_i\) and \((C, i) \to (C, i - 1)\) with cost \(0\). This constructed graph is a DAG and we essentially reduced Knapsack problem to longest path on a DAG.</p> <p>Based on the algorithm we have seen earlier, we modify the algorithm to include memoization</p> <pre><code class="language-psuedocode">   dfs(C, i):
       if marked[C][i] = true: return DP[C][i] // Modification
       if i = 0: return 0
       Cost &lt;- dfs(C, i - 1)
       if C &gt;= c_i:
           Cost &lt;- max(Cost, dfs(C - c_i, i - 1) + v_i)
       marked[C][i] &lt;- true // Modification
       DP[C][i] &lt;- cost // Modification
       return Cost
</code></pre> <p>The modified algorithm now has the time complexity \(\mathcal O(Un)\) since there are \(Un\) vertices in total with atmost 2 edges each.</p> <h2 id="general-observation">General observation</h2> <p>Dynamic Programming problems can be typically thought og as a decision-making processes. These decision problems can be converted to graphs where the states are vertices on a graph and the transitions are edges on a graph. Typically, the problem have a DP solution if the graph is a DAG and the number of states is not too large.</p> <p>The algorithm shown above can then be used as a general procedure to solve the problems. Sometimes, it is beneficial to implement the algorithms with a loop rather than recursion.</p> <h2 id="knapsack-with-unlimited-items">Knapsack with unlimited items</h2> <p>The algorithm remains pretty much the same except that teh recrusion call has <code class="language-plaintext highlighter-rouge">dfs(C - c_i, i)</code> instead of <code class="language-plaintext highlighter-rouge">dfs(C - c_i, i - 1)</code>.</p> <h2 id="knapsack-with-limited-items">Knapsack with limited items</h2> <p>We can consider another variant where item \(i\) can be used at most \(k_i\) times. Then, a similar algorithm would have the time complexity \(\mathcal (U \sum_i k_i)\).</p> <p>A better solution treats the \(i\)th item as \(\lceil \log k_i\rceil\) items. For example, if \(k_i = 8\), then divide the item as \((c_i, v_i), (2c_i, 2v_i), (4c_i, 4v_i), (c_i, v_i)\). Then the time complexity would be reduced to \(\mathcal O(U \sum_i \log k_i)\).</p> <p>However, it can be improved to \(\mathcal O(Un)\) using a <strong>monotonic queue</strong>.</p> <h2 id="2d-knapsack">2D Knapsack</h2> <p>In this variant, each item has value \(v_i\), volume integer \(c_i &gt; 0\) and a weight integer \(w_i &gt; 0\). The goal is to find a subset of items that has the total volume at most \(U\), total weight at most \(W\) and the total value is maximized. The dynamic programming algorithm has a runtime of \(\mathcal O(WUn)\).</p> <p>In the loop variant of the algorithm, it is better to iterate over the items first rather than the weights. Why is that? Furthermore, it is better to iterate decreasing the costs, because</p> <h2 id="summary">Summary</h2> <p>Many intractable problems can be efficiently solved on trees - combining with DFS enforces the computation ordering.</p> <h1 id="bellman-ford-algorithm">Bellman-Ford algorithm</h1> <h3 id="single-source-shortest-path-sssp">Single Source Shortest Path (SSSP)</h3> <p>Given a directed graph of \(n\) vertices and \(m\) edge, find the shortest paths for a vertex pair \((s, t)\).</p> <p>Recently, researchers showed that Dijkstra’s algorithm is the most optimal algorithm possible with a <em>specially</em> designed heap.</p> <p>Would dynamic programming work for SSSP? \(dp[u][i]\) represents a path from \(s\) to \(u\) using at most \(i - 1\) edges. This simply is the Bellman-Ford algorithm.</p> <h1 id="floyd-warshall-algorithm">Floyd-Warshall algorithm</h1> <p>All pair shortest path algorithm in \(O(n^3)\).</p> <h1 id="traveling-salesman-problem">Traveling Salesman Problem</h1> <p>Given.a complete graph of \(n\) vertices, every edge has a cost \(l_e\). The goal is to find a path that visits every node exactly once while minimizing the total cost. It is an NP-hard problem where the brute-force algorithm takes \(\\mathcal O(n!)\) - enumerating all possible permutations of nodes.</p> <p>There is a dynamic programming solutions of the order \(O(2^n n^2)\). This is computable with the modern computers upto \(n = 15\). We discuss the framework here -</p> <ul> <li> <p><strong>States</strong> - At node \(u\), set of all unvisited states - minimum cost to finish the rest of the task. The number of states is \(\mathcal O(n2^n)\).</p> </li> <li> <p><strong>Decision-making</strong></p> </li> <li> <p>What will be the next node to visit? If we choose \(v \in S\), then the next state will be $$(v, S - {v})$</p> </li> </ul> <p>This can be implemented using a bitmask for representing the state - saves space and is faster.</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[A collection of ideas for design algorithms and analyzing them.]]></summary></entry><entry><title type="html">Tabletop Manipulation Algorithms</title><link href="https://sudhansh6.github.io/blog/Tabletop-Rearrangement/" rel="alternate" type="text/html" title="Tabletop Manipulation Algorithms"/><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Tabletop-Rearrangement</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Tabletop-Rearrangement/"><![CDATA[<h2 id="problem">Problem</h2> <p>Given a table top with various objects, their initial positions and final goal positions, we want to find the algorithm to rearrange the objects with the <strong>least number of actions</strong> and <strong>low buffer space</strong>.</p> <h3 id="buffers">Buffers</h3> <p>If one object is obstructing the goal position of the other, we would want to use <em>buffer space</em> to free-up the goal space. In a brute force manner, we can place everything on the side and put objects in their goals states. We want to optimize the number of actions, and doing this doesn’t do well in that scenario.</p> <p>The problem in the general case reduces to traveling salesman problem or the vertex cover problem, which are both NP-hard. We want to find a good approximation algorithm to get the least amount of actions.</p> <p>We also want real-time execution time because we do not want the algorithm to run for a long time.</p> <h3 id="variations">Variations</h3> <ol> <li> <p>Mobile and mobile base</p> </li> <li> <p>Disk objects and different shape objects</p> </li> <li> <p>stacking vs not stacking objects</p> </li> <li> <p>External buffer or not</p> </li> <li> <p>Single vs multiple tables</p> </li> <li> <p>Overlaps vs non-overlapping objects</p> </li> </ol> <h3 id="search-problem">Search Problem</h3> <p>The problem can be converted to a search problem where the search space is a tree representing all the possibilities of movements of objects (to goal positions) possible with the current configuration. The actions would be of the form “move object ‘i’ to buffer”, “move object ‘i’ to goal position”, etc. At each state, we have at most \(2n\) actions (if there are \(n\) objects on the table) - moving each object to buffer or goal.</p> <p>Obviously, this is a very high-dimensional space, and rudimentary search algorithms would not fare well if used directly.</p> <p>Let us now see some algorithms that try to tackle this.</p> <h2 id="trlb">TRLB</h2> <p><a href="https://arxiv.org/abs/2110.12325">Fast High-Quality Tabletop Rearrangement in Bounded Workspace</a> does the following -</p> <ol> <li> <p>Calculate a primitive plan by assuming there is always a feasible buffer space on the table (assume you have a second table to place objects)</p> </li> <li> <p>Try executing this plan, and assign buffer as we go along the path.</p> </li> <li> <p>If we fail to assign a buffer, we add a node to mark this search path.</p> </li> <li> <p>We repeat the procedure by finding a primitive plan from this stage until we find a feasible path.</p> </li> </ol> <p>The goal of this algorithm is to quickly calculate the solutions but it has a suboptimal traveling cost (example, robot mobile base needs to move a lot). Note that this is a non-deterministic algorithm, since we calculate random primitive plans.</p> <h2 id="orla">ORLA*</h2> <p>Aims to calculate the optimal path without considering the time as a heuristic. It essentially is \(A^*\) adapted to the table-top rearrangement problem - \(f(n) = g(n) + h(n)\) where \(g(n)\) is the travel cost from start to current node and \(h(n)\) expected travel cost from current node to goal.</p> <p>It results in an optimal plan but has a very long execution time. Another point to note is that ORLA* considers a mobile base which not many papers have considered previously.</p> <h2 id="our-research">Our Research</h2> <p>We aim to build an algorithm that gives the optimal solution and also executes fast. We assume mobile base manipulation robot with arbitrary shape objects and no external buffer.</p> <h3 id="approach-1">Approach 1</h3> <p>Perform search in TRLB using MCTS to find the most optimal search plan. Basically, execute TRLB for a fixed time to get multiple feasible paths. TRLB stops as soon as it finds a feasible path, but we execute it until we get a fixed number of possible paths.</p> <h3 id="bit">BIT*</h3> <p>We initially find a suboptimal path using TRLB* - this is fast.</p> <h2 id="future-directions">Future Directions</h2> <ol> <li> <p>Extend the algorithms for multi-agent scenarios where the problem becomes much harder.</p> </li> <li> <p>Learn conflict detection in TRLB or the modified algorithm to do better backtracking - one of the key ideas for combinatorial search algorithms. This can be done via SAT solving - learn conflicts as new constraints. This is particularly important when the number of objects on the table is high with a high density.</p> </li> </ol>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[Various methods for optimal mobile tabletop rearrangement - rearrange objects on the table with the least cost.]]></summary></entry><entry><title type="html">Evaluating interactions in music</title><link href="https://sudhansh6.github.io/blog/Evaluting-Music-with-RL/" rel="alternate" type="text/html" title="Evaluating interactions in music"/><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Evaluting-Music-with-RL</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Evaluting-Music-with-RL/"><![CDATA[<p>How do we identify how well two pieces of music complement each other? From notes, scales, rhythms, chords, time signatures, and many more abstract concepts associated with music, it is an interesting problem to capture the interactions between music pieces. This problem requires concepts from signal processing</p> <p>How is music represented? A Mel spectrogram is used to represent frequencies across time with amplitudes for audio files. A MIDI file is a tabular description of beats, positions, pitch, durations and instruments. These tabular instances can easily be converted to vector representations using one-hot encodings and feature normalization.</p> <p>Using such representations, the paper <a href="https://arxiv.org/abs/2207.06983">Multitrack Music Transformer</a> attempts to capture the interactions with attention mechanisms in a transformer. To understand this better, let us look at some theory.</p> <h3 id="total-information-flow">Total Information Flow</h3> <p>The total information flow is the sum of transfer entropies from \(X\) to \(Y\) and \(Y\) to \(X\).</p> \[T_{X \to Y} + T_{Y \to X} = H(X \vert \bar X) + H(Y \vert \bar Y) - H(XY \vert \bar{XY})\] <p>If the combined sequence \(XY\) does not make sense musically, then it will have a higher entropy, leading to lower total information flow. This concept is explored in depth in this context in this paper - <a href="https://arxiv.org/abs/2402.06810">Evaluating Co-Creativity using Total Information Flow</a>.</p> <h3 id="conditional-entropy">Conditional Entropy</h3> <p>Represented as \(H(Y \vert X)\) it measures how unpredicatble \(Y\) is, given that we have \(X\). How is this useful? A pre-trained music transformer is used to model \(p(X \vert \bar X)\) which represents the probability of a particular <em>token</em> (next part of music) after seeing a particular set of tokens (music till that point).</p> <h2 id="research">Research</h2> <p>Using the features from a MIDI file directly would not yield the best results. It composes of monotonically increasing data (beat), categorical features (instruments) and repeated values (position).</p> <p>This problem can also be posed as a Reinforcement Learning Problem - using total information flow as a reward. For example, <a href="https://arxiv.org/abs/2002.03082">RL-Duet: Online Music Accompaniment Generation Using Deep Reinforcement Learning</a> formulated a Deep RL algorithm for online accompaniment generation. The generation agent learns a policy to generate a musical note (action) based on previously generated context (state). RL has potential for real-time human-machine duel improvisation.</p> <h1 id="rl-duet">RL-Duet</h1> <p>What is the goal? Create an agent that can generate music <em>interactively</em> with a human. Again, this is done via Symbolic MIDI pitch + beat. The earlier approaches used GANs which have large data requirements and usntable training. Another approach, using Gibbs sampling, iteratively modifies the music fragments based on the previous context. However, this approach cannot be done in an online fashion.</p> <p>Typical approaches in reinforcement learning for sequence generation use maximum likelihood estimation. However, to improve the perceptual quality, global coherence and harmony, specific hand-crafted music rules-based rewards work much better.</p> <p>The work in RL-Duet captures the horizontal temporal consistency and vertical harmony relations for the reward function of the RL-agent. The current state is the previously generated context (by both human and the agent) and the action as mentioned before is symbolic MIDI (note and pitch). This involves horizontal view (like linear bi-directional language modeling in NLP), vertical part, joint modeling and hand-crafted rewards.</p>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[Capturing the quantifiers required to comment on how well two pieces of music complement each other.]]></summary></entry></feed>