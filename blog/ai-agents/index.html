<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>AI Agents | Sudhansh Peddabomma</title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="Everyone is talking about agents. But what is an agent? Is it just a buzzword being thrown around? This article talks deeply about this issue along with the technical ideas associated."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/ai-agents/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> <script src="/assets/js/chat.js?e73db4280bae3cbae4d78219277155b9"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Sudhansh Peddabomma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Articles</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">AI Agents</h1> <p class="post-meta">January 6, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/notes"> <i class="fa-solid fa-tag fa-sm"></i> Notes</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>The content on this article is based on the <a href="https://github.com/pearls-lab/ai-agents-course" rel="external nofollow noopener" target="_blank">course</a> by <a href="https://prithvirajva.com" rel="external nofollow noopener" target="_blank">Prof. Prithviraj</a> at UC San Diego. This <a href="https://www.kaggle.com/whitepaper-agents" rel="external nofollow noopener" target="_blank">whitepaper</a> about AI Agents by Google is also a good read.</p> <h2 id="what-is-an-agent">What is an agent?</h2> <p>Agent is an entity with <em>agency</em>. <a href="https://minecraft.wiki/w/Agent" rel="external nofollow noopener" target="_blank">A Minecraft agent?</a>. Agents see applications within the workspaces in the form of workflow automations, household or commercial robotics, software development and personal assistants. Generally, the theme is that <em>agents</em> take actions.</p> <p>Historically, the use of agents started in the early 1900s in the field of control theory. They were used for dynamic control of flight systems, and in 1940s it expanded to flight guidance, etc. By the 1950s the concepts of MDPs and dynamic programming were being expanded to many use cases. Surprisingly, one of the first natural language chatbots, Eliza, was created as a psychotherapist simulator in the 1960s! Finally, reinforcement learning became a field of study in the 1990s for sequential decision making.</p> <h2 id="sequential-decision-making">Sequential Decision Making</h2> <p>These tasks are different from other ML problems like classification. A model that has an accuracy of 99% at each step, has a cumulative accuracy of ~30% after 120 steps!</p> <p>These problems are formalized as a Markov Decision Process - an <strong>agent</strong> performs <strong>actions</strong> in an <strong>environment</strong>, and in turn receives <strong>rewards</strong> as feedback. These configurations are distinguished as <strong>states</strong>, and the whole process can be seen as sequential decision making.</p> <p>The core components of an agent, often agreed on, are</p> <ul> <li> <strong>Grounding</strong> - Language is anchored to <em>concepts</em> in the world. Language can be grounded to different forms of information systems - images, actions and cultural norms. <ul> <li>Agency (ability to act) - At each state, an agent needs to have multiple choices to act. <em>If an agent has to select what tools to use but there’s always only one tool, is that agency?</em> The action space has to be well-defined to look for agency. Although there is a single tool call, different parameters for the tool call can probably be considered as different actions. Actions can be defined as something the agent does and changes the environment. The distinction between an agent and environment is not very clear in many cases. Although, our approximations mostly serve us well.</li> <li>Planning (Long horizon)</li> <li>Memory - <ul> <li>Short-term - What is the relevant information around the agent that it needs to use to act now</li> <li>Long term - What information has the agent already gathered that it can retrieve to take an action</li> </ul> </li> <li>Learning (from feedback) - Doesn’t necessarily always mean <em>backpropagation</em>.</li> </ul> </li> <li> <strong>Additional</strong> - <ul> <li>Embodiment (physically acting in the real-world). <em>Embodied hypothesis</em> - embodiment is necessary for AGI.</li> <li>Communication - Can the agent communicate its intentions to other agents. Very necessary pre-requisite for multi-agent scenarios.</li> <li>World Modeling - Given the state of the world and an actions, predict the next state of the world. Is <a href="https://deepmind.google/technologies/veo/veo-2/" rel="external nofollow noopener" target="_blank">Veo</a>/<a href="https://sora.com" rel="external nofollow noopener" target="_blank">Sora</a> a world model? It is an attempt for world model since they have no verifiability. <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/" rel="external nofollow noopener" target="_blank">Genie</a> is another such attempt. So is <a href="https://genesis-embodied-ai.github.io" rel="external nofollow noopener" target="_blank">Genesis</a> - this is much better if it works.</li> <li>Multi-modality - The clean text on the internet is only a few terabytes, and our models have consumed it (took use 2 decades though). YouTube has 4.3 Petabytes of new videos a day. CERN generates 1 Petabyte a day (modalities outside vision and language). Some people believe this form of scaling is the way to go. There are more distinctions -</li> </ul> </li> </ul> <table> <thead> <tr> <th>Model</th> <th>AI System</th> <th>Agent</th> </tr> </thead> <tbody> <tr> <td>GPT-4</td> <td>ChatGPT</td> <td>ChatGPT computer use</td> </tr> <tr> <td>Forward passes of a neural net</td> <td>Mixing models together</td> <td>Has agency</td> </tr> </tbody> </table> <p>It is important to remember that not every use case needs an agent and most use cases just need models or AI systems. <em>Occam’s razor</em>.</p> <h1 id="simulated-environments-and-reality">Simulated Environments and Reality</h1> <p>Why do we need simulations? Most tasks have many ways of completing them. There is no notion of <em>global</em> optimal solutions ahead of time but usually known once the task is complete.</p> <p>The agent needs to explore to find many solutions to compare and see what is the most efficient. However, exploration in the read world is expensive - wear and tear of robots, excessive compute, danger to humans, etc.</p> <p>Simulations offer an easy solution to these problems. Assign a set of rules, and let a world emerge. One of the early examples of this is <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life" rel="external nofollow noopener" target="_blank">Conway’s Game of Life</a> which theorized that complicated behaviors can emerge by just a few rules.</p> <p>From an MDP perspective, a simulation contains \(&lt;S, A, T&gt;\) where</p> <ul> <li>\(S\) is the set of all states. It consists of propositions that are true/false. Example: You are in a house, door is open, knife in drawer</li> <li>\(A\) is the set of all actions. Example: Take knife from drawer, walk through door</li> <li>\(T\) is the transition matrix - (You are in the house, you walk out of the door) -&gt; You are outside the house.</li> </ul> <p>A simulation need not have an explicit reward.</p> <h2 id="sim2real-transfer">Sim2Real Transfer</h2> <p>The ability of an agent trained in simulation transfer to reality is dependent on how good the model extrapolates out of distribution. With the current stage of agents, the simulation is made as close to reality as possible to reduce the Sim2Real gap.</p> <p>How do we measure closeness to reality? The tasks in the real world have different types of complexities -</p> <ol> <li>Cognitive complexity - Problems that requires long chains of <em>reasoning</em> - puzzles, math problems or moral dilemmas</li> <li>Perceptive complexity - Requires high levels of vision and/or precise motor skills - bird watching, threading a needle, Where’s Waldo</li> </ol> <p>Examples of simulations -</p> <ol> <li>Grid world - low cognitive and almost zero perceptive. However, this idea can arbitrarily scale to test algorithms for their generalization potential in controllable settings.</li> <li>Atari - low perceptive, medium cognitive. Atari games became very popular in 2013, when Deepmind released their <a href="https://arxiv.org/pdf/1312.5602" rel="external nofollow noopener" target="_blank">Deep Q-Net</a> paper that achieved human level skills on these games.</li> <li> <a href="https://en.wikipedia.org/wiki/Zork" rel="external nofollow noopener" target="_blank">Zork</a>, <a href="https://www.nethack.org" rel="external nofollow noopener" target="_blank">NetHack</a> - low perceptive, high cognitive. These are configurations or worlds that you purely interact with text. The worlds are actually so complex that there is no agent that is able to finish the challenge!</li> <li> <a href="https://cs.stanford.edu/people/jcjohns/clevr/" rel="external nofollow noopener" target="_blank">Clevr simulation</a> - medium perceptive, low cognitive - This simulation generates images procedurally with certain set of objects and has reasoning questions for each image.</li> <li> <a href="https://ai2thor.allenai.org" rel="external nofollow noopener" target="_blank">AI2 THOR</a> - medium perceptive, medium cognitive. Worlds with ego-centric views for robotics manipulation and navigation simulations</li> <li> <a href="https://arxiv.org/pdf/2407.18901" rel="external nofollow noopener" target="_blank">AppWorld</a> - medium perceptive, medium cognitive. A bunch of different apps that you would generally use in daily life. The agents can access apps, and the simulation also has human simulators. This simulation is one that is closest to reality in the discussed so far!</li> <li> <a href="https://www.minecraft.net/en-us" rel="external nofollow noopener" target="_blank">Minecraft</a> - medium perceptive, high cognitive. A voxel based open-world game that lets players take actions similar to early-age humans.</li> <li> <a href="https://mujoco.org" rel="external nofollow noopener" target="_blank">Mujoco</a> - high perceptive, low cognitive. It is a free and open source physics engine to aid the development of robotics.</li> <li> <a href="https://ai.meta.com/research/publications/habitat-a-platform-for-embodied-ai-research/" rel="external nofollow noopener" target="_blank">Habitat</a> - high perceptive, medium cognitive. A platform for research in embodied AI that contains indoor-world ego-centric views similar to AI2 THOR, but with much better graphics. They have recently added sound in the environment too!</li> <li> <p>High perceptive, high cognitive - Real world, and whoever gets this simulation right, wins the race to AGI. It requires people to sit down and enumerate all kinds of rules. Game Engines like Unreal and Unity are incredibly complex, and are the closest we’ve gotten.</p> <p>Some researchers try to “learn” the simulations from real-world demonstrations.</p> </li> </ol> <p>In each of these simulators, think of the complexity and reward sparsity in the environment. It is easy to build a simulator that gives rewards at a goal state than the one that gives a reward for each action. There are some open-lines of research in this domain -</p> <ol> <li>Which dimensions of complexity transfer more easily? Curriculum learning</li> <li>Can you train on lower complexity and switch to a higher complexity?</li> <li>Can we learn the world model holy grail?</li> </ol> <h2 id="how-to-make-simulations">How to make simulations?</h2> <p>As we’ve seen, simulations can range from games to real-world replications with physics involved. Most simulations are not designed keeping AI in mind. However, with the current state of AI, this is an important factor to keep in mind.</p> <p>Classical environments like in Zork/AI2 Thor/Mujoco have something known as <strong>PDDLs</strong>. Some simulations are built through AI, like <em>AI Dungeon</em> that spins up worlds for role-play games.</p> <h3 id="planning-domain-definition-language-pddl">Planning Domain Definition Language (PDDL)</h3> <p>Standard encoding for classic planning tasks. Many specific languages for creating simulations have similarities with PDDL.</p> <p>A PDDL Task consists of the following</p> <ul> <li>Objects - things in the world that interest us</li> <li>Predicates - Properties of objects that we are interested in, can be true or false</li> <li>Initial state - The state of the world that we start in</li> <li>Goal specification - Things that we want to be true</li> <li>Actions/Operators - Ways of changing the state of the world.</li> </ul> <p>These are split across two files - domain and problem <code class="language-plaintext highlighter-rouge">.pddl</code> files.</p> <p>Classic symbolic planners read PDDLs and give possible solutions. Checkout the <a href="https://planning.wiki/ref/planners/atoz" rel="external nofollow noopener" target="_blank">Planning.wiki</a>. In many cases these planners are used over reinforcement learning due to lack of algorithmic guarantees.</p> <p>There were other attempts</p> <h1 id="search-for-planning-in-simulations">Search for Planning in Simulations</h1> <h1 id="reinforcement-learning-abridged">Reinforcement Learning (Abridged)</h1> <p>We have been building chronologically, and next in line is basic RL.</p> <h2 id="terminology">Terminology</h2> <ul> <li> <p>A <strong>policy</strong> is a function that defines an agent’s behavior in the environment. Finding the optimal policy is known as the <em>control</em> problem.</p> <p>Formally, a policy is a distribution over actions for a given state.</p> \[\pi(a \vert s) = P(A_t = a \vert S_t = s)\] <p>Due to the Markov property, the policy depends only on the current state and not history. In cases where the history is needed, the state is modified to embed the history to evade this dependence.</p> <p>For a given MDP, an optimal policy always exists that achieves the maximum expected reward at every state. (This is proved using the compactness properties of the state-action space using the Banach’s fixed point theorem - refer to <a href="/blog/cse291g">these notes</a> for more details).</p> </li> <li> <p>The <strong>value function</strong> determines how good each state or actions is. Finding the optimal value functions is known as the <em>prediction</em> problem.</p> <p>There are two functions that capture the value of the current state/action of the agent</p> <ol> <li>\(V_\pi(s) = E_\pi[R_{t + 1} + \gamma V_\pi(S_{t + 1} \vert S_t = s]\) - The expected reward obtained by a policy \(\pi\) starting from a given state \(s\).</li> <li>\(Q_\pi(s, a) = E_\pi[R_{t + 1} + \gamma Q_\pi(S_{t + 1}, A_{t + 1}) \vert S_t = s, A_t = a]\) - The expected reward for a given state \(s\) upon taking a certain action \(a\). These two functions are closely related to each other, and knowing one determines the other. In general, these functions (matrices, in discrete spaces) do not have a closed form solution.</li> </ol> </li> <li> <p>A <strong>model</strong> is the agent’s representation of the environment</p> </li> </ul> <p>RL algorithms are classified under various categories</p> <ul> <li>Model free and Model-based</li> <li>Off-policy and on-policy</li> </ul> <h1 id="its-all-dynamic-programming">It’s all Dynamic Programming?</h1> <p>The core theory of RL, the properties of the Bellman equation (refer to these notes for more details) and the recursive nature of the value functions, ties to dynamic programming. This insight helps us design algorithms to solve the problems in RL (Prediction and Control).</p> <p>We ideally want the solution to the control problem since we want to define the optimal behavior of an agent in the environment. To do so, the prediction problem is a pre-cursor that needs to be solved.</p> <h2 id="policy-evaluation">Policy Evaluation</h2> <p>The prediction problem involves calculating the rewards obtained by a given policy \(\pi\). The expectation can be written as</p> \[\begin{align*} V_\pi(s) &amp;= \sum_{a \in A} \pi(a \vert s) Q_\pi(s, a) \\ &amp;= \sum_{a \in A} \pi(a \vert s) (R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’)V_\pi(s’)) \end{align*}\] <p>where \(T\) is the state-transition function of the MDP.</p> <p>Ideally, we want to find the optimal policy that reaches the maximum value at every state.</p> \[\begin{align*} V*(s) &amp;= \max_\pi V_\pi(s) \\ Q*(s, a) &amp;= \max_\pi Q_\pi(s, a) \\ \pi*(s) &amp;= \arg\max_\pi Q_\pi(s, a) \end{align*}\] <p>These can be determined (iteratively) from the Bellman’s Optimality equations -</p> \[\begin{align*} Q*(s, a) &amp;= R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’) V*(s’) &amp;= R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’) \max_{a’} Q*(s’, a’) \end{align*}\] <blockquote> <p>Note the subtlety here. Although Bellman’s optimality equations aren’t seemingly much different from the Bellman’s equations, there is a very strong claim the optimality equations make - they claim that the existence a policy that gets the maximum possible value at every state. The existence is not a trivial claim and it is the proof I referred to in the terminology. Furthermore, it turns out that a deterministic policy is just as good as a stochastic one.</p> </blockquote> <p>So how do we find these optimal values?</p> <h2 id="policy-iteration">Policy Iteration</h2> <p>Given a policy \(\pi\), we iteratively update its actions at each state to improve its value. Remember that we can <em>evaluate a policy</em> to get its value function.</p> <p>At each state, if there is an action \(a\) such that \(Q_\pi(s, a) &gt; Q_\pi(s, \pi(s))\), then the policy is <em>strictly improved</em> by updating \(\pi(s) \gets a\). In each iteration, we update the actions this way across all the states, and repeat this until the policy does not change.</p> <p>How many iterations should we repeat this for? Because the number of policies is finite (bounded by \(O(\vert A \vert^{\vert S\vert})\), we are guaranteed to reach the optimum. Each iteration costs \(O(\vert S\vert^2 \vert A\vert + \vert S\vert^3)\). Although these numbers seem big, in practice, this algorithm typically takes only a few iterations.</p> <h2 id="value-iteration">Value Iteration</h2> <p>It is similar to the policy iteration algorithm, but focuses on recursively improving the value function instead.</p> <p>We start out with a random value function, and at each state, we choose the action that gives the maximum value (with the currently set values across the states). Once the values are updated across all the states, the process is repeated until the improvement is below a threshold. At the end of the iterations, we can extract the policy from the value function deterministically (the algorithm itself is a hint to this).</p> <p>Although this seems very similar to the policy iteration algorithm, there are some key differences. We do need to reach the optimal value function to get the optimal policy - if it is close enough, we can get the optimal policy. Also, the iterations <em>asymptotically reach</em> the optimal policy and there is no upper bound to this.</p> <h2 id="limitations">Limitations</h2> <p>Although dynamic programming approaches have theoretical guarantees, they are not widely used in practice. Why?</p> <p>The curse of dimensionality. These algorithms are have very limited applicability in practice. Many environments have a very large set of states and actions. In some cases, these could be continuous as well. The iteration algorithms are computationally infeasible in such cases.</p> <h1 id="model-free-rl">Model-free RL</h1> <p>Since we cannot look at every state action combination, we resort to approximations. We explore the world (say, with Monte Carlo sampling) and build experiences to heuristically guide the policy.</p> <p>The goal is to optimize the value of an unknown MDP through experience based learning. Many real world problems are better suited to be solved by RL techniques over dynamic programming based approaches (the iterative algorithms).</p> <h2 id="monte-carlo-control">Monte Carlo Control</h2> <p>It suggests greedy policy improvements over \(V(s)\) requires a model of the MDP. However, improvement over \(Q(s, a)\) is a model-free method! (This was the importance of defining both \(V_\pi(s)\) and \(Q_\pi(s, a)\)).</p> <p>The \(Q\) function can be learned from experiences. These concepts are the foundation concepts for deep RL!</p> <p>This approach can be thought of as a hybrid approach between policy and value iteration. In these exploration/sampling based techniques, it is important to gather data about the model through exploration and not be greedy. This forms the basis of <strong>epsilon-greedy</strong> algorithms.</p> <h2 id="epsilon-greedy-exploration">\(\epsilon\)-greedy exploration</h2> <p>At each state, with a certain probability we choose to exploit (greedily take the action based on the optimal policy we developed so far) or explore (take a random action to sample more outcomes)</p> \[\pi(a \vert s) = \begin{cases} \epsilon/m + 1 - \epsilon &amp; a* = \arg\max_{a \in A} Q(s, a) \\ \epsilon/m \end{cases}\] <p>This class of algorithms also has some theory but it is limited. This core trade-off between exploration/exploitation is still a core element in the modern RL algorithms.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/data-systems-for-ml/">Data Systems for Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/NumAn/">Numerical Analysis Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/rl-theory/">Reinforcement Learning Theory</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ipl/">IPL Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/dbms/">DiBS Notes</a> </li> </div> <script>document.querySelectorAll("#table-of-contents a").forEach(function(e){e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substring(1);document.querySelectorAll(".content-section").forEach(function(e){e.classList.add("hidden")});var n=document.getElementById(t);n&&n.classList.remove("hidden")})});</script> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Sudhansh Peddabomma. Last updated: February 07, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0K9MLG0V24");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <div class="chat-toggle-container"> <button id="chat-toggle-btn" class="chat-toggle-btn"> <i class="fas fa-comments"></i> </button> </div> <div id="chat-window" class="chat-window"> <div class="chat-header"> <h5 class="mb-0">Talk to my AI</h5> <button id="close-chat" class="btn-close"> <i class="fas fa-times"></i> </button> </div> <div id="chat-messages" class="chat-messages"></div> <div class="chat-input-container"> <input type="text" id="chat-input" class="form-control" placeholder="Type a message..."> <button id="send-btn" class="btn btn-primary"> <i class="fas fa-paper-plane"></i> </button> </div> </div> </body> </html>