<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Data Systems for Machine Learning | Sudhansh Peddabomma </title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="Prompting ChatGPT is not enough. To build large-scale AI systems, it is imperative to understand how to design the proper systems to optimize all the computations. The following blog is a deep-dive into system/data design for Machine Learning frameworks."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?d0883cd261a1387cbdd04fa5cb9cc690" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/data-systems-for-ml/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/chat.js?e73db4280bae3cbae4d78219277155b9"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Sudhansh Peddabomma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Articles </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Data Systems for Machine Learning</h1> <p class="post-meta"> Created in January 08, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/notes"> <i class="fa-solid fa-tag fa-sm"></i> Notes</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>Machine Learning systems have played a pivotal role in the rapid adaptation of Ai in the world today. This domain is essential for solving future problems and also making the current architectures more efficient. This is crucial considering that companies are reactivating nuclear power plants to power AI in the real-world.</p> <p>Along with the progress in AI from small neural networks to large language models, there has been a development in the size of datasets as well. Big data arrived, and AI today relies on these internet-scale datasets. After all, doesn’t ChatGPT just do pattern-matching in the internet?</p> <p>Moreover, the compute capabilities have been scaling exponentially. Just last year (2024), NVIDIA released a new super-chip architecture <a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing" rel="external nofollow noopener" target="_blank">Blackwell</a> that has 97 billion transistors that can reach up to 1.4 exa-flops! The largest super-computer was barely able to reach 1 exa-flop. All this power in the palm of your hand…</p> <p>Richard Sutton once said, search and learning can scale unparalleled with growing computation power.</p> <p>Consider the year 2012 — AlexNet made waves showing SOTA capabilities with images. They use Stochastic Gradient Descent, dropout, convolution networks and initialization techniques. Without ML systems (CUDA, etc), the code would have been 44,000 lines with days of training! With these systems (Jax, PyTorch, TensorFlow) in place, you can achieve the same result in 100 lines within hours of training.</p> <h3 id="in-practice">In Practice</h3> <p>In industry, problems are typically of the form - improve the self-driving car’s pedestrian detection to be X-percent accurate at Y-ms latency budget. For an ML engineer is, the general approach is to design a better model with better learning efficiency followed by hyper-parameter running, pruning, distillation. An ML systems engineer would take the best model by ML researchers, specialize the implementation to target the H/W platform to reduce latency. <em>Streamlining the entire process from development to deployment</em>.</p> <h2 id="overview">Overview</h2> <p>From ad-hoc methods having diverse models and optimization algorithms with various data pre-processing techniques - we have arrived at an optimal algorithm that is <em>iterative and convergent</em>. As our models have become more and more specialized, the computation resources scaled exponentially.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17363053041704.jpg" alt=""></p> <p>Through the course of this article, we will cover deep learning basics, computational graphs, Autodiff, ML frameworks, GPUs, CUDA and collective communication.</p> <p>There are more related topics to the ones discussed here</p> <ul> <li>ML for systems</li> <li>ML Hardware design</li> </ul> <p>Unfortunately, the textbook for this content is just miscellaneous research papers.</p> <h1 id="background">Background</h1> <h2 id="dl-computation">DL Computation</h2> <p>The idea is to concatenate composable layers</p> \[\theta^{(t + 1)} = f(\theta^{(t)}, \nabla_L(\theta^{(t)}, D^{(t)})\] <p>A <strong>model</strong> is a parameterized function that describes how we map inputs to predictions. The parameters are optimized using optimization methods like <strong>SGD</strong>, Newton methods, etc. A <strong>loss function</strong> guides the model to give feedback on how well the model is performing.</p> <p>Having these basic definitions, we will build abstractions to map all the models being used today. It is not possible to build systems to support all models. A quick refresher of important models</p> <ul> <li> <strong>CNNs</strong> - Learnable filters to convolute across images to learn spatial features. The top 3 breakthrough architectures were - AlexNet, ResNet, U-Net. What are the important components in CNNs? <ul> <li>Convolution (1D, 2D, 3D)</li> <li>Matmul</li> <li>Softmax</li> <li>Element-wise operations - ReLU, add, sub, pooling, normalization, etc.</li> </ul> </li> <li> <strong>Recurrent Neural Networks</strong> - Many problems in nature are many-to-many. RNNs maintain an internal state that is updated as a sequence is processed. Arbitrary inputs and outputs can be generated, and any neural network can be used in the RNN architecture. The top 3 breakthrough architectures were - Bidirectional RNNs, LSTMs, GRU. What are the important components in RNNs? <ul> <li>Matmul</li> <li>Element-wise non-linear - ReLU, Sigmoid, Tanh</li> <li>Underlying MLP RNNs have a problem of forgetting (\(0.9*0.9*… \approx 0\)). Additionally, they lack <strong>parallelizability</strong> - both forward and backward passes have \(O(sequence length)\).</li> </ul> </li> <li> <strong>Transformers</strong> (Attention + MLP) - Treat representations of each element in the sequences as queries to access and incorporate information from a set of values. Transformers have an encoder part (BERT most famous) and a decoder part (GPT most famous). Along with these, DiT is one of the top 3 models. What are the important components in Transformers? <ul> <li>Attention - Matmul, softmax, Normalization</li> <li>MLP</li> <li>Layernorm, GeLU, etc.</li> </ul> </li> <li> <strong>Mixture of Experts</strong> - Voting from many experts is better than one expert. Latest LLMs are mostly MoEs - Grok, Mixtral, Deepseek-v3. A router (Matmul, softmax) is the novel component in MoE - it makes system design difficult.</li> </ul> <h2 id="machine-learning-systems">Machine Learning Systems</h2> <p>As mentioned before, the three pillars for the systems are data, model and compute. The foal is to express as manny as models as possible using one set of programming interface by connecting math primitives.</p> <h3 id="computational-dataflow-graph">Computational Dataflow Graph</h3> <p>A representation to show data flow in programs. A <strong>node</strong> represents the computation (operator) and an <strong>edge</strong> represents the data dependency (data flowing direction). A node can also represent the input/output tensor of the operator.</p> <h3 id="example-deep-learning-with-tensorflow-v1">Example: Deep learning with TensorFlow v1</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tinyflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span> <span class="c1"># Forward declaration 
</span><span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span><span class="n">tf</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">reduction_indices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># Loss function declaration 
</span><span class="n">W_grad</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">,</span> <span class="p">[</span><span class="n">W</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Automatic differentiation
</span><span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">assign</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">W_grad</span><span class="p">)</span> <span class="c1"># SGD update rule 
</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">})</span> <span class="c1"># Real-execution happens here
</span></code></pre></div></div> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17364786998993.jpg" alt=""></p> <p>This DAG representation opens up all possibilities of optimizations. However, creating such a graph doesn’t allow flexibility - once a graph is defined, it cannot be changed based on the input.</p> <h3 id="example-pytorch">Example: PyTorch</h3> <p>PyTorch also uses computational graphs, but it creates it on the fly. Previously, we had defined the graph and then executed it. Symbolic declaration vs imperative programming. Define-then-run vs Define-and-run. C++ vs Python.</p> <p>What are the pros and cons?</p> <table> <thead> <tr> <th> </th> <th>Good</th> <th>Bad</th> </tr> </thead> <tbody> <tr> <td>Symbolic</td> <td>Easy to optimize, much more efficient (can be 10x faster)</td> <td>The way of programming can be counter-intuitive, hard to debug and less flexible</td> </tr> <tr> <td>Imperative</td> <td>More flexible, easy to program and debug</td> <td>Less efficient and more difficult to optimize</td> </tr> </tbody> </table> <p>How does TensorFlow work in Python then? Tensorflow has Python as the interface language.</p> <p>Apart from these two famous frameworks, there were more like Caffe, DyNet, mxnet (has ability to switch between both), etc. Recently, Jax (derived from Tensorflow) has been getting more popular.</p> <h3 id="just-in-time-jit-compilation">Just-in-time (JIT) compilation</h3> <p>Ideally, we want define-and-run during development and define-then-run during deployment. However do we combine both? PyTorch introduced a deploy mode through a decorator <code class="language-plaintext highlighter-rouge">torch.compile()</code>. So is there an issue with JIT? It creates only static graphs, and cannot work with conditionals or loops in the code.</p> <h3 id="static-vs-dynamic-models">Static vs Dynamic models</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17364797895738.jpg" alt=""> Static graphs are defined and optimized only once. The execution follows a defined computation. On the other hand, dynamic graphs depend on the input. It is difficult to express complex flow-control logic and debug. The implementation is also difficult.</p> <p>As seen above, LSTMs are trying to replace the dynamics in the natural language problem.</p> <p><strong>How to handle dynamics?</strong></p> <ul> <li>Just do Define-and-run and forget about JIT - most popular unforunately :(</li> <li>Introduce Control Flow Ops - <ul> <li>Example: Switch and Merge. This can be added a computational primitive in the graph and introduce dynamics in the graph.</li> <li>These ideas are natural across all programming languages - conditionals and loops. However, the problem with this approach is that graphs becomes complex, and more importantly, how does we do back propagation? What is the gradient of “switch”? TensorFlow team has been working on this.</li> </ul> </li> <li>Piecewise compilation and guards - This approach is better adopted than control flow. <ul> <li>Case 1: A graph accepting input shapes of \([x, c1, c2]\) where \(x\) is variable. The solution is to compile for different values of \(x\) (powers of 2).</li> </ul> </li> </ul> <p>So far, we have seen representations that express the forward computations using primitives. But, how do we represent backward computations?</p> <h1 id="autodiff-ad">Autodiff (AD)</h1> <p>Derivative can be taken using the first order principles. However, this approach can be slow since we have to evaluate the function twice \(f(\theta + \epsilon) , f(\theta)\) and it is also error prone \(\theta(\epsilon^2)\).</p> <p>To optimize the derivative calculation, we pre store the gradients of primitives and map the derivative chain rules in the computational graph. There are two ways of doing this as well</p> <ol> <li>Calculating the derivative from left (inside) to right (outside) in a network - from inputs to outputs</li> <li>Calculating it from right to left - from outputs to inputs</li> </ol> <p>Both are valid approaches and we will discuss them in detail.</p> <h2 id="forward-mode-autodiff">Forward Mode Autodiff</h2> <p>We start from the input nodes, and derive the gradients all the way to the output nodes. <strong>Cons</strong> - - For \(f: R^n \to R^k\), we need \(n\) forward passes to get the gradients with respect to each input. - However, it is usually the case that \(k = 1\) (loss) and \(n\) is very large.</p> <blockquote> <p>If this is confusing, think of it this way - we want the gradient of output with respect to all parameters to update weights. However, forward mode calculates the gradient of inputs with respect to all parameters.</p> </blockquote> <h2 id="reverse-mode-autodiff">Reverse Mode Autodiff</h2> <p>We define the quantity <em>adjoint</em> \(\bar v_i = \frac{\partial y}{\partial v_i}\). We then compute each \(\bar v_i\) in the reverse topological order of the graph. This way, we can simply do one backward pass to get the necessary gradients.</p> <p>In some scientific scenarios, we can have \(k &gt;&gt; n\) where the forward mode can be more efficient.</p> <blockquote> <p>What are the size bounds of the backward graph as compared to the neural network?</p> </blockquote> <p>We construct backward graphs in a symbolic way to reuse it multiple times.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369099776396.jpg" alt=""></p> <h2 id="backpropagation-vs-reverse-mode-ad">Backpropagation vs. Reverse-mode AD</h2> <p>In old frameworks like Caffe/cuda-convnet, the backward computations were done through the forward graph itself. Newer frameworks like Tensorflow and PyTorch construct the backward graph explicitly. The reasons to do so are -</p> <ol> <li>Explicit graphs allow backward computation with any input values. They have flexibility to even calculate gradient of gradients.</li> <li>Having an explicit backward graph can help optimization!</li> <li>Gradient update rules can be efficiently implemented.</li> </ol> <h2 id="gradient-update-rules">Gradient update rules</h2> <p>Typically done via gradient descent, the weights are updated with the gradients with the following simplified rule</p> \[f(\theta, \nabla_l) = \theta - \eta \nabla_L\] <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369103443367.jpg" alt=""></p> <h1 id="architecture-overview-of-ml-systems">Architecture Overview of ML systems</h1> <p>The aim is to make the systems fast, scalable, memory-efficient, run on diverse hardware, energy efficient and easy to program/debug/deploy. Phew.</p> <p>We have discussed dataflow and Autodiff graphs. However, there are numerous things that can be added to these - graph optimization, parallelization, runtime memory, operator optimizations and compilation.</p> <h2 id="graph-optimization">Graph Optimization</h2> <p>The goal is to rewrite the original graph \(G\) as \(G’\) that is faster.</p> <p>Consider the following motivating example - Typically, convolution is followed by batch normalization. Instead of performing batch normalization, just update the weights in convolution to do everything in one step!</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369110087495.jpg" alt=""></p> <p>Note that some steps can become slower based on the hardware, but you get the general idea.</p> <p>Similarly, in attention calculations, the code is typically written with a concatenated vector of queries, keys and values. This version is optimal - it can be understood with <em>Arithmetic Intensity</em> which the ratio of #operations and #bytes. For example, an addition operation has intensity of \(1/3\) (2 loads and one store). However, fusing multiple arithmetic operations reduces the loads and stores by bringing all variables into memory once, improving the arithmetic intensity.</p> <h3 id="so-how-do-we-optimize-graphs">So how do we optimize graphs?</h3> <p>We write rules or templates for opportunities to simplify graphs. There is also implementation of <em>auto-discovering</em> optimizations in the latest libraries, we shall study these.</p> <h2 id="parallelization">Parallelization</h2> <p>The goal is to parallelize the graph computation over multiple devices. Note that devices can be connected with fast (memory communication NVLink) and slow connections (across GPUs), with up to 10x performance difference. Ideally, we do not want to describe partitioning rules for every new model that comes up. Based on these communication patterns, distributing the tasks is not an easy problem. So, we shall discuss how we partition the computational graph on a device cluster.</p> <h2 id="runtime-and-scheduling">Runtime and Scheduling</h2> <p>How do we schedule the compute, communication and memory in a way that execution is as fast as possible, communication is overlapped with compute and is subject to memory constraints?</p> <h2 id="operator-implementations">Operator Implementations</h2> <p>The goal is this layer is to get the fastest possible implementation of <code class="language-plaintext highlighter-rouge">matmul</code>s, for different hardware, different precision and different shapes.</p> <p>NVIDIA releases a GPU every 2 years, and they have rewrite all operations every time! Notably, previously, models were trained using 32-bit floating points, but now researchers are emphasizing on lower and lower precisions.</p> <p>Now, we shall delve into each of these architectures.</p> <h1 id="operator-optimization-and-compilation">Operator Optimization and Compilation</h1> <p>The goal is maximize arithmetic intensity. In general there are three ways to speed up operators</p> <h3 id="vectorization">Vectorization</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369120762344.jpg" alt=""></p> <p>The right version is faster because of the hardware - cache sizes, etc. Tensorflow and PyTorch have this built-in.</p> <h3 id="refactoring-data-layout">Refactoring data layout</h3> <p>This is again related to how data is stored in memory. For example, C++ stores matrices in row-major order. Accessing columns of a matrix can be 10x slower! Remember this while writing code to lower cache misses and reduce pointer movements.</p> <p>ML systems don’t store tensors in row or column major but in a new format called <strong>strides format</strong> - <code class="language-plaintext highlighter-rouge">A[i, j, …] = A.data[offset + i*A.strides[0] + j*A.strides[1] + …</code>. It is a generalization of row and column major storage, and it offers more flexibility - so based on the batch-sizes or other parameters in a neural network.</p> <p>Strides can separate the underlying storage and the view of the tensor. Consider the following operations</p> <ol> <li> <code class="language-plaintext highlighter-rouge">slice</code> - simply changing the offsets and shape will output the slice without any copying involved.</li> <li> <code class="language-plaintext highlighter-rouge">transpose</code> - modifying strides will transpose the tensor without any copying! For example, consider the following example <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>     <span class="n">M</span><span class="p">.</span><span class="nf">strides</span><span class="p">()</span> <span class="c1"># (24, 12, 4, 1)
</span>     <span class="n">M</span><span class="p">.</span><span class="nf">permute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
     <span class="n">M</span><span class="p">.</span><span class="n">t</span><span class="p">.</span><span class="nf">strides</span><span class="p">()</span> <span class="c1"># (12, 4, 1, 24)
</span></code></pre></div> </div> </li> <li> <code class="language-plaintext highlighter-rouge">broadcast</code> - Suppose we have to extend a tensor’s data across a dimension for performing operations with another tensor, then by simply adding <code class="language-plaintext highlighter-rouge">0</code> stride in the appropriate dimensions would be enough! Again, no copying</li> </ol> <p>Many more operations can be done without copying the data and simply modifying the strides. For example, consider the following example -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17370816868291.jpg" alt=""></p> <p>However, strides also has an issue - Memory access may become non-contiguous, and many vectorized ops require continuous storage.</p> <h3 id="summary">Summary</h3> <p>To make operators efficient, we have seen the following tactics -</p> <ol> <li>Vectorization - leverage platform-specific vectorized functions that reduce seek time</li> <li>Data layout - strides format that allow zero-copies enabling fast array-manipulations</li> <li>Parallelization on CPUs</li> </ol> <p>These were techniques for general operations. However, we can optimize certain operators with their special properties.</p> <h3 id="matmul-optimization">Matmul optimization</h3> <p>The brute-force approach takes \(\mathcal O(n^3)\). The best approach humans know is \(\mathcal O(n^{2.371552})\)!</p> <p>How to improve the speed in practice then? Recall that we are trying to increase AI = #ops/#bytes.</p> <blockquote> <p><strong>Memory Hierarchy</strong> If everything ran on registers, things would be super-fast. But, that is expensive. Remember that L1-Cache has 0.5ns latency, L2-Cache has 7ns and DRAM has 200ns (400x slower!)</p> </blockquote> <p>Let us analyze the AI of <code class="language-plaintext highlighter-rouge">matmul</code> considering the different layers of memory</p> <ol> <li>We can directly move data to registers in every iteration in inner loop</li> </ol> <h2 id="gpus-and-accelerators">GPUs and accelerators</h2> <p>Recall that parallelizing operations across threads is super useful! CPUs have some level of parallelism through SIMD operations (vectorization) but they are limited. Building on the same idea, GPUs were born.</p> <p>When we started out, the ALU units were limited by the physical space on the chips. As technology improved, we moved from 70nm process all the way 3nm process! That is, we can fit up to 20x more cores in the same area! The majority of the area on CPUs is consumed by Control and Cache, and Jensen thought, ditch those and put cores.</p> <p>Graphical Processing Unit (GPU) are tailored for matrix or tensor operations. The basic idea is to use tons of ALUs (weak but specialized) with massive parallelism (SIMD on steroids).</p> <p>There are other hardware accelerators like Tensor Processing Unit (TPU) or Application specific integrated circuit (ASIC), etc. The common theme across all these is the same - there are specialized cores. What are specialized cores? They can only compute certain computations. Specialized cores can be super powerful - <img src="/assets/img/2025-01-06-data-systems-for-ml/17370849151083.jpg" alt=""></p> <p>Companies also tried reducing precision and maintain the same performance. Additionally, they also tune the distribution of different components for specific workloads.</p> <blockquote> <p>Why does quantization work in ML systems?</p> </blockquote> <h2 id="recap">Recap</h2> <p>Consider the following question - What is the arithmetic intensity of multiplying two matrices?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Load A
Load B
C = matmul(A, B)
</code></pre></div></div> <p>Given \(A \in \mathbb{R}^{mxn}, B \in \mathbb{R}^{nxp}, C \in \mathbb{R}^{mxp}\), the number of I/O operations is \(mn + np + mp\), and the number of compute operations is \(2mnp\) since there are approximately \(mnp\) addition and multiplication operations. The arithmetic intensity is then \(\frac{\text{\#compute operations}}{\text{\#I/O operations}} = \frac{2mnp}{mn + np + mp}\). Setting \(m=n=p=2\), results in \(\frac{2x2x2x2}{2x2 + 2x2 + 2x2} = \frac{4}{3}\).</p> <p>\textit{Note.} The addition operation discussed in the previous lecture also has the same I/O operations. However, \texttt{matmul} is a denser operation that results in a higher arithmetic intensity. In practice, it takes the same time to execute matrix addition and multiplication on GPUs, which is why they are so powerful.</p> <p><code class="language-plaintext highlighter-rouge">matmul</code> is an important operation. <check></check></p> <p>Now, consider the following operations</p> <ul> <li><code class="language-plaintext highlighter-rouge">broadcast_to</code></li> <li><code class="language-plaintext highlighter-rouge">slice</code></li> <li><code class="language-plaintext highlighter-rouge">reshape</code></li> <li>Permute dimensions</li> <li><code class="language-plaintext highlighter-rouge">transpose</code></li> <li>indexing like <code class="language-plaintext highlighter-rouge">t[:, 1:5]</code> </li> </ul> <p>All these operations are optimized due to strided access in tensors. On the other hand <code class="language-plaintext highlighter-rouge">contiguous()</code> cannot take advantage of this.</p> <p>Just to recap, the strides of a tensor of shape <code class="language-plaintext highlighter-rouge">[2, 9, 1]</code> stored in row major order are <code class="language-plaintext highlighter-rouge">[9, 1, 1]</code></p> <p>Consider the cache tiling operation -</p> <ul> <li>It increases the memory allocated on Cache and memory transfers between cache and register</li> <li>It reuses the memory movement between Dram and Cache</li> <li>The arithmetic intensity <em>decreases</em> since there is more load and store</li> </ul> <h1 id="gpu-and-cuda">GPU and CUDA</h1> <p>We have seen that specialized cores offer much better performance over traditional CPUs. Consider the following basic architecture of a GPU</p> <p>Let us see the basic terminology for understanding the architecture -</p> <ul> <li> <strong>Threads</strong> - Smallest units to process a chunk of data.</li> <li> <strong>Blocks</strong> - A group of threads that share memory. Each block has many threads mapped to a <em>streaming multiprocessor</em> (SM/SMP).</li> <li> <strong>Grid</strong> - A collection of blocks that execute the same kernel.</li> <li> <strong>Kernel</strong> - CUDA program executed by many CUDA cores in parallel.</li> </ul> <p>A GPU can be made more powerful by</p> <ul> <li>Adding SMs</li> <li>Adding more cores per SM</li> <li>Making the cores more powerful - at a point of <em>diminishing rewards</em>.</li> </ul> <p>NVIDIA, the largest GPU company, has released P100, V100, A100, H100 and B100 (Blackwell) for ML development. K80, P4, T4 and L4 were a lower tier of GPUs. Let us analyze how the compute has changed across these versions</p> <ol> <li>V100 (2019 -) - 80SMs, 2048 threads/SM - $3/hour</li> <li>A100 (2020 -) - 108SMs, 2048 threads/SM - $4/hour</li> <li>H100 (2022 -) - 144SMs, 2048 threads/SM - $12/hour</li> <li>B100 and B200 (2025 -)-</li> </ol> <p>The numbers are not doubling, then how has the performance doubled? They decreased the precisions.. :(</p> <h2 id="cuda">CUDA</h2> <p><strong>What is CUDA?</strong> It is a C-like language to program GPUs, first introduced in 2007 with NVIDIA Tesla architecture. It is designed after the grid/block/thread concepts.</p> <p>CUDA programs contain a hierarchy of threads. Consider the following host code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">const</span> <span class="kt">int</span> <span class="n">Nx</span> <span class="o">=</span> <span class="mi">12</span><span class="p">;</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">Ny</span> <span class="o">=</span> <span class="mi">6</span><span class="p">;</span>

<span class="kt">dim3</span> <span class="nf">threadsPerBlock</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// 12</span>
<span class="kt">dim3</span> <span class="nf">numBlocks</span><span class="p">(</span><span class="n">Ns</span><span class="o">/</span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">x</span> <span class="p">,</span> <span class="n">Ny</span><span class="o">/</span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">y</span> <span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// (3, 2, 1) = 6</span>

<span class="c1">// the following call triggers execution of 72 CUDA threads</span>
<span class="n">matrixAddDoubleB</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">);</span>
</code></pre></div></div> <p>The GPUs are associated with constants such as</p> <ul> <li> <code class="language-plaintext highlighter-rouge">GridDim</code> - dimensions of the grid</li> <li> <code class="language-plaintext highlighter-rouge">blocking</code> - the block inter within the grid</li> <li> <code class="language-plaintext highlighter-rouge">blockDim</code> - the dimensions of a block</li> <li> <code class="language-plaintext highlighter-rouge">threadIdx</code> - the thread index within a block With these in mind, the CUDA kernel for the above code is designed as</li> </ul> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__device__</span> <span class="kt">float</span> <span class="nf">doubleValue</span><span class="p">(</span><span class="kt">float</span> <span class="n">x</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// kernel definition </span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">matrixAddDoubleB</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">])</span>
    <span class="p">{</span>
        <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="n">C</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">doubleValue</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
</code></pre></div></div> <p>The host code launched a grid of CUDA blocks, which then call the <code class="language-plaintext highlighter-rouge">matrixAdd</code> kernel. The function definition starts with <code class="language-plaintext highlighter-rouge">__global__</code> which denotes a CUDA kernel function that runs of the GPU. Each thread indexes its data using <code class="language-plaintext highlighter-rouge">blockIdx</code>, <code class="language-plaintext highlighter-rouge">blockDim</code>, <code class="language-plaintext highlighter-rouge">threadIdx</code> and execute the compute. It is the user’s responsibility to ensure that the job is correctly partitioned and the memory is handled correctly.</p> <p>The host code has a serial execution. However, the device code has SIMD parallel execution on the GPUs. When the kernel is launched, the CPU program <em>continues executing</em> without <em>halting</em> while the device code runs on the GPU. Due to this design, it is important that the device code does not have any return values - causes erroneous behavior. To get results from the GPU, <code class="language-plaintext highlighter-rouge">CUDA.synchronize</code> is used (an example will be shown later).</p> <p>It is the developers responsibility to map the data to blocks and threads. The blockDim, shapes etc should be statically declared. This is the reason why compilers like <code class="language-plaintext highlighter-rouge">torch.compile</code> requires static shapes. The CUDA interface provides a CPU/GPU code separation to the users.</p> <p>The SIMD implementation has a constraint for the control flow execution - it requires all ALUs/cores to process in the same pace. In a control flow, not all ALUs may do useful work and it can lead to up to 8 times lower peak performance.</p> <h3 id="coherent-and-divergent-execution">Coherent and Divergent execution</h3> <p>A coherent execution applied the same instructions to all data. Divergent executions do the opposite and they need to be minimized in CUDA programs. This distinction is important to note - even the latest models like the LLMs have this behavior. Concepts such as attention masking and sliding window attention are examples of divergent behavior and they need to be specially implemented to extract the most compute from the GPU.</p> <h2 id="cuda-memory-model">CUDA Memory model</h2> <p>CUDA device (SIMD execution on GPU) has its own memory called the <em>HBM</em>.</p> <p>Unlike host (CPU) memory that is stored as pages in the RAM, GPU memory does not use pages but has memory pools (bulk data) that are accessed all at once.</p> <p>Memory can be allocated <code class="language-plaintext highlighter-rouge">cudaMalloc</code> and populated with <code class="language-plaintext highlighter-rouge">cudaMemcpy</code> like usual. CUDA has a concept called <strong>pinned memory</strong> that is part of the host memory which is optimized for data transfer between CPU/GPU. Ig is not pagable by the OS and is locked, and only certain APIs can access it.</p> <p>Every thread has its own private memory space, and every block has a shared memory that all its threads can access. The HBM is the global device memory in the GPU that can be accessed by all threads. The memory complexity is to balance between speed and shared memory parallelism.</p> <p>For example, consider the program for window averaging -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mf">3.0</span>
</code></pre></div></div> <p>How can this be parallelized? Since every 3-element tuple reduction is independent, each reduction can be mapped to a CUDA core. So, each thread can compute the result for one element in the output array.</p> <p>The host code -</p> <pre><code class="language-C">int N = 1024*1024;
cudaMalloc(&amp;devInput, sizeof(float)*(N+2)); // To account for edge conditions
cudaMalloc(&amp;devOutput, sizeof(float)*N);

convolve&lt;&lt;&lt;N/THREADS_PER_BLK, THREADS_PER_BLK&gt;&gt;&gt;(N, devInput, devOutput); 
</code></pre> <p>The device code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="cp">#define THREADS_PER_BLK = 128
</span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolve</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
        <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">//thread-local variable</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span>
        <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span><span class="mf">3.</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div> <p>This program can be optimized - each element is read thrice!<br> Notice that the number of blocks assigned is much more than what a typical GPU has. This is a general practice in CUDA programming where the blocks are <em>oversubscribed</em>.</p> <p>How to optimize? The memory hierarchy can be utilized -</p> <p>The new device code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="cp">#define THREADS_PER_BLK = 128
</span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolve</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
        
        <span class="k">__shared__</span> <span class="kt">float</span> <span class="n">support</span><span class="p">[</span><span class="n">THREADS_PER_BLK</span><span class="p">];</span>
        <span class="n">support</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
        <span class="k">if</span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">){</span>
            <span class="n">support</span><span class="p">[</span><span class="n">THREADS_PER_BLK</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="n">THREADS_PER_BLK</span><span class="p">];</span>
        <span class="p">}</span>

        <span class="n">__syncthreads</span><span class="p">();</span>

        <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">//thread-local variable</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">3</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">+=</span> <span class="n">support</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">];</span>
            
        <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span><span class="mf">3.</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div> <p>We introduced a synchronization primitive here. <code class="language-plaintext highlighter-rouge">_syncthreads()</code> waits for all threads in a block to arrive at this point. Another primitive <code class="language-plaintext highlighter-rouge">cudasynchronize()</code> that syncs between host and the device.</p> <h2 id="compilation">Compilation</h2> <p>A CUDA program also needs to be converted to low-level instructions to be executed. A compiled CUDA device binary includes -</p> <ul> <li>Program text (instructions)</li> <li>Information about required resources - 128 threads per block, 8 types of local data per thread and 130 floats (520 bytes) of shared space per thread block.</li> </ul> <p>The issue is that different GPUs have different SMs. If the user asks for a static (large) number of blocks, how to handle this? The first solution is that GPUs have varying (limited) number of blocks.</p> <p>Furthermore, CUDA schedules the threadblocks to many cores using a dynamic scheduling policy that respects the resource requirements. It assumes that the thread blocks can be executed in any order. The blocks are assigned based on the available resources and the remaining ones are <em>queued</em>.</p> <h2 id="understanding-a-gpu">Understanding a GPU</h2> <p>Consider a NVIDIA GTX 980 (2014) that has the following specs -</p> <ul> <li>96KB of shared memory</li> <li>16 SMs</li> <li>2048 threads/SM</li> <li>128 CUDA cores/SM Note that the number of CUDA cores is not equal to the number of CUDA threads.</li> </ul> <p>As the GPUs became better, NVIDIA tried to increase the shared memory per SMM. This is similar to the SRAM which is very important for LLM inference.</p> <h1 id="matmul---case-study"> <code class="language-plaintext highlighter-rouge">matmul</code> - Case Study</h1> <p>Remember that over subscribing in GPUs is allowed, and identify that work can be performed in parallel. Developing the thought-process while working with CUDA is important</p> <ul> <li>Oversubscribe to keep the machine busy</li> <li>Balance workload with convergent workflows</li> <li>Minimize communication to reduce I/O</li> </ul> <p>Now, let us consider matrix multiplication. What can be parallelized? In our previous CUDA implementation, we let each thread compute one element in the result matrix. So, each thread has \(2N\) reads, and there are \(N^2\) threads, resulting in \(2N^3\) global memory access.</p> <p>We are not leveraging the fact that one element can be used to calculate many values in the result matrix. The trick is to use the shared memory space - thread tiling (similar to what we did in CPUs).</p> <p>We let each thread compute \(V \times V\) submatrix. The kernel is as follows</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mm</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">float</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">){</span>
        <span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">x</span><span class="p">,</span> <span class="o">:</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">y</span><span class="p">){</span>
            <span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">y</span><span class="p">];</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
                <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">k</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">C</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">ybase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="o">:</span><span class="p">];</span>
<span class="p">}</span>

</code></pre></div></div> <p>For this version, we have reduced the read per threads to \(NV + NV^2\) and number of threads to \(N^2/V^2\) - total reads reduce to \(N^3/V + N^3\) with \(V^2 + 2N\) float storage per thread.</p> <p>We can improve this using partial sum computations. With a small change, we get</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mm</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">float</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
        <span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">k</span><span class="p">];</span> <span class="c1">// Grabbing an area</span>
        <span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span><span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">y</span><span class="p">){</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">){</span>
                <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">y</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">C</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">ybase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="o">:</span><span class="p">];</span>
<span class="p">}</span>

</code></pre></div></div> <p>With memory read per thread reduced to \(NV^2\), total memory to \(2N^3/V\) and memory per thread to \(V^2 + 2V\). This version is pretty good for systems with a single layer of memory hierarchy. However, if we have shared memory, it can be made more efficient!</p> <p>Suppose we have an SRAM layer, we can tile hierarchically. Consider the following GPU <code class="language-plaintext highlighter-rouge">matmul</code> v3: SRAM Tiling:</p> <p>The idea is to use block shared memory to let a block compute a \(L \times L\) submatrix and each thread computes a \(V \times V\) submatrix reusing the matrices in the shared block memory.</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mm</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span> <span class="p">{</span>
    <span class="k">__shared__</span> <span class="kt">float</span> <span class="n">sA</span><span class="p">[</span><span class="n">S</span><span class="p">][</span><span class="n">L</span><span class="p">],</span> <span class="n">sB</span><span class="p">[</span><span class="n">S</span><span class="p">][</span><span class="n">L</span><span class="p">];</span>
    <span class="n">Float</span> <span class="n">c</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="n">Float</span> <span class="n">a</span><span class="p">[</span><span class="n">V</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">V</span><span class="p">];</span>
    <span class="n">Int</span> <span class="n">y</span> <span class="n">block</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="n">Iint</span> <span class="n">X</span> <span class="n">block</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">float</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
        <span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">k</span><span class="p">];</span> <span class="c1">// Grabbing an area</span>
        <span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span><span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">y</span><span class="p">){</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">){</span>
                <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">y</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">C</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">ybase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="o">:</span><span class="p">];</span>
<span class="p">}</span>

</code></pre></div></div> <blockquote> <p>Think about it this way. Initially, we performed tiling across one layer of memory balancing the tradeoffs between I/O reads and memory constraints of the threads. Now, we are adding one more layer of such tiling in a similar manner. The key is to understand the partial sums idea.</p> </blockquote> <p>Note that it is highly unlikely that the threads have a large range of execution times, but we have the <code class="language-plaintext highlighter-rouge">__syncthreads()</code> as a failsafe. The statistics of this algorithm are -</p> <ul> <li>\(2LN\) global memory access per thread block</li> <li>\(N^2/L^2\) threadblocks</li> <li>\(2N^3/L\) global memory access</li> <li>\(2VN\) shared memory access per thread</li> </ul> <p>The key addition here is was the shared memory space. For this algorithm to be efficient, the fetching from the memory has to be implemented <em>cooperatively</em> -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="kt">int</span> <span class="n">nthreads</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    
</code></pre></div></div> <p>These summarize the matrix multiplication codes implemented in the GPUs. Simple, isn’t it? Although, we have not addressed the optimal values for \(L, V\). It depends on the number of threads, registers and amount of SRAM available on the GPU - this process is called <strong>kernel tuning</strong>. There are profilers that optimize these values. It is a difficult problem since large ML models have various operations that need to be optimized together based on their processing and memory requirements. Furthermore, this is different for every GPU and there are hundreds of GPUs!</p> <p>One solution is to do brute-force - hire people and throw money at it. On the other side of things, ML researchers are building <strong>operator compilers</strong> that figure these out automatically.</p> <p>There are other GPU optimizations that are utilized in practice</p> <ul> <li>Global memory continuous read</li> <li>Shared memory bank conflict</li> <li>Pipelining - While some threads are reading, let other threads compute.</li> <li>Tensor core - Dedicated hardware component to accelerate matrix multiplications</li> <li>Lower precisions</li> </ul> <h1 id="ml-compilation">ML compilation</h1> <p>A super-hot topic during the late 2010s since there was a lot of inefficient code that was being identified. The goal is to automatically generate optimal configurations and code given users code and <em>target hardware</em>.</p> <p>Traditional compilers have to simply convert high-level code to binary instructions. The stack for ML compilers is</p> <ol> <li>Dataflow graph generation</li> <li>Optimize graphs - pruning, partitioning, distribution, etc</li> <li>Build efficient kernel code - parameter optimization</li> <li>Machine code (This step is fairly easy and already well implemented)</li> </ol> <p>The big problems in this big process are</p> <ol> <li>Programming level - Automatically transforming arbitrary (usually imperative) code into a compilable code (static dataflow graphs)</li> <li>Graph level - Automatic graph transformations to make it faster (recall how convolution and batch norm can be fused). Graph theory researchers are working on this.</li> <li>Operator level - How to use hardware and optimize standard operators like <code class="language-plaintext highlighter-rouge">matmul</code>.</li> </ol> <p>The big players in this big field are</p> <ol> <li>XLA - First compiler for ML, released along with TensorFlow in 2016 (those researchers aimed big). This turned out to be so good, that the current TensorFlow stack still uses this. Also works for PyTorch, and it is useful to deploy on TPUs.</li> <li>tvm (Tensor Virtual Machine) - It is one of the most successful open-source compiler in academia. They founded OctoML with 200M (got acquired by NVIDIA). There is no backward pass.</li> <li>2.0 - Torch based compiler, that isn’t that great in terms of optimization.</li> <li>Modular - They raised 300M, founded by the same person who created LLVM. The co-founders started swift at Apple! They had big claims - 20x faster than 2.0, not sure how true they are.</li> </ol> <p>You can think of TensorFlow and PyTorch as the front end, and the above mentioned compilers as the backend.</p> <h2 id="operator-compilation">Operator Compilation</h2> <p>Each user-level written code (for standard operations) has a library of low-level program variants, and the compiler chooses the fastest one for the given hardware.</p> <p>For example, consider a loop -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
        <span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
</code></pre></div></div> <p>Get converted to</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for xi in range(4):
    for xo in range(32):   
        C[xo * 4 + xi] A[xo * 4 + xi] + B[xo * 4 + xi]
</code></pre></div></div> <p>Which is then efficiently implemented in the GPU kernels.</p> <p>So, how do we make this happens?</p> <ul> <li>Enumerate all possibilities</li> <li>Enumerate all the (close-to-) optimal values for the hardware - register/cache</li> <li>Apply to all operators and devices</li> </ul> <p>How to search or reduce the search space and generalize?</p> <p>Note that for a certain kind of code and hardware, finding these optimal value <em>once</em> is enough.</p> <h3 id="search-via-learned-cost-model">Search via Learned Cost Model</h3> <p>The famous example is Autotvm -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17376893447233.jpg" alt=""> The code generator here is done with templates (not LLMs).We need a lot of experts to write the template to define the search space.</p> <p>To search in this parameters space, the compiler does beam search with early pruning. The cost model can be trained on the historical data.</p> <h3 id="high-level-ideas">High-level ideas</h3> <p>We represent the programs in an abstract way and build a search space with a set of transformations (that represent a good coverage of common optimizations like tiling). Then effective search with accurate cost models and transferability have to be deployed.</p> <p>So, how well are we doing in this field? If the compilers were that good, they would’ve discovered flash-attention. Okay, it’s not that bad, compilers have found good optimizations and it just goes to show how difficult this problem is.</p> <h1 id="high-level-dsl-for-cuda-triton">High-level DSL for CUDA: Triton</h1> <p>We have seen a device-specific DSL (domain-specific language). Programmers are able to squeeze the last bits of performance through this. However, it requires deep expertise and the performance optimization is very time-consuming. Maintaining codebases is complex.</p> <p>On the other hand, we have ML compilers. They prototype ideas very quickly (automatically) and the programmer does not have to worry about the low-level details. The problem is representing and searching through the search-space is difficult. Compilers were not able to find Flash-attention because the search-space wasn’t able to represent this possibility. Furthermore, code generation is a difficult problem that relies on heavy use of templates - lots of performance cliffs.</p> <p>So compared to these two extremes, Triton is in between - it is simpler than CUDA and more expressive than graph compilers. It was developed by OpenAI as a solution to the problems with CUDA and compilers.</p> <h3 id="triton-programming-model">Triton Programming Model</h3> <p>The users define tensors in SRAM directly and modify them using torch-like primitives.</p> <ul> <li>Embedded in Python - Kernels are defined in Python using triton.jit</li> <li>Supports pointer arithmetics - Users construct tensors of pointers and can (de)reference them element wise.</li> <li>However, it has shape constraints - must have power-of-two number of elements along each direction</li> </ul> <p>Consider the following example</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="kn">import</span> <span class="n">triton.language</span> <span class="k">as</span> <span class="n">tl</span>
    <span class="kn">import</span> <span class="n">triton</span>
    
    <span class="o">@</span> <span class="n">triton</span><span class="p">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">__add</span><span class="p">(</span><span class="n">z_ptr</span><span class="p">,</span> <span class="n">x_ptr</span><span class="p">,</span> <span class="n">y_ptr</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
</code></pre></div></div> <p>The triton kernel will be mapped to a single block (SM) of threads. The users are responsible for mapping to multiple blocks. Basically, the language is automating some parts (like compilers), and making the design process simpler for users (as compared to CUDA). These design philosophies are important because they help build newer mental models for users - because they offload some of the cognitive load for optimization, they can think of newer ways of optimizing with these restricted set of parameters. Consider the example of softmax calculation. This function would be slow if implemented using primitives. PyTorch implements an end-to-end kernel for softmax to increase its performance. With triton, we can construct such an end-to-end operation in a simpler manner while achieving slightly higher performance.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">triton.language</span> <span class="k">as</span> <span class="n">tl</span>
<span class="kn">import</span> <span class="n">triton</span>
<span class="nd">@triton.jit</span>
<span class="k">def</span> <span class="nf">_softmax</span><span class="p">(</span><span class="n">z_ptr</span><span class="p">,</span> <span class="n">x_ptr</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">BLOCK</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">):</span>
    <span class="c1"># Each program instance normalizes a row
</span>    <span class="n">row</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK</span><span class="p">)</span>
    <span class="c1"># Load a row of row-major X to SRAM
</span>    <span class="n">x_ptrs</span> <span class="o">=</span> <span class="n">x_ptr</span> <span class="o">+</span> <span class="n">row</span><span class="o">*</span><span class="n">stride</span> <span class="o">+</span> <span class="n">cols</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">x_ptrs</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">,</span> <span class="n">other</span> <span class="o">=</span>    <span class="nf">float</span><span class="p">(</span><span class="err">‘</span><span class="o">-</span><span class="n">inf</span><span class="err">’</span><span class="p">))</span>
    <span class="c1"># Normalization in SRAM, in FP32    
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">tl</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># This is to avoid vert large and small values
</span>    <span class="n">num</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">den</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">num</span> <span class="o">/</span> <span class="n">den</span><span class="p">;</span> 
    <span class="c1"># Write-back to HBM
</span>    <span class="n">tl</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">z_ptr</span> <span class="o">+</span> <span class="n">row</span><span class="o">*</span><span class="n">stride</span> <span class="o">+</span> <span class="n">cols</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">&lt;</span> <span class="n">N</span>
</code></pre></div></div> <p>Note that Triton achieves good performance with low time investment. However, since it is not as flexible as CUDA, achieving very high-performance is not possible with Triton.</p> <h2 id="recap-1">Recap</h2> <p>A <em>kernel</em> in the GPU is a function that is executed simultaneously by tens of thousands of threads on GPU cores. The shared memory during the GPU execution can be used as a <em>cache</em> that is used by more than one thread, avoiding multiple accesses to the global memory. <em>Over-subscribing</em> the GPU ensures that there are more blocks than SMPs present on the device, helping to hide (tail) latencies by ensuring high occupancy of the GPU.</p> <p>&lt;iNsert a point about GPU memory)</p> <p>Operations such as ReLU, batch normalization and max pooling are not arithmetically dense operations. So typically, operations such as linear layers (and layer normalization with large batches) are limited by arithmetic operations. To calculate which linear layer would have more operations, consider the FLOPs calculation for GEMM.</p> <h1 id="3-graph-optimization">(3) Graph Optimization</h1> <p>Our goal is to rewrite \(G\) as \(G’\) such that \(G’\) runs faster than \(G\) while outputting equivalent results. The straightforward solution is to use a template, wherein human experts write (sub-)graph transformation templates and an algorithm replaces these in the data flow graphs for reduction.</p> <h3 id="graph-optimization-templates-fusion">Graph Optimization Templates: Fusion</h3> <p>Fusing operators reduces I/O and kernel launching (CPU to GPU overhead, all the operations that the SM has to run). The disadvantages of this method is that creating various fused operations is difficult making the codebase unmanageable (e.g., TensorFlow).</p> <p>This also includes <strong>folding constants</strong> in a graph to replace expressions such as <code class="language-plaintext highlighter-rouge">(X + 3) + 4)</code> with <code class="language-plaintext highlighter-rouge">(X + 7)</code>.</p> <h3 id="cuda-graph">CUDA graph</h3> <p>NVIDIA allows users to capture the graph at the CUDA level.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381192844173.jpg" alt=""></p> <blockquote> <p>Is this define-then-run?</p> </blockquote> <h3 id="standard-compiler-techniques">Standard compiler techniques</h3> <ul> <li>Common subexpression elimination (CSE). The high-level idea is replacing expressions such as <code class="language-plaintext highlighter-rouge">a = b; b = c</code> with <code class="language-plaintext highlighter-rouge">a = c</code> </li> <li>Dead Code elimination (DCE). After the CSE hit, we eliminate the dead-code with unused variables.</li> </ul> <p>These both are run iteratively to reach an optimal code. These operations are every useful to eliminate parts of graph based on, say default arguments -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381197256019.jpg" alt=""></p> <h2 id="how-to-ensure-performance-gain">How to ensure performance gain?</h2> <p>When we greedily apply graph optimizations, we may miss some options that initially decrease the performance but massively increase it later. Furthermore, the same optimizations could lead to an improvement in one hardware and reduction in other. Due to the existence of hundreds of operators (200-300), thousands of graph architectures and tens of hardware backends, it is infeasible to manually design graph optimizations for all cases.</p> <p>There are other issues with template based optimizations</p> <ol> <li>Robustness - Heuristics are not generalizable across architectures and hardware</li> <li>Scalability - New operators and graph structures require newer rules</li> <li>Performance - Misses subtle optimizations specific to DNNs/hardware.</li> </ol> <p>What’s the solution?</p> <h2 id="automate-graph-transformation">Automate Graph Transformation</h2> <p>The main idea is to replace manually-designed graph optimizations with automated generation and verification of graph substitutions for tensor algebra. Basically, generate all possible substitutions and verify if they generate the same output.</p> <p>We start by enumerating all possible graphs up to a fixed size using available operators.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381201066754.jpg" alt=""> There are up to 66M graphs with 4 operators!</p> <p>Then, with a graph substitution generator, we compute the output with random input tensors. For 4 operators, we can still generate up to 28744 substitutions!</p> <p>These are further pruned based on <em>variable renaming</em> and <em>common subgraphs</em>.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381203016208.jpg" alt=""></p> <p>These substitutions are formally verified to ensure that they are equivalent mathematically for all inputs. This verification is done by using the properties of operators. For example, convolution with concatenated kernels is same as concatenation of convolutions of the same kernels.</p> <p>So this <em>automated theorem prover</em> can be used to generate valid substitutions scaling up. It takes up to 5 minutes to verify 750 substitutions and there are about 45 rules for the operators which takes about 10 minutes. Adding a new operator is easy - just provide its specifications!</p> <h3 id="incorporating-substitutions">Incorporating substitutions</h3> <p>How do we apply verified substitutions to obtain an optimized graph? The cost is based on the sum of individual operator’s cost and the cost on the target hardware. We greedily apply the substitutions to improve the performance.</p> <p>This approach can be further improved to train a model to learn which kind of substitutions optimize the graph. This was successfully implemented by TASO and it showed good results for real-life models -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381207164574.jpg" alt=""></p> <h2 id="summary-1">Summary</h2> <p>In summary for graph optimization,</p> <ol> <li>We first construct a search space</li> <li>Enumerate all possibilities for substitutions</li> <li>Prune the candidates, and select the top ones based on profile/cost model</li> <li>Apply the transformations to iteratively improve the performance.</li> </ol> <p>What could go wrong with this? The search may be slow and the evaluation of various graphs can be expensive.</p> <p>Sometimes the substitutions may only be partially equivalent, but can be orders of magnitude faster. In such cases, we can trade off accuracy for performance. E.g., Convolution vs Diluted convolution.</p> <p>Consider the following example. Suppose we have to use the same kernel to perform convolution on two different tensors. Then, we could concatenate these tensors, apply the convolution, and then apply a correction to achieve the correct result. These transformations use partial equivalent transformations yielding some speed up. These are not explorable in the previous case with fully equivalent operators.</p> <h2 id="partially-equivalent-transformations">Partially Equivalent Transformations</h2> <p>Like the previous example, we <em>mutate</em> the programs and correct them to get an optimized graph.</p> <p>The steps to do this automatically, we do something similar to before</p> <ol> <li>Enumerate all possible programs up to a fixed size using available operators</li> <li>Only consider transformations with equal shapes (in contrast with equal results as compared to before)</li> </ol> <p>With this, all the crux of the algorithm comes to the correction of the mutant programs - how do we detect which part is not equivalent and how to correct it?</p> <p>By enumeration - For each possible input and position, check if the values match. For complete correctness, this search would be \(m \times n\) for \(m\) possible inputs and \(n\) output shape. We reduce the effort by reducing \(m, n\)</p> <ul> <li> <p>Reducing \(n\) - Since neural networks are mostly multi-linear, we can make such assumptions.</p> <p>Theorem: For two multi-linear functions \(f\) and \(g\), if \(f = g\) for \(O(1)\) positions in a region, then \(f = g\) for all positions in the region.</p> <p>As a consequence, the search reduces from \(\mathcal O(mn)\) to \(\mathcal O(mr)\)</p> </li> <li> <p>Reducing \(m\) - Theorem - If \(\exists l, f(l)[p] \neq g(l)[p]\), then the probability that \(f\) and \(g\) give identical results on \(t\) random inputs is \(2^{-31t}\).</p> <p>Using this, we can run \(t\) random tests with random inputs, and if all \(t\) pass then it is very likely that \(f\) and \(g\) are equivalent.</p> </li> </ul> <p>The search space reduces to $\mathcal O(tr)$$. How does this relate to correct?</p> <h1 id="ml-compiler-retrospective">ML Compiler Retrospective</h1> <p>This field started in 2013 with Halide. It was a compiler for rendering, but since the workflow is very similar to neural networks, the later compilers draw motivation from here.</p> <p>Then came XLA in 2016-17, that has good performance but had very difficult to understand code. Companies tried other operations such as TensorRT, cuDNN and ONNX for template based graph substitution. CuDNN is still popularly used but no one understands the code since it was written in a very low level language.</p> <p>Then came <code class="language-plaintext highlighter-rouge">tvm</code> in 2018 that we’ve discussed before. In 2019-20, MLIR and Flexflow were introduced - these are layers in the compiler that provided specific optimizations. Then came 2.0 and Torch Dynamo.</p> <p>However, the community is shifting away from compilers. Why? One part is that many optimizations have been found. The main reason is that we’ve seen a certain class of neural networks architectures that work really well. For example, transformers are all the rage. So instead of focusing on compilers, people can focus on just building fused kernels for the attention mechanisms. That’s how we got flash-attention that no compiler is able to beat.</p> <h1 id="runtime">Runtime</h1> <h2 id="memory-and-scheduling">Memory and Scheduling</h2> <p>Our goal is to fit the workload on limited memory and ensure that the peak memory usage is less than the available memory.</p> <blockquote> <p>Need to add some stuff here.</p> </blockquote> <p>Consider the GPT-3 architecture -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17382918001192.jpg" alt=""></p> <p>For every model, we check the precision and multiply the number of parameters by 2 or 4 to calculate the total memory consumption.</p> <p>For the main model with 175B parameters, if each parameter is 2 bytes, then we require 350Gb of memory! How did we make this work?</p> <p>Why does this rule of thumb work? Let us check the activation sizes for different layers</p> <ol> <li>For 2D convolution: The input has the size \((bs, nc, wi, hi)\), and the output has \((bs, nc, wo, ho)\). The activation size is \(bs*nc*wo*ho*\text{sizeof(element)}\)</li> <li>For an MLP with input size \((bc, m, n)\) and output size \((bs, m, p)\), the activation size is \(bs*m*p*\text{sizeof(element)}\)</li> <li>For a transformer (ignoring activation layers and other FFLs) - the input size is \((bs, h, seq\_len)\) and the output size is \((bs, h, seq\_len). The activation size is\)bs<em>h</em>seq_len*\text{sizeof(element)}$$</li> </ol> <p>So for GPT-3, the per-layer activation assuming sequence length 1 comes to 78 or 156 Gb. Let us add some more elements to this calculation.</p> <p>The Adam Optimizer estimates the first and second moment vectors with parameters for exponential decays. It also has a step-size or learning rate. The algorithm is given by</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17382924545817.jpg" alt=""></p> <p>Along with the learning rate, since it also stores the moments, it has to store two more values for each parameter! The memory usage becomes thrice of what it should be!</p> <h3 id="lifetime-of-activations-at-training">Lifetime of activations at training</h3> <p>Because we need to store the intermediate values for the gradient steps, training an \(N\)-layer neural network would require \(O(N)\) memory. This is the main difference between training and inference. In inference, we wouldn’t need to store the parameters at all layers, so we would just need \(O(1)\) memory.</p> <p>So we’ve seen for GPT-3, we require 350 or 700 Gb. So for a sequence length of 96, we would require 7488 or 14976 Gb! These numbers are just crazy! We haven’t even considered the composite layers.</p> <p>Therefore, it is important to take care of memory.</p> <h3 id="single-device-execution">Single Device execution</h3> <p>How do we reduce the memory usage?</p> <p>Idea 1 - the input or the activation is not needed until the backward pass reaches the layer. So, we can discard some of them and recompute the missing intermediate nodes in small segments. This technique is called <em>recomputation, rematerialization, checkpoint activation, etc</em>. It’s essentially the time-space tradeoff.</p> <p>For an \(N\) layer neural network, if we checkpoint every \(K\) layers, then the memory cost reduces to</p> \[\text{Memory cost} = \mathcal O\left(\frac{N}{K}\right) + \mathcal O(K)\] <p>To minimize this, we can pick \(K = \sqrt{N}\). The total recomputation increases by \(N\) - essentially another forward pass. In PyTorch, this feature can be activated using <code class="language-plaintext highlighter-rouge">torch.utils.checkpoint</code>.</p> <p>So when do we use this? When memory is a constraint and time of training is not a concern. The memory usage also depends on the layer being checkpointed - the layers can have different out sizes. In transformers, the layer boundary is typically checkpointed. The disadvantage is that this only works for activations.</p> <blockquote> <p>why?</p> </blockquote> <p>The second idea is <strong>gradient accummulation</strong>. The activation memory is linear to batch size. The idea is to compute the gradient for the batch but will limited memory. We split the original batch into micro-batches and accumulate the gradients at each layer. We then update the weights for the complete batch.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17382936900407.jpg" alt=""></p> <p>The disadvantage of this strategy is that over-subscribing of GPUs is difficult since we have smaller matrices.</p> <p>An alternative method to save on GPU memory is to use the memory hierarchy. We have <code class="language-plaintext highlighter-rouge">SwapIn</code> (swap from CPU DRAM to HBM) and <code class="language-plaintext highlighter-rouge">SwapOut</code> (swap from HBM to CPU DRAM) that can be applied to both weights and activation. As we do a forward pass, we swap in the next layers and swap out the passed layers. You can be a bit more intelligent about it and pre-fetch the layers based on the computation and swap latencies. This strategy is becoming more practical as more companies are adopting the unified memory architecture. The memory hierarchy seems to be breaking.</p> <p>All these strategies can be used together to probably train GPT-3 on a single device but it would take forever.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387232533206.jpg" alt=""> Why do we start with gradient accumulation instead of gradient checkpointing? Checkpointing greatly increases the computation time, so we try the other alternatives first.</p> <h2 id="memory-and-compute">Memory and Compute</h2> <h3 id="quantization">Quantization</h3> <p>All our memory usage is a multiple of \(\text{sizeof(element)}\). What if we reduce that parameter?</p> <p>Quantization is the process of constraining an input from a continuous or otherwise large set of value to a discrete set. We use a lower-precision representation for data while preserving ML performance (accuracy), speeding up compute, reducing memory, saving energy, etc. Most of the edge models use quantization.</p> <p>To understand this better, let’s understand the representation of data in memory -</p> <ul> <li>An unsigned integer has the range \([0, 2^n - 1]\)</li> <li>A signed integer with \(n\)-bit has the range \([-2^{n-1} - 1, 2^{n - 1} - 1]\). To avoid saving 0 twice by storing a sign bit, computer architects decided to use <em>Two’s complement representation</em>.</li> <li>Fixed point number - An arbitrary bit is chosen as the boundary for the integer and the decimal. This representation is mainly used in security applications now.</li> <li> <p>Floating point representation - We use a sign bit, 8-bit exponent and 23 bit fraction. That is the value is, \((-1)^{sign} \times (1 + \text{ fraction}) \times 2^{\text{exponent} - 127}\).</p> <p>How do we represent 0 then? Representation-wise, we technically cannot represent 0, so we make a special representation - <em>normal vs subnormal values</em>. Whenever the exponent bits are zero, we remove the bias term \(1\) that is added to the fraction, and represent the value as \((-1)^{sign} \times \text{ fraction} \times 2^{\text{exponent} - 127}\). This expressions is only used with the exponent is \(0\). This way, we also extend the range of the representation and the smallest positive number we can represent is \(2^{-149}\).</p> <p>How about special values? Exponent with all set bits is infinity and sign is decided by the sign bit. NaN is represented in the subnormal range with exponent bits set to 1. In summary, we have</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387239819544.jpg" alt=""></p> </li> </ul> <blockquote> <p>Calculate model sizes from parameters table.</p> </blockquote> <p>Notice that the precision of floating point numbers is much higher when the values themselves are small. This is a tradeoff we make based on the applications. Here is a summary of other floating point representations -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387241256605.jpg" alt=""></p> <p>The BF16 system has been observed to provide much better performance over FP16 for neural network training. The representation lowers the precision for a higher range. It could be that the higher range can stabilize the training during exploding or vanishing gradients. During inference, the precision matters more so FP16 might perform better in some cases.</p> <p>For practice, consider the following examples</p> <ul> <li>The FP16 bit set <code class="language-plaintext highlighter-rouge">1 10001 1100000000</code> represents -7.1. Why? The bias in the exponent is always the median value subtracted by 1. Here it is \(2^4 - 1 = 15\). The exponent is then \(17 - 15 = 2\), and the fraction is \(0.5 + 0.25 = 0.75\)</li> <li>The decimal 2.5 is represented as <code class="language-plaintext highlighter-rouge">0 10000000 0100000</code>. The bias is \(2^7 - 1 = 127\)</li> </ul> <p>After these representations, newer ones came up to improve the performance in deep-learning. <img src="/assets/img/2025-01-06-data-systems-for-ml/17387248528126.jpg" alt=""> The rule of thumb is that we require higher range for training and higher precision for inference.</p> <p>As if this were not enough, for even lower compute, NVIDIA has been pushing for INT4 and FP4 to keep up with the Moore’s law.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387250679255.jpg" alt=""></p> <p>There has been no successful model with these representations, making it a very promising avenue for research.</p> <p>Alright, with these things in mind, let us come back to quantization. There have been two approaches for quantization</p> <ol> <li> <p><strong>K-Means based quantization</strong> - widely used, almost all mobile devices have it.</p> <p>Consider we have a weight matrix that we are trying to quantize. The idea is to cluster close values together into an integer or a lower-precision representation. The number of clusters is chosen based on the chosen requirements - for 2-bit quantization, number of clusters is 4.<br> To do so, we simply apply K-means. The centroids are stored in a code book whereas the matrix is simply stored in the lowest possible representation to further reduce storage. <img src="/assets/img/2025-01-06-data-systems-for-ml/17387254097673.jpg" alt=""> How do we perform a backward pass on these matrices? The gradients are accumulated based on the classes and are applied to the centroids to fine-tune them. In practice, quantization starts affecting the performance sharply only after a certain threshold. Therefore, quantization becomes a hyper-parameter tuning problem, and we can achieve significantly lower memory consumption. The number of bits used can vary with layers as well!</p> <blockquote> <p>Try coding a library with variable quantization layers. Shared code books across layers</p> </blockquote> <p>How is the run-time affected? The computations are still FP arithmetic. There is an added computation cost for weight compression/decompression and code book lookup. K-means has been quite effective with convolution networks.</p> </li> <li> <p><strong>Linear quantization</strong> - The idea is to determine a linear mapping of integers to real numbers. It can be seen as a linear optimization problem.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387262709675.jpg" alt=""></p> <p>So, we need to determine zero-point and scale parameters. These parameters are often also publicly shared on platforms such as HuggingFace. The parameters can be chosen for the whole model or for each layer based on our performance tradeoff appetite. For many popular models like Llama, the quantization is done tensor-wise.</p> <p>The parameters can easily be determined as follows</p> \[\begin{align*} S &amp;= \frac{r_{max} - r_{min}}{q_{max} - q_{min}} \\ Z &amp;= \text{round}(q_{min} - \frac{r_{min}}{S}) \end{align*}\] <p>The bit-width \(n\) determines \(q_{min} = -2^{n -1}\) and \(q_{max} = 2^{n - 1} - 1\).</p> <p>Suppose we apply linear quantization to <code class="language-plaintext highlighter-rouge">matmul</code> -</p> \[\begin{align*} Y &amp;= WX \\ S_Y(q_Y - Z_Y) &amp;= S_W(q_W - Z_W)S_X(q_X - Z_X) \\ Q_Y = \frac{S_W S_X}{S_Y} \left( q_W q_X - Z_W q_X - \underset{Z_X q_W + Z_W Z_X) + Z_Y}{\text{precomputed for inference}} \right) \end{align*}\] <p>Empirically, the factor \(\frac{S_W S_X}{S_Y}\) is between 0 and 1. Instead of using floating point multiplications, it is represented as fixed point multiplication and bit shift. Also, empirically \(Z_W\) follows normal distribution and can be approximated as 0. Thus, the heavy lifting operation is \(q_Wq_X\) which is simply integer multiplication.</p> <p>Therefore, we reduced both storage and computation time (integer arithmetic is much faster and cheaper). This can also be used to reduce FP16 to FP8 rather than integers.</p> </li> </ol> <p>In summary, we have <img src="/assets/img/2025-01-06-data-systems-for-ml/17387273312564.jpg" alt=""></p> <h1 id="large-language-models-on-cloud-and-edge">Large Language Models on Cloud and Edge</h1> <p>The first wave of revolution in ML systems was sparked by Big Data in 2010s (competitions like Netflix recommendation price), which resulted in systems such as XGBoost, Spark and GraphLab. Then in mid 2010s, deep-learning started gaining traction, and TensorFlow, TVM and Torch were created. Now, we’re in the third revolution with Generative AI. ML Systems is playing a much bigger role.</p> <p>The challenges involved are -</p> <ol> <li> <p>Memory - Llama-70B consumes 320GB VRAM just to store parameters in FP32</p> </li> <li> <p>Compute - The post-Moore era brings great demand for diverse specialized compute, system support becomes bottleneck</p> </li> </ol> <p>The design of systems depends on the paradigm that the industry is moving towards. Currently, we have cloud based models with a client-server architectures. This is how computers initially started before the age of personal computers. So, would we move towards personal AI in consumer devices?</p> <p>There are many engineering challenges involved in this. As we covered previously, there are specialized libraries and systems for each backend involving manually created optimizations. The area is very labor intensive with huge market-opportunities.</p> <p>Our approach to this has been in terms of composable optimizations to rewrite kernel codes. Furthermore, we added techniques such as parameter sharding, memory planning, operator fusion, etc to add to these optimizations. What have we learned from this journey?</p> <p>There are four abstractions we use</p> <ul> <li> <p>Computational Graphs - Graph and its extensions enable higher level program rewriting and optimization</p> </li> <li> <p>Tensor Programs - These abstractions focus on loop and layout transformation for fused operators</p> </li> <li> <p>Libraries and Runtimes - Optimizing libraries are built by vendors and engineers to accelerate key operators of interests</p> </li> <li> <p>Hardware Primitives - The hardware builders exposes novel primitives to provide native hardware acceleration</p> </li> </ul> <p>It has not been about a <strong>silver bullet system but continuous improvement and innovations</strong>. ML Engineering is going hand-in-hand with ML modeling.</p> <p>The developers of TVM are expanding it to TVMUnity to bring the compiler flow to the user. As we’ve studied, IRModule is the central abstraction in TVM. Once the user-level code is written in this form, TVM takes care of the hardware backend making it easy to the models on various architectures.</p> <p>TVM generates an optimized models, and features are continuously added with data to make it better over time.</p> <h2 id="tvm-unity">TVM Unity</h2> <h3 id="first-class-symbolic-shape-support">First-class symbolic shape support</h3> <p>Traditional models like ResNet have some key-characteristics. The compilers are being built under the assumption of these fixed parameters, but with the age of generative AI, these parameters keep changing continuously. TVMUnity leverages symbolic shift to incorporate variable sizes in the model. In essence, traditional compilers are unable to handle dynamic shapes, whereas TVMUnity allows it with symbolic support.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-47-07-image.png" alt=""></p> <p>Knowing the dependencies between different variable parameters allows for better optimizations during runtime - static memory planning for dynamic shapes</p> <h3 id="composable-tensor-optimization">Composable Tensor Optimization</h3> <p>In the early ages, we had scalar computing. Then came the age of vector computing with SIMD. Now, NVIDIA has TensorCore and TPUs became a thing for tensor computing. How do we leverage these hardware developments in our programs? We want both loop based optimization and tensor-based programs.</p> <p>The first step is to isolate the internal computation tensorized computation from external loops to create a <code class="language-plaintext highlighter-rouge">Block</code>.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-52-50-image.png" alt=""></p> <p>With this, TVMUnity does <em>Imperative Schedule Transformation</em> to search different variants of programs by changing blocks to create an optimized IR.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-53-13-image.png" alt=""></p> <p>By providing this interface to the user, where the user can mention the parameters involved, TVMUnity can perform tensorization along with graph-optimization problem.</p> <h3 id="bringing-compilation-and-libraries-together">Bringing compilation and Libraries Together</h3> <p>We have seen this tradeoff with compilers and libraries.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-57-28-image.png" alt=""></p> <p>TVMUnity provides interfaces like <code class="language-plaintext highlighter-rouge">Relax-BYOC</code> to offload the computational load to libraries such as TensoIR to leverage the library-optimized kernels. The native compilation works on top of the library offloading to increase the performance even more.</p> <p>By adding this flexibility, the compiler can update (live with library updates) to squeeze the best performance.</p> <p>Since libraries come out with data layout requirements, the compiler can use this information to optimize the remaining parts of the code.</p> <h2 id="ml-compilation-in-action">ML Compilation in Action</h2> <p>How does this architecture help with incremental developments? The users can try new optimizations in coordination with the compiler to scale to newer models.</p> <p>In language models, the compiler does targeted optimizations such a low-bit quantizations, dynamic shape planning, fusion and hardware aware optimizations, etc. Along with these, it works with KVCache etc to improve the performance.</p> <p>The <strong>MLCEngine</strong> is a universal LLM deployment engine being developed to deploy LLMs on any device. The key contribution here is <em>universal</em>. This development also revealed some key insights - iOS devices require custom Metal language kernels, Snapdragon devices require OpenCL - and this project is trying to take care of all of that.</p> <p>It also helps with structured generation with near zero overhead. This feature would be super useful for agent use cases. <a href="https://huggingface.co/spaces/mlc-ai/WebLLM-Structured-Generation-Playground" rel="external nofollow noopener" target="_blank">Surprisingly, MLCEngine does it with near zero overhead with something known as XGrammar!</a></p> <p>They also created <a href="https://webllm.mlc.ai/" rel="external nofollow noopener" target="_blank">WebLLM</a> with the new WebGPU standard to use the local compute to the browser. Yes, it uses your local GPUs and doesn’t send your data to any server! You can use this to build stuff on top of it with the typescript API and a Node package. It is an open source project too!</p> <p>Let us continue our discussion on Quantization. We discussed about quantization granularity (per-tensor, per-channel, group quantizations).</p> <p>Per-channel quantization is preferred to tensor quantization in some cases because the channels can have very different ranges, and using a single \(S\) can result in lopsided representations. Even then, some applications can lose too much on performance for per-channel quantization. In these cases, group quantization is useful.</p> <p>Group quantization is more fine-grained. E.g., per vector. It has more accuracy, less quantization error trading off some of the savings.</p> <blockquote> <p>Can we do some sort of statistical grouping here?</p> </blockquote> <p>The sweet-spot that works in practice is a two-level quantization that quantizes hierarchically. Instead of using \(r = S(q - Z)\) we use \(r = \gamma S_q (q = Z)\). \(\gamma\) is a gloating-point coarse grained scale factor, and \(S_Q\) is an integer per-vector scale factor. It can be further generalized into multi-level quantization with scale factors for each levels.</p> <p>So far, we have done quantization of weights. They are static during inference, and so everything works. What if we want to quantize the activations? The stats change with every input. This problem is solved in two ways:</p> <ol> <li>Moving average - Observed ranges are smoothed across thousands of training steps</li> <li>Calibration dataset - Figure out the range from subset of training set</li> </ol> <h2 id="mixed-precision">Mixed Precision</h2> <p>In the previous methods, we have done <em>uniform quantization</em> wherein every weight parameter is quantized to the same number of bits. However, what if we use different precision for different weights? Does this improve the performance a lot? The implementation is however very complicated since we have to deal with different data formats. Also, what formats give the best savings and high performance?</p> <p>These are hard questions. So we throw ML at the problem. We define a cost function to let a model discover the combination of formats that give the best parameters. It was a good research field, and the accuracy of models improved by 10% or so.</p> <p>After a while, <a href="https://arxiv.org/pdf/1710.03740" rel="external nofollow noopener" target="_blank">NVIDIA released a paper</a> (it’s a good read) that became the standard for mixed precision training. The intuition for this approach is as follows. Some layers are more sensitive to dynamic range and precision. For example, softmax and normalization layers have a large range of values. We identify such operations and assign them higher precision.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393289702270.jpg" alt=""></p> <p>Since Adam calculates two moments and normalizes the gradients, we use FP32 for the weight updates.</p> <p><em>Note.</em> Deepseek changed the standard with new precisions.</p> <p>Let us see how the memory of models changes with this new precision system. Again, for the largest GPT, there are 175B parameters. So it occupies 350G with FP16 for all the weights. The activations occupy 7488G assuming checkpointing at each layer boundary.</p> <p>So in this system, the master model with FP32 weights occupies 4<em>175 = 700G. The gradients occupy 2</em>175 = 350G. The running copy of the model used for inference is 2<em>175 = 350G. Finally, we need Adam mean and variance (FP32) that is 2</em>4*175 = 1400G. The rule of thumb in general is \((4 + 2 + 2 + 4 + 4)N = 16N\) memory for LLMs.</p> <h3 id="scaling-down-ml">Scaling down ML</h3> <p>Running ML on edge devices is always strongly demanded, and the market is very fragmented. It is easy to build a startup and get acquired in this space. The possible research directions are quantization, pruning, ML energy efficiency, federated ML, etc.</p> <h1 id="parallelization-1">Parallelization</h1> <p>Moore’s law came to an end. However, ML models were scaling 3x every 18 months! Why? Bigger model gives better accuracy. People have also started seeing emergent capabilities in larger models.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393299103392.gif" alt=""></p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393300313845.jpg" alt=""></p> <p>So, the models are going to get bigger. Along with this, the memory demand is increasing too. It was back in 2019 when we last fit an entire model in one GPU. Now we require 100s of GPUs just to store the model parameters. The only way out of this is parallelization. Wait! Aren’t GPUs already doing that?</p> <blockquote> <p>GPUs did data parallelization. We are talking about having multiple GPUs and using them together. All the things we considered so far assumed the entire model is on one GPU. Now, we need to distribute the model training. We just seem to be creating problems for ourselves…</p> </blockquote> <p>Intuitively there are multiple ways we can go about this</p> <ol> <li>Parallelize along the layers (cutting through depth)</li> <li>Parallelize each layer (cutting through breadth) - this one is rather complicated with more data traveling between clusters.</li> </ol> <p>Apart from these considerations, a GPU cluster also has its own constraints. There are different latency communication channels (kind of like memory hierarchy).</p> <p>Let us look at the problem from a computational lens. A model involves parameters, weight updates, model spec and the data.</p> <ul> <li>Computing - The forward pass and backward pass require compute</li> <li>Memory - The data and parameters require memory. Between these, we require communication (typically done with interconnects or network, e.g., NVLink) which is the main bottleneck in this whole setup. How do we communicate parameters and activations?</li> </ul> <p>In <strong>data parallelism</strong>, we partition the data across GPUs and give each GPU a copy of the parameters and gradients. Then, we need to synchronize the updates together across the GPUs before the next iteration.</p> <p>In <strong>model parallelism</strong>, we partition the model across GPUs and use the data to update parts of the model. This method is more complicated. Let us delve deeper.</p> <p>How do we partition a computation graph on a device cluster?</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393313839159.jpg" alt=""></p> <p>There are more strategies that consider hybrid variants - some parts of the model are inter-op, intra-op and some parameters are replicated across devices.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393315930822.jpg" alt=""></p> <p>These are the standard techniques being used for the models today.</p> <p>Let us delve deeper into these. <img src="/assets/img/2025-01-06-data-systems-for-ml/17393318019536.jpg" alt=""> In the above example, intra-op is similar to what we discussed before for matmul on GPUs - each GPU computes a partial sum. Surprisingly, we can show mathematically that this is the best we can do with inter-op.</p> <p>What are the pros and cons?</p> <ul> <li>Inter-op parallelism requires point-to-point communication but results in devices being idle.</li> <li>Intra-op parallelism keeps the devices busy but requires collective communication.</li> </ul> <p>The all-reduce operation is computationally intensive.</p> <p>Since intra-op requires more communication, this is an important aspect to consider. On the other hand, inter-op results in hardware bubble wherein some of the GPUs are idle (can be prevented to some extent with pipelining).</p> <p>In summary, inter-op parallelism requires point to point communication but results in idle devices. The devices are busy in intra-op parallelism but requires collective communication.</p> <blockquote> <p><strong>A note on terminology</strong> Previously, the literature used to talk about data and model parallelism. Data parallelism is general and precise but the term “model parallelism” is vague. That is why, we stick to the terms inter-op and intra-op parallelism which rely on the pillars of computational graph and device cluster. Furthermore, we are mainly discussing about model parallelism because of the regime in which models do not fit on a single GPU anymore.</p> </blockquote> <p>With that, let us formulate our goal - “What is the most efficient way to execute a graph using combinations of inter-op and intra-op parallelism subject to memory and communication constraints?”</p> <p>Let us equip ourselves with a metric to quantify this goal. Previously we had, Arithmetic Intensity. Now, we consider <strong>Model FLOPs Utilization (MFU)</strong></p> \[MFU = \#FLOPs/t/\text{peak FLOPs}\] <p>Where #FLOPs is the total FLOPs in the ML program, and \(t\) is the time required to finish the program. The goal is to maximize this quantity.</p> <p>Why don’t we achieve the peak FLOPs? The Peak FLOPs values are typically achieved by matrix multiplications but a neural network model like a transformer involves many layer operations (memory bounded) that lower the performance. Moreover, we have seen how optimization can greatly reduce the time of execution and increase the operations within the same time. Unoptimized code can lower the value further. Adding to these communication like we have discussed before can reduce the quantity a lot too. Finally, all these parameters also depend on the precision, code and the GPU type being used.</p> <p>How do we calculate the MFU?</p> <ol> <li>Count the number of operations (FLOPs) considering the shape of the data in the model. For example, we have seen the formula \(2mnp\) for <code class="language-plaintext highlighter-rouge">matmul</code>.</li> <li>Run the model for one forward and backward pass on the GPU to get benchmark for the running time \(t\)</li> <li>Check the GPU spec, type of cores, and their peak FLOPs</li> <li>Calculate the MFU</li> </ol> <p>What is the history of these numbers?</p> <ol> <li>The best ML system on V100 a couple years ago got 40-50% MFU - only half the peak utilization! The peak FLOPs on V100 is 112 TFLOPs, so we were only able to use 50-60 FLOPs</li> <li>With A100, we were still in the same range until FlashAttention came up, which took the MFU value to 60%! A100 has 312 FLOPs, so we are using ~160FLOPs at this stage!</li> <li>With H100, we are able to use only 30-50% depending on the model size (Larger <code class="language-plaintext highlighter-rouge">matmul</code> is better for MFU over smaller). Why did it decrease? The peak value of H100 is very high (990 TFLOPs), and our software did not catch up to this. Remember that communication also plays an important role</li> <li>This year, with B100 the peak FLOPs is 1.8 PFLOPs!</li> </ol> <p>Besides MFU, we also define <strong>Hardware FLOPs Utilization (HFU)</strong>. This quantity is to consider operations that do not contribute to the model For example, we can treat gradient checkpointing as 2 forward passes and 1 backward pass (each backward pass can be approximated as 2 forward passes due to gradient updates)</p> <h2 id="collective-communication">Collective Communication</h2> <p>In Machine Learning systems, there are usually two types of connections</p> <ol> <li>Point-to-point communication - Comprises of a sender and a receiver. Very simple.</li> <li>Collective Communication - It’s a common concept in HPC (high performance computing). There are multiple forms of this <ol> <li>Broadcast - One worker shares data with all the other workers</li> <li>Reduce(-to-one) - All the data among the workers is combined into one rank (one worker). Essentially reverse of broadcast in a way.</li> <li>Scatter - The data from one rank is split and each of the splits is <em>scattered</em> across other ranks. That is, every worker gets on part of the data.</li> <li>Gather - The reverse operation of Scatter, where different parts of the data are <em>gathered</em> into one rank.</li> <li>All-gather - Essentially gather followed by broadcast.</li> <li>Reduce-scatter - Essentially reduce followed by scatter.</li> <li>All-reduce - Reduce followed by scatter.</li> </ol> <p>Collective communication is more expensive than P2P since it can be thought of as a combination of many P2Ps. However, collective communication has been highly optimized in the past 2 decades (<code class="language-plaintext highlighter-rouge">(x)ccl</code> libraries - NVIDIA has <code class="language-plaintext highlighter-rouge">NCCL</code> the best, Microsoft has <code class="language-plaintext highlighter-rouge">MCCL</code> and Intel has <code class="language-plaintext highlighter-rouge">OneCCL</code>). The important thing to note is that collective communication is not fault tolerant (if one device fails, then everything fails).</p> </li> </ol> <h3 id="basics">Basics</h3> <p>Let us understand some more terminology for communication models. There is a terminology called as \(\alpha\beta\) model that talks about latency and bandwidth. For example, if the model is \(\alpha + n \beta\) then \(\alpha\) refers to the latency and \(\beta = 1/B\) is the reciprocal of bandwidth determined by the hardware. For smaller messages, latency (\(\alpha\)) is the dominant factor, and for larger messages (larger \(n\)), the bandwidth utilization (\(n\beta\)) dominates.</p> <p>Based on these distinctions, the community works on two mainstream algorithms/implementations</p> <ol> <li>For smaller messages, an MST based algorithm that emphasizes low latency</li> <li>For larger messages, a <strong>Ring algorithm</strong> to emphasize on bandwidth utilization.</li> </ol> <p>There are over 50+ algorithms in this area and the HPC community even got the 2023 Turing award!</p> <p>The core principle for lower latency is to minimize the number of rounds needed for communication. For example, consider the Broadcast operation.</p> <ol> <li>We first split the ranks into half, and send the message to the half without the message</li> <li>Then repeat broadcast recursively in each half</li> </ol> <p><em>Beautiful.</em></p> <p>For the core operations, we have the following communication models</p> <ul> <li>Reduce-to-one - \(\log(p) (\alpha + n \beta + n \gamma)\)</li> <li>Scatter and Gather- \(\log(p)\alpha + \frac{p - 1}{p} n\beta\)</li> <li>Broadcast - \(\log(p)(\alpha + n \beta)\)</li> </ul> <p>The remaining composite operations can simply use these primitives.</p> <p>What are the problems with this approach? Since latency is prioritized over bandwidth, some links are idle. Building on the same idea, the core principle for high-bandwidth is to use all links between every two nodes.</p> <p>The <strong>ring algorithm</strong> essentially is a logical ring that can be embedded in a physical linear array with worm-hole routing such that the “wrap-around” message does not conflict. Look at the diagram below for clarity -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17395038648341.jpg" alt=""> At every instant, all the links are used ensuring full-bandwidth. For example, reduce-scatter can be implemented as consecutive reduce and propagate ring passes.</p> <p>For this regime of communication, the core primitives of communication are</p> <ul> <li>Reduce-scatter</li> <li>Scatter</li> <li>Gather</li> <li>All-gather and the other operations can be implemented as the composites of these</li> <li>Reduce(-to-one) is reduce-scatter followed by gather</li> <li>All-reduce is reduce-scatter followed by all-gather</li> <li>Broadcast is scatter followed by all-gather.</li> </ul> <p>So, how does this all come back to ML? ML systems are usually composed of large data communications. Inter-op systems always result in P2P communications and intra-op result in collective communication.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17395041727515.jpg" alt=""></p> <p>The key point to note is that communication always happens between nodes that are partitioned differently. Remember that with tensors there are more dimensions, and the above diagram is a simplified version. Just to summarize all ideas and the motivations behind our discussions, let us go through this again.</p> <h2 id="data-parallelism">Data Parallelism</h2> <p>Data parallelism was first suggested by Dean et. al. In DistBelief based on a Parameter server. This method was mainly based on sharding the data through batches. Then, this paradigm evolved, and integrated into PyTorch as <code class="language-plaintext highlighter-rouge">DDP</code>.</p> <p>When using data parallelism, there are two solutions to update the model parameters (assuming the models can fit inside one GPU worker)</p> <h3 id="parameter-server">Parameter server</h3> <p>As proposed in the original DistBelief paper, the parameter server is to help with gradient descent (not stochastic). The gradients are accumulated across the workers (have to wait for all workers) and are used to update the parameters. These parameters have to be replicated across all workers for further forward computations (server bottleneck). The compute to communication ratio for this is 1:10 in 2021.</p> <p>This approach can be improved to some extent by using a distributed parameter server (sharded KV stores). This ensures a redundancy across different PS shards. The savings essentially comes from using AllReduce - no server bottleneck and faster gradient sharing by workers. When the parameter server nodes are same as the worker nodes in the previous setup, the configuration simplifies a lot more. The operations then become Reduce-scatter followed by Allgather.</p> <p>Even then, one slow worker can slow down the whole system.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17399335217090.jpg" alt=""> The problem is that we have been waiting for all workers to update the parameters. However, machine learning systems are error-tolerant to some extent. Deviating from the gradient can still lead to the same performance (similar to why stochastic gradient descent works).</p> <p>So adopting a form of asynchronous communication wherein the parameters are updated every time a worker finished the computation. We can set a staleness threshold to discard very old messages for improved performance. It turns out that the training stability is heavily dependent on this parameter. There were a lot of theorems for this line of work too!</p> <h3 id="all-reduce">All-Reduce</h3> <p><code class="language-plaintext highlighter-rouge">DDP</code> is a form of Allreduce type of data parallelism. Allreduce was initially implemented in Horovod (was designed by Uber!). It was further optimized by NVIDIA in NCCL. It was then adopted in PyTorch as DDP.</p> <p>Also, Allreduce is less fault-tolerant than parameter server! However, this paradigm is more popular since ML systems are somewhat robust to noisy updates. Allreduce does not have consistency mechanisms too!</p> <p>It became popular due to its simplicity as also because of the release of NVLink - more helpful for Allreduce.</p> <h2 id="model-parallelism">Model Parallelism</h2> <p>As we’ve discussed model parallelism is usually paired with data parallelism (once models stopped fitting in one worker). There are two kinds of sharding - inter-op and intra-op.</p> <h3 id="inter-operator-parallelism">Inter-operator Parallelism</h3> <p>Inter-op parallelism usually has smaller communication requirements since only outputs at stage boundaries have to be transferred. However, unless pipelining, there are many hardware bubbles. Without pipelining, we have a bubble percentage of \((D - 1)/D\) and with pipelining we have \((D - 1)/(D - 1 + N)\) where \(D\) is the number of devices and \(N\) is the number of inputs.</p> <p>Note that we are ignoring the computation models for starting GPU kernels and other overheads for simpler models.</p> <p>However, this is mainly for inference. For doing backward pass, the devices need to be executed in reverse! Pipelining won’t do. So, how do we reduce the bubbles for training?</p> <ol> <li> <p>Device Placement - Slice the branches of a neural network into multiple stages so they can be calculated concurrently.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17399350251810.jpg" alt=""> This only works for models with branches such as the Inception model. Contrastive models also include branches and can be used. Other convolutional networks and transformers cannot be used with this.</p> </li> <li>Synchronous Pipeline Parallel Schedule - Modify pipeline schedule to improve efficiency, but keep the computation and convergence semantics exactly the same as if training with a single device. The advantage is that the training process is exactly the same. However, pipeline bubbles are always present and reducing pipeline bubbles typically requires splitting inputs into smaller components, and smaller inputs may not use the hardware to the fullest extent. <ol> <li> <strong>GPipe</strong> (Google) - Partition the input batch into multiple “micro-batches”, and accumulate the gradients of the micro-batches. <img src="/assets/img/2025-01-06-data-systems-for-ml/17399351787754.jpg" alt=""> The size of the microbatch has to be chosen so that each device is utilized to the full extent to maintain high arithmetic intensity. This choice is hard to consider. Due to the structure of GPipe, the memory usage of each device increases linearly to a peak and reduces. Each device has to store the outputs from the micro-batches until the backpropagation is called.</li> <li> <strong>1F1B</strong> - To prevent this memory peak, the 1 forward, 1 backward schedule was introduced. The idea is to perform backward pass as early as possible. <img src="/assets/img/2025-01-06-data-systems-for-ml/17399357286232.jpg" alt=""> It maintains the same latency as GPipe but with a lower memory usage. (Exam)</li> <li> <strong>Interleaved 1F1B</strong> - The previous idea can be optimized further to slice the neural network into more fine-grained stages and assign multiple stages to reduce pipeline bubbles. <img src="/assets/img/2025-01-06-data-systems-for-ml/17399360266930.jpg" alt=""> This is super hard to debug.</li> <li> <p><strong>TeraPipe</strong> - This method was mainly suggested for autoregressive models. The key idea is that the computation of an input token only depends on previous tokens but not the future tokens. The bubbles are reduced by pipelining with a sequence. <img src="/assets/img/2025-01-06-data-systems-for-ml/17399362100774.jpg" alt=""></p> </li> <li> <strong>Chimera</strong> - Store bi-directional stages and combine bidirectional pipeline to further reduce bubbles. Deepseek used this! Optimized 1F1B schedule <img src="/assets/img/2025-01-06-data-systems-for-ml/17399363005548.jpg" alt=""> The only issue is that the model parameters have to be stored twice. But if you use a model with half the number of parameters, then you’re good!</li> </ol> </li> <li> <strong>Asynchronous pipeline schedules</strong> - The idea is to start the next round of forward pass before the backward pass finishes. It breaks the synchronous training semantics, and training involves stalled gradients. The algorithms may have to store multiple versions of model weights for consistency. <ol> <li> <p><strong>AMPNet</strong> - Each device performs forward pass whenever free and updates the weights after every backward pass. It does not generalize well to larger datasets. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405374850797.jpg" alt=""></p> </li> <li> <strong>Pipedream</strong> - Enforce the same version of weight for a single input batch by storing multiple weight versions. Has a similar accuracy on Image Net with a 5x speed up compared to data parallel.</li> <li> <strong>Pipedream-2BW</strong> - Reduced memory usage by only storing 2 copied and updating the weights less frequently. The weights are always stalled by 1 update. It achieved similar accuracy on larger models like BERT and GPT.</li> </ol> </li> </ol> <p><strong>Imbalanced pipelines</strong> - All these schedules work best when the stages are balanced. However, there can scenarios where the stages do not have equal workloads. There have been works for automatic stage partitioning that try to minimize maximum stage latency and maximize parallelization.</p> <p><strong>Placeto: RL-based partitioning algorithm</strong> - The state is given by the device assignment plan for a computation graph, and the actions are modifying the device assignment of a node. The rewards are latency differences between new and old placements.</p> <p>In summary, inter-operator parallelism assigns different operators of the computational graph to different devices and executed in a pipelined fashion.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17405378678788.jpg" alt=""></p> <p>Furthermore, imbalanced stages cause further pipeline bubbles, and RL-based/optimization-based automatic stage partitioning techniques are being explored.</p> <h3 id="intra-op-parallelism">Intra-op parallelism</h3> <p>Let us understand some core ideas to implement techniques in this paradigm.</p> <p>Suppose we are parallelizing one operator.</p> <ol> <li>Element-wise operations - The operations involved are typically independent of one another and can be parallelized across devices. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405380474272.jpg" alt=""> The splitting can be done based on the optimal parameters for the underlying hardware.</li> <li>Matrix multiplication - It is a 3-level nested for-loop. We have the following variations <img src="/assets/img/2025-01-06-data-systems-for-ml/17405381689985.jpg" alt=""> <img src="/assets/img/2025-01-06-data-systems-for-ml/17405381942988.jpg" alt=""> <img src="/assets/img/2025-01-06-data-systems-for-ml/17405382663939.jpg" alt=""> </li> <li>2D convolution - It is a 7-level nested for-loop as we’ve seen before. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405385743018.jpg" alt=""> </li> </ol> <p>Now, we consider how we parallelize multiple operators together. Note that data parallelism can be seen as a special case of intra-op parallelism. We will consider two main types of matrix multiplication parallelization strategies.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17405388302958.jpg" alt=""></p> <p>Different parallelization strategies for operators require different partitioning format of the same tensor. For example, with type 1 matrix multiplications, a ReLU operator following matmul would not require any repartitioning. However, in Type 2, we would require an all-gather operation.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17405389681048.jpg" alt=""></p> <p>So given these variations, our goal is to minimize the node costs (computation and communication) and the edge costs (repartitioning communication). There have been various solutions using manual design, randomized search, dynamic programming and integer linear programming. We discuss some important projects.</p> <p><strong>Model-specific strategies</strong></p> <ol> <li> <strong>AlexNet</strong> - A convolution layer was split across 2 GPUs. It improved the accuracy by 2%, because the training improved on using 2 GPUs.</li> <li> <p><strong>Megatron-LM</strong> - A large language model with 8.3B parameters that outperforms SOTA results! They noted the following - performing Type 2 matmul followed by Type 1 matmul results in only 1 all-reduce operations for both forward and backward passes. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405394028552.jpg" alt=""></p> </li> <li> <strong>GShard MoE</strong> - A multi-language translation model with 600B parameters that outperforms SOTA. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405394573494.jpg" alt=""> Deepseek used this kind of parallelization, and so they optimized all-to-all operations in their pipeline. These implementations were shared as part of their open-source week event.</li> </ol> <p><strong>Systems for Intra-op parallelism</strong></p> <ol> <li> <strong>ZeRO Optimizer</strong> - The authors noticed that data parallelism replicates gradients, optimizer states and model weights on all devices. So, they partitioned gradients, optimizer states and model weights too! Here is a summary - <img src="/assets/img/2025-01-06-data-systems-for-ml/17405396936446.jpg" alt=""> The memory cost in the above table is per device. ZeRO Stage 3 is part of PyTorch through <code class="language-plaintext highlighter-rouge">FSDP</code> (Fully Sharded Data Parallelism). <blockquote> <p>Recall why the optimizer, gradients and model weights have the mentioned memory costs. Optimizer states are stored in FP32 (factor of 4) and the rest are in FP16 (factor of 2).</p> </blockquote> <p>Due to the communication and memory costs, everyone prefers Stage 2 over Stage 1 today.</p> <ol> <li> <strong>ZeRO Stage 2</strong> - The operation <code class="language-plaintext highlighter-rouge">all-reduce</code> is <code class="language-plaintext highlighter-rouge">reduce-scatter</code> followed by <code class="language-plaintext highlighter-rouge">all-gather</code>. Using this idea, ZeRO stage 2 does the following. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405399912790.jpg" alt=""> We obtained the same results but with much larger memory savings.</li> <li> <strong>ZeRO Stage 3</strong> - Extending this idea, we partition the model across the ranks. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405402117787.jpg" alt=""> The trade-off between Stage 2 and Stage 3 is memory vs communication cost.</li> </ol> </li> <li> <strong>Mesh Tensorflow</strong> - The idea is to map tensor dimensions to mesh dimensions for parallelism. A mesh is essentially a matrix of devices.</li> <li> <strong>GSPMD</strong> - Successor for Mesh Tensorflow (a compiler based approach) but generalizes it. The users do not have to partition every tensor, the compiler takes care of automatic partitioning across the code. That is, the annotations are propagated to the whole graph.</li> <li>There are more works like Tofu and FlexFlow which we do not cover here.</li> </ol> <p>Now that we have methods for both intra-op and inter-op, how do we combine the both of them? Such kind of a hybrid approach is compulsory to scale the parallelism to clusters that have 1000s of GPUs.</p> <p>Remember that these can be combined with the approaches we described before for the best outcome.</p> <ol> <li>System-level memory optimizations - Gradient checkpointing or memory swapping</li> <li>ML-level optimizations - Quantization, Sparsification and Low-rank approximation.</li> </ol> <h2 id="auto-parallelization">Auto-parallelization</h2> <p>The motivation to develop auto-parallelization is very similar to why we designed ML compilers for operator optimization. There are many parallelism models on different model architectures. It is difficult to keep track of which method is the best for a given model and the hardware cluster configuration.</p> <p>Again, the search space is huge because there are 100-10k operators in a model, 80-200 operator types and number of GPUs in a cluster vary from 10s to 1000s! There have been a multitude of methods for this problem -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17405409086956.jpg" alt=""></p> <p>-<strong>Search-based methods</strong> simply search the best configuration for a given model and cluster. Most companies rely on this technique. It is similar to a brute-force approach.</p> <ul> <li> <strong>Learning-based methods</strong> - Uses some sort of learned heuristic to navigate the search space. The most popular work is <a href="https://arxiv.org/abs/1706.04972" rel="external nofollow noopener" target="_blank">ColocRL by Mirhoseini et al.</a> That searches the space of inter-op strategies using an ML model to sample candidates and use a policy gradient algorithm to improve the model based on the execution cost. The method was able to obtain strategies that are 20-30% better than an expert-designed assignment. However, the method is not very efficient since it consumes a lot of resources, and RL is not very efficient as well.</li> <li> <p><strong>Optimization-based methods</strong> - Search methods but with better optimizations. One good method in this category is <a href="https://arxiv.org/abs/2201.12023" rel="external nofollow noopener" target="_blank">Alpa by Zheng et al.</a>. It tries to optimize considering both inter and intra-op parallelism considering the fast and slow connections in the GPU cluster. Intra-op parallelism strategies require communication, so splitting them across ranks with NVLink is a good heuristic. Inter-op can be done across slower communication methods. The method performs the search in a hierarchical manner - inter-op followed by intra-op.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17405414966691.jpg" alt=""></p> <ul> <li>The inter-op pass, the graph is partitioned into multiple stages, and each stage is assigned different ranks. The states in the dynamic programming algorithm are formed by these choices - partitioning and the sub-mesh assignment. This step also considers the pipeline execution latency, due to different running times of each stages. The running times are given by the intra-op passes, and we have that information. Since dynamic partitioning can introduce imbalances in the pipeline increasing the bubbles, we use dynamic programming with the execution times of each stage to correctly optimize and reach the best strategy. Essentially, the inter-op pass is like the outer loop and we have the information from the inner-loops (intra-op passes) for optimization.</li> <li>The intra-op pass parallelizes the operators across sub-meshes and minimizes the communication. For example, consider the following - <img src="/assets/img/2025-01-06-data-systems-for-ml/17407105767310.jpg" alt=""> <img src="/assets/img/2025-01-06-data-systems-for-ml/17407106048963.jpg" alt=""> <blockquote> <p>What about the split cost?</p> </blockquote> </li> </ul> </li> </ul> <p>Essentially, within each stages, we try to color every node in the stage so that the execution latency of the stage on the assigned mesh is minimized such that the peak memory usage does not cross the memory budget. Note that this problem formulation has node costs (splitting the operator) and edge costs (layout conversions).</p> <p>With this hierarchical optimization, Alpa is able to</p> <ul> <li>Match specialized manual systems with GPT</li> <li>Outperform the manual baseline for Mixture of Experts by up to 8x</li> <li>Generalize to models without manual plans</li> </ul> <p>However, the paper is not popular because the industry has stuck to one type of model (transformers) and the manual expert design for these has been good enough.</p> <p>In summary, we have</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17407109626252.jpg" alt=""></p> <p>How to choose parallelism then? Transformers have been heavily optimized manually, follow Hao’s ultimate guide below. Otherwise, use the automatic compilers.</p> <p>We consider the following factors - #GPUs, Model size, JCT (Job completion time), communication bandwidth, etc.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17407110650089.jpg" alt=""></p> <blockquote> <p>What is 3D parallelism?</p> </blockquote> <h1 id="connecting-dots">Connecting dots…</h1> <p>We have studied all the concepts. Let us know see how they are used in practice.</p> <h2 id="a-bit-of-refresher">A bit of refresher</h2> <p>We’ll list out all aspects of an LLM that are relevant for implementation -</p> <p>LLMs do next-token prediction. They basically do Maximum Likelihood Estimation based on the tokens given so far (with the prompt or generated). Transformers at their core are seq2seq NNs built on top of attention mechanism.</p> <p>The attention score computes how relevant the position i’s token is to a given output token. With language, we choose the same tokens for input and output (hence, self-attention).</p> \[SelfAttention(Q, K, V) = \text{softmax} \left(\frac{QK^T}{d^{1/2}\right) V\] <p>Conceptually, we perform this calculation in two steps -</p> <ol> <li>Pre-softmax attention score $$s_{ti} = \frac{1}{\sqrt{d}} q_tk_i^T</li> <li>Weighted average via softmax - $$h = \sum_i \text{softmax}(s)_i v_i</li> </ol> <p>We also have multiple attention heads in models to calculate different features. We get \(Q, K, V\) using embedding layers \(W_Q, W_K, W_V\).</p> <p>During training, we also mask the attention so that a <em>causal relation</em> is maintained. Only attend to previous tokens. In naïve implementation, we make the entries of the attention matrix corresponding to the mask negative infinity. Can we be more smart about them and skip these calculations? Turns out masking later is more efficient due to GPUs behavior we’ve seen before - oversubscribe and don’t disturb the layout!</p> <p>Given all this, we summarize the following</p> <ol> <li>Self-attention is slow</li> <li>Layer-norm, residual - fast</li> <li>MLPs - slow</li> <li>Non-linear activations - fast</li> <li>Word and position embeddings - fast</li> <li>Loss function - fast</li> </ol> <p>So from a computing perspective, how do we make self-attention fast?</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17407127855541.jpg" alt=""></p> <p>GPT-3 has 96 of these! So we definitely need to make it fast.</p> <p>These were the core parts of an LLM. Notably, only the fast operations have been changing a lot, and the core attention and MLP mechanisms have remained.</p> <table> <thead> <tr> <th>Feature</th> <th>Vaswani et al.</th> <th>LLaMA</th> </tr> </thead> <tbody> <tr> <td>Norm Position</td> <td>Post</td> <td>Pre</td> </tr> <tr> <td>Norm Type</td> <td>LayerNorm</td> <td>RMSNorm</td> </tr> <tr> <td>Non-linearity</td> <td>ReLU</td> <td>SiLU</td> </tr> <tr> <td>Positional Encoding</td> <td>Sinusoidal</td> <td>RoPE</td> </tr> </tbody> </table> <h2 id="training-llms">Training LLMs</h2> <p>As we have noted, the three main components of implementation are compute, memory and communication. We must know</p> <ol> <li>The number of parameters of an LLM - it tells us how much memory and communication bandwidth is needed</li> <li>The FLOPs needed to train the LLM - it tells us how much compute is needed</li> <li>The memory needed to train an LLM - again, tells us about memory and communication considering activations and gradients</li> </ol> <p>Let us delve into each of these questions</p> <h3 id="calculating-number-of-parameters">Calculating number of parameters</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17407132381084.jpg" alt=""></p> <ol> <li>The embedding layer has weight matrices of size $$</li> <li> <p>The SwiGLU layer has the following equation</p> \[\text{SwiGLU}(x) = \text{Swish{(xW_1 + b_1) \cdot (xW_2 + b_1)\] <p>So in comparison with vanilla feedforward, we have one extra set of parameters.</p> </li> <li>The linear transformation in the end essentially converts embeddings to token space and shares the weights with the embedding layer (<code class="language-plaintext highlighter-rouge">tie_embd</code> in HuggingFace)</li> </ol> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17407134503575.jpg" alt=""></p> <p>Notice that the major bottleneck comes from the feedforward layers. It has \(12h^2\) parameters in LLaMa! The second bottleneck is from self-attention which has \(4h^2\) parameters.</p> <h3 id="calculating-flops">Calculating FLOPs</h3> <p>As we’ve noted multiple times, <code class="language-plaintext highlighter-rouge">matmul</code> is the most expensive operation in terms of FLOPs. We’ve seen that the total number of FLOPs is approximately \(2mnh\) for multiplying matrices of sizes \((m \times n)\) and \((n \times h)\).</p> <table> <thead> <tr> <th>Component</th> <th>Output Shape</th> <th>FLOPs</th> </tr> </thead> <tbody> <tr> <td><strong>Input:</strong></td> <td> </td> <td> </td> </tr> <tr> <td>X</td> <td>(b, s, h)</td> <td>0</td> </tr> <tr> <td><strong>Self Attention:</strong></td> <td> </td> <td> </td> </tr> <tr> <td>XW_Q, XW_K, XW_V</td> <td>(b, s, h)</td> <td>3 * 2bsh²</td> </tr> <tr> <td>RoPE</td> <td>(b, n, s, d)</td> <td>3bsnd</td> </tr> <tr> <td>P = Softmax(QKᵀ/√d)</td> <td>(b, n, s, s)</td> <td>2bs²nd + 3bs²n</td> </tr> <tr> <td>PV</td> <td>(b, n, s, d)</td> <td>2bs²nd</td> </tr> <tr> <td>AW_O</td> <td>(b, s, h)</td> <td>2bsh²</td> </tr> <tr> <td><strong>Residual Connection:</strong></td> <td>(b, s, h)</td> <td>bsh</td> </tr> <tr> <td><strong>Output from Self Attn:</strong></td> <td> </td> <td> </td> </tr> <tr> <td>X</td> <td>(b, s, h)</td> <td>0</td> </tr> <tr> <td><strong>Feed-Forward SwiGLU:</strong></td> <td> </td> <td> </td> </tr> <tr> <td>XW_gate / XW_up</td> <td>(b, s, i)</td> <td>2 * 2bshi</td> </tr> <tr> <td>Swish Activation</td> <td>(b, s, i)</td> <td>4bsi</td> </tr> <tr> <td>Element-wise *</td> <td>(b, s, i)</td> <td>bsi</td> </tr> <tr> <td>XW_down</td> <td>(b, s, h)</td> <td>2bshi</td> </tr> <tr> <td><strong>RMS Norm:</strong></td> <td>(b, s, h)</td> <td>4bsh + 2bs</td> </tr> </tbody> </table> <p>where</p> <ul> <li> <strong>b</strong>: Batch size</li> <li> <strong>s</strong>: Sequence length</li> <li> <strong>n</strong>: Number of attention heads</li> <li> <strong>d</strong>: Hidden state dimension of one head</li> <li> <strong>h</strong>: Hidden state dimension</li> <li> <strong>i</strong>: SwiGLU projection dimension</li> </ul> <p>Note that attention is a very expensive operation because we have \(s^2\) - it does not allow us to scale to large number of tokens!</p> <p>The final equation we get is</p> \[\text{Total FLOPs} = \#\text{num layers} *(6bsh^2 + 4bs^2h + 3bs^2 n + 2bsh^2 + \#\text{num layers} (6 bshi) + 2bshv\] <p>For Llama 7B, this comes out to be 63 TFLOPs! The MLP is \(\approx 55\%\) of the total FLOPs and attention is \(\approx 41 \%\)!</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17407142596268.jpg" alt=""></p> <p>To reduce the bottleneck, one might try to decrease \(h\) (the hidden dimension) to scale up the model. The other way is to reduce the sequence length.</p> <p>Note the factor of 3 above that is because of training (the model parameters and optimizer parameters). Let us recall the memory usage in models</p> <ol> <li>Model Weights - \(2 \times M\) for FP16</li> <li>Intermediate activation values <ol> <li>If we checkpoint at transformers boundary, the activation memory is \(b \times s \times h \times n\_layers\). It is same as what we need to communicate in inter-op parallelism.</li> <li>The activation inside a transformer has the size \((b, n, s, s)\). It is the bottleneck for memory! We’ll see <strong>Flash Attention</strong> on how to avoid saving the complete attention matrix</li> </ol> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17411426493026.jpg" alt=""></p> <p>In summary, these are the activations inside the transformer block.</p> </li> <li>Optimizer State - \(12 \times M\) for FP16-32 mixed-precision</li> <li>Weight Gradients + Activation Gradients - \(2 \times M\).</li> </ol> <p>Based on this analysis, we have the following observations</p> <ol> <li>All terms are at least linear with \(h\) - Useful observation for scaling up</li> <li>Dimensions of partitioning <ol> <li>\(b\) would be data parallelism</li> <li>\(d\) - Megatron based architecture</li> <li>\(s\) - Part of 4D parallelism</li> </ol> </li> </ol> <blockquote> <p>Advanced Topics (Recently published research) Megatron-style parallelism has been well adapted. How do we partition along \(s\)? We can partition \(n\) is attention (assign different heads to different GPUs) and \(s\) in MLP. Deepspeed Ulysses sequence parallelism is a recently proposed technique that partitions on \(n\) first and then partitions on \(s\). Moreover, it often happens that \(n \ll \#GPUs\) or is not a multiple of 8, then it is prudent to partition \(s\) in both attention and MLP. This is implemented in the form of <strong>ring attention</strong>.</p> </blockquote> <blockquote> <p>Rule of thumb: In many computer systems and algorithms, anything more complex than quadratic is less likely to be adapted at large scale.</p> </blockquote> <h2 id="scaling-laws">Scaling Laws</h2> <p>Continuing on our observations</p> <ol> <li>Computer is a function of \(h, i, b\)</li> <li>\(\#\)parameters is a function of \(h, i\)</li> <li>So the computer is a function \(\#\) parameters.</li> </ol> <p>But as we know, the compute budget is limited. Then we have to decide between</p> <ol> <li>Training models longer (on more data) vs training bigger models</li> <li>Collect more data vs get more GPUs</li> <li>How to choose exact \(h, i\), etc.</li> </ol> <p>Our motivation for scaling laws is to understand how large a model should we train and how much data should we use to achieve a given performance subject to a compute budget.</p> <p>For applications like statistical estimators, we can easily derive theoretical scaling laws based on mathematical axioms. However, that seems to be much difficult for language models.</p> <p>Then, how do we analyze these models? We perform empirical observations. Our analysis on machine learning has moved from mathematics based proofs to physics like experiments. Relying on observations. It became a strong trademark after the GPT-3 paper.</p> <p>Here is what we have got so far, observing based on empirical scaling laws:</p> <ul> <li>To conclude transformers are better than LSTMs, instead of spending tens of millions to train LSTM GPT-3, we extrapolated results and noticed that transformers outperform these models.</li> <li>Similarly for depth vs width of networks - 1 vs 2 layers make s a huge difference but more layers have diminishing returns after a certain number of parameters.</li> </ul> <p>Essentially, the scaling law way trains smaller models to establish a scaling law and select the optimal hyper-parameters based on the scaling law prediction. This allows us to get a heuristic on the effects of hyper-parameters in big LMs before training.</p> <p>Consider the question data vs compute trade-off - for a given compute is it better to train undertrained big models or well-trained small models?</p> \[N_{opt} (C), D_{opt}(C) = \arg_{N, D}\min_{FLOPs(N, D) = C} L(N, D)\] <p>where \(N\) is the number of parameters, \(D\) is the data and \(C\) is the compute. We obtained the following after spending billions of dollars for next-prediction loss</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17411445543561.jpg" alt=""></p> \[L(N, D) = \frac{406.4}{N^{0.34}} \frac{410.7}{D^{0.29}} + 1.69\] <p>and every LLM seems to follow this…</p> <h2 id="moe-llms">MoE LLMs</h2> <p>The key idea like we discussed before is to make each expert focus on predicting the right answer for a subset of cases.</p> <p>Instead of choosing one expert each time, it is also possible to choose multiple experts for a token, and that becomes another hyper parameter. That is, we can choose to activate two experts each time and add the output of both of them.</p> <p>Let us consider the number of parameters and analyze MoE with scaling laws.</p> <ul> <li>The number of parameters increase drastically - MLP params \(\times N/2\) (if we choose to activate 2 experts each time) where \(N\) is the number of experts. For example, Deepseek V3 has 256 experts!</li> <li>Memory for parameters increases by the same factor. However, activations don’t consume too much additional memory. Even though the parameters increase drastically, the compute only increases mildly. This is why Deepseek blew up. For much higher number of experts, the attention parameters decrease largely (sparse activations in experts), and they were able to train dense model equivalent model with much lower compute.</li> </ul> <p>People also tried to get scaling laws for MoEs and noticed that MoE is a much more compute-efficient model (it has a better scaling law).</p> <p>Let us analyze the architecture again. We stuck to a Megatron like architecture for transformers. However, it would not work with MoE since all the weights are replicated number of expert times. So we perform parallelism in the dimension of experts. <img src="/assets/img/2025-01-06-data-systems-for-ml/17411455774654.jpg" alt=""></p> <p>This way, each GPU would have different workloads (bubbles!) - A hot expert problem. Deepseek implemented many mechanisms to reduce this as much as possible.</p> <h2 id="inference">Inference</h2> <p>So far we’ve studied that LLMs are slow and expensive to serve. At least 10 A100-40GB GPUs are required to serve 175B GPT-3 in half precision and generating 256 tokens takes approximately 20 seconds.</p> <p>The key factor is that we perform autoregressive decoding - we require the current token to predict the next token. Note that we predict until the pre-defined maximum length or we reach the end of sequence.</p> <p>The inference stage can be divided into two stages</p> <ol> <li>Prefilling - Process all input tokens at once (prompt)</li> <li>Decoding phase - Process a single token generated from previous iteration (generating)</li> </ol> <p>The decoding phase is optimized with <strong>Key-value cache</strong> by saving attention keys and values during the decoding phase to avoid recomputations.</p> <p>In a broader sense, serving an LLM has more considerations during inference. For a larger group, it would be prudent to emphasize on throughput and if it is an individual then latency is a concern. How do we optimize for these scenarios?</p> <p>Different techniques have been proposed. For the single user case, a popular technique has been <strong>speculative decoding</strong>.</p> <p>The latency is essentially \(\text{latency} = \text{ step latency} \times \#\text{steps}\)</p> <p>With speculative decoding, we try and reduce the number of steps!</p> <h1 id="guest-lecture-hongyang-zhang-speculative-decoding">[Guest Lecture: Hongyang Zhang] Speculative Decoding</h1> <p>The vanilla autoregressive inference involves sampling tokens from a probability distribution that is obtained by the tokens generated so far.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-18-35-07-image.png" alt=""></p> <p>How do we move from this sequential sampling framework to a parallel one?</p> <p><strong>Speculative sampling framework</strong> (introduced in <a href="https://arxiv.org/abs/2211.17192" rel="external nofollow noopener" target="_blank">Leviathan et al</a>) uses a much smaller model that is a good approximation of the original large model. This tiny <strong>draft model</strong> is used to create a <em>draft</em> of the response in the same sequential autoregressive manner. Since the model is tiny, the inference is faster.</p> <p>The tokens generated by the draft model are fed into the LLM to generate the distributions from these tokens in parallel. The draft model can be verified by comparing the distributions of both the models for a given token.</p> <ol> <li> <p>We generate a number \(r\) between 0 and 1 uniformly, and check if \(r &lt; \min(1, p(t)/q(t))\) where \(p\) is the LLM distribution and \(q\) is the draft model distribution.</p> </li> <li> <p>If the condition holds, we accept the token. Otherwise we correct the token by sampling \(t' \sim \norm(\max(0, p - q))\). All the tokens following this corrected token are discarded, and the process is repeated again.</p> </li> </ol> <p>There is theoretical work that shows that the above procedure is equivalent to sampling from \(p\).</p> <p>Given this, how do we build the tiny model \(q\)? That is the crux of the methods in this domain.</p> <blockquote> <p>Does this framework only work for greedy sampling? No, it can be extended to non-greedy sampling techniques with some modifications.</p> </blockquote> <blockquote> <p>The tiny model used can have low rejection rates for tasks like summarization but may have high rejection rates for tasks involving reasoning. So, there’s a trade-off with the draft model overhead and the rejection rate.</p> </blockquote> <h3 id="building-q-with-eagle-v1">Building \(q\) with EAGLE-v1</h3> <p>As mentioned before, building \(q\) is a trade-off between accuracy and efficiency. They have proposed EAGLE.</p> <p>The authors of this paper observed that the initial distribution of the embeddings in the initial layers is very complicated and dynamic. However, for the top layers, the distribution becomes much simpler - that is, the distribution to predict next tokens from previous tokens.</p> <p>So, they tried to estimate the features rather than the tokens directly. In their first attempt, they trained a single transformer layer to learn the embeddings (features) in the last layer of LLMs. Essentially, instead of using the first layer tokens, they used the last layer features for the speculative decoding framework. With this, they obtained a much higher speed-up and accuracy!</p> <p>However, they noted that this model does not use feature uncertainty correctly. In the speculative decoding framework, the parallel decoding in the large LLM does not consider the previously generated</p> <p>To solve this, they tried another method by predicting the distribution by concatenating the initial token embedding with the feature from the last layer. They attribute this as adding a shifted token embedding.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-19-01-18-image.png" alt=""></p> <p>Essentially, we generate choices through the draft model rather than a single prediction. With this, we obtain a much higher speed-up and accuracy -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-18-55-53-image.png" alt=""></p> <p>Note that the attention mask for thi has to be carefully design as follows -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-19-07-29-image.png" alt=""></p> <p>Because of this, the model cannot handle long-context settings.</p> <p>The draft model can be around 1-3% (higher percentage for smaller models to account for the dimensionality of the latent space) of the larger model leading to a very high speed-up!</p> <h3 id="eagle-v2">EAGLE-v2</h3> <p>The authors noted that the method can be improved even further. They noted that the accuracy of the speculative decoding tree if higher to the top-left and lowest to bottom-right. So, they associated a confidence score to each node in the tree. Ideally, that would be \(\min(1, p(t)/q(t))\), but they found that \(q(t)\) is a good approximation and has a strong linear correlation with the actual quantity.</p> <p>With this, they build a context-aware dynamic draft tree.</p> <p>Essentially, instead of generating a tree, they do beam-search with the small model.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-19-22-55-image.png" alt=""></p> <p>As expected, this method obtained even higher speedup and accuracy!</p> <p>They didn’t stop there.</p> <h3 id="eagle-v3">EAGLE-v3</h3> <p>They published yesterday lol! With the new method that has a modified training routine and a loss function, they noticed a new scaling law that seems to perform better as the model grows. Essentially, they introduced draft model inference in the training routine to better simulate the inference routine.</p> <p>They also observed that the performance is the best when the draft model architecture is the same as the large model.</p> <blockquote> <p>The draft model just generates a draft and not the complete response. For example, it will generate 6 words, and these are used by the larger LLM. So it’s different from knowledge distillation.</p> </blockquote> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/machine-learning-systems/">Key Works in ML Systems</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ai-agents/">AI Agents</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/rl-theory/">Reinforcement Learning Theory</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/brains-and-ai/">Brains and AI</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Statistical-NLP/">Statistical Natural Language Processing</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Sudhansh Peddabomma. Last updated: March 09, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script src="/assets/js/tooltips-setup.js?53023e960fbc64cccb90d32e9363de2b"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-0K9MLG0V24');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> <div id="chat-window" class="chat-window"> <div class="chat-header"> <h5 class="mb-0">Talk to my AI</h5> <button id="close-chat" class="btn-close"> <i class="fas fa-times"></i> </button> </div> <div id="chat-messages" class="chat-messages"></div> <div class="chat-input-container"> <input type="text" id="chat-input" class="form-control" placeholder="Type a message..."> <button id="send-btn" class="btn btn-primary"> <i class="fas fa-paper-plane"></i> </button> </div> </div> </body> </html>