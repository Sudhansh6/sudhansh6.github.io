# Simulated Environments and Reality

Why do we need simulations? Most tasks have many ways of completing them. There is no notion of *global* optimal solutions ahead of time but usually known once the task is complete. 

The agent needs to explore to find many solutions to compare and see what is the most efficient. However, exploration in the read world is expensive - wear and tear of robots, excessive compute, danger to humans, etc.

Simulations offer an easy solution to these problems. Assign a set of rules, and let a world emerge. One of the early examples of this is [Conway’s Game of Life](https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life) which theorized that complicated behaviors can emerge by just a few rules. 

From an MDP perspective, a simulation contains $$<S, A, T>$$ where 
- $$S$$ is the set of all states. It consists of propositions that are true/false. Example: You are in a house, door is open, knife in drawer
- $$A$$ is the set of all actions. Example: Take knife from drawer, walk through door
- $$T$$ is the transition matrix - (You are in the house, you walk out of the door) -> You are outside the house.

A simulation need not have an explicit reward.

## Sim2Real Transfer
The ability of an agent trained in simulation transfer to reality is dependent on how good the model extrapolates out of distribution. With the current stage of agents, the simulation is made as close to reality as possible to reduce the Sim2Real gap.

How do we measure closeness to reality? The tasks in the real world have different types of complexities -
1. Cognitive complexity - Problems that requires long chains of *reasoning* - puzzles, math problems or moral dilemmas
2. Perceptive complexity - Requires high levels of vision and/or precise motor skills - bird watching, threading a needle, Where’s Waldo

Examples of simulations -
1. Grid world - low cognitive and almost zero perceptive. However, this idea can arbitrarily scale to test algorithms for their generalization potential in controllable settings. 
2. Atari - low perceptive, medium cognitive. Atari games became very popular in 2013, when Deepmind released their [Deep Q-Net](https://arxiv.org/pdf/1312.5602) paper that achieved human level skills on these games.
3. [Zork](https://en.wikipedia.org/wiki/Zork), [NetHack](https://www.nethack.org) - low perceptive, high cognitive. These are configurations or worlds that you purely interact with text. The worlds are actually so complex that there is no agent that is able to finish the challenge!
4. [Clevr simulation](https://cs.stanford.edu/people/jcjohns/clevr/) - medium perceptive, low cognitive - This simulation generates images procedurally with certain set of objects and has reasoning questions for each image.
5. [AI2 THOR](https://ai2thor.allenai.org) - medium perceptive, medium cognitive. Worlds with ego-centric views for robotics manipulation and navigation simulations
6. [AppWorld](https://arxiv.org/pdf/2407.18901) - medium perceptive, medium cognitive. A bunch of different apps that you would generally use in daily life. The agents can access apps, and the simulation also has human simulators. This simulation is one that is closest to reality in the discussed so far!
7. [Minecraft](https://www.minecraft.net/en-us) - medium perceptive, high cognitive. A voxel based open-world game that lets players take actions similar to early-age humans. 
8. [Mujoco](https://mujoco.org) - high perceptive, low cognitive. It is a free and open source physics engine to aid the development of robotics.
9. [Habitat](https://ai.meta.com/research/publications/habitat-a-platform-for-embodied-ai-research/) - high perceptive, medium cognitive. A platform for research in embodied AI that contains indoor-world ego-centric views similar to AI2 THOR, but with much better graphics. They have recently added sound in the environment too! 
10. High perceptive, high cognitive - Real world, and whoever gets this simulation right, wins the race to AGI. It requires people to sit down and enumerate all kinds of rules. Game Engines like Unreal and Unity are incredibly complex, and are the closest we’ve gotten.
    
    Some researchers try to “learn” the simulations from real-world demonstrations.
    
In each of these simulators, think of the complexity and reward sparsity in the environment. It is easy to build a simulator that gives rewards at a goal state than the one that gives a reward for each action. There are some open-lines of research in this domain -
1. Which dimensions of complexity transfer more easily? Curriculum learning
2. Can you train on lower complexity and switch to a higher complexity?
3. Can we learn the world model holy grail?



