<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>DiBS Notes | Sudhansh</title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="An introductory course for design and programming of database systems. Covers the entity-relationship (ER) approach to data modelling, the relational model of database management systems (DBMSs) and the use of query languages such as SQL. Briefly discusses query processing and the role of transaction management."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/dbms/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Sudhansh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Articles</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">DiBS Notes</h1> <p class="post-meta">January 6, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/category/notes"> <i class="fa-solid fa-tag fa-sm"></i> Notes</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="lecture-1">Lecture 1</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">03/01/2022</code></p> </blockquote> <h1 id="chapter-1-introduction">~Chapter 1: Introduction</h1> <ul> <li> <strong>Embedded databases</strong> - Databases which don’t have high amount of concurrent access, but is mostly used by a single user. These databases are implemented by SQLite in general.</li> <li> <p>Motivation for Database systems -</p> <ul> <li>Atomicity</li> <li>Concurrency</li> <li>Security</li> <li>…</li> </ul> </li> </ul> <h2 id="data-models">Data Models</h2> <p>A collection tools for describing the data, relationships in the data, semantics and other constraints.</p> <p>We start using the relational models that are implemented using SQL. Then, we shall study Entity-Relationship data model. These are used for database design. We will briefly touch upon Object-based data models. Finally, we shall see Semi-structured data model as a part of XML.</p> <p>There are also other models like Network model and Hierarchical model (old) etc.</p> <h2 id="relational-model">Relational Model</h2> <p>All the data is stored in various tables. A <strong>relation</strong> is nothing but a table. Back in the 70’s, Ted Codd (Turing Award 1981) formalized the Relational Model. According to his terminology, a table is same as a <em>relation</em>, a column is also called an <em>attribute</em>, and the rows of the table are called as <em>tuples</em>. The second major contribution he made was introducing the notion of operations. Finally, he also established a low-level database engine that could execute these operations.</p> <p>Some more terminology - <strong>Logical Schema</strong> is the overall logical structure of the database. It is analogous to the type information of a variable in a program. A <strong>physical schema</strong> is the overall physical structure of the database. An <strong>instance</strong> is the actual content of the database at a particular point in time. It is analogous to the value of a variable. The notion of <strong>physical data independence</strong> is the ability to modify the physical schema without changing the logical schema.</p> <h3 id="data-definition-language-ddl">Data Definition Language (DDL)</h3> <p>It is the specification notation for defining the database schema. DDL compiler generates a set of table templates stored in a <em>data dictionary</em>. A data dictionary contains metadata (data about data) such as database schema, integrity constraints (primary key) and authorization.</p> <h3 id="data-manipulation-language-dml">Data Manipulation Language (DML)</h3> <p>It is the language for accessing and updating the data organized by the appropriate data model. It is also known as a <em>query language</em>. There are basically two types of data-manipulation languages - <strong>Procedural DML</strong> requires a user to specify what data is needed and how to get that data; <strong>Declarative DML</strong> requires a user to specify what data is need without specifying how to get those data. Declarative/non-procedural DMLs are usually easier to learn.</p> <h3 id="sql-query-language">SQL Query Language</h3> <p>SQL query language is <strong>non-procedural</strong>! It is declarative. SQL is <strong>not</strong> a Turing machine equivalent language. There are extensions which make it so. SQL does not support actions such as input from the users, typesetting, communication over the network and output to the display. A query takes as input several tables and always returns a single table. SQL is often used embedded within a higher-level language.</p> <p><strong>Database Design</strong> involves coming up with a Logical design and Physical design.</p> <p>A <strong>Database Engine</strong> accepts these queries and parses them. It is partitioned into modules that deal with each of the responsibilities of the overall system. The functional components of a database system can be divided into</p> <ul> <li> <p>Storage manager - Actually stores the data. It takes the logical view and maps it to the physical view. It is also responsible to interact with the OS file manager for efficient storing, retrieving and updating of data. It has various components such as authorization and integrity manager, transaction manager, file manager and buffer manager. It implements several data structures as a part of the physical system implementation - data files, data dictionary and indices.</p> </li> <li> <p>Query processor - It includes DDL interpreter, DML compiler (query to low-level instructions along with query optimization) and the query evaluation engine.</p> <p>Query processing involves parsing and translation, optimization, and evaluation. Statistics of the data are also used in optimization.</p> </li> <li> <p>Transaction management - A <strong>transaction</strong> is a collection of operations that performs a single logical function in a database application. The <strong>transaction management component</strong> ensures that the database remains in a consistent state despite system failure. The <strong>concurrency control manager</strong> controls the interaction among the concurrent transactions to ensure the consistency of the data.</p> </li> </ul> <h1 id="lecture-2">Lecture 2</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">04-01-22</code></p> </blockquote> <h3 id="database-architecture">Database Architecture</h3> <p>There are various architectures such as centralized databases, client-server, parallel databases and distributed databases.</p> <h1 id="chapter-2-intro-to-relational-model">~Chapter 2: Intro to Relational Model</h1> <h3 id="attributes">Attributes</h3> <p>The set of allowed values for each attribute is called the <strong>domain</strong> of the attribute. Attribute values are required to be <em>atomic</em> - indivisible. Todd realized that having sets or divisible attributes complicates the algebra. The special value <strong><em>null</em></strong> is a member of every domain that indicates the value is unknown. The null values causes complications in the definition of many operations.</p> <p>Relations are <strong>unordered</strong>. The order of tuples is irrelevant for the operations logically.</p> <p><strong>Database Schema</strong> - The logical structure of the database.</p> <h3 id="keys">Keys</h3> <p><em>K</em> is a <strong>superkey</strong> of the relation <em>R</em> if values for <em>K</em> are sufficient to identify a unique tuple of each possible relation \(r(R)\). Superkey \(K\) is a <strong>candidate key</strong> if \(K\) is minimal. One of the candidate keys is selected to be the <strong>primary key</strong>. A <strong>foreign key</strong> constraint ensures that the value in one relation must appear in another. There is a notion of <em>referencing</em> relation and a <em>referenced</em> relation.</p> <h3 id="relational-query-languages">Relational Query Languages</h3> <p>Pure languages include Relational algebra, Tuple relational calculus and Domain relational calculus. These three languages are equivalent in computing power.</p> <h1 id="lecture-3">Lecture 3</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">06/01/2022</code></p> </blockquote> <h2 id="relational-algebra">Relational Algebra</h2> <p>An algebraic language consisting of a set of operations that take one or two relations as input and produces a new relation as their result. It consists of six basic operators -</p> <ul> <li>select \(\sigma\)</li> <li>project \(\Pi\)</li> <li>union \(\cup\)</li> <li>set difference \(-\)</li> <li>Cartesian product \(\times\)</li> <li>rename \(\rho\)</li> </ul> <p>We shall discuss each of them in detail now.</p> <h3 id="select-operation">Select Operation</h3> <p>The <strong>select</strong> operation selects tuples that <u>satisfy a given predicate</u>. So, it’s more like <code class="language-plaintext highlighter-rouge">where</code> rather than <code class="language-plaintext highlighter-rouge">select</code> in SQL. The notation is given by \(\sigma_p(r)\).</p> <h3 id="project-operation">Project Operation</h3> <p>A unary operation that returns its argument relation, with certain attributes left out. That is, it gives a subset of attributes of tuples. By definition, it should only return the attributes. However, in most cases we can return modified attributes. The notation is given by \(\Pi_{attr_1, attr_2, ...}(r)\)</p> <h3 id="composition-of-relation-operations">Composition of Relation Operations</h3> <p>The result of a relational-algebra is a relation and therefore we different relational-algebra operations can be grouped together to form <u>relational-algebra expressions</u>.</p> <h3 id="cartesian-product-operation">Cartesian-product Operation</h3> <p>It simply takes the cartesian product of the two tables. Then, we can use the select condition to select the relevant (rational) tuples.</p> <p>The <strong>join</strong> operation allows us to combine a select operation and a Cartesian-Product operation into a single operation. The join operation \(r \bowtie_\theta s = \sigma_\theta (r \times s)\). Here \(\theta\) represents the predicate over which join is performed.</p> <h3 id="union-operation">Union operation</h3> <p>This operation allows us to combine two relations. The notation is \(r \cup s\). For this operation to be vald, we need the following two conditions -</p> <ul> <li>\(r, s\) must have the same <strong>arity</strong> (the same number of attributes in a tuple).</li> <li>The attribute domains must be <u>compatible</u>.</li> </ul> <blockquote> <p>Why second?</p> </blockquote> <h3 id="set-intersection-operation">Set-Intersection Operation</h3> <p>This operator allows us to find tuples that are in both the input relations. The notations is \(r \cap s\).</p> <h3 id="set-difference-operation">Set Difference Operation</h3> <p>It allows us to find the tuples that are in one relation but not in the other.</p> <h3 id="the-assignment-operation">The assignment Operation</h3> <p>The assignment operation is denoted by \(\leftarrow\) and works like the assignment in programming languages. It is used to define temporary relation variables for convenience. With the assignment operation, a query can be written as a sequential program consisting of a series of assignments followed by an expression whose value is displayed as the result of the query.</p> <h3 id="the-rename-operation">The rename Operation</h3> <p>The expression \(\rho_x(E)\) is used to rename the expression \(E\) under the name \(x\). Another form of the rename operator is given by \(\rho_{x(A1, A2, ...)}(E)\).</p> <blockquote> <p>Difference between rename and assignment? Is assignment used to edit tuples in a relation?</p> </blockquote> <p>Are these set of relational operators enough for Turing completeness? No! Check <a href="https://www.quora.com/Turing-Completeness/Why-is-relational-algebra-not-Turing-complete#:~:text=Relational%20algebra%20clearly%20doesn't,analysis%20such%20as%20query%20optimizers." rel="external nofollow noopener" target="_blank">this</a> link for more info.</p> <h3 id="aggregate-functions">Aggregate Functions</h3> <p>We need functions such as <code class="language-plaintext highlighter-rouge">avg</code>, <code class="language-plaintext highlighter-rouge">min</code>, <code class="language-plaintext highlighter-rouge">max</code>, <code class="language-plaintext highlighter-rouge">sum</code> and <code class="language-plaintext highlighter-rouge">count</code> to operate on the multiset of values of a column of a relation to return a value. Functions such as <code class="language-plaintext highlighter-rouge">avg</code> and <code class="language-plaintext highlighter-rouge">sum</code> cannot be written using FOL or the relations we defined above. Functions such as <code class="language-plaintext highlighter-rouge">min</code> and <code class="language-plaintext highlighter-rouge">max</code> can be written using a series of queries but it is impractical. The other way of implementing this is to use the following</p> <div style="text-align:center;"> $$ \Pi_{mark}(marks) - \Pi_{m1.mark}(\sigma_{m1.mark &gt; m2.mark} \\ (\rho_{m1}(marks) \times \rho_{m2}(marks))) $$ </div> <p>However, this definitive expression is very inefficient as it turns a linear operation to a quadratic operation.</p> <p><strong>Note.</strong> The aggregates <strong>do not</strong> filter out the duplicates! For instance, consider $\gamma_{count(course_id)}(\sigma_{year = 2018}(section))$. What if a course has two sections? It is counted twice.</p> <h3 id="group-by-operation">Group By Operation</h3> <p>This operation is used to group tuples based on a certain attribute value.</p> <h2 id="equivalent-queries">Equivalent Queries</h2> <p>There are more ways to write a query in relation algebra. Queries which are <u>equivalent</u> need not be <u>identical</u>.</p> <p>In case of SQL, the database optimizer takes care of optimizing equivalent queries.</p> <h1 id="chapter-3-basic-sql">~Chapter 3: Basic SQL</h1> <h2 id="domain-types-in-sql">Domain Types in SQL</h2> <ul> <li> <code class="language-plaintext highlighter-rouge">char(n)</code> - Fixed length character string, with user-specified length \(n\). We might need to use the extra spaces in the end in the queries too!</li> <li> <code class="language-plaintext highlighter-rouge">varchar(n)</code> - Variable length strings</li> <li>…</li> </ul> <h2 id="create-table-construct">Create Table Construct</h2> <p>An SQL relation is defined using the create table command -</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">table</span> <span class="n">r</span>
	<span class="p">(</span><span class="n">A_1</span><span class="p">,</span> <span class="n">D_1</span><span class="p">,</span> <span class="n">A_2</span><span class="p">,</span> <span class="n">D_2</span><span class="p">,</span> <span class="p">...,</span> <span class="n">A_n</span><span class="p">,</span> <span class="n">D_n</span><span class="p">)</span>
</code></pre></div></div> <h2 id="integrity-constraints-in-create-table">Integrity Constraints in Create Table</h2> <p>Types of integrity constraints</p> <ul> <li>primary key \((A_1, A_2, A_3, ...)\)</li> <li>Foreign key \((A_m, ..., A_n)\) references r</li> <li>not <code class="language-plaintext highlighter-rouge">null</code> </li> </ul> <p>SQL prevents any update to the database that violates an integrity constraint.</p> <h1 id="lecture-4">Lecture 4</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">10-01-22</code></p> </blockquote> <h2 id="basic-query-structure">Basic Query Structure</h2> <ul> <li>A typical SQL query has the form:</li> </ul> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">select</span> <span class="n">A1</span><span class="p">,</span> <span class="n">A2</span><span class="p">,</span> <span class="p">...,</span> <span class="n">An</span>
<span class="k">from</span> <span class="n">r1</span><span class="p">,</span> <span class="n">r2</span><span class="p">,</span> <span class="p">...,</span> <span class="n">rm</span>
<span class="k">where</span> <span class="n">P</span>
</code></pre></div></div> <p>where, \(A_i\) are attributes, \(r_i\) are relations, and \(P\) has conditions/predicates. The result of an SQL query is a relation. SQL is case-insensitive in general.</p> <ul> <li>We shall be using PostgreSQL for the rest of the course.</li> <li>SQL names are usually case insensitive. Some databases are case insensitive even in string comparison!</li> </ul> <h3 id="select-clause">select clause</h3> <ul> <li>To force the elimination of duplicates, insert the keyword <code class="language-plaintext highlighter-rouge">distinct</code> after select. Duplicates come from <ol> <li>Input itself is a multiset</li> <li>Joining tables</li> </ol> <p>Removing duplicates imposes an additional overhead to the database engine. Therefore, it was ubiquitously decides to exclude duplicates removal in SQL.</p> </li> <li> <p>The keyword <code class="language-plaintext highlighter-rouge">all</code> specifies that duplicates should not be removed.</p> </li> <li>SQL allows renaming relations and attributes using the <code class="language-plaintext highlighter-rouge">as</code> clause. We can skip <code class="language-plaintext highlighter-rouge">as</code> in some databases like Oracle. Also, some databases allow queries with no <code class="language-plaintext highlighter-rouge">from</code> clause.</li> </ul> <h3 id="from-clause">from clause</h3> <p>If we write <code class="language-plaintext highlighter-rouge">select * from A, B</code>, then the Cartesian product of <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> is considered. This usage has some corner cases but are rare.</p> <p>### as clause</p> <p>It can be used to rename attributes as well as relations.</p> <h3 id="self-join">Self Join</h3> <p>How do we implement various levels of recursion without loops and only imperative statements? Usually, <code class="language-plaintext highlighter-rouge">union</code> is sufficient for our purposes. However, this is infeasible in case of large tables or higher levels of hierarchy.</p> <h3 id="string-operations">String operations</h3> <p>SQL also includes string operations. The operator <code class="language-plaintext highlighter-rouge">like</code> uses patterns that are describes using two special character - <code class="language-plaintext highlighter-rouge">percent %</code> - Matches any substring and <code class="language-plaintext highlighter-rouge">underscore _</code> matches any character (Use <code class="language-plaintext highlighter-rouge">\</code> as the escape character). Some databases even fully support regular expressions. Most databases also support <code class="language-plaintext highlighter-rouge">ilike</code> which is case-insensitive.</p> <h3 id="set-operations">Set operations</h3> <p>These include <code class="language-plaintext highlighter-rouge">union</code>, <code class="language-plaintext highlighter-rouge">intersect</code> and <code class="language-plaintext highlighter-rouge">except</code> (set difference). To retain the duplicates we use <code class="language-plaintext highlighter-rouge">all</code> keyword after the operators.</p> <h3 id="null-values">null values</h3> <p>It signifies an unknown value or that a value does not exist. The result of any arithmetic expression involving <code class="language-plaintext highlighter-rouge">null</code> is <code class="language-plaintext highlighter-rouge">null</code>. The predicate <code class="language-plaintext highlighter-rouge">is null</code> can be used to check for null values.</p> <h1 id="lecture-5">Lecture 5</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">11-01-22</code></p> </blockquote> <h3 id="aggregate-functions-1">Aggregate Functions</h3> <p>The <code class="language-plaintext highlighter-rouge">having</code> clause can be used to select groups which satisfies certain conditions. Predicates in the <code class="language-plaintext highlighter-rouge">having</code> clause are applied after the formation of groups whereas predicates in the <code class="language-plaintext highlighter-rouge">where</code> clause are applied before forming the groups.</p> <h2 id="nested-subqueries">Nested subqueries</h2> <p>SQL provides a mechanism for the nesting of subqueries. A subquery is a select-from-where expression that is nested within another query. The nesting can be done in the following ways -</p> <ul> <li> <code class="language-plaintext highlighter-rouge">from</code> clause - The relation can be replaced by any valid subquery</li> <li> <code class="language-plaintext highlighter-rouge">where</code> clause - The predicate can be replaced with an expression of the form <code class="language-plaintext highlighter-rouge">B &lt;operation&gt; (subquery)</code> where <code class="language-plaintext highlighter-rouge">B</code> is an attribute and <code class="language-plaintext highlighter-rouge">operation</code> will be defined later.</li> <li> <strong>Scalar subqueries</strong> - The attributes in the <code class="language-plaintext highlighter-rouge">select</code> clause can be replaced by a subquery that generates a single value!</li> </ul> <h3 id="subqueries-in-the-from-clause">subqueries in the <code class="language-plaintext highlighter-rouge">from</code> clause</h3> <p>the <code class="language-plaintext highlighter-rouge">with</code> clause provides a way of defining a temporary relation whose definition is available only to the query in which the <code class="language-plaintext highlighter-rouge">with</code> clause occurs. For example, consider the following</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">max_budget</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">as</span> 
	<span class="p">(</span><span class="k">select</span> <span class="k">max</span><span class="p">(</span><span class="n">budget</span><span class="p">))</span>
	<span class="k">from</span> <span class="n">department</span><span class="p">)</span>
<span class="k">select</span> <span class="n">department</span><span class="p">.</span><span class="n">name</span> <span class="k">from</span> <span class="n">department</span><span class="p">,</span> <span class="n">max_budget</span>
<span class="k">where</span> <span class="n">department</span><span class="p">.</span><span class="n">budget</span> <span class="o">=</span> <span class="n">max_budget</span><span class="p">.</span><span class="n">value</span>
</code></pre></div></div> <p>We can write more complicated queries. For example, if we want all departments where the total salary is greater than the average of the total salary at all departments.</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">dept_total</span><span class="p">(</span><span class="n">dept_name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="k">as</span>
	<span class="p">(</span><span class="k">select</span> <span class="n">dept_name</span><span class="p">,</span> <span class="k">sum</span><span class="p">(</span><span class="n">salary</span><span class="p">)</span> <span class="k">from</span> <span class="n">instructor</span> <span class="k">group</span> <span class="k">by</span> <span class="n">dept_name</span><span class="p">)</span>
		<span class="n">dept_total_avg</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">as</span>
		<span class="p">(</span><span class="k">select</span> <span class="k">avg</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">from</span> <span class="n">dept_total</span><span class="p">)</span>
    <span class="k">select</span> <span class="n">dept_name</span>
    <span class="k">from</span> <span class="n">dept_total</span><span class="p">,</span> <span class="n">dept_total_avg</span>
    <span class="k">where</span> <span class="n">dept_total</span><span class="p">.</span><span class="n">value</span> <span class="o">&gt;</span> <span class="n">dept_total_avg</span><span class="p">.</span><span class="n">value</span>
</code></pre></div></div> <h3 id="subqueries-in-the-where-clause">subqueries in the <code class="language-plaintext highlighter-rouge">where</code> clause</h3> <p>We use operations such as <code class="language-plaintext highlighter-rouge">in</code> and <code class="language-plaintext highlighter-rouge">not in</code>. We can also check the set membership of a subset of attributes in the same order. There is also a <code class="language-plaintext highlighter-rouge">some</code> keyword that returns a True if at least one tuple exists in the subquery that satisfies the condition. Similarly we have the <code class="language-plaintext highlighter-rouge">all</code> keyword. There is also the <code class="language-plaintext highlighter-rouge">exists</code> clause which returns True if the tuple exists in the subquery relation. For example, if we want to find all courses taught in both the Fall 2017 semester and in the spring 2018 semester. We can use the following</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">select</span> <span class="n">course_id</span> <span class="k">from</span> <span class="n">section</span> <span class="k">as</span> <span class="n">S</span>
<span class="k">where</span> <span class="n">semester</span> <span class="o">=</span> <span class="s1">'Fall'</span> <span class="k">and</span> <span class="nb">year</span> <span class="o">=</span> <span class="mi">2017</span> <span class="k">and</span>
	<span class="k">exists</span> <span class="p">(</span><span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">section</span> <span class="k">as</span> <span class="n">T</span>
			<span class="k">where</span> <span class="n">semester</span> <span class="o">=</span> <span class="s1">'Spring'</span> <span class="k">and</span> <span class="nb">year</span> <span class="o">=</span> <span class="mi">2018</span>
			<span class="k">and</span> <span class="n">S</span><span class="p">.</span><span class="n">course_id</span> <span class="o">=</span> <span class="n">T</span><span class="p">.</span><span class="n">course_id</span><span class="p">)</span>
</code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">S</code> is the correlation name and the inner query is the correlated subquery. Correspondingly, there also is a <code class="language-plaintext highlighter-rouge">not exists</code> clause.</p> <p>The <code class="language-plaintext highlighter-rouge">unique</code> construct tests whether a subquery has any duplicate tuples in its result. It evaluates to True if there are no duplicates.</p> <h3 id="scalar-subquery">Scalar Subquery</h3> <p>Suppose we have to list all the departments along with the number of instructors in each department. Then, we can do the following</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">select</span> <span class="n">dept_name</span><span class="p">,</span> 
	<span class="p">(</span><span class="k">select</span> <span class="k">count</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">from</span> <span class="n">instructor</span>
	<span class="k">where</span> <span class="n">department</span><span class="p">.</span><span class="n">dept_name</span> <span class="o">=</span> <span class="n">instructir</span><span class="p">.</span><span class="n">dept_name</span> <span class="k">as</span> <span class="n">num_instructors</span>
	<span class="p">)</span> <span class="k">from</span> <span class="n">department</span><span class="p">;</span>
</code></pre></div></div> <p>There would be a <strong>runtime error</strong> if the subquery returns more than one result tuple.</p> <h2 id="modification-of-the-database">Modification of the database</h2> <p>We can</p> <ul> <li> <p>delete tuples from a given relation using <code class="language-plaintext highlighter-rouge">delete from</code>. It deletes all tuples without a <code class="language-plaintext highlighter-rouge">where</code> clause. We need to be careful while using delete. For example, if we want to delete all instructors whose salary is less than the average salary of instructors. We can implement this using a subquery in the <code class="language-plaintext highlighter-rouge">where</code> clause. The problem here is that the average salary changes as we delete tuples from instructor. The solution for this problem is - we can compute average first and then delete without recomputation. This modification is usually implemented.</p> </li> <li> <p>insert new tuples into a give relation using <code class="language-plaintext highlighter-rouge">insert into &lt;table&gt; values &lt;A1, A2, ..., An&gt;</code>. The <code class="language-plaintext highlighter-rouge">select from where</code> statement is evaluated fully before any of its results are inserted into the relation. This is done to prevent the problem mentioned in <code class="language-plaintext highlighter-rouge">delete</code>.</p> </li> <li> <p>update values in some tuples in a given relation using <code class="language-plaintext highlighter-rouge">update &lt;table&gt; set A1 = ... where ...</code>. We can also use a <code class="language-plaintext highlighter-rouge">case</code> statement to make non-problematic sequential updates. For example,</p> <div class="language-sql highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">update</span> <span class="n">instructor</span>
	<span class="k">set</span> <span class="n">salary</span> <span class="o">=</span> <span class="k">case</span>
					<span class="k">when</span> <span class="n">salary</span> <span class="o">&lt;=</span> <span class="mi">1000</span> <span class="k">then</span> <span class="n">salary</span> <span class="o">*</span><span class="mi">1</span><span class="p">.</span><span class="mi">05</span>
					<span class="k">else</span> <span class="n">salary</span><span class="o">*</span><span class="mi">1</span><span class="p">.</span><span class="mi">03</span>
                <span class="k">end</span>
</code></pre></div> </div> </li> </ul> <p><strong><code class="language-plaintext highlighter-rouge">coalesce</code></strong> takes a series of arguments and returns the first non-null value.</p> <h1 id="lecture-6">Lecture 6</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">13-01-22</code></p> </blockquote> <h1 id="chapter-4-intermediate-sql">~Chapter 4: Intermediate SQL</h1> <p><strong>Join operations</strong> take two relations and return as a result another relation. There are three types of joins which are described below.</p> <h3 id="natural-join">Natural Join</h3> <p>Natural join matches tuples with the same values for <strong>all common attributes</strong>, and retains only one copy of each common column.</p> <blockquote> <p>Can’t do self-join using this?</p> </blockquote> <p>However, one must be beware of natural join because it produces unexpected results. For example, consider the following queries</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Correct version</span>
<span class="k">select</span> <span class="n">name</span><span class="p">,</span> <span class="n">title</span> <span class="k">from</span> <span class="n">student</span> <span class="k">natural</span> <span class="k">join</span> <span class="n">takes</span><span class="p">,</span> <span class="n">course</span>
<span class="k">where</span> <span class="n">takes</span><span class="p">.</span><span class="n">course_id</span> <span class="o">=</span> <span class="n">course</span><span class="p">.</span><span class="n">course_id</span>
<span class="c1">-- Incorrect version</span>
<span class="k">select</span> <span class="n">name</span><span class="p">,</span> <span class="n">title</span>
<span class="k">from</span> <span class="n">student</span> <span class="k">natural</span> <span class="k">join</span> <span class="n">takes</span> <span class="k">natural</span> <span class="k">join</span> <span class="n">course</span>
</code></pre></div></div> <p>The second query omits all pairs where the student takes a course in a department other than the student’s own department due to the attribute department name. Sometimes, we don’t realize some attributes are being equated because all the common attributes are equated.</p> <h3 id="outer-join">Outer join</h3> <p>One can lose information with inner join and natural join. Outer join is an extension of the join operation that avoids loss of information. It computes the join and then adds tuples from one relation that do not match tuples in the other relation to the result of the join. Outer join uses <code class="language-plaintext highlighter-rouge">null</code> to fill the incomplete tuples. We have variations of outer join such as left-outer join, right-outer join, and full outer join. Can outer join be expressed using relational algebra? Yes, think about it. In general, \((r ⟖ s) ⟖ t \neq r ⟖ (s ⟖t)\).</p> <p><strong>Note.</strong> \((r ⟖ s) ⟕ t \neq r ⟖ (s⟕t)\). Why? Consider the following</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>r | X | Y | s | Y | Z | t | Z | X | P |
  | 1 | 2 |   | 2 | 3 |   | 3 | 4 | 7 |
  | 1 | 3 |   | 3 | 4 |   | 4 | 1 | 8 |
-- LHS				-- RHS
| X | Y | Z | P |	| X | Y | Z | P |
| 1 | 2 | 3 | - |	| 4 | 2 | 3 | 7 |
| 1 | 3 | 4 | 8 |	| 1 | 3 | 4 | 8 |
</code></pre></div></div> <h2 id="views">Views</h2> <p>In some cases, it is not desirable for all users to see the entire logical model. For example, if a person wants to know the name and department of instructors without the salary, then they can use</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">view</span> <span class="n">v</span> <span class="k">as</span> <span class="k">select</span> <span class="n">name</span><span class="p">,</span> <span class="n">dept</span> <span class="k">from</span> <span class="n">instructor</span>
</code></pre></div></div> <p>A view provides a mechanism to hide certain data from the view of certain users. The view definition is not the same as creating a new relation by evaluating the query expression. Rather, a view definition causes the saving of an expression; the expression is substituted into queries using the view.</p> <p>One view may be used in the expression defining another view. A view relation \(v_1\) is said to <em>depend directly</em> on a view relation \(v_2\) if \(v_2\) is used in the expression defining \(v_1\). It is said to <em>depend on</em> \(v_2\) if there is a path of dependency. A <em>recursive</em> view depends on itself.</p> <h3 id="materialized-views">Materialized views</h3> <p>Certain database systems allow view relations to be physically stored. If relations used in the query are updated, the materialized view result becomes out of date. We need to maintain the view, by updating the view whenever the underlying relations are updated. Most SQL implementations allow updates only on simple views.</p> <h2 id="transactions">Transactions</h2> <p>A transaction consists of a sequence of query and/or update statements and is atomic. The transaction must end with one of the following statements -</p> <ul> <li> <strong>Commit work</strong> - Updates become permanent</li> <li> <strong>Rollback work</strong> - Updates are undone</li> </ul> <h2 id="integrity-constraints">Integrity Constraints</h2> <ul> <li><code class="language-plaintext highlighter-rouge">not null</code></li> <li><code class="language-plaintext highlighter-rouge">primary key (A1, A2, ..., Am)</code></li> <li><code class="language-plaintext highlighter-rouge">unique (A1, A2, ..., Am)</code></li> <li><code class="language-plaintext highlighter-rouge">check (P)</code></li> </ul> <h3 id="check-clause"> <code class="language-plaintext highlighter-rouge">check</code> clause</h3> <p>The <strong>check(P)</strong> clause specifies a predicate P that must be satisfied by every tuple in a relation.</p> <h3 id="cascading-actions">Cascading actions</h3> <p>When a referential-integrity constraint is violated, the normal procedure is to reject the action that caused the violation. We can use <code class="language-plaintext highlighter-rouge">on delete cascade</code> or <code class="language-plaintext highlighter-rouge">on update cascade</code>. Other than cascade, we can use <code class="language-plaintext highlighter-rouge">set null</code> or <code class="language-plaintext highlighter-rouge">set default</code>.</p> <h1 id="lecture-7">Lecture 7</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">17-01-22</code></p> </blockquote> <p>Continuing the previous referential-integrity constraints. Suppose we have the command</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">table</span> <span class="n">person</span><span class="p">(</span>
	<span class="n">ID</span> <span class="nb">char</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
	<span class="n">name</span> <span class="nb">char</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
	<span class="n">mother</span> <span class="nb">char</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
	<span class="n">father</span> <span class="nb">char</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
	<span class="k">primary</span> <span class="k">key</span> <span class="n">ID</span><span class="p">,</span>
	<span class="k">foreign</span> <span class="k">key</span> <span class="n">father</span> <span class="k">references</span> <span class="n">person</span><span class="p">,</span>
	<span class="k">foreign</span> <span class="k">key</span> <span class="n">mother</span> <span class="k">references</span> <span class="n">person</span>
<span class="p">)</span>
</code></pre></div></div> <p>How do we insert tuples here without violating the foreign key constraints? We can initially insert the name attributes and then the father and mother attributes. This can be done by inserting <code class="language-plaintext highlighter-rouge">null</code> in the mother/father attributes. Is there any other way of doing this? We can insert tuples by using the acyclicity among the tuples using topological sorting. There is also a third way which is supported by SQL. In this method, we can ask the database to defer the foreign key checking till the end of the transaction.</p> <h3 id="complex-check-conditions">Complex check conditions</h3> <p>The predicate in the check clause can be an arbitrary predicate that can include a subquery <strong>(?)</strong> For example,</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">check</span> <span class="p">(</span><span class="n">time_slot_id</span> <span class="k">in</span> <span class="p">(</span><span class="k">select</span> <span class="n">time_slot_id</span> <span class="k">from</span> <span class="n">time_slot</span><span class="p">))</span>
</code></pre></div></div> <p>This condition is similar to a foreign key constraint. We have to check this condition not only when the ‘section’ relation is updated but also when the ‘time_slot’ relation is updated. Therefore, <u>it is not currently supported by any database!</u></p> <h3 id="built-in-data-types-in-sql">Built-in Data Types in SQL</h3> <p>In addition to the previously mentioned datatypes, SQL supports <code class="language-plaintext highlighter-rouge">date</code>s, <code class="language-plaintext highlighter-rouge">interval</code>s, <code class="language-plaintext highlighter-rouge">timestamp</code>s, and <code class="language-plaintext highlighter-rouge">time</code>. Whenever we subtract <code class="language-plaintext highlighter-rouge">date</code> from a <code class="language-plaintext highlighter-rouge">date</code> or <code class="language-plaintext highlighter-rouge">time</code> from <code class="language-plaintext highlighter-rouge">time</code>, we get an <code class="language-plaintext highlighter-rouge">interval</code>.</p> <p>Can we store files in our database? Yes! We can store large objects as</p> <ul> <li> <code class="language-plaintext highlighter-rouge">blob</code> - Binary large object - Large collection of uninterpreted binary data (whose interpretation is left to the application outside of the database system).</li> <li> <code class="language-plaintext highlighter-rouge">clob</code> character large object - Large collection of</li> </ul> <p>Every database has its own limit for the maximum file size.</p> <h3 id="index-creation">Index Creation</h3> <p>An <strong>index</strong> on an attribute of a relation is a data structure that allows the database system to find those tuples in the relation that have a specified value for that attribute efficiently, without scanning through all the tuples of the relation. We create an index with the <code class="language-plaintext highlighter-rouge">create index</code> command</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">index</span> <span class="o">&lt;</span><span class="n">name</span><span class="o">&gt;</span> <span class="k">on</span> <span class="o">&lt;</span><span class="n">relation</span><span class="o">-</span><span class="n">name</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">attribute</span><span class="p">):</span>
</code></pre></div></div> <p>Every database automatically creates an index on the primary key.</p> <h3 id="authorization">Authorization</h3> <p>We can assign several forms of authorization to a database</p> <ul> <li> <strong>Read</strong> - allows reading, but no modification of data</li> <li> <strong>Insert</strong> - allows insertion of new data, but not modification of existing data</li> <li> <strong>Update</strong> - allows modification, but not deletion of data</li> <li> <strong>Delete</strong> - allows deletion of data</li> </ul> <p>We have more forms on the schema level</p> <ul> <li> <strong>Index</strong> - allows creation and deletion of indices</li> <li> <strong>Resources</strong>, <strong>Alteration</strong> </li> <li> <strong>Drop</strong> - allows deleting relations</li> </ul> <p>Each of these types of authorizations is called a <strong>privilege</strong>. These privileges are assigned on specified parts of a database, such as a relation, view or the whole schema.</p> <p>The <code class="language-plaintext highlighter-rouge">grant</code> statement is used to confer authorization.</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">grant</span> <span class="o">&lt;</span><span class="n">privilege</span> <span class="n">list</span><span class="o">&gt;</span> <span class="k">on</span> <span class="o">&lt;</span><span class="n">relation</span> <span class="k">or</span> <span class="k">view</span><span class="o">&gt;</span> <span class="k">to</span> <span class="o">&lt;</span><span class="k">user</span> <span class="n">list</span><span class="o">&gt;</span>
<span class="c1">-- Revoke statement to revoke authorization</span>
<span class="k">revoke</span> <span class="k">select</span> <span class="k">on</span> <span class="n">student</span> <span class="k">from</span> <span class="n">U1</span><span class="p">,</span> <span class="n">U2</span><span class="p">,</span> <span class="n">U3</span>
</code></pre></div></div> <ul> <li> <code class="language-plaintext highlighter-rouge">&lt;user list&gt;</code> can be a user-id, <strong>public</strong> or a <em>role</em>. Granting a privilege on a view does no imply granting any privileges on the underlying relations.</li> <li> <code class="language-plaintext highlighter-rouge">&lt;privilege-list&gt;</code> may be <strong>all</strong> to revoke all privileges the revokee may hold.</li> <li>If <code class="language-plaintext highlighter-rouge">&lt;revoke-list&gt;</code> includes <strong>public</strong>, all users lost the privilege except those granted it explicitly.</li> <li>If the same privilege was granted twice to the same user by different grantees, the user may retain the privilege after the revocation. All privileges that depend on the privilege being revoked are also revoked.</li> </ul> <p>One of the major selling points of Java was a <em>garbage collector</em> that got rid of <code class="language-plaintext highlighter-rouge">delete</code>/<code class="language-plaintext highlighter-rouge">free</code> and automatically freed up unreferenced memory. This action is done via a <em>counter</em> which keeps a count of the variables that are referencing a memory cell. SQL uses a similar counter for keeping track of permissions of various objects. However, this approach fails in case of cycles in the dependency graph. For instance, consider the following situation</p> <p><img src="/assets/img/Databases/image-20220117101819769.png" alt="image-20220117101819769"></p> <p>This problem does not occur in case of programming languages. The solution to this problem is <strong><em><code class="language-plaintext highlighter-rouge">TODO</code></em></strong>.</p> <p>What about garbage collection when the program is huge? Is it efficient? Currently, many optimizations, like <em>incremental garbage collection</em>, have been implemented to prevent freezing of a program for garbage collection. Even after this, Java is not preferred for real-time applications. However, programmers prefer Java because of the ease of debugging and writing programs.</p> <h3 id="roles">Roles</h3> <p>A <strong>role</strong> is a way to distinguish among various users as far as what these users can access/update in the database.</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="n">a</span> <span class="k">role</span> <span class="o">&lt;</span><span class="n">name</span><span class="o">&gt;</span>
<span class="k">grant</span> <span class="o">&lt;</span><span class="k">role</span><span class="o">&gt;</span> <span class="k">to</span> <span class="o">&lt;</span><span class="n">users</span><span class="o">&gt;</span>
</code></pre></div></div> <p>There are a couple more features in authorization which can be looked up in the textbook.</p> <p>We can also give authorization on views.</p> <h3 id="other-authorization-features">Other authorization features</h3> <ul> <li> <p>references privilege to create foreign key</p> <p><code class="language-plaintext highlighter-rouge">grant reference (dept_name) on department to Mariano</code></p> </li> <li> <p>transfer of privileges</p> <div class="language-sql highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">grant</span> <span class="k">select</span> <span class="k">on</span> <span class="n">department</span> <span class="k">to</span> <span class="n">Amit</span> <span class="k">with</span> <span class="k">grant</span> <span class="k">option</span><span class="p">;</span>
<span class="k">revoke</span> <span class="k">select</span> <span class="k">on</span> <span class="n">department</span> <span class="k">from</span> <span class="n">Amit</span><span class="p">,</span> <span class="n">Satoshi</span> <span class="k">cascade</span><span class="p">;</span>
<span class="k">revoke</span> <span class="k">select</span> <span class="k">on</span> <span class="n">deparment</span> <span class="k">from</span> <span class="n">Amit</span><span class="p">,</span> <span class="n">Satoshi</span> <span class="k">restrict</span><span class="p">;</span>
</code></pre></div> </div> </li> </ul> <h1 id="lecture-8">Lecture 8</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">18-01-22</code></p> </blockquote> <h1 id="-chapter-5-advanced-sql">~ Chapter 5: Advanced SQL</h1> <p>Programming languages with automatic garbage collection cannot clean the data in databases. That is, if you try using large databases, then your system may hang.</p> <h2 id="jdbc-code">JDBC code</h2> <p><code class="language-plaintext highlighter-rouge">DriverManager.getConnection("jdbc:oracle:thin:@db_name")</code> is used to connect to the database. We need to close the connection after the work since each connection is a process on the server, and the server can have limited number of processes. In Java we check the <code class="language-plaintext highlighter-rouge">null</code> value using <code class="language-plaintext highlighter-rouge">wasNull()</code> function (not intuitive).</p> <p>Prepared statements are used to take inputs from the user without SQL injection. We can also extract metadata using JDBC.</p> <h2 id="sql-injection">SQL injection</h2> <p>The method where hackers insert SQL commands into the database using SQL queries. This problem is prevented by using <code class="language-plaintext highlighter-rouge">prepared statement</code>s.</p> <p>This lecture was cut-short, and hence has less notes.</p> <h1 id="lecture-9">Lecture 9</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">20-01-22</code></p> </blockquote> <h2 id="functions-and-procedures">Functions and Procedures</h2> <p>Functions and procedures allow ‘business logic’ to be stored in the database and executed from SQL statements.</p> <p>We can define a function using the following syntax</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">function</span> <span class="o">&lt;</span><span class="n">name</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">params</span><span class="p">)</span>
	<span class="k">returns</span> <span class="o">&lt;</span><span class="n">datatype</span><span class="o">&gt;</span>
	<span class="k">begin</span>
		<span class="p">...</span>
	<span class="k">end</span>
</code></pre></div></div> <p>You can return scalars or relations. We can also define external language routines in other programming languages. These procedures can be more efficient than the ones defined in SQL. We can declare external language procedures and functions using the following.</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">procedure</span> <span class="o">&lt;</span><span class="n">name</span><span class="o">&gt;</span><span class="p">(</span><span class="k">in</span> <span class="n">params</span><span class="p">,</span> <span class="k">out</span> <span class="n">params</span> <span class="p">(</span><span class="o">?</span><span class="p">))</span>
<span class="k">language</span> <span class="o">&lt;</span><span class="n">programming</span><span class="o">-</span><span class="k">language</span><span class="o">&gt;</span>
<span class="k">external</span> <span class="n">name</span> <span class="o">&lt;</span><span class="n">file_path</span><span class="o">&gt;</span>
</code></pre></div></div> <p>However, there are security issues with such routines. To deal with security problems, we can</p> <ul> <li> <strong>sandbox techniques</strong> - using a safe language like Java which cannot access/damage other parts of the database code.</li> <li>run external language functions/procedures in a separate process, with no access to the database process’ memory.</li> </ul> <h2 id="triggers">Triggers</h2> <p>When certain actions happen, we would like the database to react and do something as a response. A <strong>trigger</strong> is a statement that is executed automatically by the system as a side effect of a modification to the database. To design a trigger mechanism, we must specify the conditions under which the trigger is to be executed and the actions to be taken when the trigger executes. The syntax varies from database to database and the user must be wary of it.</p> <p>The SQL:1999 syntax is</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">trigger</span> <span class="o">&lt;</span><span class="n">name</span><span class="o">&gt;</span> <span class="k">after</span> <span class="p">[</span><span class="k">update</span><span class="p">,</span> <span class="k">insert</span><span class="p">,</span> <span class="k">delete</span><span class="p">]</span> <span class="k">of</span> <span class="o">&lt;</span><span class="n">relation</span><span class="o">&gt;</span> <span class="k">on</span> <span class="o">&lt;</span><span class="n">attributes</span><span class="o">&gt;</span>
<span class="k">referencing</span> <span class="k">new</span> <span class="k">row</span> <span class="k">as</span> <span class="n">nrow</span>
<span class="k">referencing</span> <span class="k">old</span> <span class="k">row</span> <span class="k">as</span> <span class="n">orow</span>
<span class="p">[</span><span class="k">for</span> <span class="k">each</span> <span class="k">row</span><span class="p">]</span>
	<span class="p">...</span>
</code></pre></div></div> <p>If we do not want the trigger to be executed for every row update, then we can use statement level triggers. This ensures that the actions is executed for all rows affected by a transaction. We use <code class="language-plaintext highlighter-rouge">for each</code> instead of <code class="language-plaintext highlighter-rouge">for each row</code> and we reference tables instead of rows.</p> <p>Triggers need not be used to update materialized views, logging, and many other typical use cases. Use of triggers is not encouraged as they have a risk of unintended execution.</p> <h2 id="recursion-in-sql">Recursion in SQL</h2> <p>SQL:1999 permits recursive view definition. Why do we need recursion? For example, if we want to find which courses are a prerequisite (direct/indirect) for a specific course, we can use</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="k">recursive</span> <span class="n">rec_prereq</span><span class="p">(</span><span class="n">course_id</span><span class="p">,</span> <span class="n">prereq_id</span><span class="p">)</span> <span class="k">as</span> <span class="p">(</span>
	<span class="k">select</span> <span class="n">course_id</span><span class="p">,</span> <span class="n">prereq_id</span> <span class="k">from</span> <span class="n">prereq</span>
	<span class="k">union</span>
	<span class="k">select</span> <span class="n">rec_prereq</span><span class="p">.</span><span class="n">course_id</span><span class="p">,</span> <span class="n">prereq</span><span class="p">.</span><span class="n">prereq_id</span><span class="p">,</span>
	<span class="k">from</span> <span class="n">rec_prereq</span><span class="p">,</span> <span class="n">prereq</span>
	<span class="k">where</span> <span class="n">rec_prereq</span><span class="p">.</span><span class="n">prereq_id</span> <span class="o">=</span> <span class="n">prereq</span><span class="p">.</span><span class="n">course_id</span>
<span class="p">)</span> <span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">rec_prereq</span><span class="p">;</span>
</code></pre></div></div> <p>This example view, <code class="language-plaintext highlighter-rouge">rec_prereq</code> is called the <em>transitive closure</em> of the <code class="language-plaintext highlighter-rouge">prereq</code> relation. Recursive views make it possible to write queries, such as transitive closure queries, that cannot be written without recursion or iteration. The alternative to recursion is to write a procedure to iterate as many times as required.</p> <p>The final result of recursion is called the <strong>fixed point</strong> of the recursive view. Recursive views are required to be <strong>monotonic</strong>. This is usually achieved using <code class="language-plaintext highlighter-rouge">union</code> without <code class="language-plaintext highlighter-rouge">except</code> and <code class="language-plaintext highlighter-rouge">not in</code>.</p> <h2 id="advanced-aggregation-features">Advanced Aggregation Features</h2> <h3 id="ranking">Ranking</h3> <p>Ranking is done in conjunction with an order by specification. Can we implement ranking with the knowledge we have currently? Yes, we can use count() to check how many tuples are ahead of the current tuple.</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">select</span> <span class="o">*</span><span class="p">,</span>  <span class="p">(</span><span class="k">select</span> <span class="k">count</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">from</span> <span class="n">r</span> <span class="k">as</span> <span class="n">r2</span> <span class="k">where</span> <span class="n">r2</span><span class="p">.</span><span class="n">t</span> <span class="o">&gt;</span> <span class="n">r1</span><span class="p">.</span><span class="n">t</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">from</span> <span class="n">r</span> <span class="k">as</span> <span class="n">r1</span>
</code></pre></div></div> <p>However, this is \(\mathcal O(n^2)\). Also, note that the above query implements <em>sparse rank</em>. <em>Dense rank</em> can be implemented using the <code class="language-plaintext highlighter-rouge">unique</code> keyword. Rank in SQL can be implemented using <code class="language-plaintext highlighter-rouge">rank() over ([order by A desc])</code>.</p> <p>Ranking can be done within partitions within the dataset. This is done using <code class="language-plaintext highlighter-rouge">partition by</code>. The whole query is given by</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">select</span> <span class="n">ID</span><span class="p">,</span> <span class="n">dept_name</span><span class="p">,</span> <span class="n">rank</span><span class="p">()</span> <span class="n">over</span>
	<span class="p">(</span><span class="k">partition</span> <span class="k">by</span> <span class="n">dept_name</span> <span class="k">order</span> <span class="k">by</span> <span class="n">GPA</span> <span class="k">desc</span><span class="p">)</span>
<span class="k">from</span> <span class="n">dept_grades</span>
<span class="k">order</span> <span class="k">by</span> <span class="n">dept_name</span><span class="p">,</span> <span class="n">dept_rank</span>
</code></pre></div></div> <p><u>Multiple rank clauses can occur in a single select clause!</u> Ranking is done after applying <code class="language-plaintext highlighter-rouge">group by</code> clause/aggregation. Finally, if we want only the top few ranks, we can use <code class="language-plaintext highlighter-rouge">limit</code>. However, this method is restrictive as we can’t select top-n in each partition and it is inherently non-deterministic. This is because ties are broken arbitrarily. It is usually better to select directly using the rank attribute by embedding the relation in an outer query.</p> <p>Ranking has other function such as</p> <ul> <li> <code class="language-plaintext highlighter-rouge">percent_rank</code> gives percentile</li> <li> <code class="language-plaintext highlighter-rouge">cume_dist</code> gives fraction</li> <li> <code class="language-plaintext highlighter-rouge">row_number</code> (non-deterministic)</li> </ul> <p>SQL:1999 permits the user to specify <code class="language-plaintext highlighter-rouge">nulls first</code> or <code class="language-plaintext highlighter-rouge">nulls last</code>.</p> <p>For a given constant \(n\), the function <code class="language-plaintext highlighter-rouge">ntile(n)</code> takes the tuples in each partition in the specified order, and divides them into \(n\) buckets with equal number of tuples.</p> <h2 id="windowing">Windowing</h2> <p>Here are the examples of window specifications</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">between</span> <span class="k">rows</span> <span class="n">unbounded</span> <span class="k">preceding</span> <span class="k">and</span> <span class="k">current</span>
<span class="k">rows</span> <span class="n">nbounded</span> <span class="k">preceding</span>
<span class="k">range</span> <span class="k">between</span> <span class="mi">10</span> <span class="k">preceding</span> <span class="k">and</span> <span class="k">current</span> <span class="k">row</span>
<span class="k">range</span> <span class="n">interval</span> <span class="mi">10</span> <span class="k">day</span> <span class="k">preceding</span>
<span class="c1">-- Given a relation transaction</span>
<span class="c1">-- where value is positive for a deposite and </span>
<span class="c1">-- negative for a withdrawal, find total balance</span>
<span class="c1">-- of each account after each transaction on it</span>
<span class="k">select</span> <span class="n">account_number</span><span class="p">,</span> <span class="n">date_time</span><span class="p">,</span> 
	<span class="k">sum</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="n">over</span> 
		<span class="p">(</span><span class="k">partition</span> <span class="k">by</span> <span class="n">account_number</span>
        <span class="k">order</span> <span class="k">by</span> <span class="n">date_time</span>
        <span class="k">rows</span> <span class="n">unbounded</span> <span class="k">preceding</span><span class="p">)</span>
   	<span class="k">as</span> <span class="n">balance</span>
<span class="k">from</span> <span class="n">transaction</span>
<span class="k">order</span> <span class="k">by</span> <span class="n">account_number</span><span class="p">,</span> <span class="n">date_time</span>
</code></pre></div></div> <p>We can perform windowing within partitions.</p> <h1 id="lecture-10">Lecture 10</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">24-01-22</code></p> </blockquote> <p>We will cover one last concept in SQL and then move on to ER models.</p> <h2 id="olap">OLAP</h2> <p>OLAP stands for Online Analytical Processing. It allows interactive analysis of data, allowing data to be summarized and viewed in different ways in an online fashion. Data that can be modeled as dimension attributes and measure attributes are called <strong>multidimensional data</strong>. <strong>Measure attributes</strong> measure some value that can be aggregated upon. <strong>Dimension attributes</strong> define the dimension on which measure attributes are viewed.</p> <p>Items are often represented using <strong>cross-tabulation</strong> (cross-tab), also referred to as a <strong>pivot table</strong>. The dimension attributes form the row and column headers. The measure attributes are mentioned in each individual cell. Similarly, we can create a <strong>data cube</strong> which is a multidimensional generalization of a cross-tab. We can represent cross-tabs using relations. These can be used in SQL with <code class="language-plaintext highlighter-rouge">null</code> representing the total aggregates (despite the confusion).</p> <p>The <code class="language-plaintext highlighter-rouge">cube</code> operation in SQL computes the union of <code class="language-plaintext highlighter-rouge">group by</code>’s on every subset of the specified attributes. The function <code class="language-plaintext highlighter-rouge">grouping()</code> can be applied on an attribute to check if the <code class="language-plaintext highlighter-rouge">null</code> value represents ‘all’ or not. It returns 1 if the value is a null value representing all. The <code class="language-plaintext highlighter-rouge">rollup</code> construct generates union on every prefix of a specified list of attributes. It can be used to generate aggregates at multiple levels of a hierarchy.</p> <h3 id="olap-operations">OLAP Operations</h3> <ul> <li> <strong>Pivoting</strong> - Changing the dimensions used in a cross-tab</li> <li> <strong>Slicing</strong> - Creating a cross-tab for fixed values only. Sometimes called <strong>dicing</strong> when values for multiple dimensions are fixed.</li> <li> <strong>Rollup</strong> - Moving from finer-granularity data to a coarser granularity.</li> <li> <strong>Drill down</strong> - Opposite of rollup.</li> </ul> <p>Early OLAP systems precomputed all possible aggregates in order to provide online response. Since this is infeasible, it suffices to precompute some aggregates and compute others on demand from pre-computed aggregates.</p> <h1 id="chapter-6-database-design-using-the-er-model">~Chapter 6: Database Design using the ER Model</h1> <p>How do we design schemas for a database? Is there any systematic way? We shall study this in the following chapter. The entity-relationship model proves useful in modelling the data.</p> <p>When we design a database, we initially characterize the data needs of the database users. Then, we choose a data model to translate the requirements into a conceptual schema. The conceptual schema is designed using the ER model, and the implementation can be done in different ways such as the relation model. We do this in the final step where we move from an abstract data model to the implementation in the database.</p> <p>Why do we care about good design? A bad design can have <em>redundancy</em> - repeats information which might cause data inconsistency and <em>incompleteness</em> which might make certain aspects of the enterprise difficult or impossible to model.</p> <h2 id="er-model">ER Model</h2> <p><strong>Entity</strong> is a thing or an object in the enterprise that is distinguishable from other objects. It is described by a set of attributes. A <strong>relationship</strong> is an association among several entities. These models are represented graphically using the ER diagram.</p> <h3 id="entity-sets">Entity sets</h3> <p>An <strong>entity set</strong> is a set of entities of the same type that share the same properties. For example, it can be a set of all persons (each of which is an entity). A subset of the attributes form a primary key of the entity set.</p> <p>Entity sets can be represented graphically using rectangles and attributes listed inside it. The primary keys are underlined.</p> <h3 id="relationship-sets">Relationship sets</h3> <p>A <strong>relationship set</strong> is a mathematical relation among \(n \geq 2\) entities, each taken from entity sets.</p> \[\{(e_1, e_2, \dots, e_n \vert e_1 \in E_1, e_2 \in E_2, \dots, e_n \in E_n\}\] <p>where \((e_1, e_2, \dots, e_n)\) is a relationship. We draw a line between the related entities to represent relationships. Relationship sets are represented using diamonds.</p> <p>An attribute can also be associated with a relationship set. This is shown using a rectangle with the attribute name inside connected to the relationship set diamond with a dotted line.</p> <h3 id="roles-1">Roles</h3> <p>The entity sets of a relationship need not be distinct. In such a case, we assign ‘roles’ to the entity sets.</p> <h3 id="degree-of-a-relationship-set">Degree of a relationship set</h3> <p>The degree of a relationship set is defined as the number of entities associated with the relationship set. Most relationship sets are binary. We can’t represent all ternary relations as a set of binary relations.</p> <p>There are no null values in the ER model. This becomes an issue in case of ternary relationships. Such problems are prevented in binary relationships. For example, think about the ER model of people and their parents. If someone has only one parent, then it is difficult to represent this using a ternary relationship between people, fathers and mothers. Instead, we could have two separate father and mother relationships. Binary relationships also provide the flexibility of mapping multiple entities to the same entity between two entity sets. While this is also possible in ternary relationships, we have more options in case of binary relationships.</p> <p>Does ternary relationship convey more information than binary relationship in any case? Yes, that is why we can’t represent all ternary relations as a set of binary relations. For instance, think about the instructor, project and student mapping. There are many combinations possible here which can’t be covered using binary relationships.</p> <h3 id="complex-attributes">Complex attributes</h3> <p>So far, we have considered atomic attributes in the relation model. The ER model does not impose any such requirement. We have <strong>simple</strong> and <strong>composite</strong> attributes. A composite attribute can be broken down into more attributes. For instance, we can have first and last name in name. We can have <em>single-valued</em> and <em>multi-valued</em> attributes. We can also have <em>derived</em> attributes. Multivalued attributes are represented inside curly braces <code class="language-plaintext highlighter-rouge">{}</code>.</p> <h3 id="mapping-cardinality-constraints">Mapping cardinality constraints</h3> <p>A mapping cardinality can be one-to-one, one-to-many, many-to-one or many-to-many.</p> <h1 id="lecture-11">Lecture 11</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">25-01-22</code></p> </blockquote> <p>We express cardinality constraints by drawing either a directed line \((\to)\), signifying ‘one’, or an undirected line \((-)\), signifying ‘many’, between the relationship set and the entity set.</p> <p>Let us now see the notion of participation.</p> <p><strong>Total participation</strong> - every entity in the entity set participates in at least one relationship in the relationship set. This is indicated using a double line in the ER diagram.</p> <p><strong>Partial participation</strong> - some entities may not participate in any relationship in the relationship set.</p> <p>We can represent more complex constraints using the following notation. A line may have an associated minimum and maximum cardinality, shown in the form ‘l..h’. A minimum value of 1 indicates total participation. A maximum value of 1 indicates that the entity participates in at most one relationship. A maximum value of * indicates no limit.</p> <p>How do we represent cardinality constraints in Ternary relationships? We allow at most one arrow out of a ternary (or greater degree) relationship to indicate a cardinality constraint. For instance, consider a ternary relationship R between A, B and C with arrows to B, then it indicates that each entity in A is associated with at most one entity in B for an entity in C.</p> <p>Now, if there is more than one arrow, the understanding is ambiguous. For example, consider the same setup from the previous example. If there are arrows to B and C, it could mean</p> <ul> <li>Each entity in A is associated with a unique entity from B and C, or</li> <li>Each pair of entities from (A, B) is associated with a unique C entity, and each pair (A, C) is associated with a unique entity in B.</li> </ul> <p>Due to such ambiguities, more than one arrows are typically not used.</p> <h3 id="primary-key-for-entity-sets">Primary key for Entity Sets</h3> <p>By definition, individual entities are distinct. From the database perspective, the differences are expressed in terms of their attributes. A key for an entity is a set of attributes that suffice to distinguish entities from each other.</p> <h3 id="primary-key-for-relationship-sets">Primary key for Relationship Sets</h3> <p>To distinguish among the various relationships of a relationship set we use the individual primary keys of the entities in the relationship set. That is, for a relationship set \(R\) involving entity sets \(E_1, E_2, \dots, E_n\), the primary key is given by the union of the primary keys of \(E_1, E_2, \dots, E_n\). If \(R\) is associated with any attributes, then the primary key includes those too. The choice of the primary key for a relationship set depends on the mapping cardinality of the relationship set.</p> <p><strong>Note.</strong> In one-to-many relationship sets, the primary key of the <strong>many</strong> side acts as the primary key of the relationship set.</p> <h3 id="weak-entity-sets">Weak Entity Sets</h3> <p>Weak entity set is an entity set whose existence depends on some other entity set. For instance, consider the section and course entity set. We cannot have a section without a course - an existence dependency. What if we use a relationship set to represent this? This is sort of redundant as both section and course have the course ID as an attribute. Instead of doing this, we can say that section is a weak entity set identified by course.</p> <p>In ER diagrams, a weak entity set is depicted via a double rectangle. We underline the discriminator of a weak entity set with a dashed line, and the relationship set connecting the weak entity set (using a double line) to the identifying strong entity set is depicted by a double diamond. The primary key of the strong entity set along with the discriminators of the weak entity set act as a primary key for the weak entity set.</p> <p>Every weak entity set must be associated with an <strong>identifying entity set</strong>. The relationships associating the weak entity set with the identifying entity set is called the <strong>identifying relationship</strong>. Note that the relational schema we eventually create from the weak entity set will have the primary key of the identifying entity set.</p> <h3 id="redundant-attributes">Redundant Attributes</h3> <p>Sometimes we often include redundant attributes while associating two entity sets. For example, the attribute <code class="language-plaintext highlighter-rouge">course_id</code> was redundant in the entity set section. However, when converting back to tables, some attributes get reintroduced.</p> <h2 id="reduction-to-relation-schemas">Reduction to Relation Schemas</h2> <p>Entity sets and relationship sets can be expressed uniformly as <em>relation schemas</em> that represent the content of the database. For each entity set and relationship set there is a unique schema that is assigned the name of the corresponding entity set or relationship set. Each schema has a number of columns which have unique names.</p> <ul> <li>A strong entity set reduces to a schema with the same attributes.</li> <li>A weak entity set becomes a table that includes a column for the primary key of the identifying strong entity set.</li> <li>Composite attributes are flattened out by creating separate attribute for each component attribute.</li> <li>A multivalued attribute \(M\) of an entity \(E\) is represented by a separate schema \(EM\). Schema \(EM\) has attributes corresponding to the primary key of \(E\) and an attribute corresponding to multivalued attribute \(M\).</li> <li>A many-to-many relationship set is represented as a schema with attributes for the primary keys of the two participating entity sets, and any descriptive attributes of the relationships set.</li> <li>Many-to-one and one-to-many relationship sets that are total on the many-side can be represented by adding an extra attribute to the ‘many’ side. If it were not total, null values would creep up. It is better to model such relationships as many-to-many relationships so that the model needn’t be changed when the cardinality of the relationship is changed in the future.</li> </ul> <h3 id="extended-er-features">Extended ER Features</h3> <p><strong>Specialization</strong> - Overlapping and Disjoint; Total and partial.</p> <p>How do we represent this in the schema? Form a schema for the higher-level entity and for the lower-level entity set. Include the primary key of the higher level entity set and local attributes in that of the local one. However, the drawback of such a construction is that we need to access two relations (higher and then lower) to get information.</p> <p><strong>Generalization</strong> - Combine a number of entity sets that share the same features into a higher-level entity set.</p> <p><strong>Completeness constraint</strong> specifies whether or not an entity in the higher-level entity set must belong to at least on of the lower-level entity sets within a generalization (total and partial concept). Partial generalization is the default.</p> <p><strong>Aggregation</strong> can also be represented in the ER diagrams.</p> <ul> <li>To represent aggregation, create a schema containing - primary key of the aggregated relationship, primary key of the associated entity set, and any descriptive attributes.</li> </ul> <blockquote> <p>I don’t understand aggregation</p> </blockquote> <h3 id="design-issues">Design issues</h3> <p><img src="/assets/img/Databases/image-20220126214411806.png" alt="image-20220126214411806"></p> <h1 id="lecture-12">Lecture 12</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">27-01-22</code></p> </blockquote> <h3 id="binary-vs-non-binary-relationships">Binary vs. Non-Binary Relationships</h3> <p>We had discussed this previously in <a href="#degree-of-a-relationship-set">this</a> section. In general, any non-binary relationship can be represented using binary relationships by creating an artificial entity set. We do this by replacing \(R\) between entity sets \(A, B, C\) by an entity set \(E\), and three relationship sets \(R_i\) relating \(E\) and \(i \in \{A, B, C\}\). We create an identifying attribute for E and add any attributes of \(R\) to \(E\). For each relationship \((a_i, b_i, c_i)\) in \(R\), we</p> <ul> <li>create a new entity \(e_i\) in the entity set \(E\)</li> <li>add \((e_i, j_i)\) to \(R_j\) for \(j \in \{A, B, C\}\)</li> </ul> <p><img src="/assets/img/Databases/image-20220215211541849.png" alt="image-20220215211541849"></p> <p>We also need to translate constraints (which may not be always possible). There may be instances in the translated schema that cannot correspond to any instance of \(R\). We can avoid creating an identifying attribute for \(E\) by making it a weak entity set identified by the three relationship sets.</p> <h3 id="er-design-decisions">ER Design Decisions</h3> <p><strong>Important Points</strong></p> <ul> <li>A weak entity set can be identified by multiple entity sets.</li> <li>A weak entity set can be identified by another weak entity set (indirect identification).</li> <li>In SQL, the value in a foreign key attribute can be null (the attribute of the relation having the fks constraint).</li> </ul> <h3 id="uml">UML</h3> <p>The <strong>Unified Modeling Language</strong> has many components to graphically model different aspects of an entire software system. The ER diagram notation we studied was inspired from the UML notation.</p> <h1 id="chapter-7-functional-dependencies">~Chapter 7: Functional Dependencies</h1> <p>When programmers usually skip the design phase, they run into problems with their relational database. We shall briefly mention these problems and see the motivation for this chapter.</p> <p>A good relational design does not have repetition of information, and no unnecessary null values. The only way to avoid the repetition of information is to decompose a relation to different schemas. However, all decompositions are not <strong>lossless</strong>.</p> <h1 id="lecture-13">Lecture 13</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">31-01-22</code></p> </blockquote> <p>The term <strong>lossy decomposition</strong> does not imply loss of tuples but rather the loss of information (relation) among the tuples. How do we formalise this idea?</p> <h2 id="lossless-decomposition---1">Lossless Decomposition - 1</h2> <p>Let \(R\) be a relations schema and let \(R_1\) and \(R_2\) form a decomposition of \(R = R_1 \cup R_2\). A decomposition if lossless if there is no loss of information by replacing \(R\) with the two relation schemas \(R_1, R_2\). That is,</p> \[\pi_{R_1}(r) \bowtie \pi_{R_2}(r) = r\] <p>Note that this relations must hold for all <strong>instances</strong> to call the decomposition lossless. And, conversely a decomposition is lossy if</p> \[r \subset \pi_{R_1}(r) \bowtie \pi_{R_2}(r)\] <p>We shall see the sufficient condition in a <a href="#lossless-decomposition---2">later section</a>.</p> <h2 id="normalization-theory">Normalization theory</h2> <p>We build the theory of functional/multivalued dependencies to decide whther a particular relation is in a “good” form.</p> <h2 id="functional-dependencies">Functional Dependencies</h2> <p>An instance of a relations that satisfies all such real-world constriants is called a <strong>legal instance</strong> of the relation. <u>A functional dependency is a generalization of the notion of a key</u>.</p> <p>Let \(R\) be a relation schema and \(\alpha, \beta \subseteq R\). The functional dependency \(\alpha \to \beta\) holds on \(R\) iff for any legal relations \(r(R)\) whenever two tuples \(t_1, t_2\) of \(r\) agree on the attributes \(\alpha\), they also agree on the attributes \(\beta\). That is,</p> \[\alpha \to \beta \triangleq t_1[\alpha] = t_2[\alpha] \implies t_1[\beta] = t_1[\beta]\] <h3 id="closure-properties">Closure properties</h3> <p>If \(A \to B\) and \(B \to C\) Then \(A \to C\). The set of <strong>all</strong> functional dependencies logically implied by a functional dependency set \(F\) is the <strong>closure</strong> of \(F\) denoted by \(F^+\).</p> <h3 id="keys-and-functional-dependencies">Keys and Functional Dependencies</h3> <p>\(K\) is a superket for relation schema \(R\) iff \(K \to R\). \(K\) is a candidate key for \(R\) iff</p> <ul> <li>\(K \to R\) and</li> <li>for no \(A \subset K\), \(A \to R\)</li> </ul> <p>Functional dependencies allow us to express constraints that cannot be expressed using super keys.</p> <h3 id="use-of-functional-dependencies">Use of functional dependencies</h3> <p>We use functional dependencies to test relations to see if they are legal and to specify constraints on the set of legal relations.</p> <p><strong>Note.</strong> A specific instance of a relation schema may satisfy a functional dependency even if that particular functional dependency does not hold across all legal instances.</p> <h2 id="lossless-decomposition---2">Lossless Decomposition - 2</h2> <p>A decomposition of \(R\) into \(R_1\) and \(R_2\) is lossless decomposition if at least one of the following dependecnies is in \(F^+\)</p> <ul> <li> \[R_1 \cap R_2 \to R_1\] </li> <li> \[R_1 \cap R_2 \to R_2\] </li> </ul> <p>The above functional dependencies are a necessary condition only if all constraints are functional dependencies.</p> <h2 id="dependency-preservation">Dependency Preservation</h2> <p>Testing functional dependency constrinats each time the database is updated can be costly. If testing a functional dependency can be done by considering just one relation, then the cost of testing this constraint is low. A decomposition that makes it computaitonally hard to enforce functional dependencies is sat to be <strong>not dependency preserving</strong>.</p> <h1 id="lecture-14">Lecture 14</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">01-02-22</code></p> </blockquote> <h2 id="boyce-codd-normal-form">Boyce-Codd Normal Form</h2> <p>There are a few designs of relational schema which prevent redundancies and have preferable properties. One such design format is BCNF.</p> <p>A relation schema \(R\) is in BCNF with respect to a set \(R\) of functional dependencies if <strong>for all</strong> functional dependencies in \(F^+\) of the form \(\alpha \to \beta\) where \(\alpha, \beta \subseteq R\), at least one of the following holds</p> <ul> <li>\(\alpha \to \beta\) is trivial (\(\beta \subseteq \alpha\))</li> <li>\(\alpha\) is a superkey for \(R\).</li> </ul> <p>Let \(R\) Be a schema \(R\) That is not in BCNF. Let \(\alpha \to \beta\) Be the FD that causes a violation of BCNF. Then, to convert \(R\) to BCNF we decompose it to</p> <ul> <li> \[\alpha \cup \beta\] </li> <li> \[R - (\beta - \alpha)\] </li> </ul> <p><strong><em>Example.</em></strong> Consider the relation \(R= (A, B, C)\) with \(F \{A \to B, B \to C\}\). Suppose we have the following decompositions</p> <ul> <li> \[R_1 = (A, B), R_2 = (B, C)\] <p>This decompositions is lossless-join and also dependency preserving. Notice that it is dependency preserving even though we have the \(A \to C\) constraint. This is because \(A \to C\) is implied from the other two constraints.</p> </li> <li> \[R = (A, B), R_2 = (A, C)\] <p>This decomposition is lossless but is not dependency preserving.</p> </li> </ul> <h3 id="bcnf-and-dependency-preservation">BCNF and Dependency Preservation</h3> <p><u>It is not always possible to achieve both BCNF and dependency preservation</u>.</p> <h2 id="third-normal-form">Third Normal Form</h2> <p>This form is useful when you are willing to allow a small amount of data redundancy in exchange for dependency preservation.</p> <p>A relations \(R\) Is in third normal form (3NF) if <strong>for all</strong> \(\alpha \to \beta \in F^+\) <strong>at least</strong> one of the following holds</p> <ul> <li>\(\alpha \to \beta\) is trivial</li> <li>\(\alpha\) is a super key for \(R\)</li> <li>Each attribute \(A\) In \(\beta - \alpha\) is contained in a candidate key for \(R\).</li> </ul> <p>There are 1NF and 2NF forms but they are not very important. <u>If a relation is in BCNF, then it is in 3NF.</u></p> <h3 id="redundancy-in-3nf">Redundancy in 3NF</h3> <p>Consider \(R\) which is in 3NF \(R = (J, K, L)\) and \(F = \{JK \to L, L \to K \}\). Then, we can have the following instance</p> <table> <thead> <tr> <th>J</th> <th>K</th> <th>L</th> </tr> </thead> <tbody> <tr> <td>p1</td> <td>q1</td> <td>k1</td> </tr> <tr> <td>p2</td> <td>q1</td> <td>k1</td> </tr> <tr> <td>p3</td> <td>q1</td> <td>k1</td> </tr> <tr> <td>null</td> <td>q2</td> <td>k2</td> </tr> </tbody> </table> <h3 id="3nf-and-dependency-preservation">3NF and Dependency Preservation</h3> <p>It is always possible to obtain a 3NF design without sacrificing losslessness or dependency preservation. However, we may have to use null values (like above) to represent some of the possible meaningful relationships among data items.</p> <h3 id="goals-of-normalisation">Goals of Normalisation</h3> <p>A “good” schema consists of lossless decompositions and preserved dependencies. We can use 3NF and BCNF (preferable) for such purpose.</p> <p>There are database schemas in BCNF that do not seem to be sufficiently normalised. <em>Multivalued dependencies, Insertion anomaly, …</em></p> <h2 id="functional-dependency-theory">Functional Dependency Theory</h2> <h3 id="closure-of-a-set-of-functional-dependencies">Closure of a set of functional dependencies</h3> <p>We can compute \(F^+\) from \(F\) by repeatedly applying <strong>Armstrong’s Axioms</strong></p> <ul> <li> <strong>Reflexive rule</strong> - If \(\beta \subseteq \alpha\), then \(\alpha \to \beta\)</li> <li> <strong>Augmentation rule</strong> - If \(\alpha \to \beta\), then \(\gamma\alpha \to \gamma\beta\)</li> <li> <strong>Transitivity rule</strong> - If \(\alpha \to \beta\) and \(\beta \to \gamma\) then \(\alpha \to \gamma\)</li> </ul> <p>It is trivial to see that these rules are <strong>sound</strong>. However, showing that these rules are <strong>complete</strong> is much more difficult.</p> <p>Additional rules include (can be derived from above)-</p> <ul> <li> <strong>Union rule</strong> - If \(\alpha \to \beta\) and \(\alpha \to \gamma\), then \(\alpha \to \beta\gamma\)</li> <li> <strong>Decomposition rule</strong> - If \(\alpha \to \beta\gamma\), then \(\alpha \to \beta\)</li> <li> <strong>Pseudo-transitivity rule</strong> - If \(\alpha \to \beta\) and \(\gamma\beta \to \delta\), then \(\alpha \gamma \to \delta\)</li> </ul> <h3 id="closure-of-attribute-sets">Closure of Attribute Sets</h3> <p>Given a set of attributes \(\alpha\), define the <strong>closure</strong> of \(\alpha\) <strong>under</strong> \(F\) (denoted by \(\alpha^+\)) as the set of attributes that are functionally determined by \(\alpha\) under \(F\). We use the following procedure to compute the closure of A</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>result := A
while (change) do
	for each beta to gamma in F do
		begin
			if beta in result then result = result union gamma
    end
</code></pre></div></div> <p>The time complexity of this algorithm is \(\mathcal O(n^3)\) where \(n\) is the number of attributes.</p> <p>There are several uses of the attribute closure algorithm</p> <ul> <li>To test if \(\alpha\) is a superkey, we compute \(\alpha\) and check if it contains all attributes of the relation</li> <li>To check if a functional dependency \(\alpha \to \beta\) holds, see if \(\beta \subseteq \alpha^+\)</li> <li>For computing the closure of \(F\). For each \(\gamma \subseteq R\), we find \(\gamma\) and for each \(S \subset \gamma^+\), we output a functional dependency \(\gamma \to S\).</li> </ul> <h3 id="canonical-cover">Canonical Cover</h3> <p>A <strong>Canonical cover</strong> of a functional dependency set \(F\) is the minimal set of functional dependencies such that its closure is \(F^+\).</p> <p>An attribute of a functional dependency in \(F\) is <strong>extraneous</strong> if we can remove it without changing \(F^+\). Removing an attribute from the left side of a functional dependency will make it a stronger constraint.</p> <h1 id="lecture-15">Lecture 15</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">05-02-22</code></p> </blockquote> <h3 id="extraneous-attributes">Extraneous attributes</h3> <p>Removing an attribute from the left side of a functional dependency makes it a stronger constraint. Attribute A is extraneous in \(\alpha\) if</p> <ul> <li> \[A \in \alpha\] </li> <li>\(F\) logically implies \((F - \{\alpha \to \beta\}) \cup \{(\alpha - A) \to \beta\}\)</li> </ul> <p>To test this, consider \(\gamma = \alpha - \{A\}\). Check if \(\gamma \to \beta\) can be inferred from \(F\). We do this by computing \(\gamma^+\) using the dependencies in \(F\), and if it includes all attributes in \(\beta\) then \(A\) is extraneous in \(\alpha\).</p> <p>On the other hand, removing an attribute from the right side of a functional dependency could make it a weaker constraint. Attribute A is extraneous in \(\beta\) if</p> <ul> <li> \[A \in \beta\] </li> <li> <p>The set of functional dependencies</p> <p>\((F - \{\alpha \to \beta\}) \cup \{\alpha \to ( \beta - A)\}\) logically implies \(F\).</p> </li> </ul> <p>To test this, consider \(F’ = (F - \{\alpha \to \beta\}) \cup \{\alpha \to ( \beta - A)\}\) and check if \(\alpha^+\) contains A; if it does, then \(A\) is extraneous in \(\beta\).</p> <h2 id="canonical-cover-revisited">Canonical Cover Revisited</h2> <p>A <strong>canonical cover</strong> for \(F\) is a set of dependencies \(F_c\) such that</p> <ul> <li>\(F\)(\(F_c\)) logically implies all dependencies in \(F_c\)(\(F\))</li> <li>No functional dependency in \(F_c\) contains an extraneous attribute</li> <li><u>Each left side of functional dependency in $$F_c$$ is unique</u></li> </ul> <p>To compute a canonical cover for \(F\) do the following</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fc = F
repeat
	Use the union rule to replace any dependencies in Fc of the form
		alpha -&gt; beta and alpha -&gt; gamma with
    alpha -&gt; beta gamma
	Find a functional dependency alpha to beta 
	in Fc with an extraneous attribute 
	in alpha or in beta
	Delete extraneous attribute if found
until Fc does not change
</code></pre></div></div> <h2 id="dependency-preservation---3">Dependency Preservation - 3</h2> <p>Let \(F_i\) be the set of dependencies \(F^+\) that include only the attribtues in \(R_i\). A decomposition is <strong>dependency preserving</strong> if \((F_1 \cup \dots \cup F_n)^+ = F^+\). However, we can’t use this definition to test for dependency preserving as it takes exponential time.</p> <p>We test for dependency preservation in the following way. To check if a dependency \(\alpha \to \beta\) is preserved in a decomposition, apply the following test</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>result = alpha
repeat
	for each Ri in the decomposition
		t = (result cap Ri)^+ cap Ri
		result = result \cup t
until result does not change
</code></pre></div></div> <p>If the result contains all attributes in \(\beta\), then the functional dependency \(\alpha \to \beta\) is preserved. This procedure takes polynomial time.</p> <h2 id="testing-for-bcnf">Testing for BCNF</h2> <p>To check if a non-trivial dependency \(\alpha \to \beta\) cause a violation of BCNF, compute \(\alpha^+\) and verify that it includes all attributes of \(R\). Another simpler method is to check only the dependencies in the given set \(F\) for violations of BCNF rather than checking all dependencies in \(F^+\). If none of the dependencies in \(F\) cause a violation, then none of the dependencies in \(F^+\) will cause a violation. <em>Think.</em></p> <p>However, the simplified test using only \(F\) is incorrect when testing a relation in a decomposition of \(R\). For example, consider \(R = (A, B, C, D, E)\), with \(F = \{ A \to B, BC \to D\}\) and the decomposition \(R_1 = (A, B), R_2 = (A, C, D, E)\). Now, neither of the dependencies in \(F\) contain only attributes from \((A, C, D, E)\) so we might be misled into thinking \(R_2\) satisfies BCNF.</p> <p>Therefore, testing decomposition requires the restriction of \(R^+\) to that particular set of tables. If one wants to the use the original set of dependencies \(F\), then they must check that \(\alpha^+\) either includes no attributes of \(R_i - \alpha\) or includes all attributes of \(R_i\) for every set of attributes \(\alpha \subseteq R\). If a condition \(\alpha \to \beta \in F^+\) violates BCNF, then the dependency \(\alpha \to (\alpha^+ - \alpha) \cap R_i\) can be shown to hold on \(R_i\) and violate BCNF.</p> <p>In conclusion, the BCNF decomposition algorithm is given by</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>result = R
done = false
compute F+
while (not done) do
	if (there is a schema Ri in result that is not in BCNF)
		let alpha to beta be a nontrivial functional dependency
      that holds on Ri such that alpha to Ri
      is not in F+
      and alpha cap beta is null
 		result = {(result - Ri),(Ri - beta),(alpha, beta)}
 	else done = true
		
</code></pre></div></div> <p>Note that each \(R_i\) is in BCNF and decomposition is lossless-join.</p> <h2 id="3nf">3NF</h2> <p>The main drawback of BCNF is that is may not be dependency preserving. Through 3NF, we allow some redundancy to acquire dependency preserving along with lossless join.</p> <p>To test for 3NF, we only have to check the FDs in \(F\) and not all the FDs in \(F^+\).We use attribute closure to check for each dependency \(\alpha \to \beta\), if \(\alpha\) is a super key. If \(\alpha\) is not a super key, we need to check if each attribute in \(\beta\) is contained in a candidate key of \(R\).</p> <p>However, this test is shown to be NP-hard, but the decomposition into third normal form can be done in polynomial time.</p> <blockquote> <p>Doesn’t decomposition imply testing? No, one relation can have many 3NF decompositions.</p> </blockquote> <h3 id="3nf-decomposition-algorithm">3NF Decomposition Algorithm</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let Fc be a canconical cover for F;
i = 0;
/* initial schema is empty */
for each FD alpha to beta in Fc do
	if none of the schemas Rj (j &lt;= i) contains alpha beta
		then begin
			i = i + 1
			Ri = alpha beta
		end
/* Here, each of the FDs will be contained in one of the Rjs */
if none of the schemas Rj (j &lt;= i) contains a candidate key for R
  then begin
    i = i + 1
    Ri = any candidate key for R
  end
/* Here, there is a relation contianing the candidate key of R */
/* Optionally remove redundant relations */
repeat
  if any schema Rj is contained in another schema Rk
    then delete Rj
      Rj = Ri;
      i = i - 1
  return (R1, ..., Ri)
</code></pre></div></div> <p>Guaranteeing that the above set of relations are in 3NF is the easy part. However, proving that the decomposition is lossless is difficult.</p> <h3 id="comparison-of-bcnf-and-3nf">Comparison of BCNF and 3NF</h3> <p>3NF has redundancy whereas BCNF may not be dependency preserving. The bigger problem is 3Nf allows certain function dependencies which are not super key dependencies. However, none of the SQL implementations today support such FDs.</p> <h2 id="multivalued-dependencies">Multivalued Dependencies</h2> <p>Let \(R\) be a relation schema and let \(\alpha, \beta \subseteq R\) . The multivalued dependency</p> \[\alpha \to\to \beta\] <p>holds on \(R\) if in any legal relation \(r(R)\), for all pairs for tuples \(t_1, t_2\) in \(r\) such that \(t_1[\alpha] = t_2[\alpha]\), there exists tuples \(t_3\) and \(t_4\) in \(r\) such that</p> \[t_1[\alpha] = t_2[\alpha] = t_3[\alpha] = t_4[\alpha] \\ t_3[\beta] = t_1[\beta] \\ t_3[R - \beta] = t_2[R - \beta] \\ t_4[\beta] = t_2[\beta] \\ t_4[R - \beta] = t_1[R - \beta]\] <p>Intuitively, it means that the relationship between \(\alpha\) and \(\beta\) is independent of the remaining attributes in the relation. The tabular representation of these conditions is given by</p> <p><img src="/assets/img/Databases/image-20220217224504065.png" alt="image-20220217224504065"></p> <p>The definition can also be mentioned in a more intuitive manner. Consider the attributes of \(R\) that are partitioned into 3 nonempty subsets \(W, Y, Z\). We say that \(Y \to\to Z\) iff for all possible relational instances \(r(R)\),</p> \[&lt;y_1, z_1, w_1&gt; \in r \text{ and } &lt; y_1, z_2, w_2 &gt; \in r \\ \implies \\ &lt;y_1, z_1, w_2 &gt; \in r \text{ and } &lt;y_1, z_2, w_1 &gt; \in r\] <p><strong>Important Points</strong>-</p> <ul> <li>If \(Y \to\to Z\) then \(Y \to\to W\)</li> <li>If \(Y \to Z\) then \(Y \to\to Z\)</li> </ul> <blockquote> <p>why?</p> </blockquote> <p>The closure \(D^+\) of \(D\) is the set of all functional and multivalued dependencies logically implied by \(D\). We are not covering the reasoning here.</p> <h2 id="fourth-normal-form">Fourth Normal Form</h2> <p>A relation schema \(R\) is in 4NF rt a set \(D\) of functional and multivalued dependencies if for all multivalued dependencies in \(D^+\) in the form \(\alpha \to\to \beta\), where \(\alpha, \beta \subseteq R\), at least one of the following hold</p> <ul> <li>\(\alpha \to\to \beta\) is trivial</li> <li>\(\alpha\) is a super key for schema \(R\)</li> </ul> <p><u>If a relation is in 4NF, then it is in BCNF.</u> That is, 4NF is stronger than BCNF. Also, 4NF is the generalisation of BCNF for multivalued dependencies.</p> <h3 id="restriction-of-mvds">Restriction of MVDs</h3> <p>The restriction of \(D\) to \(R_i\) is the set \(D_i\) consisting of</p> <ul> <li>All functional dependencies in \(D^+\) that include only attributes of \(R_i\)</li> <li>All multivalued dependencies of the form \(\alpha \to \to (\beta \cap R_i)\) where \(\alpha \in R_i\) and \(\alpha \to\to \beta \in D^+\).</li> </ul> <h3 id="4nf-decomposition-algorithm">4NF Decomposition Algorithm</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>result = {R};
done = false;
compute D+
Let Di denote the restriction of D+ to Ri
while(not done)
	if (there is a schema Ri in result not in 4NF)
		let alpha to to beta be a nontrivial MVD that holds
		on Ri such that  alpha to Ri is not in Di and 
		alpha cap beta is null
		result = {result - Ri, Ri - beta, (alpha, beta)}
	else done = true
</code></pre></div></div> <p>This algorithm is very similar to that of BCNF decomposition.</p> <h1 id="lecture-16">Lecture 16</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">07-02-22</code></p> </blockquote> <h2 id="further-normal-forms">Further Normal Forms</h2> <p><strong>Join dependencies</strong> generalise multivalued dependencies and lead to <strong>project-join normal form (PJNF)</strong> also known as <strong>5NF</strong>. A class of even more general constraints leads to a normal form called <strong>domain-key normal form</strong>. There are hard to reason with and no set of sound and complete set of inference rules exist.</p> <h2 id="overall-database-design-process">Overall Database Design Process</h2> <p>We have assumed \(R\) is given. In real life, we can get it based on applications through ER diagrams. However, one can consider \(R\) to be generated from a single relations containing all attributes that are of interest (called <strong>universal relation</strong>). Normalisation breaks this \(R\) into smaller relations.</p> <p>Some aspects of database design are not caught by normalisation. For example, a <strong>crosstab</strong>, where values for on attribute become column names, is not captured by normalisation forms.</p> <h2 id="modeling-temporal-data">Modeling Temporal Data</h2> <p><strong>Temporal data</strong> have an associated time interval during which the data is valid. A <strong>snapshot</strong> is the value of the data at a particular point in time. Adding a temporal component results in functional dependencies being invalidated because the attribute values vary over time. A <strong>temporal functional dependency</strong> \(X \xrightarrow{\tau} Y\) holds on schema \(R\) if the functional dependency \(X \to Y\) holds on all snapshots for all legal instances \(r(R)\).</p> <p>In practice, database designers may add start and end time attributes to relations. SQL standard [start, end). In modern SQL, we can write</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">period</span> <span class="k">for</span> <span class="n">validtime</span> <span class="p">(</span><span class="k">start</span><span class="p">,</span> <span class="k">end</span><span class="p">)</span>
<span class="k">primary</span> <span class="k">key</span> <span class="p">(</span><span class="n">course_id</span><span class="p">,</span> <span class="n">validtime</span> <span class="k">without</span> <span class="k">overlaps</span><span class="p">)</span>
</code></pre></div></div> <h1 id="chapter-8-complex-data-types">~Chapter 8: Complex Data Types</h1> <p>Expected to read from the textbook.</p> <ul> <li>Semi-Structured Data</li> <li>Object Orientation</li> <li>Textual Data</li> <li>Spatial Data</li> </ul> <h2 id="semi-structured-data">Semi-Structured Data</h2> <p>Many applications require storage of complex data, whose schema changes often. The relational model’s requirement of atomic data types may be an overkill. JSON (JavaScript Object Notation) and XML (Extensible Markup Language) are widely used semi-structured data models.</p> <p><strong>Flexible schema</strong></p> <ul> <li> <strong>Wide column</strong> representation allow each tuple to have a different set of attributes and can add new attributes at any time</li> <li> <strong>Sparse column</strong> representation has a fixed but large set of attributes but each tuple may store only a subset.</li> </ul> <p><strong>Multivalued data types</strong></p> <ul> <li>Sets, multi-sets</li> <li>Key-value map</li> <li>Arrays</li> <li>Array database</li> </ul> <h3 id="json">JSON</h3> <p>It is a verbose data type widely used in data exchange today, There are efficient data storage variants like BSON</p> <h3 id="knowledge-representation">Knowledge Representation</h3> <p>Representation of human knowledge is a long-standing goal of AI. <strong>RDF: Resource Description Format</strong> is a simplified representation for facts as triples of the form (subject, predicate, object). For example, (India, Population, 1.7B) is one such form. This form has a natural graph representation. There is a query language called SparQL for this representation. <strong>Linked open data</strong> project aims to connect different knowledge graphs to allow queries to span databases.</p> <p>To represent n-ary relationships, we can</p> <ul> <li>Create an artificial entity and link to each of the n entities</li> <li>Use <strong>quads</strong> instead of triples with context entity</li> </ul> <h2 id="object-orientation">Object Orientation</h2> <p><strong>Object-relational data model</strong> provides richer type system with complex data types and object orientation. Applications are often written in OOP languages. However, the type system does not match relational type system and switching between imperative language and SQL is cumbersome.</p> <p>To use object-orientation with databases, we could build an <strong>object-relational database</strong>, adding object-oriented features to a relational database. Otherwise, we could automatically convert data between OOP and relational model specified by <strong>object-relational mapping</strong>. <strong>Object-oriented database</strong> is another option that natively supports object-oriented data and direct access from OOP. The second method is widely used now.</p> <h3 id="object-relational-mapping">Object-Relational Mapping</h3> <p>ORM systems allow</p> <ul> <li>specification of mapping between OOP objects and database tuples</li> <li>Automatic modification of database</li> <li>Interface to retrieve objects satisfying specified conditions</li> </ul> <p>ORM systems can be used for websites but not for data-analytics applications!</p> <h2 id="textual-data">Textual Data</h2> <p><strong>Information retrieval</strong> basically refers to querying of unstructured data. Simple model of keyword queries consists of fetching all documents containing all the input keywords. More advanced models rank the relevance of documents.</p> <h3 id="ranking-using-tf-idf">Ranking using TF-IDF</h3> <p>Term is a keyword occurring in a document query. The <strong>term frequency</strong> \(TF(d, t)\) is the relevance of a term \(t\) to a document \(d\). It is defined by</p> \[TF(d, t) = \log( 1+ n(d, t)/n(d))\] <p>where \(n(d, t)\) is the number of occurrences of term \(t\) in document \(d\) and \(n(d)\) is the number of terms in the document \(d\).</p> <p>The <strong>inverse document frequency</strong> \(IDF(t)\) is given by</p> \[IDF(t) = 1/n(t)\] <p>This is used to give importance to terms that are rare. <strong>Relevance</strong> of a document \(d\) to a set of terms \(Q\) can be defined as</p> \[r(d, Q) = \sum_{t \in Q} TF(d, t)*IDF(t)\] <p>There are other definitions that take <strong>proximity</strong> of words into account and <strong>stop words</strong> are often ignored.</p> <h1 id="lecture-17">Lecture 17</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">08-02-22</code></p> </blockquote> <p>The TF-IDF method in search engine was did not work out as web designers added repeated occurrences of words on their website to increase the relevance. There were plenty of shady things web designers could do in order to increase the page relevance. To prevent this problem, Google introduced the model of <strong>PageRank</strong>.</p> <h3 id="ranking-using-hyperlinks">Ranking using Hyperlinks</h3> <p>Hyperlinks provide very important clues to importance. Google’s PageRank measures the popularity/importance based on hyperlinks to pages.</p> <ul> <li>Pages hyperlinked from many pages should have higher PageRank</li> <li>Pages hyperlinked from pages with higher PageRank should have higher PageRank</li> </ul> <p>This model is formalised by a <strong>random walk</strong> model. Let \(T[i, j]\) be the probability that a random walker who is on page \(i\) will click on the link to page \(j\). Then, PageRank[j] for each page \(j\) is defined as</p> \[P[j] = \delta/N + (1 - \delta)*\sum_{i = 1}^n(T(i, j)*P(j))\] <p>where \(N\) is the total number of pages and \(\delta\) is a constant usually set to \(0.15\). As the number of pages are really high, some sort of bootstrapping method (Monte Carlo simulation) is used to approximate the PageRank. PageRank also can be fooled using mutual link spams.</p> <h3 id="retrieval-effectiveness">Retrieval Effectiveness</h3> <p>Measures of effectiveness</p> <ul> <li> <strong>Precision</strong> - what % of returned results are actually relevant.</li> <li> <strong>Recall</strong> - what percentage of relevant results were returned</li> </ul> <h2 id="spatial-data">Spatial Data</h2> <p>Not covered</p> <h1 id="chapter-9-application-development">~Chapter 9: Application Development</h1> <h2 id="http-and-sessions">HTTP and Sessions</h2> <p>The HTTP protocol is <strong>connectionless</strong>. That is, once the server replied to a request, the server closes the connection with the client, and forgets all about the request. The motivation to this convention is that it reduces the load on the server. The problem however is authentication for every connection. Information services need session information to acquire user authentication only once per session. This problem is solved by <strong>cookies</strong>.</p> <p>A <strong>cookie</strong> is a small piece of text containing identifying information</p> <ul> <li>sent by server to browser on first interaction to identify session</li> <li>sent by browser to the server that created the cookie on further interactions (part of the HTTP protocol)</li> <li>Server saved information about cookies it issued, and can use it when serving a request. E.g, authentication information, and user preferences</li> </ul> <p>Cookies can be stored permanently or for a limited time.</p> <p><strong>Java Servlet</strong> defines an API for communication between the server and app to spawn threads that can work concurrently.</p> <h2 id="web-services">Web Services</h2> <p>Web services are basically URLs on which we make a request to obtain results.</p> <p>Till HTML4, local storage was restricted to cookies. However, this was expanded to any data type in HTML5.</p> <h2 id="http-and-https">HTTP and HTTPS</h2> <p>The application server authenticates the user by the means of user credentials. What if a hacker scans all the packets going to the server to obtain a user’s credentials? So, HTTPS was developed to encrypt the data sent between the browser and the server. How is this encryption done? The server and the browser need to have a common key. Turns out, there are crypto techniques that can achieve the same.</p> <p>What if someone creates a middleware that simulates a website a user uses? This is known as <strong>man in the middle attack</strong>. How do we know we are connected to the authentic website? The basic idea is to have a <strong>public key</strong> of the website and send data encrypted via this public key. Then, the website uses its own <strong>private key</strong> to decrypt the data. This conversion is reversible. As in, the website encrypts the data using its own private key which can be decoded by the user using the public key.</p> <p>How do we get public keys for millions of websites out there? We use <strong>digital certificates</strong>. Let’s say we have a website’s public key and that website has the public key of the user (via their website). The website then encrypts the user’s public key using its private key to generate a digital certificate. This digital certificate can be advertised on the user’s website to allow other users to check the authenticity of the user’s website. Now, another user can obtain this certificate, decrypt it using the first website’s private key, and verify the authenticity of the user’s webpage. These verifications are maintained as a hierarchical structure to maintain digital certificates of millions of websites.</p> <h2 id="cross-site-scripting">Cross Site Scripting</h2> <p>In cross site scripting, the user’s session for one website is used in another website to execute actions at the server of the first website. For example, suppose a bank’s website, when logged in, allows the user to transfer money by visiting the link <code class="language-plaintext highlighter-rouge">xyz.com/?amt=1&amp;to=123</code>. If another website has a similar link (probably for displaying an image), then it can succeed in transferring the amount if the user is still logged into the bank. This vulnerability is called called <strong>cross-site scripting (XSS)</strong> or <strong>cross-site request forgery (XSRF/CSRF)</strong>. <strong>XSRF</strong> tokens are a form of cookies that are used to check these cross-site attacks (CORS from Django).</p> <h1 id="lecture-18">Lecture 18</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">14-02-22</code></p> </blockquote> <h2 id="application-level-authorisation">Application Level Authorisation</h2> <p>Current SQL standard does not allow fine-grained authorisation such as students seeing their own grades but not others. <strong>Fine grained (row-level) authorisation</strong> schemes such as Oracle Virtual Private Database (VPD) allows predicates to be added transparently to all SQL queries.</p> <h1 id="chapter-10-big-data">~Chapter 10: Big Data</h1> <p>Data grew in terms of volume (large amounts of data), velocity (higher rates of insertions) and variety (many types of data) in the recent times. This new generation of data is known as <strong>Big Data</strong>.</p> <p>Transaction processing systems (ACID properties) and query processing systems needed to be made scalable.</p> <h2 id="distributed-file-systems">Distributed File Systems</h2> <p>A distributed file system stores data across a large collection of machines, but provides a single file-system view. Files are replicated to handle hardware failure, and failures were to be detected and recovered from.</p> <h3 id="hadoop-file-system-architecture">Hadoop File System Architecture</h3> <p>A single namespace is used for an entire cluster. Files are broken up into blocks (64 MB) and replicated on multiple <em>DataNodes</em>. A client finds the location of blocks from <em>NameNode</em> and accesses the data from <em>DataNodes</em>.</p> <p>The key idea of this architecture is using large block sizes for the actual file data. This way, the metadata would be reduced and the <em>NameNode</em> can store the <em>DataNodes</em> info in a much more scalable manner.</p> <p>Distributed file systems are good for <u>millions of large files</u>. However, distributed file systems have very high overheads and poor performance with billions of smaller tuples. Data coherency also needs to be ensured (write-once-read-many access model).</p> <h2 id="sharding">Sharding</h2> <p>It refers to partitioning data across multiple databases. Partitioning is usually done on some <strong>partitioning attributes</strong> known as <strong>partitioning keys</strong> or <strong>shard keys</strong>. The advantage to this is that it scales well and is easy to implement. However, it is not transparent (manually writing all routes and queries across multiple databases), removing load from an overloaded database is not easy, and there is a higher change of failure. Sharding is used extensively by banks today.</p> <h2 id="key-value-storage-systems">Key Value Storage Systems</h2> <p>These systems store large numbers of small sized records. Records are partitioned across multiple machines, and queries are routed by the system to appropriate machine. Also, the records are replicated across multiple machines to ensure availability. Key-value stores ensure that updates are applied to all replicas to ensure consistency.</p> <p>Key-value stores may have</p> <ul> <li> <u>uninterpreted bytes</u> with an associated key</li> <li> <u>Wide-column</u> with associated key</li> <li>JSON</li> </ul> <p><strong>Document stores</strong> store semi-structured data, typically JSON. Key-value stores support <code class="language-plaintext highlighter-rouge">put</code>, <code class="language-plaintext highlighter-rouge">get</code> and <code class="language-plaintext highlighter-rouge">delete</code>. Some systems also support <strong>range queries</strong> on key values. Document stores also support queries on non-key attributes. <u>Key value stores are not full database systems</u>. They have no/limited support for transactional updates and applications must manage query processing on their own. These systems are therefore known as <strong>NoSQL</strong> systems.</p> <h2 id="parallel-and-distributed-databases">Parallel and Distributed Databases</h2> <p><strong>Replication</strong> to ensure availability. <strong>Consistency</strong> implemented using majority protocols. <strong>Network partitions</strong> involve a network can break into two or more parts, each with active systems that can’t talk to other parts. In presence of partitions, cannot guarantee both availability and consistency - <strong>Brewer’s CAP</strong> theorem. Traditional database systems choose consistency, and most web applications choose availability.</p> <h1 id="lecture-19">Lecture 19</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">15-02-22</code></p> </blockquote> <h2 id="mapreduce-paradigm">MapReduce Paradigm</h2> <p>The goal here is to be able to run many queries/scripts across a large number of machines. <code class="language-plaintext highlighter-rouge">Map</code> and <code class="language-plaintext highlighter-rouge">Reduce</code> have similar functionalities as seen in Python. Programmers realised many operations can be reduced to a sequence of map and reduce actions (popular in functional programming).</p> <p>Google formalised the notion of map-reduce for web-crawling and other web-development needs as map-reduce workers. This paradigm was used along with distributed file systems.</p> <p>The default input for the map operations is a line.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>map(k, v) -&gt; list(k1, v1)
reduce(k1, list(v1)) -&gt; v2
</code></pre></div></div> <p>However, map-reduce code for database queries was large. So, the developers at Facebook came up with Hive which converts SQL queries to map-reduce queries.</p> <h2 id="algebraic-operations">Algebraic Operations</h2> <p>We shall study these as a part of <strong>Spark</strong>.</p> <h1 id="lecture-20">Lecture 20</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">17-02-22</code></p> </blockquote> <h2 id="algebraic-operations-in-spark">Algebraic Operations in Spark</h2> <p><strong>Resilient Distributed Dataset (RDD)</strong> abstraction is a collection of records that can be stored across multiple machines. RDDs can be created by applying algebraic operations on other RDDs. This is a generalisation to RA where the operators can be any piece of code. <u>RDDs can be lazily computed when needed.</u> As in, the tree is executed only on specific functions such as <code class="language-plaintext highlighter-rouge">saveAsTextFile()</code> or <code class="language-plaintext highlighter-rouge">collect()</code>.</p> <p>Spark makes use of Java Lambda expressions with the following syntax.</p> <div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">-&gt;</span> <span class="nc">ArrayasList</span><span class="o">(</span><span class="n">s</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">" "</span><span class="o">)).</span><span class="na">iterate</span><span class="o">()</span>
</code></pre></div></div> <p>RDDs in Spark can be typed in programs, but not dynamically.</p> <h2 id="streaming-data">Streaming Data</h2> <p>Streaming data refers to data that arrives in a continuous fashion in contrast to <strong>data-at-rest</strong>.</p> <p>Approaches to querying streams-</p> <ul> <li> <strong>Windowing</strong> - Break up stream into windows and queries are run on windows.</li> <li> <strong>Continuous Queries</strong> - Queries written e.g. in SQL, output partial result based on stream seen so far; query results are updated continuously.</li> <li> <strong>Algebraic operators on streams</strong> - Operators are written in an imperative language.</li> <li> <strong>Pattern Matching</strong> - <em>Complex Even Processing (CEP)</em> systems. Queries specify patterns, system detects occurrences of patterns and triggers actions.</li> <li> <strong>Lambda architecture</strong> - Split the stream into two, one output goes to stream processing system and the other to a database for storage.</li> </ul> <p>There are stream extensions to SQL - Tumbling window, Hopping window, Sliding window and Sessions windows.</p> <h3 id="publish-subscribe-systems">Publish Subscribe Systems</h3> <p><strong>Public-subscribe (pub-sub)</strong> systems provide convenient abstraction for processing streams. For example, Apache Kafka</p> <h2 id="graph-databases">Graph Databases</h2> <p>A <strong>graph data model</strong> can be seen as a generalisation of the ER model. Every entity can be seen as a node, and every binary relationship is an edge. Higher degree relationships can be expressed as multiple binary relationships.</p> <p>Check out <em>Neo4J</em>. Query languages for graph databases make it easy for graph traversal.</p> <h1 id="lecture-21">Lecture 21</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">28-02-22</code></p> </blockquote> <h3 id="parallel-graph-processing">Parallel graph processing</h3> <p>Two popular approaches have been devised for parallel processing on very large graphs</p> <ul> <li>Map-reduce and algebraic frameworks</li> <li><strong>Bulk synchronous processing (BSP)</strong></li> </ul> <h3 id="bulk-synchronous-processing">Bulk Synchronous Processing</h3> <p>Each vertex of a graph has data associated with it. The vertices are partitioned across multiple machines, and state of the nodes are kept in-memory. Now, in each step (<em>superstep</em>)</p> <ul> <li>Nodes process received messages</li> <li>Update their state</li> <li>Send further messages or vote to halt</li> <li>Computation ends when all nodes vote to halt, and there are no pending messages</li> </ul> <p>The method is synchronous as the computation is done in steps. However, this method is not fault tolerant as all the computations need to be recomputed in case of a failure. Checkpoints can be created for restoration.</p> <h1 id="chapter-11-data-analytics">~Chapter 11: Data Analytics</h1> <ul> <li> <strong>Data Analytics</strong> - The processing of data to infer patterns, correlations, or models for prediction.</li> <li>Data often needs to be <strong>extracted</strong> from various source formats, <strong>transformed</strong> to a common schema, and <strong>loaded</strong> into the <u>data warehouse</u>. (ETL)</li> <li> <strong>Data mining</strong> extends techniques developed by ML onto large datasets</li> <li>A <strong>data warehouse</strong> is a repository of information gathered from multiple sources, stored under a unified schema at a single site. It also permits the study of historical trends. The common schema is <u>optimised for querying and not transactions</u>. The schema is most often <strong>denormalized</strong> (faster query time).</li> <li>Data in warehouses can be stored as <strong>fact tables</strong> or <strong>dimension tables</strong>. The attributes of fact tables can be usually viewed as <strong>measure attributes</strong> (aggregated upon) or <strong>dimension attributes</strong> (small ids that are foreign keys to dimension tables).</li> <li>A fact table branching out to multiple dimension schema is a <strong>star schema</strong>. A <strong>snowflake schema</strong> has multiple levels of dimension tables (can have multiple fact tables).</li> <li>A <strong>data lake</strong> refers to repositories which allow data to be stored in multiple formats without schema integration. Basically, data is just dumped for future use.</li> <li>Data warehouses often use <strong>column-oriented storage</strong>.</li> </ul> <h1 id="chapter-12-physical-storage-systems">~Chapter 12: Physical Storage Systems</h1> <p>The performance of a database engine depends on the way data is stored underneath. The storage hierarchy typically used is as follows</p> <p><img src="/assets/img/Databases/image-20220312223422611.png" alt="image-20220312223422611"></p> <p>Tertiary storage is used for data archives in today’s world. Data is read as a cache-line from the main memory (lookahead sorta). Similarly, to account for even higher latency of the flash memory, we read one page at a time.</p> <h2 id="storage-interfaces">Storage Interfaces</h2> <p>The way we interface with the storage also has a great impact on the performance. We have the following standards</p> <ul> <li>SATA (Serial ATA) - Supports upto 6 Gbps (v3)</li> <li>SAS (Serial Attached SCSI) - Supports upto 12 Gbps (v3)</li> <li>NVMe (Non-Volatile Memory Express) works with PCIe connecters and gives upto 24 Gbps.</li> </ul> <h1 id="lecture-22">Lecture 22</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">03-03-22</code></p> </blockquote> <p>A large number of disks are connected by a high-speed network to a number of servers in a <strong>Storage Area Network (SAN)</strong>. The <strong>Network Attached Storage (NAS)</strong> provides a file system interface using a network file system protocol like FTP. SAN can be connected to multiple computers and gives a view of a local disk. It has fault tolerance and replication. A NAS pretends to be a file system unlike SAN.</p> <h2 id="magnetic-disks">Magnetic Disks</h2> <p>The surface of the platter is divided into 50k-100k circular <strong>tracks</strong>. Each track is divided into 500-1000 (on inner tracks) or 1000-2000 (on outer tracks) <strong>sectors</strong>. A <strong>cylinder</strong> is a stack of platters.</p> <p>The performance of a disk is measured via its <strong>access time</strong> which involves seek time and rotational latency, <strong>I/O operations per second (IOPS)</strong>, <strong>Mean/Annualized time to failure (MTTF)</strong> and <strong>data-transfer rate</strong>. We can tune the performance by changing parameters such as <strong>Disk block size</strong> and <strong>Sequential/Random access pattern</strong>. MTTF decreases as disk ages.</p> <p><strong><em>Note.</em></strong> Suppose the MTTF of a single disk is \(t\). How do we calculate the average failing time of a disk in a set of \(n\) disks? The probability that a disk fails in a given hour is \(1/t\). The probability that one of the disks in \(n\) fails is \(1 - (1 - 1/t)^n\). However, if \(t\) is large, it is simply \(n/t\). That is, on an average a disk fails in every \(t/n\) hours in a set of \(n\) disks.</p> <p>In a random access pattern, every request requires a <strong>seek</strong>. This method results in lower transfer rates. Current disks allow up to 50-200 IOPS.</p> <h2 id="flash-storage">Flash storage</h2> <p>A <strong>NAND flash</strong> is widely used for storage in contrast to a <strong>NOR flash</strong>. A page can only be written once, and it must be erased to allow rewriting. Flash storage does page-at-a-time read. If we try for a byte read, then the control lines take up a lot of storage, and the capacity goes down.</p> <p>A <strong>solid state disk</strong> uses standard block-oriented disk interfaces, but store data on multiple flash storage devices internally. We can use SSD using the SATA interface. An erase in flash storage happens in unit of <strong>erase block</strong>, and <strong>remapping</strong> of logical page addresses to physical page addresses avoids waiting for erase. The remapping is carried out by <strong>flash translation layer</strong>. After 100000 to 1000000 erases, the erase block becomes unreliable and cannot be use due to <strong>wear leveling</strong>.</p> <p>A SLC tolerates about \(10^6\) erases. A QLC has 4 voltage levels (2 bits can be stored in 1 physical bit). These are much less tolerant to erases (about \(10^3\) erases). <strong>Wear leveling</strong> normalises the erases in a region of the flash storage by storing cold data in the part where a lot of erases have been done.</p> <p>The performance of an SSD is measured through the data transfer rates. SSDs also support parallel reads. <strong>Hybrid disks</strong> combine small amount of flash cache with large magnetic disks.</p> <p>Recently, Intel has come up with the 3D-XPoint memory technology which is shipped as Intel Optane. It allows lower latencies than flash SSDs.</p> <h3 id="raid">RAID</h3> <p><strong>Redundant Arrays of Independent Disks</strong> is a set of disk organization techniques that manage a large number of disks <u>providing a view of a single disk.</u> The idea is that some disk out of a set of <em>N</em> disks will fails much higher than the chance that a specific single disk will fail. We expect <u>high capacity, high speed, and high reliability</u> from this system.</p> <p>In a way, we improve the reliability of the storage system using redundancy. For example, the simplest way to do this is <strong>mirroring</strong> (or shadowing) where we just duplicate all disks. <u>The **mean time to data loss** depends on the mean time to failure and the mean time to repair</u>. For example, if the MTTF is 100000 hours and the mean time to repair is 10 hours, then we get the mean time to data loss as \(500\times 10^6\) hours. How do we get this? The probability that one of the disk fails is \(2*10^{-5}\). Now, what is the probability that the other disk fails within the repair time? It is \(2* 10^{-4}\). Now, at this point we have data loss. Therefore, the mean time to data loss would be \(2.5 *10^8\) for one disk. As we have two disks, we get \(5 * 10^8\). Data loss occurs when both disks fail.</p> <p>The two main goals of parallelism in a disk system are to load balance multiple small accesses to increase throughput and parallelise large accesses to reduce the response time. We do this via bit-level stripping or <strong>block-level striping</strong>. In block level striping, with n disks, block \(i\) of a file goes to disk to disk(\(i\%n\)) + 1. Now, requests for the same file can run in parallel increasing the transfer rate.</p> <ul> <li>RAID level 0 : Block-striping; non-redundant. Used in high-performance applications where data loss is not critical.</li> <li>RAID level 1: Mirrored disks with block striping. Popular for best write performance and applications such as storing log files in a database system.</li> </ul> <p>RAID also <strong>parity blocks</strong> that stores the XOR of bits from the block of each disk. Parity block \(j\) stores XOR of bits from block \(j\) of each disk. This helps in recovery of data in case of a single disk failure (XOR the parity bit with the remaining blocks on various disks). Parity blocks are often spread across various disks for obvious reasons.</p> <ul> <li> <p>RAID level 5 - Block-interleaved Distributed Parity. This is nice but writes are slower. The cost of recovery is also high. <em>Think</em>.</p> </li> <li> <p>RAID level 6 - It has a P + Q redundancy scheme where 2 error correction blocks are stored instead of a single parity block. Two parity blocks guard against multiple(2) disk failures.</p> </li> </ul> <p>There are other RAID levels which are not used in practice.</p> <h1 id="lecture-23">Lecture 23</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">07-03-22</code></p> </blockquote> <p><strong>Software RAID</strong> vs. <strong>Hardware RAID</strong>. Copies are written sequentially to guard against corruption in case of power failure. There are couple of hardware issues</p> <ul> <li> <strong>Latent sector failures</strong> - Data successfully written earlier gets damaged which can result in data loss even if only one disk fails.</li> <li> <strong>Data scrubbing</strong> - Continuous scans for latent failures, and recover from copy/parity.</li> <li> <strong>Hot swapping</strong> - Replacement of disk while the system is running without powering it down. This reduces the time to recovery, and some hardware RAID systems support this.</li> </ul> <h1 id="chapter-13-data-storage-structures">~Chapter 13: Data Storage Structures</h1> <h2 id="file-organization">File Organization</h2> <p>The database is stored as a collection of <em>files</em>. Each file is a sequence of <em>records</em>, and each record is a sequence of fields. We will assume the following</p> <ul> <li>Fixed record size</li> <li>Each file has records of one particular type only</li> <li>Different files are used for different relations</li> <li>Records are smaller than a disk block</li> </ul> <h3 id="fixed-length-records">Fixed length records</h3> <p>We store the records contiguously, and access a record based on the index and the offset. There might be fragmentation at the end of the block. How do we handle deleted records? We shall link all the free records on a free list.</p> <h3 id="variable-length-records">Variable length records</h3> <p>Strings are typically variable sized. Each record has variable length attributes represented by fixed size (offset, length) with the actual data stored after all fixed length attributes. Null values are represented by null-value bitmap. How do we structure these records in a block?</p> <h3 id="slotted-page-structure">Slotted Page Structure</h3> <p>A slotted page header contains -</p> <ul> <li>number of record entries</li> <li>end of free space in the block</li> <li>location and size of each record</li> </ul> <p>The records are stored contiguously after the header. Disk pointers point to the header and not directly to the record.</p> <h3 id="storing-large-objects">Storing Large Objects</h3> <p>Records were assumed to be smaller than pages. Otherwise, we store the records as files. In Postgres, the large attribute is automatically broken up into smaller parts.</p> <h2 id="organisation-of-records-in-files">Organisation of Records in Files</h2> <p>Records can be stored as</p> <ul> <li> <strong>Heap</strong> - Record can be placed anywhere in the file where there is space. We maintain a hierarchical free space map of two levels usually.</li> <li> <strong>Sequential</strong> - Store records in sorted sequential order, based on the value of the search key of each record.</li> <li><strong>B+ tree file organization</strong></li> <li><strong>Hashing</strong></li> </ul> <p>Some databases also support <strong>multi-table clustering file organisation</strong> that allows records of different relations to be stored in the same file.</p> <h2 id="metadata-and-partitioning">Metadata and Partitioning</h2> <p>The <strong>data dictionary</strong> or <strong>system catalog</strong> stored <strong>metadata</strong> such as</p> <ul> <li>names of relations</li> <li>names, types and lengths of attributes of each relation</li> <li>names and definitions of views</li> <li>integrity constraints</li> </ul> <h1 id="lecture-24">Lecture 24</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">08-03-22</code></p> </blockquote> <h3 id="partitioning">Partitioning</h3> <p><strong>Table partitioning</strong> - Records in a relation can be partitioned into smalled relations that are stored separately - <strong>Horizontal partitioning</strong>. Store each attribute of a relation separately - <strong>vertical partitioning</strong>. Also known as <strong>columnar representation</strong> or <strong>column oriented storage</strong>. This is a good idea for data analytics but not for transaction processing. The benefits of this representation include</p> <ul> <li>Reduced IO if only some attributes are accessed</li> <li>Improved CPU cache performance</li> <li>Improved Compression</li> <li>Vector Processing on modern CPU architectures</li> </ul> <p>The disadvantages are</p> <ul> <li>Tuple reconstruction is difficult</li> <li>Tuple deletion and updates are difficult</li> <li>Cost of decompression</li> </ul> <p>Some databases support a hybrid model which has both row and column representation.</p> <p><strong>Note.</strong> ORC and Parquet use file formats with columnar storage inside file. These are log file formats.</p> <h2 id="storage-access">Storage Access</h2> <p>Blocks are units of both storage allocation and data transfer. At the disk layer, a page is the physical unit. <strong>Buffer</strong> - The portion of the main memory to store copies of the disk blocks.</p> <h3 id="buffer-manager">Buffer Manager</h3> <p><strong>Pinned block</strong> - A memory block that is not allowed to be written back to the disk. A <strong>pin</strong> is done before reading/writing data from a block. An <strong>unpin</strong> done when read/write is complete. Multiple concurrent pin/unpin operations are possible. There are also <strong>shared and exclusive locks</strong> on buffer.</p> <h3 id="buffer-replacement-policies">Buffer Replacement Policies</h3> <p>Most OS replace the block using the LRU strategy. However, this is not suitable in many database operations. Therefore, a database system can query plan to predict future references. There are <strong>toss-immediate</strong> and <strong>MRU</strong> strategies too.</p> <h1 id="chapter-14-indexing">~Chapter 14: Indexing</h1> <p>A <strong>search key</strong> is a set of attributes used to look up records in a file. An <strong>index file</strong> consists of records (called <strong>index entries</strong>) of the form \(search-key \mid pointer\). These files are usually much smaller than the original file. We have two basic kinds of indices</p> <ul> <li> <strong>Ordered indices</strong> - SEarch keys are stored in a sorted order</li> <li> <strong>Hash indices</strong> - search keys are distributed uniformly across “buckets” using a “hash function”.</li> </ul> <h2 id="index-evaluation-metrics">Index Evaluation Metrics</h2> <ul> <li>Access types supported by the indices. These include searching for records with a specified value or records falling in a specified range of values.</li> <li>Access time</li> <li>Insertion time</li> <li>Deletion time</li> <li>Space overhead</li> </ul> <h2 id="ordered-indices">Ordered Indices</h2> <p>A <strong>clustering index</strong> in a sequentially ordered file, is the index whose search key specifies the sequential order of the file. It is also called as the <strong>primary index</strong> that is not to be confused with primary key. A <strong>secondary index/nonclustering</strong> is an index whose search key specifies an order different from the sequential order of the file.</p> <p>An index sequential file is a sequential file ordered on a search key, with a clustering index on the search key.</p> <h3 id="dense-index-files">Dense index Files</h3> <p>A <strong>dense index</strong> is an index for which there is a record in the index-file for every search-key value in the file. Also, every index-record made with a dense index need not be mapped to an index as we use the following structure.</p> <p><img src="/assets/img/Databases/image-20220407153333197.png" alt="image-20220407153333197"></p> <p>A <strong>sparse index</strong> on the other hand contains index records for only some search-key values. To locate a record with a search-key value \(K\), we first find an index record with largest search-key value \(&lt; K\). Then, we search the file sequentially starting at the record to which the index record points. For unclustered index, we create a sparse index on top of a dense index (multilevel index).</p> <h1 id="lecture-25">Lecture 25</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">10-03-22</code></p> </blockquote> <p>Sparse indices take less space and have less maintenance overhead in comparison to dense indices. However, they are generally slower than dense indices.</p> <p><strong>Note.</strong> Secondary indices have to be dense.</p> <p>We use <strong>lexicographic ordering</strong> for composite search keys.</p> <h2 id="b-tree">B\(^+\)-Tree</h2> <p>We will ignore duplicate keys for now. The number of children for every node lies within a certain specified range for that tree. In a \(B^+\)-Tree we have \(n\) pointers and \(n-1\) values separating them. A pointer between values \(a\) and \(b\) will point to values \(c\) that satisfy \(a \leq c &lt; b\). It is not necessary for the internal nodes to be full.</p> <p>Formally, a \(B^+\)-tree is a rotted tree satisfying the following properties</p> <ul> <li>All paths from the root to a leaf are of the same length.</li> <li>Each node that is not a root or a leaf has between \(\lceil{n/2}\rceil\) and \(n\) children.</li> <li>A lead node has between \(\lceil (n - 1)/2 \rceil\) and \(n - 1\) values.</li> <li>If a root is not a leaf, it has at least 2 children, and if a root is a lead, it can have between \(0\) and \(n - 1\) values.</li> </ul> <p>A typical node looks like \(P_! \mid K_1 \mid \dots \mid K_{n - 1} \mid P_n\). Here \(K_i\) are the search-key values and \(P_i\) are pointers to children or records (buckets of records). Also, \(K_1 &lt; \dots &lt; K_{n - 1}\).</p> <h3 id="leaf-nodes">Leaf nodes</h3> <p>For \(i = 1, \dots, n - 1\), pointer \(P_i\) points to a file record with search-key value \(K_i\).</p> <p>Pointers help us keep the nodes logically close but they need not be physically close. The non-lead levels of the \(B^+\) tree form a hierarchy of sparse indices. The level below root has at least \(2* \lceil n/2 \rceil\) values, the next level has \(2* \lceil n/2 \rceil* \lceil n/2 \rceil\), and so on. So if there are \(K\) search key values in the file, the tree height is no more than \(\lceil \log_{\lceil n/1 \rceil} K\rceil\).</p> <h3 id="queries-on-b-trees">Queries on \(B^+\)-trees</h3> <p><strong>Range queries</strong> finds all records with search key values in a given range. These are implemented as iterators.</p> <p>To handle non-unique keys, create a composite key that indexes into the duplicate values. Search for an index can be implemented as a range query. If the index is clustering, then all accesses are sequential. However, if the index if non-clustering, each record access may need an I/O operation.</p> <h3 id="insertion-on-b-trees">Insertion on \(B^+\)-trees</h3> <p>Insertion is easy when the nodes are not full. However, when nodes are full, we would have to split the nodes. We split a node through the parent, by adding a splitting value in the parent node. We do this recursively, and if the root gets full, we create a new root. We insert from leaves because the leaves hold the pointers to records.</p> <p><img src="/assets/img/Databases/image-20220407164819276.png" alt="image-20220407164819276"></p> <p>The above image gives the formal algorithm.</p> <h3 id="deletion-on-b-trees">Deletion on \(B^+\)-trees</h3> <p>We need to ensure that there are at least a minimum number of values in each node. The complexity of the updates is of the order \(\mathcal O( \log_{\lceil n/2 \rceil}K)\). The height of the tree decreases when a node has very few children. Note that a deleted value can still appear as a separator in the tree after the deletion. Also, the average node occupancy depends on the insertion order (2/3rds with random and 1/2 with insertion in sorted order).</p> <h1 id="lecture-26">Lecture 26</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">14-03-22</code></p> </blockquote> <p>If we allow non-unique keys, we can store a key with multiple pointers. However, the complexity comes in terms of deletion. Worst case complexity may be linear.</p> <h3 id="b-tree-file-organisation">\(B^+\)-Tree file Organisation</h3> <p>Leaf nodes in a \(B^+\) tree file organisation store records, instead of pointers. As records are larger than pointers, the maximum number of records that can be stored in a lead node is less than the number of pointers in a non-leaf node. To improve space utilisation, we can involve more sibling nodes in redistribution during splits and merges. Involving 2 siblings in redistribution results in each node having at least \(\lfloor 2n/3 \rfloor\) entries.</p> <p>Record relocation and secondary indices - If a record moves, all secondary indices that store record pointers have to be updated. Therefore, node splits in \(B^+\) tree file organisation become very expensive. The solution to this is use a search key of the \(B^+\) tree file organisation instead o record pointer in a secondary index. For example, consider students database sorted using roll numbers with names as a secondary index. If the records move, we would need to update all the “name” index pointers. So what we do is, make the “name” index pointers point to the “roll number” index pointers instead of the records directly. Since “roll number” is a clustered index, no relocation of secondary indices is required.</p> <h3 id="indexing-strings">Indexing Strings</h3> <p>How do we use variable length strings as keys? We use <strong>prefix compression</strong> along with variable fanout (?). In the internal nodes, we can use simplified separators.</p> <h2 id="bulk-loading-and-bottom-up-build">Bulk Loading and Bottom-Up Build</h2> <p>Inserting entries one-at-a-time into a \(B^+\)-tree requires \(\geq\) 1 I/O per entry assuming leaves don’t fit in the memory.</p> <ul> <li>Sort entries first, and insert in a sorted order. This will have much improved I/O performance.</li> <li>Build a \(B^+\)tree <strong>bottom-up</strong>. AS before sort the entries, and then create tree layer-by-layer starting with the leaf level.</li> </ul> <p>However, the above two methods expect a bulk insertion. What do we do if we have a sudden burst of inserts? We will look at alternatives later.</p> <h3 id="b-tree-index-files">B-Tree Index Files</h3> <p>Similar to \(B^+\)-tree but B-tree allows search-key values to appear only once and eliminates redundant storage of search keys. The pointers to the records are stored in the internal nodes too! The problem with this approach is that the tree becomes taller. There is minimal advantage too.</p> <p>Indexing on flash has a few issues, as writes are no in-place and it eventually requires a more expensive erase.</p> <p>A key idea is to use large node size to optimise disk access, but structure data within a node using a tree with small node size, instead of using an array for faster cache access (so that all nodes fit inside a single cache line).</p> <h2 id="hashing">Hashing</h2> <h3 id="handling-bucket-overflows">Handling bucket overflows</h3> <p><strong>Overflow chaining</strong> - The overflow buckets of a given bucket are chained together in a linked list. The above scheme is called <strong>closed addressing</strong> or <strong>open/closed hashing</strong>.</p> <p>Overflow can happen due to insufficient buckets or skewness in the data.</p> <p>Hashing is not used widely on disks but is used in-memory.</p> <p><strong>Covering indices</strong> - Attributes that are added to index to prevent the control from fetching the entire record.</p> <p>Some databases allow creation of indices on foreign keys.</p> <p>Indices over tuples can be problematic for a few queries due to lexicographic ordering.</p> <h1 id="lecture-27">Lecture 27</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">15-03-22</code></p> </blockquote> <h2 id="write-optimised-indices">Write Optimised Indices</h2> <p>Performance of \(B^+\) trees can be poor for write intensive workloads. This is because we require one I/O per leaf, assuming all internal nodes are in memory. There are two approaches to reducing cost of writes</p> <ul> <li>Log-structured merge tree</li> <li>Buffer tree</li> </ul> <h3 id="log-structured-merge-lsm-tree">Log Structured Merge (LSM) Tree</h3> <p>Consider only insert queries for now. Records are first inserted into in-memory L0 tree. When the in-memory tree is full, we move the records to the disk in the L1 tree. \(B^+\)-tree is constructed using bottom-up by merging the existing L1 tree with records from L0 tree. The goal is to minimise random I/O.</p> <p>The benefits are that inserts are done using sequential I/O operations and the leaves are full avoiding space wastage. The drawbacks are that queries have to search multiple trees and the entire context of each level is copied multiple times. <strong>Bloom filters</strong> avoid lookups in most trees. The idea is to use hash functions and bitmaps.</p> <p>How about deletes? They will now incur a lot of I/O. We do a logical delete by inserting a new delete entry. Updates are handled as inserts followed by deletes.</p> <p>LSM trees were introduced for disk-based indices. These are useful to minimise erases with flash-based indices.</p> <h3 id="buffer-tree">Buffer Tree</h3> <p>Each internal node of \(B^+\)-tree has a buffer to store inserts. The inserts are moved to lower levels when the buffer is full. With a large buffer, many records are moved to lower level at each time. Therefore, per record I/O decreases.</p> <p>The benefits are less overhead on queries, and it can be used with any tree index structure. However, they have more random I/O than LSM trees.</p> <h2 id="spatial-and-temporal-indices">Spatial and Temporal Indices</h2> <p>A <strong>k-d tree</strong> is a structure used for indexing multiple dimensions. Each level of k-d tree partitions the space into two, and we cycle through the dimensions at each level. Range queries do not have \(\log\) complexity bounds in this index structure.</p> <p>Queries can mix spatial (contains, overlaps) and non-spatial conditions.</p> <p>The <strong>k-d-B</strong> tree extends the k-d tree to allow multiple child nodes for each internal node. This is well suited for secondary storage.</p> <p>Each node in a <strong>quadtree</strong> is associated with a rectangular region of space. Similarly, we can cut across more dimensions in each level.</p> <h3 id="r-tree">R-tree</h3> <p>The motivation behind this structure was to store objects in the spatial domain in a single leaf. We try to create minimally overlapping bounding boxes for all the objects and create a structure similar to a \(B^+\)-tree. Multiple paths may need to be searched, but the performance is good in practice.</p> <p>Suppose we want to insert a new object that overlaps with many bounding boxes. We choose the box which overlaps the least, or the box which has the lowest change in size on the addition. Now, insertion is done via the <strong>clustering algorithm</strong>. The clustering is also done via some heuristics such as minimum overlap of bounding boxes. Greedy heuristics are often used.</p> <h1 id="lecture-28">Lecture 28</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">17-03-22</code></p> </blockquote> <h3 id="indexing-temporal-data">Indexing Temporal Data</h3> <p>A time interval has a start and an end time. A query may ask for all tuples that are valid at a point in time or during a time interval. We can use a spatial index called an <strong>R-tree</strong> for indexing.</p> <h1 id="chapter-15-query-processing">~Chapter 15: Query Processing</h1> <p>Database engines often apply optimisations based on statistics over data which are approximate. An annotated expression specifying a detailed execution strategy is called an <strong>evaluation plan</strong>.</p> <p><strong>Query optimisation</strong> chooses an evaluation plan with the lowest cost (a metric based on approximated statistics).</p> <h2 id="measures-of-query-cost">Measures of Query Cost</h2> <p>Many factors contribute to time cost such as disk access, CPU, and network communication. Cost can be measured on <strong>response time</strong> or <strong>total resource consumption</strong>. As estimating the time is more difficult, we often resort to resource consumption for optimisation. This metric is also useful is shared databases. For our purposes, we will just consider costs related to I/O time.</p> <p>Now, the disk cost is estimated as the sum of average seeks, blocks read, and blocks written. For simplicity we just use the <strong>number of block transfers from disk</strong> and the <strong>number of seeks</strong>. Then, we get \(b \times t_T + S\times t_S\). On a high end magnetic disk, \(t_S = 4ms, t_T = 0.1ms\) and on a SSD, \(t_S = 20-90\mu s, t_T = 2-10 \mu s\) for 4KB blocks.</p> <p>We assume no data is available in the buffer.</p> <h2 id="selection-operation">Selection Operation</h2> <p><code class="language-plaintext highlighter-rouge">A1</code>- <strong>Linear Search</strong> - Assume that file is stored sequentially.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = b/2*t_T + 1*t_S
</code></pre></div></div> <p>We do not consider binary search as it requires a lot more (random) accesses and access time is high in disks.</p> <h3 id="selection-using-indices">Selection using Indices</h3> <p><code class="language-plaintext highlighter-rouge">A2</code> - <strong>Clustering index, Equality on key</strong> - Retrieve a single record that satisfied the corresponding equality condition.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = (h_i + 1)*(t_T + t_S)
</code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">h_i</code> is the height of the index (in \(B^+\)-tree?), and since we are doing random I/O for each case, we need to add both seek and transfer time.</p> <p><code class="language-plaintext highlighter-rouge">A3</code>-<strong>Clustering index, equality on non-key</strong> - Retrieve multiple records. Records will be on consecutive blocks. Let \(b\) be the number of blocks containing matching records.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = h_i*(t_T + t_S) + t_S + t_T*b
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">A4</code>-<strong>Secondary index, equality on key/non-key</strong></p> <p>If the search-key is a candidate key, then</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = (h_i + 1)*(t_T + t_S)
</code></pre></div></div> <blockquote> <p>Why?</p> </blockquote> <p>Otherwise, each of <code class="language-plaintext highlighter-rouge">n</code> matching records may b on a different block</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = (h_i + n)*(t_T + t_S)
</code></pre></div></div> <p>It might be cheaper to scan the whole relation as sequential access is easier than random I/O.</p> <p><code class="language-plaintext highlighter-rouge">A5</code>-<strong>Clustering index, comparison</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = linear_cost for &lt;
		 = index_equality_cost + linear_cost
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">A6</code>-<strong>Non-clustering index, comparison</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = cost + cost_records (I/O)
</code></pre></div></div> <p>The difference between clustering and non-clustering indices is that we would have to fetch records in case of non-clustering indices in order to read the non-clustering index attribute. This is not the case in case of a clustering index.</p> <p>Let me write my understanding of all this</p> <blockquote> <p>Indices are just files with pointers. Basically, scanning through indices is faster than scanning through a sequence of entire records, so we use indices. Instead of storing indices sequentially, we use \(B^+\) trees so that its faster. We can’t do this with records directly because records are big and they may not reside in a single file (I think).</p> <p>Now, clustering indices are indices whose order is same as the order of the records. So, once we fetch a record for an index, all records corresponding to the next indices will be in a sequence. Therefore, we won’t have additional seek time.</p> <p>However, this is not the case for non-clustering indices. If we want records corresponding to the next indices of the current index, we’d have additional seek time as the records may lie in different blocks.</p> </blockquote> <h3 id="implementation-of-complex-selections">Implementation of Complex Selections</h3> <p>How do we implement conjunctions? If all the attributes in the conjunction are indexed, then it is straightforward. We will just take the intersection of all results. Otherwise, test all the other conditions after fetching the records into the memory buffer.</p> <p>Also, as we discussed before, we can use a composite index.</p> <p>Disjunctions are a slightly different. If we have all indexed attributes, we just take the union. Otherwise, we just have to do a linear scan. Linear scan is also the best way in most cases for negation.</p> <h2 id="bitmap-index-scan">Bitmap Index Scan</h2> <p>We have seen that index scans are useful when less number of records match in the case of secondary indices. If more records match, we should prefer a linear scan. How do we decide the method beforehand? The <strong>bitmap index scan</strong> algorithm is used in PostgreSQL.</p> <p>We create a bitmap in memory with a bit for each page in the relation. A record ID is just the page ID and the entry number. We initially do an index scan to find the relevant pages, and mark these pages as 1 in the bitmap. After doing this, we just do a linear scan fetching only pages with bit set to 1.</p> <p>How is the performance better? It is same as index scan when only a few bits are set, and it is same as a linear scan when most bits are set. Random I/O is avoided in both cases.</p> <h2 id="sorting">Sorting</h2> <p>For relations that fit in memory, we can use quicksort. Otherwise, we use <strong>external sort merge</strong>.</p> <p><img src="/assets/img/Databases/image-20220407235807034.png" alt="image-20220407235807034"></p> <p><img src="/assets/img/Databases/image-20220407235757873.png" alt="image-20220407235757873"></p> <h1 id="lecture-29">Lecture 29</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">21-03-22</code></p> </blockquote> <h2 id="join-operation">Join Operation</h2> <h3 id="nested-loop-join">Nested Loop Join</h3> <p>Requires no indices and can be used with any kind of join condition. It is very expensive though, as it is quadratic in nature. Most joins can be done in linear time in one of the relations, as most joins are foreign key joins. In the worst case, there would memory enough to hold only one block of each relation. The estimated cost then is, <code class="language-plaintext highlighter-rouge">n_r*b_s + b_r</code> block transfers and <code class="language-plaintext highlighter-rouge">n_r + b_r</code> seeks.</p> <h3 id="block-nested-loop-join">Block Nested-Loop Join</h3> <p>We first do block matching and then tuple matching. Asymptotically, this looks same as the above method but it decreases the I/O cost as the number of seeks come down.</p> <h3 id="indexed-nested-loop-join">Indexed Nested-Loop Join</h3> <p>An index is useful in equi-join or natural join. For each tuple \(t_r\) in the outer relation \(r\), we use the index to look up tuples in \(s\) satisfy the join condition with tuple \(t_r\). In the worst case, the buffer has space for only one page of \(r\), and for each tuple in \(r\), we perform an index lookup on \(s\). Therefore, the cost of the join is <code class="language-plaintext highlighter-rouge">b_r(t_T + t_S) + n_r*C</code>, where \(c\) is the cost of traversing index and fetching all matching \(s\) tuples for one tuple of \(r\). The second term is the dominating term. We should use the fewer tuples relations as the outer relation.</p> <h3 id="merge-join">Merge Join</h3> <p>Sort both relations on their join attribute, and merge the sorted relations. Cost is <code class="language-plaintext highlighter-rouge">(b_r + b_s)*t_T + (ceil(b_r/b_b) + ceil(b_S/b_b))*t_S</code> along with the cost of sorting.</p> <p><strong>Hybrid merge-join</strong> - If one relation is sorted, an the other has a secondary \(B^+\)-tree on the join attribute, then we can merge the sorted relation with the leaf entries of the \(B^+\)-tree. Then we sort the result on the addresses of the unsorted relation’s tuples. Finally, we scan the unsorted relation in physical address order and merge with the previous result, to replace addresses by actual tuples.</p> <h3 id="hash-join">Hash Join</h3> <p>The goal in the previous methods was to simplify the relations so that they fit in the memory. Along with this, we can also parallelise our tasks.</p> <p>In this method, we hash on the join attributes and then merge each of the partitions. It is applicable for equi-joins and natural joins.</p> <p><img src="/assets/img/Databases/image-20220408003107650.png" alt="image-20220408003107650"></p> <p>The value \(n\) and the hash function \(h\) are chosen such that each \(s_i\) fits in the memory. Typically, \(n\) is chosen as \(\lceil b_s/M \rceil *f\) where \(f\) is a <strong>fudge factor</strong>. The probe relation need not fit in memory. We use <strong>recursive partitioning</strong> if number of partitions is greater than number of pages in the memory.</p> <p><strong>Overflow resolution</strong> can be done in the build phase. Partition \(s_i\) is further partitioned using a different hash function. <strong>Overflow avoidance</strong> performs partitioning carefully to avoid overflows during the build phase. Both methods fail with a high number of duplicates.</p> <p>Cost of hash join is <code class="language-plaintext highlighter-rouge">(3(b_r + b_s) + 4n_h)*t_T + 2t_T(ceil(b_r/b_b) + ceil(b_s/b_b))</code>. Recursive partitioning adds a factor of \(\log_{\lfloor M/bb\rfloor - 1}(b_s/M)\). If the entire build can be kept in the memory, then no partitioning is required and cost estimate goes down to \(b_r + b_s\).</p> <p>Can we not build an entire index on \(s\) instead of hash join? Building an on-disk index is very expensive on disk. Indices have to be maintained which is an overhead.</p> <p><strong>Hybrid Hash-join</strong> keeps the first partition of the build relation in memory. This method is most useful when \(M \gg \sqrt b_s\).</p> <h3 id="complex-joins">Complex Joins</h3> <p>Similar methods to that of selection can be used here. That is conjunction of \(n\) conditions requires intersections of the result of \(n\) joins. In disjunction, we take the union of the join results. This method works for sets but not for multi-sets! For multi-sets, we can make sets out of the records.</p> <h3 id="joins-on-spatial-data">Joins on Spatial Data</h3> <p>There is no simple sort order for spatial joins. Indexed nested loops join with spatial indices such as R-trees, quad-trees and k-d-B-trees. Nearest neighbour joins can be done with tiling.</p> <h2 id="other-operations">Other operations</h2> <p><strong>Duplicate elimination</strong> can be implemented via hashing or sorting. An optimisation is to delete duplicates during run generation as well as at intermediate merge steps. <strong>Projection</strong> can be done by performing projection on each tuple. <strong>Aggregation</strong> can implemented similar to duplicate elimination. Sorting and hashing can be used to bring tuples in the same group together, and then the aggregate functions can be applied on each group.</p> <h1 id="lecture-30">Lecture 30</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">22-03-22</code></p> </blockquote> <h2 id="set-operations-1">Set Operations</h2> <p>These are fairly straightforward using merge-join after sorting or hash-join.</p> <h2 id="outer-join-1">Outer Join</h2> <p>During merging, for every tuple \(t_r\) from \(r\) that do not match any tuple in \(s\), output \(t_r\) padded with nulls.</p> <h2 id="evaluation-of-expressions">Evaluation of Expressions</h2> <p>We have two method to evaluate an entire expression tree</p> <ul> <li> <strong>Materialisation</strong> - Generate results of an expression whose inputs are relations or are already computed, materialize (store) it on disk</li> <li> <strong>Pipelining</strong> - Pass on tuples to parent operations even as an operation is being executed</li> </ul> <h3 id="materialisation">Materialisation</h3> <p>We evaluate one operation at a time, and store each temporary result on the disk. this method is always applicable, but the cost is high. The overall cost is the sum of costs of individual operations and the cost of writing intermediate results to the disk.</p> <p><strong>Double buffering</strong> - Use two output buffers for each operation, when one is full write it to disk while the other is getting filled.</p> <h3 id="pipelining">Pipelining</h3> <p>We evaluate several operations simultaneously, passing the results of one operation on to the next. However, this is not always possible in case of aggregation, sorts and hash-joins. It is executed in two ways -</p> <ul> <li> <strong>Demand driven</strong> - In lazy evaluation, the system repeatedly requests next tuple from the top level operation. The operation has to maintain states. Pull model.</li> <li> <strong>Producer driven</strong> - In eager pipelining the operators produce tuples eagerly and pass them up to their parents. Push model.</li> </ul> <h2 id="blocking-operations">Blocking Operations</h2> <p>They cannot generate any output until all the input is consumed. For example, sorting, aggregation, etc. They often have two sub-operations, and we can treat them as separate operations. All operations in a <strong>pipeline</strong> stage run concurrently.</p> <h2 id="query-processing-in-memory">Query Processing in Memory</h2> <p>In early days, memory was the bottleneck. So, engineers had to reduce the I/O. Query was compiled to machine code, and compilation usually avoids many overheads of interpretations to speed up query processing. This was often done via generation of Java byte code with JIT compilation. Column oriented storage was preferred as it allowed vector operations, and cache conscious algorithms were used.</p> <h1 id="lecture-31">Lecture 31</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">24-03-22</code></p> </blockquote> <h3 id="cache-conscious-algorithms">Cache conscious algorithms</h3> <p>The goal is to minimise the cache misses.</p> <ul> <li> <strong>Sorting</strong> - We can use runs that are as large as L3 cache to avoid cache misses during sorting of a run. Then merge runs as usual in merge sort.</li> <li> <strong>Hash-join</strong> - We first create partitions such that build + probe partitions fit in memory. Then, we sub partition further such that sub partition and index fit in L3 cache. This speeds up probe phase.</li> <li>Lay out attributes of tuples to maximise cache usage. Store often accessed attributes adjacent to each other.</li> <li>Use multiple threads for parallel query processing. Cache miss leads to stall of one thread, but others can proceed.</li> </ul> <h1 id="chapter-16-query-optimisation">~Chapter 16: Query Optimisation</h1> <p>As we have seen before, there are multiple ways to evaluate a given query. The cost difference can be magnanimous in some cases. A plan is evaluated on cost formulae, statistical information and statistical estimation of intermediate results. Most databases support <code class="language-plaintext highlighter-rouge">explain &lt;query&gt;</code> that gives the details of the evaluation plan.</p> <h2 id="generating-equivalent-expressions">Generating Equivalent Expressions</h2> <p>Two queries are equivalent in the (multi)set version if both of them generate the same (multi)set of tuples on <strong>every legal database instance</strong>. Note that we ignore the order of tuples in relational algebra.</p> <ul> <li> <p>Conjunctive selection operations can be deconstructed into a sequence of individual selections</p> \[\sigma_{\theta_1 \land \theta_2}(E) \equiv \sigma_{\theta_1} (\sigma_{\theta_2}(E))\] </li> <li> <p>Selection operations are commutative</p> \[\sigma_{\theta_1} (\sigma_{\theta_2}(E)) \equiv \sigma_{\theta_2} (\sigma_{\theta_1}(E))\] </li> <li> <p>Only the last in a sequence of project operations is needed, the others can be omitted.</p> \[\Pi_{L_1}(\dots(\Pi_{L_n}(E))\dots) \equiv \Pi_{L_1}(E)\] </li> <li> <p>Selections can be combined with Cartesian products and theta joins.</p> \[\begin{align} \sigma_{\theta}(E_1 \times E_2) &amp;\equiv E_1 \bowtie_\theta E_2 \\ \sigma_{\theta_1}(E_1 \bowtie_{\theta_2} E_2) &amp; \equiv E_1 \bowtie_{\theta_1 \land \theta_2} E_2 \end{align}\] </li> <li> <p><strong>Theta-join and natural joins</strong> operations are <u>commutative</u> as well as <u>associative</u>. However, order will not be the same in SQL.</p> <p>sc \((E_1 \bowtie_{\theta_1} E_2)\bowtie_{\theta_2 \land \theta_3} E_3 \equiv(E_1 \bowtie_{\theta_1 \land \theta_3} E_2)\bowtie_{\theta_2 } E_3\)</p> <p>where \(\theta_2\) contains attributes only from \(E_2\) and \(E_3\).</p> </li> <li> \[\sigma_{\theta_1 \land \theta_2} (E_1 \bowtie_\theta E_2) \equiv (\sigma_{\theta_1}(E_1)) \bowtie_\theta (\sigma_{\theta_2}(E_2))\] </li> <li> <p>Projection distributes over join. Throw out useless attributes before joining.</p> <p><img src="/assets/img/Databases/image-20220416151913983.png" alt="image-20220416151913983"></p> </li> <li> <p>We also have the usual set operations equivalences. Selection operation distributes over \(\cup, \cap, -\).</p> </li> <li> <p>We can also come up with rules involving left outer join (⟖), aggregations and group by’s.</p> \[\sigma_\theta(E_1 ⟕ E_2) \equiv (\sigma_\theta(E_1) ⟕ E_2)\] <p>where \(\theta\) does not involve attributes from \(E_2\) that are not in \(E_1\). If it involves only the attributes from \(E_2\) and is <u>null rejecting</u>, we can convert the left outer join to inner join.</p> </li> <li> </li> <li> \[_A\gamma_{count(A)}(s_1 \bowtie_{s_1.A = s_2.A}s_2) \equiv \Pi_{A, c_1 \times c_2}(_A\gamma_{count(A)}(s_1) \bowtie_{s_1.A = s_2.A} {_A}\gamma_{count(A)}(s_2))\] </li> <li> \[\sigma_\theta({_A}\gamma_{agg(B)}(E)) \equiv {_A}\gamma_{agg(B)}(\sigma_\theta(E))\] <p>where \(\theta\) uses only attributes from the grouping attributes.</p> </li> </ul> <p>There were 300 rules in SQL server in 2008!</p> <h1 id="lecture-32">Lecture 32</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">28-03-22</code></p> </blockquote> <p>Note that left/right outer join is not commutative! An optimiser has to consider the cost not just the size. Sometimes, more tuples might be faster due to indices. Associativity is some times helpful in join when the join result of, say, \(r2, r3\) is much larger than that of \(r1, r2\). In that case, we compute the smaller join first. One must also beware about the overhead of applying all these transformations.</p> <p>There are other optimisations such as detecting duplicate sub-expressions and replacing them by one copy. Dynamic Programming is also put to use. The algorithms for transformation of evaluation plans must also be taken into account. Practical query optimisers either enumerate all plans and choose the best plan using cost, or they use heuristics to choose a plan.</p> <h3 id="cost-based-optimisation">Cost based optimisation</h3> <p>If we have \(r_1 \bowtie \dots \bowtie r_n\), we have \((2(n - 1))!/(n - 1)!\). We use dynamic programming to store the least-cost join order. Using dynamic programming, we are bringing down factorial order to an exponential order \(3^n\). The cost of each join is evaluated by interchanging selection and join operations based on indices. Further optimisation is done by only considering <strong>left-deep join trees</strong> where the rhs of a join is a relation and not an intermediate join. After this, the time complexity is \(\mathcal O(n2^n)\) and space complexity is \(\mathcal O(2^n)\).</p> <p>How about sort orders? Certain sort orders can make subsequent operations cheaper. However, we don’t consider this much. The Volcano project also considers physical equivalence rules.</p> <h3 id="heuristic-optimisation">Heuristic Optimisation</h3> <p>Heuristic optimisation transforms the query-tree by using a set of rules that typically improve execution performance. Nested subqueries hinder optimisation techniques.</p> <p>System-R used heuristics for aggregates. We also need to check <u>optimisation cost budget</u> and <u>plan caching</u>. As some applications use the same query repeatedly, we can try and use the same evaluation plan based on a heuristic on statistics.</p> <h2 id="statistics-for-cost-estimation">Statistics for Cost Estimation</h2> <p>We consider \(n_r\) (no. of tuples), \(b_r\) (no. of blocks), \(I_r\) (size of a tuple), \(f_r\) (blocking factor \(b_r = \lceil n_r/f_r\rceil\)) and \(V(A, r)\) (no. of distinct values). Histograms are used to compute statistics.</p> <h1 id="lecture-33">Lecture 33</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">29-03-22</code></p> </blockquote> <h3 id="selection-size-estimation">Selection size estimation</h3> <ul> <li> \[\sigma_{A = v}(r) \approx n_r/V(A, r)\] </li> <li>Assuming, \(\min\) and \(\max\) are available - \(\sigma_{A \leq v}(r) = \begin{cases} 0 &amp;&amp; v &lt; \min(A, r) \\ n_r \cdot \frac{v - \min(A, r)}{\max(A, r) - \min{A, r}} \end{cases}\)</li> </ul> <p>These estimates are refined using updates in the histograms. Similarly, we can derive size estimates for complex selections.</p> <h3 id="join-size-estimation">Join size estimation</h3> <ul> <li>If \(R \cap S = \phi\), then \(r \bowtie s = r \times s\).</li> <li>If \(R \cap S\) is a key in \(R\), then a tuple of \(s\) will join with at most one tuples from \(r\) -&gt; \(r \bowtie s \leq s\).</li> <li>If \(R \cap S\) in \(S\) is a foreign key in \(S\) referencing \(R\), then \(r \bowtie s = s\).</li> <li>If the common attribute is not a key, then the size is \((n_r*n_s)/V(A, s)\) if every tuple in \(R\) produces a tuple in the join.</li> </ul> <p>Similarly, we have other size estimations.</p> <p>For projection, we have \(\Pi_A(r) = V(A, r)\), and for aggregation we have \({_G}\gamma_A(r) = V(G, r)\). There are estimates for set operations too!</p> <p>In summary, these estimates work well in practice, but the errors are multiplied across multiple queries. In worst cases, they might hamper the performance.</p> <h2 id="additional-optimisations">Additional Optimisations</h2> <h3 id="optimising-nested-subqueries">Optimising Nested Subqueries</h3> <p>SQL treats the nested subquery as a function with a few parameters - This evaluation is known as <strong>correlated evaluation</strong>. The parameters to the function are known as <strong>correlation variables</strong>. This method is inefficient because a large number of call may be made for the nested query that results in unnecessary random I/O.</p> <p>However, every nested subquery in SQL can be written in terms of joins. SQL optimisers try to do this. One must be beware of duplicates during this conversion. The (left)<strong>semijoin</strong> operator ⋉ is defined as - A tuple \(r_i\) appears \(n\) times in \(r ⋉_\theta s\) if it appears \(n\) times in \(r\), and there is atleast on matching tuple \(s_i\) in \(s\). This operator is often used by optimisers to maintain the duplicate count. Similarly, for <code class="language-plaintext highlighter-rouge">not exists</code>, we have <strong>anti semijoin</strong> \(\bar ⋉\).</p> <p><strong>Decorrelation</strong> is the process of replacing a nested query by a query with a join/semi-join. This process is a bit non-trivial in case of scalar subqueries. Note that relational algebra can’t deal with exceptions.</p> <h3 id="materialised-views">Materialised views</h3> <p>The values of the view are computed and stored. The re-computation during updates is expensive. Therefore, we adopt <u>incremental view maintenance</u>. The changes to a relation or expressions are referred to as its <strong>differential</strong>.</p> <p>To explain the above, consider a materialised view of a join. For a new insert, we find the corresponding matching tuples for join and add them. Similarly for deletes. We can do this due to distributivity of \(\bowtie\) and \(\cup\).</p> <p>Project is a more difficult operation due to duplicates. Therefore, we maintain a count for how many times the set of attributes occur. Aggregates can also be done in a similar way.</p> <p>To handle expressions, the optimiser might have to change the evaluation plan. For example, the tree structure in join order may not be efficient if indices are present during insertions.</p> <h1 id="lecture-34">Lecture 34</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">28-03-22</code></p> </blockquote> <p>We had discussed about view maintenance, but how do we use materialised views? A query optimiser can replace sub-expressions with appropriate views if the user writes the queries only in terms of relations. Sometimes, the opposite can also be useful.</p> <p>Materialised view selection and index selection are done based on typical system <strong>workload</strong>. Commercial database systems provide <u>tuning</u> tools that automate this process.</p> <h3 id="top-k-queries">Top-K queries</h3> <p>The query optimiser should consider that only the top-k results are required from a huge relation. This can be done via indexed nested loops join with the relation that is being used for sorting as the outer relation. There are other alternatives too.</p> <h3 id="multi-query-optimisation">Multi-query Optimisation</h3> <p>Multiple queries with common sub-routines can be done in parallel.</p> <h3 id="parametric-query-optimisation">Parametric Query Optimisation</h3> <p>The evaluation plan can change based on the input parameters to the queries. To optimise this, we divide the range of the parameter into different partitions and choose a good evaluation plan for each partition.</p> <h1 id="chapter-17-transactions">~Chapter 17: Transactions</h1> <p>A <strong>transaction</strong> is a <em>unit</em> of program execution. It access and possible updates various data items. It also guarantees some well-defined robustness properties. To discuss transactions, we move to a level below queries where each <em>atomic</em> instruction is performed. We need to ensure correctness during failures and concurrency.</p> <p>In OS, we have seen that mutexes are used for concurrency. However, we need a higher level of concurrency in databases.</p> <p><strong>ACID guarantees</strong> refer to Atomicity (Failures), Consistency (Correctness), Isolation (Concurrency) and Durability (Failures). There is a notion of <u>consistent state</u> and <u>consistent transaction</u>. Durability refers to persistence in case of failures. Atomicity refers to all-or-nothing for each update. Partial updates are reversed using logs. Two concurrent transactions must execute as if they are unaware of the other in isolation. In conclusion, ACID transactions are a general systems abstraction.</p> <p>Concurrency increases processor and disk utilisation. It also reduces the average response time. Isolated refers to concurrently executing actions but showing as if they were occurring serially/sequentially.</p> <h2 id="serialisability">Serialisability</h2> <p>A schedule is <strong>serialisable</strong> if it is equivalent to a serial schedule. We are assuming that transactions that are run in isolation are atomic, durable and preserve consistency.</p> <p><strong>Conflict serialisable</strong> schedules are a subset of serialisable schedules that detect or prevent conflict and avoid any ill effects. To understand these, we will consider <code class="language-plaintext highlighter-rouge">read</code>s and <code class="language-plaintext highlighter-rouge">write</code>s.</p> <p><strong>Conflicting instructions</strong> - Instructions form two transactions <strong>conflict</strong> only if one or both <em>update</em> the <em>same shared</em> item. For example, <code class="language-plaintext highlighter-rouge">write A</code> in T1 conflicts with <code class="language-plaintext highlighter-rouge">read A</code> in T2. These are similar to RAW, WAW, etc conflicts seen in Architecture course.</p> <p>We can swap the schedules of non-conflicting schedules and obtain a serial schedule. Such schedules are conflict serialisable. Conflict equivalence refers to the equivalence between the intermediate schedules obtained while swapping. More formally,</p> <p><img src="/assets/img/Databases/image-20220416193446900.png" alt="image-20220416193446900"></p> <p>We are skipping <strong>view equivalence</strong>. There are other notions of serialisability (considering a group of operations).</p> <h3 id="testing-for-conflict-serialisability">Testing for Conflict Serialisability</h3> <p>A <strong>precedence graph</strong> is a directed graph where vertices are transaction IDs and edges represent conflicting instructions with arrows showing the temporal order. Then, we can perform topological sorting to check for serialisability. A schedule is serialisability iff its precedence graph is acyclic.</p> <h1 id="lecture-35">Lecture 35</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">04-04-22</code></p> </blockquote> <p>We shall discuss a series of concepts in isolation now.</p> <h3 id="recoverable-schedules">Recoverable Schedules</h3> <p>If a transaction \(T_i\) reads a data item previously written by a transaction \(T_j\), then the commit operation of \(T_j\) appears before the commit operation of \(T_i\). These schedules are recoverable.</p> <h3 id="cascading-rollbacks">Cascading rollbacks</h3> <p>A single transaction failure leads to a series of transaction rollbacks. That is, uncommitted transactions must be rolled back.</p> <h3 id="cascadeless-schedules">Cascadeless Schedules</h3> <p>Cascading rollbacks cannot occur in these schedules. For each pair of transactions \(T_i\) and \(T_j\) such that \(T_j\) reads a data item previously written by \(T_i\), the commit operation \(T_i\) appears before the read operation of \(T_j\). That is, they allow reading of committed values only and disallow <em>dirty reads</em>. Every cascadeless schedule is recoverable.</p> <h3 id="weak-levels-of-isolation">Weak levels of Isolation</h3> <p>Some applications can tolerate schedules that are not serialisable. For example, in statistics we only want approximations. Ideally, we’d like serialisable, recoverable and preferably cascadable.</p> <p>There are levels of isolation defined in SQL-92 like - read committed, read uncommitted, repeatable read, and serialisable. Most often, databases run in read committed mode.</p> <h3 id="concurrency-control-mechanisms">Concurrency Control Mechanisms</h3> <p>We have a ton of theory for implementing whatever we have discussed. There are locking schemes, timestamp schemes, optimistic/lazy schemes and multi-versions (similar to master-slave mechanism in parallel cores of architecture).</p> <h2 id="transactions-in-sql">Transactions in SQL</h2> <p>We have predicate operations in SQL. A tuple might fail a predicate before a transaction, but passes it after the transaction has completed. Some databases lock the matched tuples for consistency, but that does not always work. That is, we need “predicate locking”, not just key-based locks that locks all <em>possible</em> matching tuples. Phantom reads refer to not matching tuples that are just added.</p> <p>In SQL, a transaction begins implicitly. <strong>commit work</strong> commits current transaction and begins a new one. <strong>rollback work</strong> causes current transaction to abort. Isolation levels can be set at database level or at the start of a transaction.</p> <h1 id="chapter-18-concurrency-control">~Chapter 18: Concurrency Control</h1> <h2 id="lock-based-protocols">Lock based protocols</h2> <p>A lock is a mechanism to control concurrent access to a data item. Items can be locked in two modes</p> <ul> <li> <strong>exclusive</strong> (X) mode - Data item can be both read as well as written. <code class="language-plaintext highlighter-rouge">lock-X</code> </li> <li> <strong>shared</strong> (S) mode - Data item can only be read. <code class="language-plaintext highlighter-rouge">lock-S</code> </li> </ul> <p>These requests are made implicitly using a concurrency control manager. Not that X lock can’t be obtained when a S lock is held on an item.</p> <p>A <strong>locking protocol</strong> is a set of rules followed by all transactions while requesting and releasing locks. They enforce serialisability by restricting the set of possible schedules. To handle <strong>deadlocks</strong>, we need to roll back some transactions. <strong>Starvation</strong> is also possible if concurrency control manager is badly designed. This occurs because X-locks can’t be granted when S-lock is being used.</p> <h3 id="two-phase-locking-protocol">Two-Phase Locking Protocol</h3> <p>This protocol ensures conflict-serialisable schedules. We have two phases</p> <ul> <li>Phase 1: Growing phase. A transaction can obtain locks but not release them</li> <li>Phase 2: Shrinking phase. It’s the opposite of the above.</li> </ul> <p>The transactions can be serialised in the order of their lock points (the point where a transaction has acquired its final lock). However, this protocol does not ensure protection from deadlocks. It also does not ensure recoverability. There are extensions that ensure recoverability from cascading roll-back.</p> <ul> <li> <strong>Strict two-phase locking</strong> - A transaction must hold all its exclusive locks till it commits/aborts.</li> <li> <strong>Rigorous two-phase locking</strong> - A transaction must hold all locks till commit/abort.</li> </ul> <p>We mostly use the second protocol.</p> <p>Two-phase locking is not necessary condition for serialisability. That is, there are conflict serialisable schedules that cannot be obtained if the two-phase locking protocol is used.</p> <p>Given a locking protocol, a schedule \(S\) is <strong>legal</strong> under a locking protocol if it can be generated by a set of transactions that follow the protocol. A protocol <strong>ensures</strong> serialisability if all legal schedules under that protocol are serialisable.</p> <p>In the two-phase locking protocol. we can upgrade (S to X) in the growing phase and downgrade (X to S) in the shrinking phase.</p> <h1 id="lecture-36">Lecture 36</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">05-04-22</code></p> </blockquote> <h2 id="implementation-of-locking">Implementation of Locking</h2> <p>A <strong>lock manager</strong> can be implemented as a separate process. The lock manager maintains an in-memory data-structure called a <strong>lock table</strong> to record granted locks and pending requests. A lock table is implemented as a hash table with queues.</p> <h2 id="graph-based-protocols">Graph-Based Protocols</h2> <p>They impose a partial ordering \(\to\) on the set \(D = \{d_1, \dots, d_n\}\) of all the data items. If \(d_i \to d_j\), then any transaction accessing both \(f_i\) and \(f_j\) must access \(d_i\) before accessing \(d_j\).</p> <p>A tree protocol ensures conflict serialisability as well as freedom from deadlock. However, it does not guarantee recoverability and it has other issues.</p> <h2 id="deadlocks">Deadlocks</h2> <p>There are other deadlock prevention strategies like</p> <ul> <li> <strong>wait-die</strong> - non-preemptive. Older transaction may wait for younger one to release the data item, and the younger transaction never wait for older ones. They are rolled back instead.</li> <li> <strong>wound-wait</strong> - preemptive. Older transaction <em>wounds</em> (forces rollback) of a younger transaction instead of waiting for it.</li> </ul> <p>In both the schemes, a rolled back transaction restarts with its original timestamp.</p> <ul> <li>Time-out based schemes</li> </ul> <h3 id="deadlock-recovery">Deadlock Recovery</h3> <p>There are two ways for rollback</p> <ul> <li>Total rollback</li> <li>Partial rollback - Roll back victim transaction only as far as necessary to release locks that another transaction in cycle is waiting for.</li> </ul> <p>A solution for starvation is that the oldest transaction in the deadlock set is never chosen as victim of rollback.</p> <h2 id="multiple-granularity">Multiple Granularity</h2> <p>A lock table might be flooded with entries when a transaction requires coarser granularity. The levels of granularity we use are database, area, file and record. What if we only use coarse granularities? The problem is that it’s not effective in terms of concurrency.</p> <p>We use <strong>intention locks</strong> to take care of hierarchy of granularities.</p> <h3 id="intention-lock-modes">Intention Lock Modes</h3> <p>In addition to S and X, we have</p> <ul> <li> <strong>Intention-shared</strong> IS - Indicates explicit locking at a lower level of the tree but only with shared locks.</li> <li> <strong>Intention-exclusive</strong> IX - Indicates explicit locking at a lower level with exclusive or shared locks.</li> <li> <strong>Shared and intention exclusive</strong> SIX - The subtree rooted by that node is locked explicitly in shared mode and explicit locking is being done at a lower level with exclusive-mode locks.</li> </ul> <p>Intention locks allow a higher level node to be locked in S or X mode without having to check all descendent nodes. That is, to get a lock at the bottom level, we need to start taking intention locks from the root. Also, we have the following <strong>compatibility matrix</strong>.</p> <p><img src="/assets/img/Databases/image-20220417153625206.png" alt="image-20220417153625206"></p> <p>The query engine decides all the locks based on the input queries. It follows adaptive lock granularity when it can’t decide. There is also a notion of <strong>lock granularity escalation</strong> in adaptive locking where the query engine shifts to a coarser granularity in case there are too many locks at a particular level.</p> <p>Now, we discuss locking in the case of predicate reads. Consider an insert. You can lock the entire relation to ensure consistency, but when someone inserts a new record there won’t be any lock on it. So, we use the following rules.</p> <p><img src="/assets/img/Databases/image-20220417155825429.png" alt="image-20220417155825429"></p> <p>Note that one also locks the metadata in two-phase locking scheme that helps it ensure serialisability.</p> <h2 id="phantom-phenomenon">Phantom Phenomenon</h2> <p>A transaction \(T_1\) performs a predicate read, and a transaction \(T_2\) inserts a matching tuple while \(T_1\) is active but after predicate read. As a result, some of these schedules are not serialisable.</p> <h3 id="handling-phantoms">Handling Phantoms</h3> <p>If the conflict is at the data level, locking the metadata will prevent insertion and deletion in the relation. However, this provides very low concurrency for insertions and deletions.</p> <h3 id="index-locking-to-prevent-phantoms">Index Locking to prevent Phantoms</h3> <p>Every relation must have at least one index. A transaction \(T_i\) that performs a lookup must lock all the index leaf nodes that it accesses in \(S\)-mode. That is, it locks a range. A transaction \(T_j\) that inserts, updates or deletes a tuple \(t_i\) in a relation \(r\) must update all indices to \(r\). It can’t acquire X-lock in the locked range of \(T_i\). So phantom reads won’t occur when the rules of two-phase locking protocol must be observed. The key idea here is that, tuples in the matching range will be sequential do to the index.</p> <h3 id="next-key-locking-to-prevent-phantoms">Next-Key Locking to prevent Phantoms</h3> <p>This method provides higher concurrency. It locks all values that satisfy index lookup, and also lock the next key value in the index.</p> <p>Note that the above locks are done in the \(B^+\) trees.</p> <h1 id="lecture-37">Lecture 37</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">07-04-22</code></p> </blockquote> <h2 id="timestamp-ordering-protocol">Timestamp Ordering Protocol</h2> <p>The timestamp-ordering protocol guarantees serialisability since all the arcs in the precedence graph are of the form having nodes with edges from smaller timestamp to larger timestamp. This protocol ensures freedom from deadlock as no transaction ever waits. However, the schedule may not be cascade free and may not even be recoverable.</p> <p>To make it recoverable, we have the following solutions</p> <ul> <li>All the writes are done at the end of the timestamp. A transaction that aborts is restarted with a new timestamp.</li> <li>Limited form of locking - wait for data to be committed before reading it</li> <li>Use commit dependencies to ensure responsibility.</li> </ul> <h2 id="thomas-write-rule">Thomas’ Write Rule</h2> <p>Modified version of the TSO in which obsolete <strong>write</strong> operations may be ignored under certain circumstances. When a transaction items to write to a data item Q which will be rewritten by another transaction, then the original write is considered as obsolete. \(TS(T) &lt; W_\text{timestamp}(Q)\). So, rather than rolling back the original transaction as the TSO does, we ignore the write operation in the original transaction. In this way, Thomas’ Write Rules allows greater potential concurrency. It allows some view serialisable schedules that are not conflict-serialisable.</p> <p>The rule comes into the picture in case of <strong>blind writes</strong> - A write is done without a preceding read.</p> <h2 id="validation-based-protocol">Validation Based Protocol</h2> <p>It uses timestamps, but they are not pre-decided. The validation is performed at commit time to detect any out-of-serialisation order reads/writes. It is also known as <strong>optimistic concurrency control</strong> since transaction executes fully in the hope that all will go well during validation. This is done in three phases</p> <ul> <li>Read and execution phase. Writes are done to temporary variables.</li> <li>Validation phase</li> <li>Write phase</li> </ul> <p>Each transaction has 3 timestamps corresponding to start of execution, validation phase, and write phase. The validation time stamps are used in the protocol. In validation, we check that for all \(T_i, T_j\) such that \(TS(T_i) &lt; TS(T_j)\), one of the following must hold</p> <ul> <li> \[finishTS(T_i) &lt; startTS(T_j)\] </li> <li>\(startTS(T_j) &lt; finishTS(T_i) &lt; validationTS(T_j)\) and the set of data items written by \(T_i\) does not intersect with the set of data items read by \(T_j\).</li> </ul> <p>This is when the validation succeeds.</p> <h2 id="multiversion-concurrency-control">Multiversion Concurrency Control</h2> <p>Multiversion schemes keep old versions of data item to increase concurrency. There are variants such as</p> <ul> <li>Multiversion Timestamp Ordering</li> <li>Multiversion Two-Phase Locking</li> <li>Snapshot isolation</li> </ul> <p>The key ideas are that</p> <ul> <li>Each successful write results in the creation of a new version that labeled using timestamps.</li> <li>When a read operation is issued, we select the appropriate timestamp based on the timestamp of transaction issuing read and return the value of the selected version.</li> </ul> <h3 id="multiversion-timestamp-ordering">Multiversion Timestamp ordering</h3> <p>Each data item \(Q\) has a sequence of versions \(&lt; Q_1, \dots, Q_n&gt;\) each of which have</p> <ul> <li>Content</li> <li>Write timestamp</li> <li>Read timestamp</li> </ul> <p>If \(T\) issues a read or write, let \(Q_k\) be the version with the highest <u>write</u> timestamp that has a value less than the timestamp of \(T\). Then for a read, we return the value from \(Q_k\), and for a write we overwrite if both the timestamps are equal. We roll back \(T\) if \(TS(T) &lt; R\_timestamp\). Otherwise, we simply create a new entry.</p> <p>Like the basic TSP, recoverability is not ensured.</p> <h3 id="multiversion-two-phase-locking">Multiversion Two-Phase Locking</h3> <p>Differentiates between read-only transactions and update transactions. Update transactions follow rigorous two-phase locking. Read of a data item returns the latest version of the item. The first write of \(Q\) by \(T\) results in creation of a new version \(Q_i\) and the timestamp is updated after the completion of the transaction. After the transaction \(T\) completes, \(TS(T_i) = \texttt{ts-counter} + 1\) and \(W\_timestamp(Q) = TS(T_i)\) for all versions of \(Q\) that it creates. Then, the <code class="language-plaintext highlighter-rouge">ts-counter</code> is incremented. All of this must be done atomically.</p> <p>In read only transactions, <code class="language-plaintext highlighter-rouge">ts-counter</code> is assigned to the timestamp. As a result, only serialisable schedules are produced.</p> <p>The issues with multiversion schemes are they increase storage overhead and there are issues with keys constraint checking and indexing with multiple versions.</p> <h2 id="snapshot-isolation">Snapshot Isolation</h2> <p>A transaction \(T\) executing with snapshot isolation</p> <ul> <li>Takes snapshot of the committed data at start</li> <li>Always reads/modified data in its own snapshot</li> <li>Updates of concurrent transactions are not visible to \(T\)</li> <li>Writes of \(T\) are complete when it commits</li> </ul> <p>So, <strong>first committer wins</strong> rule is being used. <strong>Serialisable snapshot isolation (SSI)</strong> is an extension that ensures serialisability. However, there are some anomalies in this.</p> <h1 id="lecture-38">Lecture 38</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">11-04-22</code></p> </blockquote> <h1 id="chapter-19">~Chapter 19:</h1> <p>A transaction failure can occur as a result of</p> <ul> <li>logical errors - internal error condition in the transaction. These can be somewhat prevented by <code class="language-plaintext highlighter-rouge">assert</code> statements.</li> <li>system error - database must terminate the transaction due to errors like deadlock.</li> </ul> <p><strong>System crash</strong> - Occurs due to a power failure, hardware or software failure. A <strong>failstop assumption</strong> refers to the assumptions that non-volatile storage contents are not corrupted by a system crash. A <strong>disk failure</strong> destroys disk storage.</p> <p>Logging helps in recovery. To recover from failure, we first find inconsistent blocks by</p> <ul> <li>We compare two copies of every disk block which is expensive, or</li> <li>Use logs sort of mechanism</li> </ul> <p>and then overwrite the inconsistent blocks. We also need to ensure atomicity despite failures.</p> <h2 id="log-based-recovery">Log-Based Recovery</h2> <p>A <strong>log</strong> is a sequence of log records that keep information of update activities on the database. When transaction \(T\) starts, it registers itself by writing \((T start)\) log record. Before \(T\) executes \(write(X)\), it writes a log record \((T, X, V_1, V_2)\). Upon finishing the last statement, the log record \((T commit)\) is written. There are two approaches using log</p> <ul> <li>Immediate database modification - write to buffer which will write to disk when before the transaction commits. Log record is written before database item is written.</li> <li>Deferred database modification - writes are done only after commit. Not used frequently.</li> </ul> <p><u>A transaction is said to have committed when its commit log record is output to stable storage.</u> Also, the log records are interleaved for concurrent transactions.</p> <h2 id="concurrency-control-and-recovery">Concurrency Control and Recovery</h2> <p>We assume that if a transaction \(T\) has modified an item, no other transaction can modify the same item until \(T\) has committed or aborted. This is equivalent to <em>strict two phase locking</em>.</p> <ul> <li>\(undo(T_i)\) restores the value of all data items updated by \(T_i\) to their old values, going backwards from the last log record for \(T_i\). For each restoration, we add a log record of \((T_i, X, V)\). When undo of a transaction is complete, a log record \((T_i abort)\) is written out.</li> <li>\(redo(T_i)\) sets the value of all data items updated by \(T_i\) to the new values, going forward from the first log record of \(T_i\). The log is unchanged here.</li> </ul> <h2 id="recovering-from-failure">Recovering from Failure</h2> <p>When recovering from failure,</p> <ul> <li>Transaction \(T_i\) needs to be undone if the log contains the record \((T_i start)\) but does not contain \((T_i commit)\) or \((T_i abort)\)</li> <li>It needs to be redone if it contains \((T_i start)\) and \((T_i commit)\) or \((T_i abort)\).</li> </ul> <p>Note that the second step is wasteful in some cases. We recovered before, and we are doing it again. This is known as <strong>repeating history</strong>.</p> <h3 id="checkpoints">Checkpoints</h3> <p>Processing the entire log can be slow. We streamline recovery procedures by periodically performing checkpointing.</p> <ul> <li>Output all log records currently residing in the main memory onto stable storage</li> <li>Output all modified buffer blocks to the disk</li> <li>Write a log record \((checkpoint L)\) onto stable storage where \(L\) is a list of all transactions active at the time of checkpoint</li> <li>All updates are stopped while doing checkpointing.</li> </ul> <p>The log records before a checkpoint are not needed!</p> <h1 id="lecture-39">Lecture 39</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">12-04-22</code></p> </blockquote> <p>## Recovery Algorithm</p> <p><img src="/assets/img/Databases/image-20220417183018051.png" alt="image-20220417183018051"></p> <p><img src="/assets/img/Databases/image-20220417183027590.png" alt="image-20220417183027590"></p> <p><img src="/assets/img/Databases/image-20220417183036363.png" alt="image-20220417183036363"></p> <h2 id="log-record-buffering">Log Record Buffering</h2> <p>Log records are buffered in main memory, instead of being output directly to stable storage. When the buffer is full, a <strong>log force</strong> operation is performed.</p> <p>As we had said, before a block of data in the main memory is output to the database, all log records pertaining to data in that block must have been output to stable storage. This rule is called as <strong>write-ahead logging</strong> or WAL rule. Strictly speaking, this is only required for undo transactions.</p> <h2 id="database-buffering">Database Buffering</h2> <p>The recovery algorithm supports the <strong>no-force policy</strong> - updated blocks need not be written to disk when transaction commits. However, <strong>force policy</strong> is a more expensive commit. The recovery algorithm also supports the <strong>steal policy</strong> - blocks containing updates of uncommitted transactions can be written to disk, even before the transaction commits.</p> <p>No updates should be in progress on a block when it is output to disk. It can be ensured by acquiring exclusive locks on the block before writing a data item. Locks can be released once the write is completed. Such locks held for short durations are called <strong>latches</strong>.</p> <p>In summary, we acquire latch, do log flush, write block to disk and finally release the latch.</p> <p>The <strong>dual paging</strong> problem refers to the extra I/O that is done when fetching a page from swap to buffer and then moving it to the disk. This can be prevented by letting the OS pass the control to the database when it needs to evict a page from the buffer. The database outputs the page to the database instead of the swap space, and release the page from the buffer.</p> <h2 id="fuzzy-checkpointing">Fuzzy Checkpointing</h2> <p>to avoid long interruption of normal processing during checkpointing, we allow updates to happen during checkpointing. We permit transactions to proceed with their actions once we note the list \(M\) of modified buffer blocks. We store a pointer to the <strong>checkpoint</strong> record in a fixed position <strong>last_checkpoint</strong> on disk once the all modified buffer blocks in \(M\) are output to disk. During recovery, we use this last checkpoint.</p> <h2 id="failure-with-loss-of-nonvolatile-storage">Failure with Loss of Nonvolatile Storage</h2> <p>Technique similar to checkpointing used to deal with the loss of non-volatile storage. That is, we periodically <strong>dump</strong> the entire content of the database to stable storage. No transaction may be active during the dump procedure. We perform the following -</p> <ul> <li>output all log records from main memory to storage</li> <li>output all buffer blocks onto the disk</li> <li>copy the contents of the database to stable storage</li> <li>output a record \((dump)\) to lon on stable storage.</li> </ul> <p>There are versions of <strong>fuzzy dump</strong> and <strong>online dump</strong>.</p> <h2 id="remote-backup-systems">Remote Backup Systems</h2> <p>We need to <strong>detect failure</strong> at the backup site using heartbeat messages and perform <strong>transfer of control</strong> to take control at the backup site. The log records are copied at the backup before, and recovery can be initiated. Once the primary site goes back up, we give back the control.</p> <p>The <strong>time to recover</strong> is very important to reduce delay in takeover. Therefore, the backup sire periodically processed the redo log records, performs a checkpoint, and can then delete earlier parts of the log.</p> <p>A <strong>hot-spare</strong> configuration permits very fast takeover - backup continually processes redo logs as they arrive and when the failure is detected, the backup rolls back incomplete transactions and is ready to process new transactions. An alternative to remote system backup is to use distributed systems.</p> <p>We have the following levels of durability</p> <ul> <li> <strong>One safe</strong> - Commit as soon as transaction’s commit log record is written at primary.</li> <li> <strong>Two-very safe</strong> - Commit when transaction’s commit log record is written at primary and backup.</li> <li> <strong>Two-safe</strong> - Proceed as in two-very-safe is both primary and backup are active.</li> </ul> <p>We also need to reduce latency for communication. We add a <strong>near site</strong> backup close to the primary site. Log records are replicated both at near site and remote backup site. If primary fails, remote backup site gets latest log records, which it may have missed, from near site.</p> <hr> <h4 id="end-of-course">END OF COURSE</h4> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/automata/">Automata Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ipl/">IPL Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/NumAn/">Numerical Analysis Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/operating-systems/">Operating System Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/philosophy/">Philosophy Notes</a> </li> </div> <script>document.querySelectorAll("#table-of-contents a").forEach(function(e){e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substring(1);document.querySelectorAll(".content-section").forEach(function(e){e.classList.add("hidden")});var n=document.getElementById(t);n&&n.classList.remove("hidden")})});</script> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Sudhansh Peddabomma. Last updated: April 09, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0K9MLG0V24");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>