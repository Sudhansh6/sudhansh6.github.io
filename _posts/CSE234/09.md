# Large Language Models on Cloud and Edge

The first wave of revolution in ML systems was sparked by Big Data in 2010s (competitions like Netflix recommendation price), which resulted in systems such as XGBoost, Spark and GraphLab. Then in mid 2010s, deep-learning started gaining traction, and TensorFlow, TVM and Torch were created. Now, we're in the third revolution with Generative AI. ML Systems is playing a much bigger role.

The challenges involved are -

1. Memory - Llama-70B consumes 320GB VRAM just to store parameters in FP32

2. Compute - The post-Moore era brings great demand for diverse specialized compute, system support becomes bottleneck

The design of systems depends on the paradigm that the industry is moving towards. Currently, we have cloud based models with a client-server architectures. This is how computers initially started before the age of personal computers. So, would we move towards personal AI in consumer devices? 

There are many engineering challenges involved in this. As we covered previously, there are specialized libraries and systems for each backend involving manually created optimizations. The area is very labor intensive with huge market-opportunities. 

Our approach to this has been in terms of composable optimizations to rewrite kernel codes. Furthermore, we added techniques such as parameter sharding, memory planning, operator fusion, etc to add to these optimizations. What have we learned from this journey?

There are four abstractions we use

- Computational Graphs - Graph and its extensions enable higher level program rewriting and optimization

- Tensor Programs - These abstractions focus on loop and layout transformation for fused operators

- Libraries and Runtimes - Optimizing libraries are built by vendors and engineers to accelerate key operators of interests

- Hardware Primitives - The hardware builders exposes novel primitives to provide native hardware acceleration

It has not been about a **silver bullet system but continuous improvement and innovations**. ML Engineering is going hand-in-hand with ML modeling.

The developers of TVM are expanding it to TVMUnity to bring the compiler flow to the user. As we've studied, IRModule is the central abstraction in TVM. Once the user-level code is written in this form, TVM takes care of the hardware backend making it easy to the models on various architectures. 

TVM generates an optimized models, and features are continuously added with data to make it better over time.

## TVM Unity

### First-class symbolic shape support

Traditional models like ResNet have some key-characteristics. The compilers are being built under the assumption of these fixed parameters, but with the age of generative AI, these parameters keep changing continuously. TVMUnity leverages symbolic shift to incorporate variable sizes in the model. In essence, traditional compilers are unable to handle dynamic shapes, whereas TVMUnity allows it with symbolic support.

![](/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-47-07-image.png)

Knowing the dependencies between different variable parameters allows for better optimizations during runtime - static memory planning for dynamic shapes

### Composable Tensor Optimization

In the early ages, we had scalar computing. Then came the age of vector computing with SIMD. Now, NVIDIA has TensorCore and TPUs became a thing for tensor computing. How do we leverage these hardware developments in our programs? We want both loop based optimization and tensor-based programs. 

The first step is to isolate the internal computation tensorized computation from external loops to create a `Block`. 

![](/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-52-50-image.png)

With this, TVMUnity does *Imperative Schedule Transformation* to search different variants of programs by changing blocks to create an optimized IR.

![](/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-53-13-image.png)

By providing this interface to the user, where the user can mention the parameters involved, TVMUnity can perform tensorization along with graph-optimization problem.

### Bringing compilation and Libraries Together

We have seen this tradeoff with compilers and libraries.

![](/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-57-28-image.png)

TVMUnity provides interfaces like `Relax-BYOC` to offload the computational load to libraries such as TensoIR to leverage the library-optimized kernels. The native compilation works on top of the library offloading to increase the performance even more. 

By adding this flexibility, the compiler can update (live with library updates) to squeeze the best performance. 

Since libraries come out with data layout requirements, the compiler can use this information to optimize the remaining parts of the code. 

## ML Compilation in Action

How does this architecture help with incremental developments? The users can try new optimizations in coordination with the compiler to scale to newer models. 

In language models, the compiler does targeted optimizations such a low-bit quantizations, dynamic shape planning, fusion and hardware aware optimizations, etc. Along with these, it works with KVCache etc to improve the performance. 

The **MLCEngine** is a universal LLM deployment engine being developed to deploy LLMs on any device. The key contribution here is *universal*. This development also revealed some key insights - iOS devices require custom Metal language kernels, Snapdragon devices require OpenCL - and this project is trying to take care of all of that. 

It also helps with structured generation with near zero overhead. This feature would be super useful for agent use cases. [Surprisingly, MLCEngine does it with near zero overhead with something known as XGrammar!](https://huggingface.co/spaces/mlc-ai/WebLLM-Structured-Generation-Playground)

They also created [WebLLM](https://webllm.mlc.ai/) with the new WebGPU standard to use the local compute to the browser. Yes, it uses your local GPUs and doesn't send your data to any server! You can use this to build stuff on top of it with the typescript API and a Node package. It is an open source project too!
