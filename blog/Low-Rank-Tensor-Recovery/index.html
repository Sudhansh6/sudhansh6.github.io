<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Low Rank Tensor Recovery for Joint Probability Distribution | Sudhansh Peddabomma</title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="An introduction to Tensors and notation. Summary of a few papers."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/Low-Rank-Tensor-Recovery/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Sudhansh Peddabomma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Articles</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Low Rank Tensor Recovery for Joint Probability Distribution</h1> <p class="post-meta">January 4, 0001</p> <p class="post-tags"> <a href="/blog/0001"> <i class="fa-solid fa-calendar fa-sm"></i> 0001 </a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> Research</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="tensors">Tensors</h1> <ul> <li>A scalar can be represented by a <strong>single</strong> component with <strong>zero</strong> basis vectors for each component.</li> <li>A vector of 3 dimensions can be specified using <strong>three</strong> components and <strong>one</strong> basis vector for each component.</li> <li>A stress matrix can be represented by <strong>nine</strong> components (we are considering 3 dimensions). That is <strong>two</strong> basis vectors for each component.</li> </ul> <p>All of the above mathematical objects fall under a broad class of <strong>Tensors</strong>.</p> <blockquote> <p><strong>Tensors</strong>: In an \(m\) dimensional space, a tensor of rank \(n\) is a mathematical object that has \(n\) indices, \(m^n\) components and obeys certain transformation rules.</p> </blockquote> <p>Generally, \(m=3\) in real-life scenarios. \(m=4\) is used while discussing relativity.</p> <blockquote> <p><strong>Rank of a tensor</strong> is defined as the number of basis vectors needed to fully specify a component of the tensor.</p> </blockquote> <h2 id="transformation-rules-of-tensors">Transformation rules of tensors</h2> <ul> <li>A tensor is an object that transforms like a tensor</li> <li>A tensor is an object that is an object that is invariant under a change of coordinate system with components that change according to a special set of mathematical formulae</li> </ul> <p>In simpler terms, the actual tensor is invariant to a change in the coordinate systems. For example, the displacement vector pointing from \(A\) to \(B\) ( a \(1\) rank tensor) does not change when coordinates change. The vector will still be pointing from \(A\) to \(B\). However, the way we represent the vector, i.e., the components changes.</p> <h3 id="matrices-neq-tensors">Matrices \(\neq\) Tensors</h3> <p>Matrices are a form of representation of rank-\(2\) tensors. They are just an array of numbers. Although, Tensors require detailed specification and are invariant under change of coordinate systems.</p> <h2 id="einstein-notation-for-tensors">Einstein Notation for Tensors</h2> <p>Both subscripts and superscripts are used for indices in tensors.</p> <ul> <li> <p>Any twice repeated index in a single <strong>term</strong> is summed over. Index \(= 1, 2, \cdots , n\). Typically, \(n = 3\)</p> <p>For example, \(a_{ij}b_j = a_{i1} + a_{i2} + a_{i3}\)</p> <p>Here, \(j\) is a <em>dummy</em> variable and \(i\) is a <em>free</em> variable.</p> <p>Dummy indices can be replaced, whereas free indices can’t be. A free index takes only <strong>one</strong> value in an expression.</p> </li> <li> <p>No index may occur \(3\) or more times in a given term.</p> <p>\(a_{ij}b_{ij} \checkmark\) \(a_{ii}b_{ij} \pmb\times\) \(a_{ij}b_{jj} \pmb\times\)</p> <p>We count subscripts and superscripts together. That is,</p> <p>\(a_j^j\) - \(j\) is a dummy variable</p> </li> <li> <p>In an equation involving Einstein notation, the free indices on both sides must match.</p> </li> </ul> <h4 id="brackets-in-einstein-notation">Brackets in Einstein Notation</h4> <ol> <li>Combine terms outside parentheses with each term inside parentheses separately.</li> <li>From each of the combined terms, use the largest count of each index as the final count in the overall term.</li> </ol> <h3 id="kronecker-delta">Kronecker Delta</h3> \[\delta_{ij} \equiv \delta_j^i \equiv \delta^{ij} \equiv \begin{cases}1 &amp; i=j\\ 0 &amp; i \neq j \end{cases}\] <h2 id="linear-algebra-of-tensors">Linear algebra of Tensors</h2> <p><img src="/assets/img/0001-01-01-Low-Rank-Tensor-Recovery/image-20210617172019075.png" alt="image-20210617172019075"></p> <p><img src="/assets/img/0001-01-01-Low-Rank-Tensor-Recovery/image-20210617172120225.png" alt="image-20210617172120225"></p> <p><img src="/assets/img/0001-01-01-Low-Rank-Tensor-Recovery/image-20210617172135319.png" alt="image-20210617172135319"></p> <p>For the following analysis, we shall be using the <em>superscript</em> notation to denote each element of a vector.</p> <blockquote> <p>Rectangular coordinate system - A coordinate system (\(x^i\)) in \(\mathbb R^n\) is rectangular if the distance between \(2\) points \(C(x^1, x^2, \cdots)\) and \(D(x^1, x^2, \cdots)\) is given by : \(\sqrt{\delta_{ij} \Delta x^i \Delta x^j}\)</p> </blockquote> <p><strong>Curvilinear coordinates</strong></p> <ul> <li>\(P\) is a point in a coordinate system (\(x^i\)) in \(\mathbb R^n\) given by \(P : (x^1,x^2, \cdots)\)</li> <li>\((\bar x^i)\) is another coordinate system in \(\mathbb R^n\) such that the coordinates of \(P\) in this system are \((\bar x^1,\bar x^2, \cdots)\)</li> <li>Suppose that \(\bar x^i = \bar{x}^i(x^1, x^2, \cdots) : \mathcal{F}\). If the functions \(\bar x^i(x^1, x^2, \cdots)\) are all real valued, have continuous \(2\)nd partial derivatives everywhere, and are all invertible then \(\mathcal{F}\) is called a coordinate transformation.</li> </ul> <blockquote> <p>The Jacobian is non-zero over a region \(U\) in \(\mathbb R^n\) iff the corresponding transformation \(\mathcal{T}\) is locally bijective in that region \(U\).</p> <p>The Jacobian matrix of the inverse transformation \(\mathcal{T}^{-1}\) is the inverse of the Jacobian matrix of \(\mathcal{T}\)</p> </blockquote> <p>The above was taken from <a href="https://www.youtube.com/playlist?list=PLdgVBOaXkb9D6zw47gsrtE5XqLeRPh27_" rel="external nofollow noopener" target="_blank">here</a></p> <h1 id="recovering-joint-probability-of-discrete-random-variables-from-pairwise-marginals">Recovering Joint Probability of Discrete Random Variables from Pairwise Marginals</h1> <h3 id="tensor-algebra">Tensor Algebra</h3> <h4 id="canonical-polyadic-decomposition">Canonical Polyadic Decomposition</h4> <p>If an \(N\)-way tensor \(X \in \mathbb R^{I_1 \times I_2 \times \cdots \times I_N}\) has Canonical Polyadic rank \(F\), it can be written as</p> <div style="text-align: center"> $$ X(i_1, i_2, \cdots, i_N) = \sum_{f = 1}^{F} \lambda(f) \prod_{n = 1}^N A_n(i_n, f) $$ </div> <p>where \(A_n \in \mathbb R^{I_n \times F}\) is called the mode-\(n\) latent factor. In the above, \(\lambda = [\lambda(1), \cdots, \lambda(F)]^T\) with \(\|\lambda\|_0 = F\) is employed to ‘absorb’ the norms of columns.</p> <p>In other words an \(N\)-way tensor \(\mathcal{Z} \in \mathbb{R}^{I_1 \times I_2 \times \cdots \times I_N}\) representing the joint PMF of \(N\) discrete RVs where \(\mathcal{Z}(x_1, x_2, \cdots, x_N) = p(X_1 = x_1 , X_2 = x_2, \cdots, X_N = x_N)\), admits a CPD if it can be decomposed as a sum of \(F\) rank-\(1\) tensors. Denoting \(a \otimes b\) as the outer-product (Kronecker Product) of two vectors, the CPD model is:</p> <div style="text-align: center"> $$ \mathcal{Z} = \sum_{f = 1}^F \lambda(f)A_1(:,f)\otimes A_2(:,f) \otimes \cdots \otimes A_N(:,f) $$ </div> <p>Here, \(F\) is the rank of the tensor.</p> <h3 id="joint-pmf-recovery-a-tensor-perspective">Joint PMF Recovery: A Tensor Perspective</h3> <p>Any joint PMF admits a naive Bayes (NB) model representation; i.e., any joint PMF can be generated from a latent variable model with just one hidden variable. It follows that the joint PMF of \(\{Z_n\}^N_{n=1}\) can always be decomposed as:</p> <div style="text-align: center"> $$ Pr(i_1, i_2, \cdots, i_N) = \sum_{f = 1}^F Pr(f) \prod_{n=1}^N Pr(i_n | f) $$ </div> <p>where \(Pr(f) := Pr(H = f)\) is the prior distribution of a latent variable \(H\) and \(Pr(i_n|f) := Pr(Z_n = z_n^{(i_n)} | H = f)\) are the conditional distributions.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Modulo-Compressed-Sensing/">Modulo Compressed Sensing</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Lattices-in-Cryptography/">Lattices in Cryptography and Quantum Computers</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/GANs-for-Compressed-Sensing/">Generative Adversarial Networks for Compressed Sensing</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Mathematics-for-Finance/">Mathematics for Finance</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Object-Detection/">Object Detection</a> </li> </div> <script>document.querySelectorAll("#table-of-contents a").forEach(function(e){e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substring(1);document.querySelectorAll(".content-section").forEach(function(e){e.classList.add("hidden")});var n=document.getElementById(t);n&&n.classList.remove("hidden")})});</script> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Sudhansh Peddabomma. Last updated: October 15, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0K9MLG0V24");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>