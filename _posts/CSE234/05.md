# `matmul` - Case Study

Remember that over subscribing in GPUs is allowed, and identify that work can be performed in parallel. Developing the thought-process while working with CUDA is important
- Oversubscribe to keep the machine busy
- Balance workload with convergent workflows
- Minimize communication to reduce I/O

Now, let us consider matrix multiplication. What can be parallelized? In our previous CUDA implementation, we let each thread compute one element in the result matrix. So, each thread has $$2N$$ reads, and there are $$N^2$$ threads, resulting in $$2N^3$$ global memory access. 

We are not leveraging the fact that one element can be used to calculate many values in the result matrix. The trick is to use the shared memory space - thread tiling (similar to what we did in CPUs).

We let each thread compute $$V \times V$$ submatrix. The kernel is as follows

```cuda
    __global__ void mm(float A[N][N], float B[N][N], float C[N][N]) {
        int ybase = blockIdx.y * blockDim.y + threadIdx.y;
        int ybase = blockIdx.y * blockDim.y + threadIdx.y;
    
    float C[V][V] {0};
    float a[N], b[N];
    for (int x = 0; x < V; ++x){
        a[:] = A[xbase * V + x, :];
        for (int y = 0; y < V; ++y){
            b[:] = B[:, ybase * V + y];
            for (int k = 0; k < N; ++k){
                c[x][y] += a[k]*b[k];
            }
        }
    }
    C[xbase * V: xbase*V + V, ybase * V: ybase*V + V] = c[:];
}

```

For this version, we have reduced the read per threads to $$NV + NV^2$$ and number of threads to $$N^2/V^2$$ - total reads reduce to $$N^3/V + N^3$$ with $$V^2 + 2N$$ float storage per thread. 

We can improve this using partial sum computations. With a small change, we get


```cuda
    __global__ void mm(float A[N][N], float B[N][N], float C[N][N]) {
        int ybase = blockIdx.y * blockDim.y + threadIdx.y;
        int ybase = blockIdx.y * blockDim.y + threadIdx.y;
    
    float C[V][V] {0};
    float a[N], b[N];
    for (int k = 0; k < N; ++k){
        a[:] = A[xbase * V: xbase * V + V, k]; // Grabbing an area
        b[:] = B[k, ybase * V:ybase * V + V];
        for (int y = 0; y < V; ++y){
            for (int x = 0; x < V; ++x){
                c[x][y] += a[x]*b[y];
            }
        }
    }
    C[xbase * V: xbase*V + V, ybase * V: ybase*V + V] = c[:];
}

```
With memory read per thread reduced to $$NV^2$$, total memory to $$2N^3/V$$ and memory per thread to $$V^2 + 2V$$. This version is pretty good for systems with a single layer of memory hierarchy. However, if we have shared memory, it can be made more efficient!

Suppose we have an SRAM layer, we can tile hierarchically. Consider the following GPU `matmul` v3: SRAM Tiling:

The idea is to use block shared memory to let a block compute a $$L \times L$$ submatrix and each thread computes a $$V \times V$$ submatrix reusing the matrices in the shared block memory.


```cuda
    __global__ void mm(float A[N][N], float B[N][N], float C[N][N]) {
    __shared__ float sA[S][L], sB[S][L];
    Float c[V][V] = {0};
    Float a[V], b[V];
    Int y block = blockIdx.y;
    Iint X block = blockIdx.x;
        int ybase = blockIdx.y * blockDim.y + threadIdx.y;
        int ybase = blockIdx.y * blockDim.y + threadIdx.y;
    
    float C[V][V] {0};
    float a[N], b[N];
    for (int k = 0; k < N; ++k){
        a[:] = A[xbase * V: xbase * V + V, k]; // Grabbing an area
        b[:] = B[k, ybase * V:ybase * V + V];
        for (int y = 0; y < V; ++y){
            for (int x = 0; x < V; ++x){
                c[x][y] += a[x]*b[y];
            }
        }
    }
    C[xbase * V: xbase*V + V, ybase * V: ybase*V + V] = c[:];
}

```

> Think about it this way. Initially, we performed tiling across one layer of memory balancing the tradeoffs between I/O reads and memory constraints of the threads. Now, we are adding one more layer of such tiling in a similar manner. The key is to understand the partial sums idea. 


Note that it is highly unlikely that the threads have a large range of execution times, but we have the `__syncthreads()` as a failsafe. The statistics of this algorithm are -
- $$2LN$$ global memory access per thread block
- $$N^2/L^2$$ threadblocks 
- $$2N^3/L$$ global memory access
- $$2VN$$ shared memory access per thread


The key addition here is was the shared memory space. For this algorithm to be efficient, the fetching from the memory has to be implemented *cooperatively* - 

```cuda 
    int nthreads = blockDim.y * blockDim.x;
    int tid = threadIdx.y * blockDim.x + threadIdx.x;
    
```

These summarize the matrix multiplication codes implemented in the GPUs. Simple, isn’t it? Although, we have not addressed the optimal values for  $$L, V$$. It depends on the number of threads, registers and amount of SRAM available on the GPU - this process is called **kernel tuning**. There are profilers that optimize these values. It is a difficult problem since large ML models have various operations that need to be optimized together based on their processing and memory requirements. Furthermore, this is different for every GPU and there are hundreds of GPUs!

One solution is to do brute-force - hire people and throw money at it. On the other side of things, ML researchers are building **operator compilers** that figure these out automatically. 

There are other GPU optimizations that are utilized in practice
- Global memory continuous read
- Shared memory bank conflict
- Pipelining - While some threads are reading, let other threads compute. 
- Tensor core - Dedicated hardware component to accelerate matrix multiplications
- Lower precisions 

# ML compilation
A super-hot topic during the late 2010s since there was a lot of inefficient code that was being identified. The goal is to automatically generate optimal configurations and code given users code and _target hardware_.

Traditional compilers have to simply convert high-level code to binary instructions. The stack for ML compilers is 
1. Dataflow graph generation
2. Optimize graphs - pruning, partitioning, distribution, etc
3. Build efficient kernel code - parameter optimization
4. Machine code (This step is fairly easy and already well implemented)

The big problems in this big process are 
1. Programming level - Automatically transforming arbitrary (usually imperative) code into a compilable code (static dataflow graphs)
2. Graph level - Automatic graph transformations to make it faster (recall how convolution and batch norm can be fused). Graph theory researchers are working on this.
3. Operator level - How to use hardware and optimize standard operators like `matmul`.


The big players in this big field are 
1. XLA - First compiler for ML, released along with TensorFlow in 2016 (those researchers aimed big). This turned out to be so good, that the current TensorFlow stack still uses this. Also works for PyTorch, and it is useful to deploy on TPUs. 
2. tvm (Tensor Virtual Machine) - It is one of the most successful open-source compiler in academia. They founded OctoML with 200M (got acquired by NVIDIA). There is no backward pass.
3. 2.0 - Torch based compiler, that isn’t that great in terms of optimization. 
4. Modular - They raised 300M, founded by the same person who created LLVM. The co-founders started swift at Apple! They had big claims - 20x faster than 2.0, not sure how true they are.


You can think of TensorFlow and PyTorch as the front end, and the above mentioned compilers as the backend. 

## Operator Compilation
Each user-level written code (for standard operations) has a library of low-level program variants, and the compiler chooses the fastest one for the given hardware. 

For example, consider a loop -
```python
    for x in range(128):
        C[x] = A[x] * B[x]
```

Get converted to 

```
for xi in range(4):
    for xo in range(32):   
        C[xo * 4 + xi] A[xo * 4 + xi] + B[xo * 4 + xi]
```

Which is then efficiently implemented in the GPU kernels. 

So, how do we make this happens?
- Enumerate all possibilities
- Enumerate all the (close-to-) optimal values for the hardware - register/cache 
- Apply to all operators and devices 

How to search or reduce the search space and generalize?

Note that for a certain kind of code and hardware, finding these optimal value *once* is enough.

### Search via Learned Cost Model 
The famous example is Autotvm -

 ![](/assets/img/2025-01-06-data-systems-for-ml/17376893447233.jpg)
The code generator here is done with templates (not LLMs).We need a lot of experts to write the template to define the search space. 

To search in this parameters space, the compiler does beam search with early pruning. The cost model can be trained on the historical data. 

### High-level ideas
We represent the programs in an abstract way and build a search space with a set of transformations (that represent a good coverage of common optimizations like tiling). Then effective search  with accurate cost models and transferability have to be deployed. 

So, how well are we doing in this field? If the compilers were that good, they would’ve discovered flash-attention. Okay, it’s not that bad, compilers have found good optimizations and it just goes to show how difficult this problem is. 

# High-level DSL for CUDA: Triton
We have seen a device-specific DSL (domain-specific language). Programmers are able to squeeze the last bits of performance through this. However, it requires deep expertise and the performance optimization is very time-consuming. Maintaining codebases is complex. 

On the other hand, we have ML compilers. They prototype ideas very quickly (automatically) and the programmer does not have to worry about the low-level details. The problem is representing and searching through the search-space is difficult. Compilers were not able to find Flash-attention because the search-space wasn’t able to represent this possibility. Furthermore, code generation is a difficult problem that relies on heavy use of templates - lots of performance cliffs. 

So compared to these two extremes, Triton is in between - it is simpler than CUDA and more expressive than graph compilers. It was developed by OpenAI as a solution to the problems with CUDA and compilers. 

### Triton Programming Model
The users define tensors in SRAM directly and modify them using torch-like primitives.
- Embedded in Python - Kernels are defined in Python using triton.jit 
- Supports pointer arithmetics - Users construct tensors of pointers and can (de)reference them element wise. 
- However, it has shape constraints - must have power-of-two number of elements along each direction

Consider the following example
```python
    import triton.language as tl
    import triton
    
    @ triton.jit
    def __add(z_ptr, x_ptr, y_ptr, N)
        offsets = tl.arange(0, 1024)
```

The triton kernel will be mapped to a single block (SM) of threads. The users are responsible for mapping to multiple blocks. Basically, the language is automating some parts (like compilers), and making the design process simpler for users (as compared to CUDA). These design philosophies are important because they help build newer mental models for users - because they offload some of the cognitive load for optimization, they can think of newer ways of optimizing with these restricted set of parameters.