# Runtime
<missing content>

## Memory and Scheduling
Our goal is to fit the workload on limited memory and ensure that the peak memory usage is less than the available memory. 

<Missing content>

Consider the GPT-3 architecture - 

![](/assets/img/2025-01-06-data-systems-for-ml/17382918001192.jpg)

For every model, we check the precision and multiply the number of parameters by 2 or 4 to calculate the total memory consumption. 

For the main model with 175B parameters, if each parameter is 2 bytes, then we require 350Gb of memory! How did we make this work? 

Why does this rule of thumb work? Let us check the activation sizes for different layers
1. For 2D convolution: The input has the size $$(bs, nc, wi, hi)$$, and the output has $$(bs, nc, wo, ho)$$. The activation size is $$bs*nc*wo*ho*\text{sizeof(element)}$$
2. For an MLP with input size $$(bc, m, n)$$ and output size $$(bs, m, p)$$, the activation size is $$bs*m*p*\text{sizeof(element)}$$
3. For a transformer (ignoring activation layers and other FFLs) - the input size is $$(bs, h, seq\_len)$$ and the output size is $$(bs, h, seq\_len). The activation size is $$bs*h*seq\_len*\text{sizeof(element)}$$

So for GPT-3, the per-layer activation assuming sequence length 1 comes to 78 or 156 Gb. Let us add some more elements to this calculation.

The Adam Optimizer estimates the first and second moment vectors with parameters for exponential decays. It also has a step-size or learning rate. The algorithm is given by

![](/assets/img/2025-01-06-data-systems-for-ml/17382924545817.jpg)

Along with the learning rate, since it also stores the moments, it has to store two more values for each parameter! The memory usage becomes thrice of what it should be!

### Lifetime of activations at training

Because we need to store the intermediate values for the gradient steps, training an $$N$$-layer neural network would require $$O(N)$$ memory. This is the main difference between training and inference. In inference, we wouldn’t need to store the parameters at all layers, so we would just need $$O(1)$$ memory. 

So we’ve seen for GPT-3, we require 350 or 700 Gb. So for a sequence length of 96, we would require 7488 or 14976 Gb! These numbers are just crazy! We haven’t even considered the composite layers. 

Therefore, it is important to take care of memory. 

### Single Device execution
How do we reduce the memory usage? 

Idea 1 - the input or the activation is not needed until the backward pass reaches the layer. So, we can discard some of them and recompute the missing intermediate nodes in small segments. This technique is called *recomputation, rematerialization, checkpoint activation, etc*. It’s essentially the time-space tradeoff. 

For an $$N$$ layer neural network, if we checkpoint every $$K$$ layers, then the memory cost reduces to 

$$
    \text{Memory cost} = \mathcal O\left(\frac{N}{K}\right) + \mathcal O(K)
$$

To minimize this, we can pick $$K = \sqrt{N}$$. The total recomputation increases by $$N$$ - essentially another forward pass. In PyTorch, this feature can be activated using `torch.utils.checkpoint`. 

So when do we use this? When memory is a constraint and time of training is not a concern. The memory usage also depends on the layer being checkpointed - the layers can have different out sizes. In transformers, the layer boundary is typically checkpointed. The disadvantage is that this only works for activations. 

> why?

The second idea is **gradient accummulation**. The activation memory is linear to batch size. The idea is to compute the gradient for the batch but will limited memory. We split the original batch into micro-batches and accumulate the gradients at each layer. We then update the weights for the complete batch. 

![](/assets/img/2025-01-06-data-systems-for-ml/17382936900407.jpg)

The disadvantage of this strategy is that over-subscribing of GPUs is difficult since we have smaller matrices. 

An alternative method to save on GPU memory is to use the memory hierarchy. We have `SwapIn` (swap from CPU DRAM to HBM) and `SwapOut` (swap from HBM to CPU DRAM) that can be applied to both weights and activation. As we do a forward pass, we swap in the next layers and swap out the passed layers. You can be a bit more intelligent about it and pre-fetch the layers based on the computation and swap latencies. This strategy is becoming more practical as more companies are adopting the unified memory architecture. The memory hierarchy seems to be breaking.


All these strategies can be used together to probably train GPT-3 on a single device but it would take forever. 

![](/assets/img/2025-01-06-data-systems-for-ml/17387232533206.jpg)
Why do we start with gradient accumulation instead of gradient checkpointing? Checkpointing greatly increases the computation time, so we try the other alternatives first. 

## Memory and Compute

### Quantization
All our memory usage is a multiple of $$\text{sizeof(element)}$$. What if we reduce that parameter?

Quantization is the process of constraining an input from a continuous or otherwise large set of value to a discrete set. We use a lower-precision representation for data while preserving ML performance (accuracy), speeding up compute, reducing memory, saving energy, etc. Most of the edge models use quantization.

To understand this better, let’s understand the representation of data in memory -
- An unsigned integer has the range $$[0, 2^n - 1]$$
- A signed integer with $$n$$-bit has the range $$[-2^{n-1} - 1, 2^{n - 1} - 1]$$. To avoid saving 0 twice by storing a sign bit, computer architects decided to use *Two’s complement representation*.
- Fixed point number - An arbitrary bit is chosen as the boundary for the integer and the decimal. This representation is mainly used in security applications now.
- Floating point representation - We use a sign bit, 8-bit exponent and 23 bit fraction. That is the value is, $$(-1)^{sign} \times (1 + \text{ fraction}) \times 2^{\text{exponent} - 127}$$. 
    
    How do we represent 0 then? Representation-wise, we technically cannot represent 0, so we make a special representation - *normal vs subnormal values*. Whenever the exponent bits are zero, we remove the bias term $$1$$ that is added to the fraction, and represent the value as $$(-1)^{sign} \times \text{ fraction} \times 2^{\text{exponent} - 127}$$. This expressions is only used with the exponent is $$0$$. This way, we also extend the range of the representation and the smallest positive number we can represent is $$2^{-149}$$. 
    
    How about special values? Exponent with all set bits is infinity and sign is decided by the sign bit. NaN is represented in the subnormal range with exponent bits set to 1. In summary, we have 
    
    ![](/assets/img/2025-01-06-data-systems-for-ml/17387239819544.jpg)
