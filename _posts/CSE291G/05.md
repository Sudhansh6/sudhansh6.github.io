# Model-free methods

Policy iteration and Value iteration are closely related to each other. For both the algorithms, we need to evaluate a policy to find the corresponding value function. However, in many cases, we do not know the exact transition and reward functions. In other cases, the environment can have a large number of states, making it impossible to model it. 

For such situations, we rely on **Monte-Carlo methods**. Any method that solves a problem by generating suitable random numbers and observing that a fraction of numbers obey some property or properties, can be classified as a Monte Carlo method. The key ideas here are using a *sampling technique* for a heuristic *estimator*. These methods do not make use of the Markov assumption much, making them much more generalizable. 

The idea is to learn directly from episodes of experience without a prior knowledge of MDP transitions (rewards). Since the idea relies on episodes, one caveat is that it can only be applied to *episodic MDPs* - episodes have to terminate. 

## Prediction Problem 

Let us consider the first problem - estimating $$V_\pi(s)$$ for a state. Instead of updating after every action, we update after each episode by taking the mean reward across all sample trajectories sampled from this stage. The *first-visit* algorithm is given by 

$$
    \tilde V_\pi(s) = \frac{1}{m} \sum G_i^s
$$

Where $$G_i^s$$ is the total reward after $$s$$ first appears in the episode. Even though the number of states is large, itâ€™s nonetheless finite. Using this fact, we can show theoretically that the value above can be bounded. The convergence time is also associated with the underlying transition probabilities (rare states require more episodes to appear in the trajectory). 

There are more questions to answer. For example, is this a biased or unbiased estimator? 

$$
\begin{align*}
    \mathbb E(\tilde V_\pi(s)) &= \frac{1}{m} \sum_i \mathbb E(G_i^s) \\
    &= V_\pi(s) \quad \because \text{ Markovian assumption}
\end{align*}
$$

How about doing a second-visit algorithm or every-visit algorithm? They are valid approaches too, the theoretical analysis slightly varies. The estimators may not be unbiased but they use the data more efficiently. That is useful in cases where sampling is expensive. The *every-visit algorithm* typically has a higher bias (due to dependencies in an episode for the occurrences) but lower variance and higher efficiency.

These differences are important to understand. For example, in the latest [o3-mini](https://openai.com/index/openai-o3-mini/) model, they observed that the every-visit variant of an RL algorithm obtained much better performance than the first-visit variant.

In contrast, consider estimating the $$Q_\pi(s, a)$$ function. A policy might choose a certain action for a given state. However, to get all the Q-values, the policy must account for exploring all actions at different states to get a good estimate. The exploration probability is captured by the **epsilon-greedy class of algorithms**.

