# Lecture 8

We are reaching the end of the *equations in one variable* theme.

## Order of convergence

Let $$\{p_n\}$$ be a sequence that converges to $$p$$ with $$p_n \neq p$$ for any $$n$$. If there are positive constants $$\lambda$$ and $$\alpha$$ such that 



$$
\lim_n \frac{\vert p_{n + 1} - p \vert}{\vert p_n - p \vert^\alpha} = \lambda
$$



then the **order of convergence** of $$\{p_n\}$$ to $$p$$ is $$\alpha$$ with *asymptomatic error* $$\lambda$$. An iterative technique of the form $$p_n = g(p_{n - 1})$$ is said to be of order $$\alpha$$ if the sequence $$\{p_n\}$$ converges to the solution $$p = g(p)$$ with order $$\alpha$$.

In general, a sequence with a high order of convergence converges more rapidly than a sequence with a lower order. The asymptotic constant affects the speed of convergence but not to the extent of the order.

Two cases of order are given special attention

- If $$\alpha = 1$$ and $$\lambda < 1$$, the sequence is **linearly convergent**.
- If $$\alpha  = 2$$, the sequence is **quadratically convergent**.

### Order of convergence of fixed point iteration method?

Consider the fixed point iteration $$p_{n + 1} = f(p_n)$$. The Mean Value Theorem gives


$$
\begin{align}
p_{n + 1} - p &= f(p_n) - f(p) \\
&= f'(\xi_n)(p_n - p)
\end{align}
$$



where $$\xi_n$$ lies between $$p_n$$ and $$p$$, hence $$\lim_n\xi_n = p$$. Therefore,



$$
\lim_n \frac{\vert p_{n + 1} - p \vert}{\vert p_n - p \vert} = \lim_n \vert f'(\xi_n)\vert = \vert f'(p)\vert
$$



The convergence of a fixed point iteration method is thus **linear** if $$f’(p) \neq 0$$. 

We need to have $$f’(p) = 0$$ for a higher order of convergence. 

> why?

***Theorem.*** Let $$p$$ be a solution of the equation $$x = f(x)$$. Let $$f’(p) = 0$$ and $$f’'$$ be continuous with $$\vert f''(x) \vert < M$$ nearby $$p$$. Then there exists a $$\delta > 0$$ such that, for $$p_0 \in [p - \delta, p + \delta]$$, the sequence defined $$p_n = f(p_{n - 1})$$ converges at least quadratically to $$p$$. Moreover, for sufficiently large values of $$n$$



$$
\vert p_{n+1} - p \vert < \frac{M}{2}\vert p_n - p \vert^2
$$



For quadratically convergent fixed point methods, we should search for functions whose derivatives are zero at the fixed point. If we have the root-finding problem for $$g(x) = 0$$, then the easiest way to construct a fixed-point problem would be 



$$
\begin{align}
f(x) &= x - \phi(x)g(x)\\
f'(x) &= 1 - \phi'(x)g(x) - \phi(x)g'(x) \\
0 &= f'(p) = 1 - \phi(p)g'(p) \\
& \implies \phi(p) = {g'(p)}^{-1}
\end{align}
$$



where $$\phi$$ is a differentiable function, to be chosen later. Therefore, define $$phi(x) = {g’(x)}^{-1}$$ which gives


$$
p_{n + 1} = f(p_n) = p_n - \frac{g(p_n)}{g'(p_n)}
$$



This is the Newton-Raphson method! We have assumed that $$g’(p) \neq 0$$ in the above analysis. The NR/Secant method will not work if this assumption fails.

## Multiplicity of a zero

Let $$g: [a, b] \to \mathbb R$$ be a function and let $$p \in [a, b]$$ be a zero of $$g$$. We say that $$p$$  is a **zero of multiplicity** $$m$$ of $$g$$ if for $$x \neq p$$, we can write $$g(x) = (x - p)^mq(x)$$ with $$\lim_{x \to p}q(x) \neq 0$$.

Whenever $$g$$ has a simple zero ($$m = 1$$) at $$p$$, then the NR method works well for $$g$$. However, NR does not give a quadratic convergence if the order of the zero is more than 1.
