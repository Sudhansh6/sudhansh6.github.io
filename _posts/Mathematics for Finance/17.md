### Donsker's Invariance Principle

A discrete random walk can be represented as $S_n = \xi_1 + \cdots + \xi_n$. When $n$ becomes very large, the magnitude of the random walk scales by $\frac{1}{\sqrt{n}}$. This is due to the Central Limit Theorem - $\lim_{n \to \infty} S_n/\sqrt{n} \sim \mathcal N(0, 1)$. This is a useful fact to note while visualizing the random walk to fit the evolution appropriately in space. Also, 

$$
W_n(t) = \begin{cases} \frac{S_k}{\sqrt{n}} & t = \frac{k}{n}, k = 0, 1, \dots, n \\ \text{linearly interpolate otherwise}\end{cases}
$$

Again, from the CLT, $\lim_{n \to \infty} W_n(t) \to \mathcal N(0, t)$. The above formulation takes care of appropriately representing large random walks in a bounded space (probabilistically). Alternately, $W_n$ can be understood as a function from the space $C([0, 1] \to \mathbb R)$ - continuous functions from $[0, 1]$ to $\mathbb R$. 

Now, we are interested in performing operations $F$ on these random walks - 

$$
\lim_{n \to \infty} \mathbb E[F(W_n)] \to \mathbb E[F(W)]
$$

for all bounded continious functions $F: C \to \mathbb R$ (maps each walk $W_n$ to a real number). Here, $W$ is a standard Brownian motion. All the above equation is saying is that the discretized formulation $W_n$ is equivalent to a standard Brownian motion for practical purposes when $n$ is large.

For example, let $F(x) = \int_{0}^1 x(t) dt$. Then,

$$
\begin{align*}

\vert F(x) - F(y) \vert &\leq \int_0^1 \vert x(t) - y(t) \vert dt \\
&\leq  \max_t \vert x(t) - y(t) \vert = \|x - y\|_\infty 
\end{align*}
$$

### Binomial Model as a Random Walk

We have $S_t = S_{t - 1} \cdot \xi_t$ where $P(\xi_t = 1) = p, P(\xi_t = -1) = 1 - p$. How do we model this as a difference equation?

$$
\begin{align*}
S_t - S_{t - 1} &= S_{t - 1} (\xi_t - 1) \\
\equiv \frac{dS_t}{S_t} &= \xi_t - 1

\end{align*}
$$

The RHS should be equated to $a \cdot dt + b \cdot dW_t$ for showing similarities with a random walk.  Suppose we integrate with the above expression

$$
\begin{align*}
S_t - S_0 = \int_0^t S_u (a) du + \int_0^t S_u b dW_u 
\end{align*}
$$

We evaluate the above expression using **Ito's Stochastic Integral**. 

## Ito's Stochastic Integral

How do we calculate the integral $\int_0^t Y_s dW_s$? We formulate it as a combination of Riemman's sums - 

1. Let $Y_t = 1_{(u, v]}(s) \cdot H$ where $H \in L^2(\mathcal F_u)$
   
   $$
   M_t = \int_0^t Y_s dW_s = \begin{cases}
 0, &t \leq u \\ 
H(W_t - W_u), &u \leq t \leq v \\
H(W_v - W_u), & t \geq v
\end{cases} = H(W_{t \wedge v} - W_{t \lor u})
   $$
   
   **Claim.** $M$ is a **continuous** **martingale** and $\mathbb E[M_t^2] = \mathbb E[\int_0^t Y_s^2 ds]$
   
   *Proof.* The continuous part is trivial since $W_{t}$ is continuous. We need to show that $\mathbb E[M_t \vert \mathcal F_s] \substack{?}{=} M_s$ for $s< t$
   
   $$
   \begin{align*}
\mathbb E[M_t \vert F_s] &= \mathbb E[H(W_t - W_u) \vert F_s] \\
&= \mathbb E[HW_t \vert F_s] - E[HW_u \vert F_s] \\
&= HW_s - HW_u = M_s
\end{align*}
   $$
   
   due to the martingale property of $W_t$. Finally, for the second moment, we have
   
   $$
   \begin{align*}
\mathbb E(M_t^2) &= \mathbb E[H^2(W_{t \wedge v} - W_{t \wedge u})] \\
&= \begin{cases} 
0 & t \leq u \\
\mathbb E[H^2(W_t - W_u)^2], & u \leq t \leq v \\
\mathbb E[H^2(W_v - W_u)^2], & t\geq u
\end{cases} \\
&= \begin{cases} 
0 & t \leq u \\
\mathbb E[H^2](t - u), & u \leq t \leq v \\
\mathbb E[H^2](v - u), & t\geq u
\end{cases} \\ 
&= \mathbb E[H^2(t \wedge u - t \wedge u)] = \mathbb E\left[\int_0^t H^2 1_{(u, v]} (s)ds\right]
\end{align*}
   $$

2. Now, we need to add such $Y_k$'s for the required interval (Riemman sum). $Y_s = \sum_{k = 1}^n 1_{(u_k, v_k]} (s) H_k$ and $H_k \in L^2 (\mathcal F_{u_k})$ - 
   
   $$
   \int_0^t Y_s dW_s := \sum_{k = 1}^n \int_0^t Y_s^{(k)} dW_s
   $$
   
   Since $Y_s$ is a continuous martingale in $t$. Also, the  **Ito's Isometry** property states that
   
   $$
   \mathbb E((S_0^t Y_s dW_s)^2) = \mathbb E(\int_0^t Y_s^2 ds)
   $$
   
   This is easy to show considering non-overlapping intervals. *Hint*. Consider conditional expectation using Tower law.
