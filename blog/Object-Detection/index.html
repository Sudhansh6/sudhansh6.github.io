<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Object Detection | Sudhansh Peddabomma </title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="A brief survey of object detection methods in 2023."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/Object-Detection/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/chat.js?e73db4280bae3cbae4d78219277155b9"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Sudhansh Peddabomma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Articles </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Object Detection</h1> <p class="post-meta"> Created in January 15, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> Research</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>What</strong> objects are <strong>where</strong>? Object detection can be performed using either traditional (1) image processing techniques or modern (2) deep learning networks.</p> <ul> <li>Image processing techniques generally don’t require historical data for training and are unsupervised in nature. OpenCV is a popular tool for image processing tasks. <ul> <li>Pros: Hence, those tasks do not require annotated images, where humans labeled data manually (for supervised training).</li> <li>Cons: These techniques are restricted to multiple factors, such as complex scenarios (without unicolor background), occlusion (partially hidden objects), illumination and shadows, and clutter effect.</li> </ul> </li> <li>Deep Learning methods generally depend on supervised or unsupervised learning, with supervised methods being the standard in computer vision tasks. The performance is limited by the computation power of GPUs, which is rapidly increasing year by year. <ul> <li>Pros: Deep learning object detection is significantly more robust to occlusion, complex scenes, and challenging illumination.</li> <li>Cons: A huge amount of training data is required; the process of image annotation is labor-intensive and expensive. For example, labeling 500’000 images to train a custom DL object detection algorithm is considered a small dataset. However, many benchmark datasets (MS COCO, Caltech, KITTI, PASCAL VOC, V5) provide the availability of labeled data.</li> </ul> </li> </ul> <p>The field of object detection is not as new as it may seem. In fact, object detection has evolved over the past 20 years. The progress of object detection is usually separated into two separate historical periods (before and after the introduction of Deep Learning):</p> <ul> <li>Object Detector Before 2014 – Traditional Object Detection period <ul> <li>Viola-Jones Detector (2001), the pioneering work that started the development of traditional object detection methods</li> <li>HOG Detector (2006), a popular feature descriptor for object detection in computer vision and image processing</li> <li>DPM (2008) with the first introduction of bounding box regression Object Detector</li> </ul> </li> <li>After 2014 – Deep Learning Detection period Most important two-stage object detection algorithms <ul> <li>RCNN and SPPNet (2014) - Region CNN</li> <li>Fast RCNN and Faster RCNN (2015)</li> <li>Mask R-CNN (2017)</li> <li>Pyramid Networks/FPN (2017)</li> <li>G-RCNN (2021) - Granulated RCNN</li> </ul> </li> <li>Most important one-stage object detection algorithms <ul> <li>YOLO (2016)</li> <li>SSD (2016)</li> <li>RetinaNet (2017)</li> <li>YOLOv3 (2018)</li> <li>YOLOv4 (2020)</li> <li>YOLOR (2021)</li> <li>YOLOv7 (2022)</li> <li>YOLOv8 (2023)</li> </ul> </li> </ul> <p>In general, deep learning based object detectors extract features from the input image or video frame. An object detector solves two subsequent tasks:</p> <ul> <li>Task #1: Find an arbitrary number of objects (possibly even zero), and</li> <li>Task #2: Classify every single object and estimate its size with a bounding box.</li> </ul> <h3 id="yolo---you-only-look-once">YOLO - You Only Look Once</h3> <p>It is a popular family of real-time object detection algorithms. The original YOLO object detector was first released in 2016. It was created by Joseph Redmon, Ali Farhadi, and Santosh Divvala. At release, this architecture was much faster than other object detectors and became state-of-the-art for real-time computer vision applications.</p> <p><img src="/assets/img/Object%20Detection/Untitled.png" alt="Untitled"></p> <p>The algorithm works based on the following four approaches:</p> <ul> <li> <p>Residual blocks - This first step starts by dividing the original image (A) into NxN grid cells of equal shape, where N in our case is 4 shown on the image on the right. Each cell in the grid is responsible for localizing and predicting the class of the object that it covers, along with the probability/confidence value.</p> <p><img src="/assets/img/Object%20Detection/Untitled%201.png" alt="Untitled"></p> </li> <li> <p>Bounding box regression -</p> <p>The next step is to determine the bounding boxes which correspond to rectangles highlighting all the objects in the image. We can have as many bounding boxes as there are objects within a given image.</p> <p>YOLO determines the attributes of these bounding boxes using a single regression module in the following format, where Y is the final vector representation for each bounding box.</p> \[Y = [pc, bx, by, bh, bw, c1, c2]\] <p>where \(pc\) is the probability score of the grid containing the object. \(c1, c2\) correspond to the object classes - ball and player.</p> </li> <li> <p>Intersection Over Unions or IOU for short</p> <p>Most of the time, a single object in an image can have multiple grid box candidates for prediction, even though not all of them are relevant. The goal of the IOU (a value between 0 and 1) is to discard such grid boxes to only keep those that are relevant. Here is the logic behind it:</p> <ul> <li>The user defines its IOU selection threshold, which can be, for instance, 0.5.</li> <li>Then YOLO computes the IOU of each grid cell which is the Intersection area divided by the Union Area.</li> <li>Finally, it ignores the prediction of the grid cells having an IOU ≤ threshold and considers those with an IOU &gt; threshold.</li> </ul> <p><img src="/assets/img/Object%20Detection/Untitled%202.png" alt="Untitled"></p> </li> <li> <p>Non-Maximum Suppression.</p> <p>Setting a threshold for the IOU is not always enough because an object can have multiple boxes with IOU beyond the threshold, and leaving all those boxes might include noise. Here is where we can use NMS to keep only the boxes with the highest probability score of detection.</p> </li> </ul> <p>Although YOLO was state-of-the-art when it released, with much faster detection, it had a few disadvantages. It is unable to detect smaller objects within a grid as each grid is designed for single object detection. It is unable to detect new or unusual shapes, and the same loss function is used for both small and large bounding boxes which creates incorrect localizations.</p> <p><img src="/assets/img/Object%20Detection/Untitled%203.png" alt="Untitled"></p> <p><strong>YOLOv2</strong> improvised over the existing architecture using <strong><a href="https://paperswithcode.com/method/darknet-19" rel="external nofollow noopener" target="_blank">Darknet-19</a></strong> as new architecture (Darknet is an open-source neural network framework in C and CUDA),</p> <ul> <li>batch normalization - reduced overfitting using a regularization effect.</li> <li>higher resolution of inputs,</li> <li>convolution layers with anchors - Replaces fully connected laters with anchor boxes to prevented predicting the exact coordinate of bounding boxes. Recall improved, accuracy decreased. Anchor boxes are predefined grids with certain aspect ratios spatially located in an image. These boxes are checked for an object probability score and are selected accordingly.</li> <li>dimensionality clustering - The previously mentioned anchor boxes are automatically found by YOLOv2 using k-means dimensionality clustering with k=5 instead of performing a manual selection. This novel approach provided a good tradeoff between the recall and the precision of the model.</li> <li>Fine-grained features - YOLOv2 predictions generate 13x13 feature maps, which is of course enough for large object detection. But for much finer objects detection, the architecture can be modified by turning the 26 × 26 × 512 feature map into a 13 × 13 × 2048 feature map, concatenated with the original features.</li> </ul> <p><strong>YOLOv3</strong> is an incremental improvement using Darknet-53 instead of Darknet-19. <strong>YOLOv4</strong> is an optimized for parallel computations. This architecture, compared to YOLOv3, adds the following information for better object detection:</p> <ul> <li>Spatial Pyramid Pooling (SPP) block significantly increases the receptive field, segregates the most relevant context features, and does not affect the network speed.</li> <li>Instead of the Feature Pyramid Network (FPN) used in YOLOv3, YOLOv4 uses <strong><a href="https://bio-protocol.org/exchange/minidetail?type=30&amp;id=9907669" rel="external nofollow noopener" target="_blank">PANet</a></strong> for parameter aggregation from different detection levels.</li> <li>Data augmentation uses the mosaic technique that combines four training images in addition to a self-adversarial training approach.</li> <li>Perform optimal hyper-parameter selection using genetic algorithms.</li> </ul> <p><strong>YOLOR</strong> is based on the unified network which is a combination of explicit and implicit knowledge approaches - (1) feature alignment, (2) prediction alignment for object detection, and (3) canonical representation for multi-task learning.</p> <p>Then, <strong>YOLOX</strong>, using a modified version of YOLOv3, decoupled classification and localization increasing the performance of the model. <a href="https://medium.com/mlearning-ai/yolox-explanation-mosaic-and-mixup-for-data-augmentation-3839465a3adf" rel="external nofollow noopener" target="_blank">Mosaic and MixUp data augmentation approaches</a> were added. Removed the anchor-based system that used to perform clustering under the hood. Introduced SimOTA instead of IoU.</p> <p><strong>YOLOv5</strong> uses Pytorch rather than Darknet, and has 5 different model sizes. <strong>YOLOv6</strong> was developed for industrial applications and introduced three improvements - a hardware-friendly backbone and neck design, an efficient decoupled head, and a more effective training strategy.</p> <p><strong>YOLOv7</strong> - <strong><a href="https://arxiv.org/pdf/2207.02696.pdf" rel="external nofollow noopener" target="_blank">Trained bag-of-freebies sets new state-of-the-art for real-time object detectors</a>.</strong> It reformed the architecture by integrating Extended Efficient Layer Aggregation Network (E-ELAN) which allows the model to learn more diverse features for better learning. The term <strong>bag-of-freebies</strong> refers to improving the model’s accuracy without increasing the training cost, and this is the reason why YOLOv7 increased not only the inference speed but also the detection accuracy.</p> <h3 id="r-cnn---region-based-convolutional-neural-networks">R-CNN - Region-based Convolutional Neural Networks</h3> <p><strong>R-CNN</strong> models first select several proposed regions from an image (for example, anchor boxes are one type of selection method) and then label their categories and bounding boxes (e.g., offsets). These labels are created based on predefined classes given to the program. They then use a convolutional neural network (CNN) to perform forward computation to extract features from each proposed area.</p> <p>In 2015, <strong>Fast R-CNN</strong> was developed to significantly cut down train time. While the original R-CNN independently computed the neural network features on each of as many as two thousand regions of interest, Fast R-CNN runs the neural network once on the whole image. This is very comparable to YOLO’s architecture, but YOLO remains a faster alternative to Fast R-CNN because of the simplicity of the code.</p> <p>At the end of the network is a novel method known as Region of Interest (ROI) Pooling, which slices out each Region of Interest from the network’s output tensor, reshapes, and classifies it (Image Classification). This makes Fast R-CNN more accurate than the original R-CNN.</p> <p><strong>Mask R-CNN</strong> is an advancement of Fast R-CNN. The difference between the two is that Mask R-CNN added a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN; it can run at 5 fps.</p> <h3 id="other-approaches">Other Approaches</h3> <p><strong>SqueezeDet</strong> was specifically developed for autonomous driving, where it performs object detection using computer vision techniques. Like YOLO, it is a single-shot detector algorithm. In SqueezeDet, convolutional layers are used only to extract feature maps but also as the output layer to compute bounding boxes and class probabilities. The detection pipeline of SqueezeDet models only contains single forward passes of neural networks, allowing them to be extremely fast.</p> <p><strong>MobileNet</strong> is a single-shot multi-box detection network used to run object detection tasks. This model is implemented using the Caffe framework.</p> <h3 id="references">References</h3> <ul> <li> <p><a href="https://www.datacamp.com/blog/yolo-object-detection-explained" rel="external nofollow noopener" target="_blank">Datacamp YOLO</a></p> </li> <li> <p><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks" rel="external nofollow noopener" target="_blank">Stanford CNNs</a> - Good site</p> </li> <li> <p><a href="https://viso.ai/deep-learning/yolov7-guide/" rel="external nofollow noopener" target="_blank">Viso AI YOLOv7</a></p> </li> <li> <p><a href="https://viso.ai/deep-learning/object-detection/" rel="external nofollow noopener" target="_blank">Viso AI Object Detection</a></p> </li> <li> <p><a href="https://viso.ai/computer-vision/what-is-computer-vision/" rel="external nofollow noopener" target="_blank">Viso AI Computer Vision</a></p> </li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/machine-learning-systems/">Key Works in ML Systems</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/data-systems-for-ml/">Data Systems for Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ai-agents/">AI Agents</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/rl-theory/">Reinforcement Learning Theory</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/brains-and-ai/">Brains and AI</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Sudhansh Peddabomma. Last updated: March 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script src="/assets/js/tooltips-setup.js?53023e960fbc64cccb90d32e9363de2b"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-0K9MLG0V24');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> <div id="chat-window" class="chat-window"> <div class="chat-header"> <h5 class="mb-0">Talk to my AI</h5> <button id="close-chat" class="btn-close"> <i class="fas fa-times"></i> </button> </div> <div id="chat-messages" class="chat-messages"></div> <div class="chat-input-container"> <input type="text" id="chat-input" class="form-control" placeholder="Type a message..."> <button id="send-btn" class="btn btn-primary"> <i class="fas fa-paper-plane"></i> </button> </div> </div> </body> </html>