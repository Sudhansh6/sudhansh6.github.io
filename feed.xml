<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://sudhansh6.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sudhansh6.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-19T20:59:55+00:00</updated><id>https://sudhansh6.github.io/feed.xml</id><title type="html">Sudhansh</title><entry><title type="html">Object Detection</title><link href="https://sudhansh6.github.io/blog/Object-Detection/" rel="alternate" type="text/html" title="Object Detection"/><published>2024-01-15T00:00:00+00:00</published><updated>2024-01-15T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Object-Detection</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Object-Detection/"><![CDATA[<p><strong>What</strong> objects are <strong>where</strong>? Object detection can be performed using either traditional (1) image processing techniques or modern (2) deep learning networks.</p> <ul> <li>Image processing techniques generally don’t require historical data for training and are unsupervised in nature. OpenCV is a popular tool for image processing tasks. <ul> <li>Pros: Hence, those tasks do not require annotated images, where humans labeled data manually (for supervised training).</li> <li>Cons: These techniques are restricted to multiple factors, such as complex scenarios (without unicolor background), occlusion (partially hidden objects), illumination and shadows, and clutter effect.</li> </ul> </li> <li>Deep Learning methods generally depend on supervised or unsupervised learning, with supervised methods being the standard in computer vision tasks. The performance is limited by the computation power of GPUs, which is rapidly increasing year by year. <ul> <li>Pros: Deep learning object detection is significantly more robust to occlusion, complex scenes, and challenging illumination.</li> <li>Cons: A huge amount of training data is required; the process of image annotation is labor-intensive and expensive. For example, labeling 500’000 images to train a custom DL object detection algorithm is considered a small dataset. However, many benchmark datasets (MS COCO, Caltech, KITTI, PASCAL VOC, V5) provide the availability of labeled data.</li> </ul> </li> </ul> <p>The field of object detection is not as new as it may seem. In fact, object detection has evolved over the past 20 years. The progress of object detection is usually separated into two separate historical periods (before and after the introduction of Deep Learning):</p> <ul> <li>Object Detector Before 2014 – Traditional Object Detection period <ul> <li>Viola-Jones Detector (2001), the pioneering work that started the development of traditional object detection methods</li> <li>HOG Detector (2006), a popular feature descriptor for object detection in computer vision and image processing</li> <li>DPM (2008) with the first introduction of bounding box regression Object Detector</li> </ul> </li> <li>After 2014 – Deep Learning Detection period Most important two-stage object detection algorithms <ul> <li>RCNN and SPPNet (2014) - Region CNN</li> <li>Fast RCNN and Faster RCNN (2015)</li> <li>Mask R-CNN (2017)</li> <li>Pyramid Networks/FPN (2017)</li> <li>G-RCNN (2021) - Granulated RCNN</li> </ul> </li> <li>Most important one-stage object detection algorithms <ul> <li>YOLO (2016)</li> <li>SSD (2016)</li> <li>RetinaNet (2017)</li> <li>YOLOv3 (2018)</li> <li>YOLOv4 (2020)</li> <li>YOLOR (2021)</li> <li>YOLOv7 (2022)</li> <li>YOLOv8 (2023)</li> </ul> </li> </ul> <p>In general, deep learning based object detectors extract features from the input image or video frame. An object detector solves two subsequent tasks:</p> <ul> <li>Task #1: Find an arbitrary number of objects (possibly even zero), and</li> <li>Task #2: Classify every single object and estimate its size with a bounding box.</li> </ul> <h3 id="yolo---you-only-look-once">YOLO - You Only Look Once</h3> <p>It is a popular family of real-time object detection algorithms. The original YOLO object detector was first released in 2016. It was created by Joseph Redmon, Ali Farhadi, and Santosh Divvala. At release, this architecture was much faster than other object detectors and became state-of-the-art for real-time computer vision applications.</p> <p><img src="/assets/img/Object%20Detection/Untitled.png" alt="Untitled"/></p> <p>The algorithm works based on the following four approaches:</p> <ul> <li> <p>Residual blocks - This first step starts by dividing the original image (A) into NxN grid cells of equal shape, where N in our case is 4 shown on the image on the right. Each cell in the grid is responsible for localizing and predicting the class of the object that it covers, along with the probability/confidence value.</p> <p><img src="/assets/img/Object%20Detection/Untitled%201.png" alt="Untitled"/></p> </li> <li> <p>Bounding box regression -</p> <p>The next step is to determine the bounding boxes which correspond to rectangles highlighting all the objects in the image. We can have as many bounding boxes as there are objects within a given image.</p> <p>YOLO determines the attributes of these bounding boxes using a single regression module in the following format, where Y is the final vector representation for each bounding box.</p> \[Y = [pc, bx, by, bh, bw, c1, c2]\] <p>where \(pc\) is the probability score of the grid containing the object. \(c1, c2\) correspond to the object classes - ball and player.</p> </li> <li> <p>Intersection Over Unions or IOU for short</p> <p>Most of the time, a single object in an image can have multiple grid box candidates for prediction, even though not all of them are relevant. The goal of the IOU (a value between 0 and 1) is to discard such grid boxes to only keep those that are relevant. Here is the logic behind it:</p> <ul> <li>The user defines its IOU selection threshold, which can be, for instance, 0.5.</li> <li>Then YOLO computes the IOU of each grid cell which is the Intersection area divided by the Union Area.</li> <li>Finally, it ignores the prediction of the grid cells having an IOU ≤ threshold and considers those with an IOU &gt; threshold.</li> </ul> <p><img src="/assets/img/Object%20Detection/Untitled%202.png" alt="Untitled"/></p> </li> <li> <p>Non-Maximum Suppression.</p> <p>Setting a threshold for the IOU is not always enough because an object can have multiple boxes with IOU beyond the threshold, and leaving all those boxes might include noise. Here is where we can use NMS to keep only the boxes with the highest probability score of detection.</p> </li> </ul> <p>Although YOLO was state-of-the-art when it released, with much faster detection, it had a few disadvantages. It is unable to detect smaller objects within a grid as each grid is designed for single object detection. It is unable to detect new or unusual shapes, and the same loss function is used for both small and large bounding boxes which creates incorrect localizations.</p> <p><img src="/assets/img/Object%20Detection/Untitled%203.png" alt="Untitled"/></p> <p><strong>YOLOv2</strong> improvised over the existing architecture using <strong><a href="https://paperswithcode.com/method/darknet-19">Darknet-19</a></strong> as new architecture (Darknet is an open-source neural network framework in C and CUDA),</p> <ul> <li>batch normalization - reduced overfitting using a regularization effect.</li> <li>higher resolution of inputs,</li> <li>convolution layers with anchors - Replaces fully connected laters with anchor boxes to prevented predicting the exact coordinate of bounding boxes. Recall improved, accuracy decreased. Anchor boxes are predefined grids with certain aspect ratios spatially located in an image. These boxes are checked for an object probability score and are selected accordingly.</li> <li>dimensionality clustering - The previously mentioned anchor boxes are automatically found by YOLOv2 using k-means dimensionality clustering with k=5 instead of performing a manual selection. This novel approach provided a good tradeoff between the recall and the precision of the model.</li> <li>Fine-grained features - YOLOv2 predictions generate 13x13 feature maps, which is of course enough for large object detection. But for much finer objects detection, the architecture can be modified by turning the 26 × 26 × 512 feature map into a 13 × 13 × 2048 feature map, concatenated with the original features.</li> </ul> <p><strong>YOLOv3</strong> is an incremental improvement using Darknet-53 instead of Darknet-19. <strong>YOLOv4</strong> is an optimized for parallel computations. This architecture, compared to YOLOv3, adds the following information for better object detection:</p> <ul> <li>Spatial Pyramid Pooling (SPP) block significantly increases the receptive field, segregates the most relevant context features, and does not affect the network speed.</li> <li>Instead of the Feature Pyramid Network (FPN) used in YOLOv3, YOLOv4 uses <strong><a href="https://bio-protocol.org/exchange/minidetail?type=30&amp;id=9907669">PANet</a></strong> for parameter aggregation from different detection levels.</li> <li>Data augmentation uses the mosaic technique that combines four training images in addition to a self-adversarial training approach.</li> <li>Perform optimal hyper-parameter selection using genetic algorithms.</li> </ul> <p><strong>YOLOR</strong> is based on the unified network which is a combination of explicit and implicit knowledge approaches - (1) feature alignment, (2) prediction alignment for object detection, and (3) canonical representation for multi-task learning.</p> <p>Then, <strong>YOLOX</strong>, using a modified version of YOLOv3, decoupled classification and localization increasing the performance of the model. <a href="https://medium.com/mlearning-ai/yolox-explanation-mosaic-and-mixup-for-data-augmentation-3839465a3adf">Mosaic and MixUp data augmentation approaches</a> were added. Removed the anchor-based system that used to perform clustering under the hood. Introduced SimOTA instead of IoU.</p> <p><strong>YOLOv5</strong> uses Pytorch rather than Darknet, and has 5 different model sizes. <strong>YOLOv6</strong> was developed for industrial applications and introduced three improvements - a hardware-friendly backbone and neck design, an efficient decoupled head, and a more effective training strategy.</p> <p><strong>YOLOv7</strong> - <strong><a href="https://arxiv.org/pdf/2207.02696.pdf">Trained bag-of-freebies sets new state-of-the-art for real-time object detectors</a>.</strong> It reformed the architecture by integrating Extended Efficient Layer Aggregation Network (E-ELAN) which allows the model to learn more diverse features for better learning. The term <strong>bag-of-freebies</strong> refers to improving the model’s accuracy without increasing the training cost, and this is the reason why YOLOv7 increased not only the inference speed but also the detection accuracy.</p> <h3 id="r-cnn---region-based-convolutional-neural-networks">R-CNN - Region-based Convolutional Neural Networks</h3> <p><strong>R-CNN</strong> models first select several proposed regions from an image (for example, anchor boxes are one type of selection method) and then label their categories and bounding boxes (e.g., offsets). These labels are created based on predefined classes given to the program. They then use a convolutional neural network (CNN) to perform forward computation to extract features from each proposed area.</p> <p>In 2015, <strong>Fast R-CNN</strong> was developed to significantly cut down train time. While the original R-CNN independently computed the neural network features on each of as many as two thousand regions of interest, Fast R-CNN runs the neural network once on the whole image. This is very comparable to YOLO’s architecture, but YOLO remains a faster alternative to Fast R-CNN because of the simplicity of the code.</p> <p>At the end of the network is a novel method known as Region of Interest (ROI) Pooling, which slices out each Region of Interest from the network’s output tensor, reshapes, and classifies it (Image Classification). This makes Fast R-CNN more accurate than the original R-CNN.</p> <p><strong>Mask R-CNN</strong> is an advancement of Fast R-CNN. The difference between the two is that Mask R-CNN added a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN; it can run at 5 fps.</p> <h3 id="other-approaches">Other Approaches</h3> <p><strong>SqueezeDet</strong> was specifically developed for autonomous driving, where it performs object detection using computer vision techniques. Like YOLO, it is a single-shot detector algorithm. In SqueezeDet, convolutional layers are used only to extract feature maps but also as the output layer to compute bounding boxes and class probabilities. The detection pipeline of SqueezeDet models only contains single forward passes of neural networks, allowing them to be extremely fast.</p> <p><strong>MobileNet</strong> is a single-shot multi-box detection network used to run object detection tasks. This model is implemented using the Caffe framework.</p> <h3 id="references">References</h3> <ul> <li> <p><a href="https://www.datacamp.com/blog/yolo-object-detection-explained">Datacamp YOLO</a></p> </li> <li> <p><a href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks">Stanford CNNs</a> - Good site</p> </li> <li> <p><a href="https://viso.ai/deep-learning/yolov7-guide/">Viso AI YOLOv7</a></p> </li> <li> <p><a href="https://viso.ai/deep-learning/object-detection/">Viso AI Object Detection</a></p> </li> <li> <p><a href="https://viso.ai/computer-vision/what-is-computer-vision/">Viso AI Computer Vision</a></p> </li> </ul>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[A brief survey of object detection methods in 2023.]]></summary></entry><entry><title type="html">Numerical Analysis Notes</title><link href="https://sudhansh6.github.io/blog/NumAn/" rel="alternate" type="text/html" title="Numerical Analysis Notes"/><published>2022-01-10T00:00:00+00:00</published><updated>2022-01-10T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/NumAn</id><content type="html" xml:base="https://sudhansh6.github.io/blog/NumAn/"><![CDATA[<h1 id="lecture-1">Lecture 1</h1> <p><em>Numerical Analysis</em> is the study of various methods</p> <ul> <li>to solve <ul> <li>differential equations,</li> <li>systems of linear equations,</li> <li>\(f(x) = \alpha\).</li> </ul> </li> <li>to approximate functions and</li> <li>to study the corresponding errors.</li> </ul> <p><strong>Example.</strong> \(e = \lim_{n \to \inf} (1 + 1/n)^n)\). How do we approximate \(e\) to an arbitrary accuracy? Since, the \(exp\) function is infinitely many times differentiable, we can approximate the function using Taylor’s theorem to any degree of precision we want.</p> <div style="text-align: center;"> $$ \text{Taylor's theorem} \\ f(x) = f(a) + f'(a)(x - a) + \cdots + \\ \frac{f^{(k)}(a)}{k!}(x - a)^k + \frac{f^{(k + 1)}(c)}{k!}(x - a)^{k + 1} $$ </div> <p>Using this, we get</p> <div style="text-align: center;"> $$ e = 1 + \frac{1}{1!} + \frac{1}{2!} + \cdots + \\ \frac{1}{n!} + \frac{e^c}{(n + 1)!} $$ </div> <p>where \(c\) is some real number between 0 and 1. Here, the error term at the \(n\)-th approximation is the term \(e^c/(n + 1)!\). We know that \(e^c\) is less than 3 so we can compute \(n\) where the error term is less than the prescribed error!</p> <p>For example, suppose we want the error to be less than \(10^{-10}\), then we use \(e^c/(n + 1)! &lt; 10^{-10}\).</p> <p><strong>Note.</strong> The last term is not the error in our approximation! We are choosing \(e^c = 3\) for no particular reason. It can be any value greater than the true value of \(e^c\). For instance, we can take \(e^c = 1000\) too. If we want our calculated value \(e\) value to be precise till the 10th decimal, then we ensure that \(1000/(n + 1)! &lt; 10^{-10}\) by tuning \(n\) appropriately.</p> <h2 id="aspects-of-numerical-analysis">Aspects of numerical analysis</h2> <ul> <li>The theory behind the calculation, and</li> <li>The computation.</li> </ul> <p>Some calculators have a loss of information due to limits in precision. This is due to the round-off error. We need to be able to detect and deal with such cases.</p> <h1 id="lecture-2">Lecture 2</h1> <h2 id="finite-digit-arithmetic">Finite digit arithmetic</h2> <p>When we do arithmetic, we allow for infinitely many digits. However, in the computational world, each representable number has only a fixed and finite number of digits. In most cases, the machine arithmetic is satisfactory, but at times problems arise because of this discrepancy. The error that is produced due to this issue is called the <strong>round-off</strong> error.</p> <p>A 64-bit representation is used for a real number. The first bit is a <strong>sign indicator</strong>, denoted by <em>s</em>. This is followed by an 11-bit exponent, <em>c</em>, called the <strong>characteristic</strong>, and a 52-bit binary fraction, <em>f</em>, called the <strong>mantissa</strong>. To ensure that numbers with small magnitude are equally representable, 1023 is subtracted from the characteristic, so the range of the exponent is actually from -1023 to 1024.</p> <p>Thus, the system gives a floating-point number of the form</p> <div style="text-align:center;"> $$ (-1)^s\cdot2^{c - 1023}\cdot(1 + f) $$ </div> <p>Since 52 binary digits correspond to between 16 and 17 decimal digits, we can assume that a number represented in this system is accurate till the 16th decimal.</p> <blockquote> <p>Can’t a single number be represented in many ways through this representation? Think about 1 for instance.</p> </blockquote> <p>The smallest positive number that can be represented is with \(s = 0, c = 1, f = 0\), so it is</p> <div style="text-align:center;"> $$ 2^{-1022}\cdot(1 + 0) \approx 0.22251 \times 10^{-307} $$ </div> <p>Numbers occurring in calculations that have a magnitude less than this number result in <strong>underflow</strong> and are generally set to 0.</p> <p>The largest positive number is</p> <div style="text-align:center;"> $$ 2^{1023}\cdot (2 - 2^{-52}) $$ </div> <p>Numbers above this would result in an <strong>overflow</strong>.</p> <h2 id="floating-point-representation">Floating-point representation</h2> <p>We will use numbers of the form</p> <div style="text-align:center;"> $$ \pm 0.d_1d_2\dots d_kd_{k + 1} \times 10^n $$ </div> <p>with \(1 \leq d_1 \leq 9\). There are two ways to get the floating-point representations \(fl(p)\) of a positive number \(p\)</p> <ul> <li><strong>Chopping.</strong> We simply chop off the \(d_{k + 1}, d_{k + 2}, \dots\) and write \(0.d_1d_2\dots d_k \times 10^n\)</li> <li><strong>Rounding.</strong> We add \(5 \times 10^{n - (k + 1)}\) to the number and then chop the result to obtain \(0.\delta_1\delta_2\dots \delta_k \times 10^n\)</li> </ul> <h2 id="errors">Errors</h2> <p>This approximate way of writing numbers is bound to create errors. If \(p\) is a real number and if \(p^*\) is its approximation then the <strong>absolute error</strong> is \(\|p - p ^*\|\) while the <strong>relative error</strong> is \(\|p - p ^*\|/p\) whenever \(p \neq 0\). Relative error is more meaningful as it takes into account the size of the number \(p\).</p> <p><strong>Note.</strong> Relative error can be negative as we take the value of \(p\) in the denominator!</p> <h2 id="significant-digits">Significant digits</h2> <p>We say that the number \(p^*\) approximates \(p\) to \(t\) significant digits if \(t\) is the largest non-negative integer for which</p> <div style="text-align:center;"> $$ \frac{\|p - p^*\|}{p} &lt; 5 \times 10^{-t} $$ </div> <h2 id="finite-digit-arithmetic-1">Finite Digit Arithmetic</h2> <p>The arithmetic in machines in defined by the following -</p> <div style="text-align:center;"> $$ x \oplus y := fl(fl(x) + fl(y)) $$ </div> <h1 id="lecture-3">Lecture 3</h1> <h2 id="major-sources-of-errors">Major sources of errors</h2> <p>One of the major sources of errors is cancellation of two nearly equal numbers. Suppose we have</p> <div style="text-align:center"> $$ fl(x) = 0.d_1d_2\dots d_p\alpha_{p + 1}\dots\alpha_k \times 10^n \\ fl(y) = 0.d_1d_2\dots d_p\beta_{p + 1}\dots\beta_k \times 10^n \\ fl(fl(x) - fl(y)) = 0.\gamma_{p + 1}\dots\gamma_k \times 10^{n - p} $$ </div> <p>In the above set of equations, the difference \(x \ominus y\) has \(k - p\) digits of significance compared to the \(k\) digits of significance of \(x\) and \(y\). The number of significant digits have reduced which leads to errors in further calculations.</p> <p>Another way the errors creep in is when we divide by number of small magnitude or multiply by numbers of large magnitude. This is because the error gets multiplied by a factor which increases its absolute value.</p> <p>Consider the expression \(-b \pm \sqrt{b^2 - 4ac}/2a\). By default, we consider positive square roots. Here, if \(b^2\) is large compared to \(4ac\) then the machine is likely to treat \(4ac\) as zero. How do we get around this error? <strong>Rationalization</strong></p> <div style="text-align:center"> $$ \frac{-b + \sqrt{b^2 - 4ac}}{2a} \times \frac{-b - \sqrt{b^2 - 4ac}}{-b - \sqrt{b^2 - 4ac}} = \frac{-2c}{b + \sqrt{b^2 - 4ac}} $$ </div> <p>​ Notice what happened here. Previously, the roots would have become zero because \(4ac\) could have been approximated as zero. However, this does not happen after rationalization! Also, do note that this won’t work when \(b&lt;0\) as we are considering positive square roots. If \(b&lt;0\), we can use the formula without rationalization. Think about the other root using such cases. Thus, the cancellation error can come in two major ways:</p> <ul> <li>when we cancel two nearly equal numbers, and</li> <li>when we subtract a small number from a big number</li> </ul> <h3 id="errors-propagate">Errors propagate</h3> <p>Once an error is introduced in a calculation, any further operation is likely to contain the same or a higher error.</p> <h2 id="can-we-avoid-errors">Can we avoid errors?</h2> <p>Errors are inevitable in finite arithmetic. There are some measures that we can take to try to avoid errors, or minimize them. For example, a general degree 3 polynomial can be written as a nested polynomial.</p> <div style="text-align:center"> $$ ax^3 + bx^2 + cx + d - x(x(ax + b) + c) + d $$ </div> <p>Computations done using this form will typically have a smaller error as the number of operations has reduced (from 5 to 3). Now, we are done with the first theme of our course - <strong><em>Machine arithmetic</em></strong>.</p> <h1 id="lecture-4">Lecture 4</h1> <h2 id="roots-of-equations">Roots of equations</h2> <p>The zeros of a function can give us a lot of information regarding the function. Why specifically zero? Division is in general more difficult to implement in comparison to multiplication. Division is implemented via the solution to roots of an equation in computers. There are other reasons such as eigenvalue decomposition. We shall see all of this later in the course.</p> <p><strong><em>Theorem.</em></strong> <em>Intermediate Value Theorem</em>. If \(f: [a, b] \to \mathbb R\) is a continuous function with \(f(a)\cdot f(b) &lt; 0\) then there is a \(c \in [a, b]\) such that \(f(x) = 0\).</p> <h3 id="bisection-method">Bisection method</h3> <p>The bisection method is implemented using the above theorem. To begin, we set \(a_1 = a\) and \(b_1 = b\), and let \(p_1\) be the midpoint of \([a, b]\);</p> <ul> <li>If \(f(p_1) = 0\), then we are done</li> <li>If \(f(p_1)f(a_1)&gt;0\), then the subinterval \([p_1, b_1]\) contains a root of \(f\), we then set \(a_2 = p_1\) and \(b_2 = b_1\)</li> <li>If \(f(p_1)f(b_1)&gt;0\), then the subinterval \([a_1, p_1]\) contains a root of \(f\), we then set \(a_2 = a_1\) and \(b_2 = p_1\)</li> </ul> <p>We continue this process until we obtain the root. Each iteration reduces the size of the interval by half. Therefore, it is beneficial to choose a small interval in the beginning. The sequence of \(\{p_i\}\) is a Cauchy sequence. IT may happen that none of the \(p_i\) is a root. For instance, the root may be an irrational number and \(a_1, b_1\) may be rational numbers. There are three criteria to decide to stop the bisection process.</p> <ul> <li>\(\|f(p_n)\| &lt; \epsilon\) - not very good because it may take a long time to achieve it, or that it may be achieved easily and yet \(p_n\) is significantly many steps away from a root of \(f\)</li> <li>\(\|p_n - p_{n-1}\| &lt;\epsilon\) is not good because it does not take the function \(f\) into account. While \(p_n\) and \(p_{n-1}\) may be close enough but the actual root may still be many steps away.</li> <li>\(p_n \neq 0 \text{ and } \frac{\|p_n - p_{n-1}\|}{\|p_n\|} &lt; \epsilon\) looks enticing as it mimics the relative error but again it does not take \(f\) into account.</li> </ul> <p>The bisection method has significant drawbacks - It is relatively slow to converge, and a good intermediate approximation might be inadvertently discarded. However, the method has the important property that is always converges to a solution.</p> <h1 id="lecture-5">Lecture 5</h1> <h2 id="fixed-points-and-roots">Fixed points and roots</h2> <p>A <em>fixed point</em> of \(f : [a,b] \to \mathbb R\) is \(p \in [a,b]\) such that \(f(p) = p\). Finding a fixed point of the function \(f\) is equivalent to finding the root of the function \(g(x) = f(x) - x\). We can create many such functions which give fixed points of \(f\) on solving for the roots of the function.</p> <p><strong><em>Theorem.</em></strong> <em>Fixed Point Theorem</em>. If \(f: [a,b] \to [a,b]\) is continuous then \(f\) has a fixed point. If, in addition, \(f’(x)\) exists on \((a, b)\) and \(\vert f’(x)\vert \leq k &lt; 1\) for all \(x \in (a, b)\) then \(f\) has a unique fixed point in \([a, b]\).</p> <p>The hypothesis is sufficient but not necessary!</p> <h3 id="fixed-point-iteration">Fixed point iteration</h3> <p>We start with a continuous \(f: [a,b] \to [a,b]\). Take any initial approximation \(p_0 \in [a,b]\) and generate a sequence \(p_n = f(p_{n - 1})\). If the sequence \(\{p_n\}\) converges to \(p \in [a, b]\) then</p> <div style="text-align:center"> $$ \begin{align} f(p) &amp;= f(\lim_n p_n) \\ &amp;= \lim_n f(p_n) = \lim_n p_{n + 1}\\ &amp;= p \end{align} $$ </div> <p>This method is called the <em>fixed point iteration method</em>. It’s convergence is not guaranteed.</p> <p>For instance, consider the roots of the equation \(x^3 + 4x^2 - 10 = 0\). The following functions can be used to find the roots using the fixed points.</p> <div style="text-align:center"> $$ \begin{align} x &amp;= g_1(x) = x - (x^3 + 4x^2 - 10)\\ x &amp;= g_2(x) = (\frac{10}{x} - 4x)^{1/2} \\ x &amp;= g_3(x) = \frac{1}{2}(10 - x^3)^{1/2} \end{align} $$ </div> <p>The results using different \(g\)’s in the Fixed Point Iteration method are surprising. The first two functions diverge, and the last one converges. This problem is because the hypothesis of the FPT does not hold in the first two functions. While the derivative \(g’(x)\) fails to satisfy in the FPT, a closer look tells us that it is enough to work on the interval \([1, 1.5]\) where the function \(g_3\) is strictly decreasing.</p> <p>So now we have the question - How can we find a fixed point problem that produces a sequence that reliably and rapidly converges to a solution to a given root-finding problem?</p> <h1 id="lecture-6">Lecture 6</h1> <h2 id="newton-raphson-method">Newton-Raphson method</h2> <p>This is a particular fixed point iteration method. Assume that \(f: [a, b] \to \mathbb R\) is twice differentiable. Let \(p \in [a, b]\) be a solution of the equation \(f(x) = 0\). If \(p_0\) is another point in \([a, b]\) then Taylor’s theorem gives</p> \[\begin{align} 0 &amp;= f(p) \\ &amp;= f(p_0) + (p - p_0)f'(p_0) + \frac{(p - p_0)^2}{2}f''(\xi) \end{align}\] <p>for some \(\xi\) between \(p\) and \(p_0\). We assume that \(\vert p - p_0 \vert\) is very small. Therefore, \(0 \approx f(p_0) + (p - p_0)f’(p_0)\). This sets the stage for the Newton-Raphson method, which starts with an initial approximation \(p_0\) and generate the sequence \(\{p_n\}\) by</p> \[p_n = p_{n - 1} - \frac{f(p_{n - 1})}{f'(p_{n - 1})}\] <p>What is the geometric interpretation of this method? It approximates successive tangents. The Newton-Raphson method is evidently better than the fixed point iteration method. However, it is important to note that \(\vert p - p_0 \vert\) is need to be small so the third term in the Taylor’s polynomial can be dropped.</p> <p><strong><em>Theorem.</em></strong> Let \(f: [a, b] \to \mathbb R\) be twice differentiable. If \(p \in (a, b)\) is such that \(f(p) = 0\) and \(f’(p) \neq 0\), then there exists a \(\delta &gt; 0\) such that for any \(p_0 \in [p - \delta, p + \delta]\), the Newton-Raphson method generates a sequence \(\{p_n\}\) converging to \(p\).</p> <p>This theorem is seldom applied in practice, as it does not tell us how to determine the constant \(\delta\). In a practical application, an initial approximation is selected and successive approximations are generated by the method. These will generally either converge quickly to the root, or it will be clear that convergence is unlikely.</p> <h1 id="lecture-7">Lecture 7</h1> <h3 id="problems-with-the-newton-raphson-method">Problems with the Newton-Raphson method</h3> <p>We have</p> \[p_n = p_{n - 1} - \frac{f(p_{n - 1})}{f'(p_{n - 1})}\] <p>One major problem with this is that we need to compute the value of \(f’\) at each step. Typically, \(f'\) is far more difficult to compute and needs more arithmetic operations to calculate than \(f\).</p> <h2 id="secant-method">Secant method</h2> <p>This method is a slight variation to NR to circumvent the above problem. By definition,</p> \[f'(a) = \lim_{x \to a} \frac{f(a) - f(x)}{a - x}\] <p>If we assume that \(p_{n - 2}\) is reasonable close to \(p_{n - 1}\) then</p> \[f'(p_{n - 1}) \approx \frac{f(p_{n - 1}) - f(p_{n - 2})}{p_{n - 1} - p_{n - 2}}\] <p>This adjustment is called as the <em>secant method</em>. The geometric interpretation is that we use successive secants instead of tangents. Note that we can use the values of \(f(p_{n - 2})\) from the previous calculations to prevent redundant steps. We need two initial guesses in this method.</p> <p>Secant method is efficient in comparison to Newton-Raphson as it requires only a single calculation in each iteration whereas NR requires 2 calculations in each step.</p> <h2 id="the-method-of-false-position">The method of false position</h2> <p>The NR or the Secant method may give successive approximations which are on one side of the root. That is \(f(p_{n - 1}) \cdot f(p_n)\) need not be negative. We can modify this by taking the pair of approximations which are on both sides of the root. This gives the <strong><em>regula falso method</em></strong> or the <em>method of false position</em>.</p> <p>We choose initial approximations \(p_0\) and \(p_1\) with \(f(p_0)\cdot f(p_1) &lt; 0\). We then use Secant method for successive updates. If in any iteration, we have \(f(p_{n - 1})\cdot f(p_n) &gt; 0\), then we replace \(p_{n - 1}\) by \(p_{n - 2}\).</p> <p>The added requirement of the regula falsi method results in more calculations than the Secant method.</p> <h2 id="comparison-of-all-the-root-finding-methods">Comparison of all the root finding methods</h2> <ul> <li>The bisection method guarantees a sequence converging to the root but it is a slow method.</li> <li>The other methods are sure to work, once the sequence is convergent. The convergence typically depends on the initial approximations being very close to the root.</li> <li>Therefore, in general, bisection method is used to get the initial guess, and then NR or the Secant method is used to get the exact root.</li> </ul> <h1 id="lecture-8">Lecture 8</h1> <p>We are reaching the end of the <em>equations in one variable</em> theme.</p> <h2 id="order-of-convergence">Order of convergence</h2> <p>Let \(\{p_n\}\) be a sequence that converges to \(p\) with \(p_n \neq p\) for any \(n\). If there are positive constants \(\lambda\) and \(\alpha\) such that</p> \[\lim_n \frac{\vert p_{n + 1} - p \vert}{\vert p_n - p \vert^\alpha} = \lambda\] <p>then the <strong>order of convergence</strong> of \(\{p_n\}\) to \(p\) is \(\alpha\) with <em>asymptomatic error</em> \(\lambda\). An iterative technique of the form \(p_n = g(p_{n - 1})\) is said to be of order \(\alpha\) if the sequence \(\{p_n\}\) converges to the solution \(p = g(p)\) with order \(\alpha\).</p> <p>In general, a sequence with a high order of convergence converges more rapidly than a sequence with a lower order. The asymptotic constant affects the speed of convergence but not to the extent of the order.</p> <p>Two cases of order are given special attention</p> <ul> <li>If \(\alpha = 1\) and \(\lambda &lt; 1\), the sequence is <strong>linearly convergent</strong>.</li> <li>If \(\alpha = 2\), the sequence is <strong>quadratically convergent</strong>.</li> </ul> <h3 id="order-of-convergence-of-fixed-point-iteration-method">Order of convergence of fixed point iteration method?</h3> <p>Consider the fixed point iteration \(p_{n + 1} = f(p_n)\). The Mean Value Theorem gives</p> \[\begin{align} p_{n + 1} - p &amp;= f(p_n) - f(p) \\ &amp;= f'(\xi_n)(p_n - p) \end{align}\] <p>where \(\xi_n\) lies between \(p_n\) and \(p\), hence \(\lim_n\xi_n = p\). Therefore,</p> \[\lim_n \frac{\vert p_{n + 1} - p \vert}{\vert p_n - p \vert} = \lim_n \vert f'(\xi_n)\vert = \vert f'(p)\vert\] <p>The convergence of a fixed point iteration method is thus <strong>linear</strong> if \(f’(p) \neq 0\) and \(f’(p) &lt; 1\).</p> <p>We need to have \(f’(p) = 0\) for a higher order of convergence.</p> <blockquote> <p>why?</p> </blockquote> <p><strong><em>Theorem.</em></strong> Let \(p\) be a solution of the equation \(x = f(x)\). Let \(f’(p) = 0\) and \(f’'\) be continuous with \(\vert f''(x) \vert &lt; M\) nearby \(p\). Then there exists a \(\delta &gt; 0\) such that, for \(p_0 \in [p - \delta, p + \delta]\), the sequence defined \(p_n = f(p_{n - 1})\) converges at least quadratically to \(p\). Moreover, for sufficiently large values of \(n\)</p> \[\vert p_{n+1} - p \vert &lt; \frac{M}{2}\vert p_n - p \vert^2\] <p>For quadratically convergent fixed point methods, we should search for functions whose derivatives are zero at the fixed point. If we have the root-finding problem for \(g(x) = 0\), then the easiest way to construct a fixed-point problem would be</p> \[\begin{align} f(x) &amp;= x - \phi(x)g(x)\\ f'(x) &amp;= 1 - \phi'(x)g(x) - \phi(x)g'(x) \\ 0 &amp;= f'(p) = 1 - \phi(p)g'(p) \\ &amp; \implies \phi(p) = {g'(p)}^{-1} \end{align}\] <p>where \(\phi\) is a differentiable function, to be chosen later. Therefore, define \(phi(x) = {g’(x)}^{-1}\) which gives</p> \[p_{n + 1} = f(p_n) = p_n - \frac{g(p_n)}{g'(p_n)}\] <p>This is the Newton-Raphson method! We have assumed that \(g’(p) \neq 0\) in the above analysis. The NR/Secant method will not work if this assumption fails.</p> <h2 id="multiplicity-of-a-zero">Multiplicity of a zero</h2> <p>Let \(g: [a, b] \to \mathbb R\) be a function and let \(p \in [a, b]\) be a zero of \(g\). We say that \(p\) is a <strong>zero of multiplicity</strong> \(m\) of \(g\) if for \(x \neq p\), we can write \(g(x) = (x - p)^mq(x)\) with \(\lim_{x \to p}q(x) \neq 0\).</p> <p>Whenever \(g\) has a simple zero (\(m = 1\)) at \(p\), then the NR method works well for \(g\). However, NR does not give a quadratic convergence if the order of the zero is more than 1.</p> <h1 id="lecture-9">Lecture 9</h1> <p><strong>Note.</strong> \(n\)-digit arithmetic deals with \(n\) significant digits and not \(n\) places after the decimal.</p> <h2 id="order-of-the-fixed-point-iteration-method">Order of the fixed point iteration method</h2> <p>Summarizing the last lecture we have</p> <ul> <li>If we have a function \(g(x)\) whose roots are to be found, we can convert it to a fixed point problem by appropriately constructing a \(f(x)\).</li> <li>If we construct \(f(x)\) such that it’s derivative is non-zero at the root, then the fixed point iteration method is <u>linear</u>.</li> <li>Otherwise, if the derivative is zero, then the fixed point iteration method is quadratic or higher. For example, we constructed such a \(f(x)\) which mirrored the Newton-Raphson method.</li> <li>In the Newton-Raphson method itself, if the root is a simple zero of \(g\), the method has quadratic convergence. However, if it is not a simple zero if \(g\) then the method may not have a quadratic convergence.</li> </ul> <p>Can we modify NR to overcome the limitation of multiplicity of the zero?</p> <h2 id="modified-newton-raphson">Modified Newton-Raphson</h2> <p>For a given \(g(x)\), we define a function \(\mu\)</p> \[\mu(x) = \frac{g(x)}{g'(x)}\] <p>If \(x = p\) is a xero of \(g\) with multiplicity \(m\), we have</p> \[\mu(x) = (x - p)\frac{q(x)}{mq(x) + (x - p)q'(x)}\] <p>Notice that \(x = p\) is a simple root of \(\mu\). Further, assume \(g, q\) are continuous. Then, if \(g(x)\) has no other zero in a neighborhood of \(x = p\) then \(\mu(x)\) will also not have any other zero in that neighborhood. We can now apply Newton-Raphson method to \(\mu(x)\).</p> <p>The fixed point iteration is given by</p> \[\begin{align} f(x) &amp;= x - \frac{\mu(x)}{\mu'(x)} \\ &amp;= x - \frac{g(x)/g'(x)}{(g'(x)^2 - g(xg''(x)))/g'(x)^2} \\ &amp;= x - \frac{g(x)g'(x)}{g'(x)^2 - g(x)g''(x)} \end{align}\] <p>This iteration will converge to \(p\) with at least the quadratic order of convergence. The only theoretical drawback with this method is that we now need to compute \(g’’(x)\) at each step. Computationally, the denominator of the formula involves cancelling two nearly equal terms (\(x = p\) is a root of both \(g, g’\)).</p> <p>Note that if \(x = p\) is a simple zero, the modified Newton-Raphson still bodes well. It’s just that there are a lot more calculations in the modified NR method.</p> <h2 id="an-other-methods">An other methods?</h2> <p>There are many methods other than the 4 we considered so far. Suppose that \(\{p_n\}\) converges to \(p\) linearly. For large enough \(n\), we have \((p_{n + 1} - p)^2 \approx (p_n - p)(p_{n + 2} - p)\) which further gives</p> \[p \approx p_n - \frac{(p_{n + 1} - p_n)^2}{p_{n + 2} - 2p_{n + 1} + p_n} = \hat p_n\] <p>This is called <strong>Aitken’s</strong> \(\mathbf{\Delta^2}\)<strong>-method</strong> of accelerating convergence. So, if we have a sequence \(\{p_n\}\) converging to \(p\) linearly, we can come up with an alternate sequence \(\{\hat p_n\}\) using the original sequence that converges faster.</p> <p>This brings us to the end of the second theme of our course - <em>Equations in one variable</em>.</p> <h1 id="lecture-10">Lecture 10</h1> <p>We begin the third theme of our course - <em>Interpolation</em></p> <p>Polynomials are very well studied functions. They have the form \(P(x) = a_nx^n + \cdots + a_1x + a_0\). Given any continuous function \(f: [a, b] \to \mathbb R\), there exists a polynomial that is as close to the given function as desired. In other words, we can construct a polynomial which exactly matches the function in a finite interval. This is known as <strong><em>Weierstrass approximation theorem</em></strong>. Another reason to prefer polynomials is that the derivatives of polynomials are also polynomials.</p> <h2 id="taylor-polynomials">Taylor polynomials</h2> <p>We can consider the polynomials formed by Taylor’s theorem. However, these polynomials approximate the function only at a single point. The advantage of using these polynomials is that the error between the function and the polynomial can be determined accurately. For ordinary computational purposes it is more efficient to use methods that include information at various points.</p> <h2 id="lagrange-interpolating-polynomials">Lagrange interpolating polynomials</h2> <p>Let \(f\) be a function with \(f(x_0) = y_0\). Is there a polynomial \(P(x)\) with \(P(x_0) = y_0\).</p> <p>The simplest case is \(P(x) = y_0\). If we have two points \(x_0\) and \(x_1\), then we can have \(P(x) = y_0\frac{x - x_1}{x_0 - x_1} + y_1\frac{x - x_0}{x_1 - x_0}\) . We can generalize this for more number of points.</p> <p>Let \(x_0, x_1, \dots, x_n\) be distinct \((n + 1)\)-points and let \(f\) be a function with \(f(x_i) = y_i, \forall i \in [n]\). We want to find a polynomial \(P\) that equals \(f\) at these points. To do this, we first solve \(n + 1\) special problems, where \(y_i = \delta_{i, n + 1}\). We find polynomials \(L_{n, i}\) with</p> \[L_{n, i}(x_j) = \delta_{i, j} = \cases{0 &amp; i $\neq$ j \\ 1 &amp; i = j}\] <p>For a fixed \(i\), \(L_{n, i}(x_j) = 0\) for \(j \neq i\). So \((x - x_j)\) divides \(L_{n, i}(x)\) for each \(j \neq i\). Since the points \(x_i\) are all distinct, we have that the product of all such \((x - x_j)\)‘s divides \(L_{n, i}(x)\). We define \(L_{n, i}(x)\) as</p> \[L_{n, i}(x) = \frac{(x - x_0)\dots(x - x_{i - 1})(x - x_{i + 1})\dots (x - x_n)}{(x_i - x_0)\dots(x_i - x_{i - 1})(x_i - x_{i + 1})\dots(x_i - x_n)}\] <p>then \(L_{n, i}(x_j) = \delta_{i, j}\). Now, \(P\) can be constructed as</p> \[P(x) = y_0L_{n, 0}(x) + \dots + y_nL_{n, n}(x)\] <p>The validity of this polynomial can be checked easily.</p> <h1 id="lecture-11">Lecture 11</h1> <blockquote> <p>What if you take a linear function and use \(n &gt;2\) points for Lagrange interpolation? Will the final function be linear? It should be.</p> <p>For example, in class we considered \(f(1) = 1\) and \(f(2) = 1\). These values gave a constant polynomial.</p> </blockquote> <p>In general, for \((n + 1)\)-points, the interpolating polynomial will have degree <strong>at most</strong> \(n\).</p> <h2 id="uniqueness-of-the-interpolating-polynomial">Uniqueness of the interpolating polynomial</h2> <p>For a given set of \((n + 1)\) points, we can have infinitely many polynomials which interpolate it. However, there exists a <strong>unique polynomial with degree</strong> \(\mathbf {\leq n}\). This result follows from the well-known theorem -</p> <p><strong><em>Theorem.</em></strong> A polynomial of degree \(n\) has at most \(n\) distinct zeroes.</p> <p><strong>Corollary.</strong> A polynomial with degree \(\leq n\) with \((n + 1)\) zeroes is the zero polynomial.</p> <h2 id="error-of-the-interpolating-polynomial">Error of the interpolating polynomial</h2> <p><strong><em>Theorem.</em></strong> Let \(f:[a,b] \to \mathbb R\) be \((n + 1)\)-times continuously differentiable. Let \(P(x)\) be the polynomial interpolating \(f\) at distinct \((n + 1)\) points \(x_0, x_1, \dots, x_n \in [a, b]\). Then, for each \(x \in [a, b]\), there exists \(\xi(x) \in (a, b)\) with</p> \[f(x) = P(x) + \frac{f^{(n + 1)}(\xi(x))}{(n + 1)!}(x - x_0)(x - x_1)\cdots (x - x_n)\] <blockquote> <p>How? Intuition?</p> </blockquote> <p>Using the above theorem, we can calculate the <u>maximum possible value of the absolute error</u> in an interval.</p> <p><strong>Note.</strong> While checking for extreme values in an interval, do not forget to check the value of the function at the edge of the interval!</p> <h1 id="lecture-12">Lecture 12</h1> <p><strong>Note.</strong> The value of \(\xi(x)\) for the error calculation depends on the point \(x\) at which error is being calculated.</p> <h3 id="practical-difficulties-with-lagrange-polynomials">Practical difficulties with Lagrange Polynomials</h3> <p>To use the error form, we need some information about \(f\) in order to find its derivative. However, this is often not the case. Also, the computations of the lower degree interpolating polynomials does not quite help the computations of the higher degree ones. We would like to find a method that helps in computing the interpolating polynomials cumulatively.</p> <h3 id="cumulative-calculation-of-interpolating-polynomials">Cumulative calculation of interpolating polynomials</h3> <p>Let us assume that \(f\) is given on distinct nodes \(x_0, x_1, \dots, x_n\). Now, the constant polynomial for the node \(x_0\) will be \(P_0(x) = f(x_0)\) and that for the node \(x_1\) will be \(Q_0(x) = f(x_1)\). Using Lagrange interpolation, we have</p> \[\begin{align} P_1(x) &amp;= \frac{x - x_1}{x_0 - x_1}f(x_0) + \frac{x - x_0}{x_1 - x_0}f(x_1) \\ &amp;= \frac{(x - x_1)P_0(x) - (x - x_0)Q_0(x)}{(x_0 - x_1)} \end{align}\] <p>Can we generalize this? Let us try to construct the quadratic polynomial. Now, suppose we have \(P_1(x)\) and $Q_1(x)$. The quadratic polynomial for the nodes \(x_0, x_1, x_2\) is given by,</p> \[\begin{align} P_2(x) &amp;= \frac{x - x_2}{x_0 - x_2}\left[\frac{x - x_1}{x_0 - x_1}f(x_0) + \frac{x - x_0}{x_1 - x_0}f(x_1)\right] \\ - &amp;\frac{x - x_0}{x_0 - x_2}\left[\frac{x - x_2}{x_1 - x_2}f(x_1) + \frac{x - x_1}{x_2 - x_1}f(x_2)\right] \\ \\ &amp;= \frac{(x - x_2)P_1(x) - (x - x_0)Q_1(x)}{x_0 - x_2} \end{align}\] <p>We shall see the general formula in the next lecture.</p> <h1 id="lecture-13">Lecture 13</h1> <p>Before we move on to the general formula, let us take the previous calculations one step further. Suppose we had to calculate the cubic polynomial in terms of the quadratic polynomials. The tedious way to do this is to expand each formula and substitute. We also have an easy way to do this. Recall the ‘unique polynomial’ theorem from last week. If we guess the formula of the cubic polynomial using induction, then all we have to do is check the value of the function at the 4 points which define it. If the value matches, then it is the polynomial we are looking for due to uniqueness.</p> <h2 id="nevilles-formula">Neville’s formula</h2> <p>Let \(f\) be defined on \(\{x_0, x_1, \dots, x_n\}\). Choose two distinct nodes \(x_i\) and \(x_j\). Let \(Q_i\) be the polynomial interpolating \(f\) on all nodes except \(x_i\), and let \(Q_j\) be the one interpolating \(f\) on all nodes except \(x_j\). If \(P\) denotes the polynomial interpolating \(f\) on all notes then</p> \[P(x) = \frac{(x - x_j)Q_j(x) - (x - x_i)Q_i(x)}{x_i - x_j}\] <p>In Neville’s formula we can get the interpolating for higher degree from any two polynomials for two subsets of nodes which are obtained by removing a single node. Through such cumulative calculations, we can calculate the interpolating polynomials up to a certain degree until we get the required accuracy. Neville’s method gives the values of the interpolating polynomials at a specific point, without having to compute the polynomials themselves.</p> <h2 id="divided-differences">Divided Differences</h2> <p>Given the function \(f\) on distinct \((n + 1)\) nodes, there is a unique polynomial \(P_n\) interpolating \(f\) on these nodes. We define \(f[x_0, \dots, x_n]\) to be the coefficient of \(x^n\) in \(P_n\). Now, it follows readily that the value of \(f[x_0, \dots, x_n]\) does not depend on the ordering of the nodes \(x_i\). Now, we shall try to get a recurrence formula for the coefficients \(f[x_0, \dots, x_n]\).</p> <p>Let \(P_{n - 1}\) and \(Q_{n - 1}\) be the polynomials interpolating \(f\) on the nodes \(x_0, \dots, x_{n - 1}\) and \(x_1, \dots, x_n\) respectively. We can get \(P_n\) from these two polynomials using Neville’s method. The coefficient of \(x^n\) in \(P_n\) is then</p> \[\frac{\text{coefficient of } x^{n - 1} \text{ in } Q_{n - 1} - \text{coefficient of } x^{n - 1} \text{ in } P_{n - 1}}{x_n - x_0} \\ = \frac{f[x_1, \dots, x_n] - f[x_0, \dots, x_{n - 1}]}{x_n - x_0}\] <p>Also note that for \(i &lt; n, P_n(x_i) = P_{n - 1}(x_i)\). That is, \(P_n - P_{n - 1} = \alpha(x - x_0)\dots (x - x_{n - 1})\) where \(\alpha\) is a real number. Hence, \(f[x_0, \dots, x_n] = \alpha\) and we have</p> \[P_n = P_{n - 1} + (x - x_0)\dots (x - x_{n - 1})f[x_0, \dots, x_n]\] <p>This formula is known as Newton’s finite differences formula.</p> <h1 id="lecture-14">Lecture 14</h1> <p>We have</p> \[P_n - P_{n - 1} = f[x_0, \dots, x_n](x - x_0)\dots (x - x_{n - 1}) \\ \\ f[x_0, \dots, x_n] = \frac{f[x_1, \dots, x_n] - f[x_0, \dots, x_{n - 1}]}{x_n - x_0}\] <p>Since the order of the nodes does not matter, we can traverse the recursion in a forward/backward manner. The forward formula is given by,</p> \[P_n(x) = f(x_0) + f[x_0, x_1](x - x_0) + f[x_0, x_1, x_2](x - x_0)(x - x_1) + \\ \cdots + f[x_0, x_1, \dots, x_n](x - x_0)\cdots (x - x_{n - 1})\] <p>The backward formula simply replaces \(i\) by \(n - i\) for \(i \in [0, \dots, n]\). For clarity, look at the following example.</p> <p><img src="/assets/img/Numerical Analysis/image-20220127150002932.png" alt="image-20220127150002932"/></p> <h3 id="nested-form-of-the-interpolating-polynomial">Nested form of the interpolating polynomial</h3> \[P_n(x) = f(x_0) + (x - x_0)\big[f[x_0, x_1] + (x - x_1)[f[x_0, x_1, x_2] + \\ \cdots + (x - x_{n - 1})f[x_0, \dots, x_n]\big]\] <p>Neste form of the interpolating polynomial is useful for computing the polynomials \(P_n\) effectively.</p> <h3 id="divided-differences-as-a-function">Divided differences as a function</h3> <p>We now give a definition of the divided differences when some of the nodes may be equal to each other. By the Mean Value Theorem, \(f[x_0, x_1] = f’(\xi)\) for some \(\xi\) between \(x_0\) and \(x_1\). In fact, we also have the following theorem,</p> <p><strong><em>Theorem.</em></strong> If \(f\) is \(n\)-times continuously differentiable on \([a, b]\) then</p> \[f[x_0, \dots, x_n] = \frac{f^{(n)}(\xi)}{n!}\] <p>for some \(\xi \in [a, b]\).</p> <p>Since \(f[x_0, x_1] = f'(\xi)\) for some \(\xi\) between \(x_0\) and \(x_1\), we define \(f[x_0, x_0] = f’(x_0) = \lim_{x_1 \to x_0}f[x_0, x_1]\). Similarly, we define \(f[x_0, \dots, x_n]\) in a similar way using limits. For instance,</p> \[f[x_0, x_1, x_0] = \frac{f[x_0, x_1]- f'(x_0)}{x_1 - x_0}\\ f[x_0, x_0, x_0] = f^{(2)}(x_0)/2\] <p>We have thus defined \(f[x_0, \dots, x_n]\) in general. Now, by letting the last \(x_n\) as variable \(x\), we get a function of x: \(f[x_0, \dots, x_{n - 1}, x]\). This function is continuous.</p> \[f[x_0, x] = \begin{cases} \frac{f(x) - f(x_0)}{x - x_0} &amp; x \neq x_0 \\ f'(x_0) &amp; x = x_0 \end{cases}\] <h1 id="ma214-post-midsem-notes">MA214 post-midsem notes</h1> <ul> <li> <p><strong>Composite numerical integration</strong> - Newton-Cotes doesn’t work for large intervals. Therefore, we divide the interval into sub-parts. We are essentially doing a spline sorta thing instead of a higher degree polynomial. \(\begin{align} \int_a^b f(x)dx &amp;= \sum_{j = 1}^{n/2} \left\{\frac{h}{3}[f(x_{2j - 2} + 4f(x_{2j - 1} + f(x_{2j}] - \frac{h^5}{90}f^{(4)}(\xi_j)\right\} \\ &amp;= \frac{h}{3}\left[f(a) + 2\sum_{j = 1}^{n/2 - 1}f(x_{2j}) + 4\sum_{j = 1}^{n/2}f(x_{2j - 1}) + f(b)\right] - \frac{b - a}{180}h^4f^{(4)}(\mu) \end{align}\)</p> </li> <li> <p>Error in composite trapezoidal rule is \(\frac{b - a}{12}h^2f''(\mu)\), and in composite Simpson’s rule is \(\frac{b - a}{180}h^4f''(\mu)\).</p> </li> <li> <p>The round-off error does not depend on the number of calculations in the composite methods. We get \(e(h) \leq hn\epsilon = (b - a)\epsilon\) Therefore, integration is stable.</p> </li> <li> <p><strong>Adaptive Quadrature method</strong> - \(S(a, b) - \frac{h^5}{90}f^{(4)}(\xi) \approx S(a, \frac{a+ b}{2}) + S(\frac{a + b}{2}, b) - \frac{1}{16}\frac{h^5}{90}f^{(4)}(\xi')\) <strong>We will assume \(f^{(4)}(\xi) \approx f^{(4)}(\xi’)\).</strong> Using this assumption, we get that composite Simpson’s rule with \(n = 2\) is <strong>15</strong> times better than normal Simpson’s rule. If one of the subintervals has error more than \(\epsilon/2\), then we divide it even further.</p> <blockquote> <p>Check this properly!</p> </blockquote> </li> <li> <p><strong>Gaussian Quadrature method</strong> -</p> <p>Choose points for interval in an optimal way and not equally spaced. We choose \(x_i\) and \(c_i\) to minimise the error in \(\int_a^bf(x)dx \approx \sum_{i = 1}^nc_if(x_i)\) There are \(2n\) parameters, then the largest class of polynomials is the set of polynomials with degree \(2n - 1\) for the approximation to be exact.</p> <p><strong>Note.</strong> Work with special cases of polynomials like \(1, x, x^2, \dots\) to get the values of the coefficients easily. (because all polynomials in the set must satisfy the approximation)</p> <p><strong>Legendre polynomials</strong> - There exist polynomials \(\{P_n(x)\}\) for \(n = 0, 1, \dots\)satisfying</p> <ul> <li>\(P_n(x)\) is a monic polynomials</li> <li>\(\int_{-1}^1P(x)P_n(x) = 0\) whenever the degree of \(P(x)\) is less than \(n\).</li> </ul> <p>For example, \(P_0(x) = 1, P_1(x) = x\). \(P_2\) can be computed from \(P_0\) and \(P_1\) as \(\int P_0P_2\) and \(\int P_1P_2\) are 0.</p> <p><img src="/assets/img/Numerical Analysis/image-20220323155057163.png" alt="image-20220323155057163"/></p> </li> <li> <p><strong>Multidimensional integrals</strong> - Composite trapezoidal rule has square of the number of function evaluations required for a single integral (for 2D).</p> <p><img src="/assets/img/Numerical Analysis/image-20220323161302337.png" alt="image-20220323161302337"/></p> <blockquote> <p>See if problems to be practiced here.</p> </blockquote> </li> <li> <p><strong>Improper integrals</strong> - Function is unbounded or the interval is unbounded. We will deal with functions where the function is unbounded on the left end.</p> </li> <li> <blockquote> <p><strong><em>Aside.</em></strong> \(a^b\) is defined as \(\exp(b \log a)\) in the complex domain. \(\log\) is not defined as the inverse of \(\exp\) as \(\exp\) is neither surjective (doesn’t take the value 0) nor injective (periodic in \(\mathbb C\)). \(\exp\) is defined by the power series and so is \(\log\). The solution set of \(z\) for \(e^z = w\) where \(z, w \in \mathbb C\) is given by \(\{\log |w| + \iota(\arg w + 2\pi k) : k \in \mathbb Z\}\)</p> </blockquote> <p>If \(f(x) = \frac{g(x)}{(x - a)^p}\) where \(0 &lt; p&lt; 1\) and \(g: [a,b] \to \mathbb R\) is continuous then the improper integral \(\int_a^b f(x)dx\) exists. Assume \(g\) is 5-times continuously differentiable. We can estimate the integral of \(f(x)\) using the following</p> <ul> <li>Get \(P_4(x)\) which is the 4th degree Taylor’s polynomial of \(g\).</li> <li>Get the <strong>exact</strong> value of \(\int_a^b P_4(x)/(x - a)^p\).</li> <li>Get the value of the difference by defining the value at \(z = a\) as \(0\) using composite Simpson’s rule.</li> </ul> <blockquote> <p>Why can’t we do Simpson’s on everything? That would lead to a similar thing. <em>Think</em>. Also, we require 4 times continuously differentiable for Simpson’s</p> </blockquote> <p>The other type of improper integral involves infinite limits of integration. The basic integral is of the type \(\int_a^\infty 1/x^p dx\) for \(p &gt; 1\). Then, we substitute \(x = 1/t\) and proceed.</p> </li> <li> <p><strong><em>Ordinary Differential Equations</em></strong> We shall develop numerical methods to get solutions to ODEs at a given point. Then, we can use interpolation to get an approximate continuous solution.</p> <p>A function \(f(t, y)\) is said to satisfy a <strong>Lipschitz condition</strong> in the variable \(y\) on a set \(D \subset \mathbb R^2\) if a constant \(L &gt; 0\) exists with \(\mid f(t, y_1) - f(t, y_2) \mid \leq L\mid y_1 - y_2 \mid\) <img src="/assets/img/Numerical Analysis/image-20220323191549861.png" alt="image-20220323191549861"/></p> <p>IVP is well-posed if it has a unique solutions, and an IVP obtained by small perturbations also has a unique solution. We consider IVPs of the form \(dy/dt = f(t, y)\), \(a \leq t \leq b\), \(y(a) = \alpha\).</p> <p><img src="/assets/img/Numerical Analysis/image-20220323193242124.png" alt="image-20220323193242124"/></p> </li> <li> <p><strong>Euler’s method</strong> - We generate <em>mesh points</em> and interpolate. As we are considering IVPs of a certain form, we can just use 1st degree Taylor’s polynomial to approximate the solution to the IVP. We take \(q_0 = \alpha\) and \(w_{i + 1} = w_i + hf(t_i, w_i)\) for \(i \geq 0\) where \(w_i \approx y(t_i)\). The error grows as \(t\) increases, but it is controlled due to the stability of Euler’s method. It grows in a linear manner wrt to \(h\).</p> </li> <li> <p><u>Error in Euler’s method</u> - Suppose \(\mid y''(t) \mid \leq M\), then \(\mid y(t_i) - w_i \mid \leq \frac{hM}{2L}(\exp (L(t_i - a)) - 1)\). What about round-off errors? We’ll get an additional factor of \(\delta/hL\) in the above expression along with the constant \(\delta_0 \exp (L(t_i - a))\). Therefore, \(h = \sqrt(2\delta/M)\).</p> </li> <li> <p><strong>Local truncation error</strong> - <strong>\(\tau_{i + 1}(h) = \frac{y_{i + 1} - y_i}{h} - \phi(t_i, y_i)\).</strong> It is just \(hM/2\) for Euler’s method (\(\phi\) refers to the Taylor polynomial). We want truncation error to be as \(\mathcal O(h^p)\) for as large \(p\) as possible.</p> <p><strong>Higher order Taylor methods</strong> - Assume \(f\) is \(n\)-times continuously differentiable. We get \(\mathcal O(n)\) for \(n\)th degree Taylor polynomial. However, the number of computations are a bit high.</p> <blockquote> <p>Practice problems on this</p> </blockquote> </li> <li> <p>What about interpolation? We should use cubic Hermite interpolation (to match the derivative too).</p> </li> <li> <p>Now, we try to reduce the computation of higher order derivatives. <strong>Runge-Kutta methods</strong> - Based off Taylor’s theorem in two variables.</p> <p><img src="/assets/img/Numerical Analysis/image-20220323210229698.png" alt="image-20220323210229698"/></p> <p>Order 2- We get \(a = 1, \alpha = h/2, \beta = f(t, y)h/2\) by equating \(af(t + \alpha, y + \beta)\) to \(T(t, y) = f(t, y) + h/2 f'(t, y)\). This specific Runge-Kutta method of Order 2 is known as the <strong>midpoint-method</strong>. (2D of Taylor order 2) \(w_{i + 1} = w_i + hf\left(t_i + \frac{h}{2}, w_i + \frac h 2 f(t_i, w_i)\right)\) The number of nesting \(f\)‘s represents the order of the differential equation.</p> <p>Suppose we try the form \(a_1f(t, y) + a_2 f(t + \alpha_2, y + \delta_2 f(t, y))\) containing 4 parameters to approximate. We still get \(\mathcal O(n^2)\) as there is only one nesting.</p> </li> <li> <p>However, the flexibility in the parameters allows us to derive the <strong>Modified Euler method</strong>. \(w_{i + 1} = w_i + \frac h 2[f(t_i, w_i) + f(t_{i + 1}, w_i + hf(t_i, w_i))]\) <strong>Higher-order Runge-Kutta methods</strong> - The parameter values are used in the <strong>Heun’s method</strong>.</p> <p><img src="/assets/img/Numerical Analysis/image-20220323212528107.png" alt="image-20220323212528107"/></p> <p>The most common Runge-Kutta is order 4 whose local truncation error is \(\mathcal O(n^4)\).</p> <p><img src="/assets/img/Numerical Analysis/image-20220323212907583.png" alt="image-20220323212907583"/></p> </li> <li> <p><u>Error control in Runge-Kutta methods</u>. Adaptive step size for lower error. Single step approximation - uses \(i-\) for \(i\). Given an \(\epsilon &gt; 0\), we need to be able to give a method that gives \(\mid y(t_i) - w_i\mid &lt; \epsilon\). \(y(t_{i + 1}) = y(t_i) + h\phi(t_i, y(t_i), h) + \mathcal O(h^{n + 1}) \\\) Local truncation error assumes \(i\)th measurement is correct to find error in the \(i + 1\)th measurement. We get \(\tau_{i + 1}(h) \approx \frac 1 h (y(t_{i + 1}) - w_{i + 1})\) assuming \(y_i \approx w_i\). For \(n\)th degree truncation error, we get \(\tau_{i + 1}(h) \approx \frac 1 h (w^{n + 1}_{i + 1} - w^{n}_{i + 1})\). After a few approximations, we get that the local truncation error changes by a factor of \(q^n\) when the step size changes by a factor of \(q\).</p> </li> <li> <p><strong>Runge-Kutta-Fehlberg Method</strong> - It uses a Runge-Kutta method with local truncation error of order five. We change the step size if \(q &lt; 1\). These methods are just an analogue of adaptive quadrature methods of integrals.</p> </li> <li> <p><strong>Multi-step methods</strong>. Methods that use the approximation at more than one previous mesh point to determine the approximation at the next point. The general equation is implicit where \(w_{i + 1}\) occurs on both sides of the equation. Implicit methods are more accurate than explicit methods.</p> <p><img src="/assets/img/Numerical Analysis/image-20220324124220485.png" alt="image-20220324124220485"/></p> </li> <li> <p><strong>Predictor-Corrector Method</strong> - How do we solve implicit methods? We can use the root-finding procedures we learnt. All of this can get quite cumbersome. We just use implicit methods to improve the prediction of the explicit methods. We insert the solution of explicit method (prediction) and insert it on the rhs of the implicit method (correction).</p> </li> <li> <p><strong>Consistency and Convergence</strong></p> <p>One-step difference method is <strong>consistent</strong> if \(\lim_{h \to 0}\max_{1 \leq i \leq n}\mid \tau_i(h)\mid = 0\). But we also need the global measure - <strong>convergence</strong> - \(\lim_{h \to 0}\max_{1 \leq i \leq n}\mid y_i(t_i) - w_i\mid = 0\)</p> <p><strong>stability</strong> considering round-off errors. For the function \(\phi\) satisfying Lipschitz condition with a \(h_0\), the one-step difference method <strong>is convergent iff it is consistent</strong>. The local truncation error is bounded, and we get \(\mid y(t_i) - q_i \mid \leq \frac {\tau(h)}{L}e^{L(t_i - a)}\).</p> <p>Analysis of consistency, convergence, and stability is difficult for multi-step methods. Adams-* methods are stable.</p> </li> <li> <p><strong><em>Numerical Linear Algebra</em></strong> - Basics - \(Ax = b\) has a unique solution iff \(A\) is invertible, and it does not have a unique or has no solution otherwise.</p> <p><strong>Cramer’s rule</strong> - \(x_j = \frac{\det A_j}{\det A}\). However, this is cumbersome. Determinant of an upper triangular matrix is product of the diagonal entries.</p> <p><strong>Gaussian Elimination Method</strong> - \(A = LU\) for most matrices \(A\). \(L\) is a lower triangular matrix and \(U\) is an upper triangular matrix. Form the augmented matrix \([ A \mid b]\). Linear combination of rows and swapping of rows can be performed. What is the total number of arithmetic operations?</p> <p>For converting to triangular - We use \((n - i)\) divisions for each row and \((n - i + 1)\) multiplications for each column of each row. Also, we have \((n - i)(n - i + 1)\) subtractions. In total, we have \((n - i)(n - i + 2)\) multiplications. Summing, we get \(\mathcal O(n^3)\) multiplications (\((2n^3 + 3n^2 - 5n)/6\)) and subtractions \(((n^3 - n)/3)\).</p> <p>For back substitution - multiplication is \((n^2 + n)/2\) and subtraction is \((n^2 - n)/2\).</p> </li> <li> <p>We have not considered finite digit arithmetic for GEM previously. The error dominates the solution when the pivot has a low absolute value. In general, we need to ensure that the pivot does not have very low magnitude by interchanging rows (followed by interchanging columns for triangular form if needed) - <strong>partial pivoting</strong>. However, this might not be enough to get rid of the rounding error. Therefore, we need to consider <strong>scaled partial pivoting</strong>. Define \(s_i\) as the maximum magnitude in the \(i\)th row. Now, the first pivot is chosen by taking the row with the maximum value of \(a_{i1}/s_i\). The operation count order still remains the same. You need not calculate scale factors more than once.</p> </li> <li> <p><strong>LU Decomposition</strong> - The conversion of \(A\) to a triangular form using the above method can be represented as a sequence of matrix multiplications (if \(A\) does not require any row interchanges). The inverse of a matrix depicting operations on \(A\) can be seen a matrix depicting the same inverse operations on \(A\). In the end, we get \(A = (L^{(1)} \dots L^{(n - 1)})(M^{(1)}\dots M^{(n-1)}A)\) where each \(M^{(i)}\) represents the action that uses \(A_{ii}\) as a pivot. Once we get \(L\) and \(U\), we can solve \(y = Ux\) and \(Ly = b\) separately. There are multiple decompositions possible which are eliminated by imposing conditions on the triangular matrices. One such condition is setting \(L_{ii} = U_{ii}\) which is known as <em>Cholesky Decomposition</em>.</p> <p>We had assumed that row interchanges are not allowed. However, we can build <strong>permutation matrices</strong> for row interchanges which will be of the form of an identity matrix with row permutations.</p> <p>Therefore, we get <strong>PLU decomposition</strong>.</p> </li> <li> <p><strong>Diagonally dominant matrices</strong> - An \(n \times n\) matrix \(A\) is said to be diagonally dominant when \(\mid a_{ii} \mid \geq \sum^n \mid a_{ij} \mid\) for all rows. <u>A strongly diagonally dominant matrix is invertible</u>. Such matrices will not need row interchanges, and the computations will be stable wrt the round off errors.</p> <blockquote> <p>Why this instead of \(\mid a_{ii} \mid \geq \max \mid a_{ij} \mid\)</p> </blockquote> <p>A matrix \(A\) is <strong>positive definite</strong> if \(x^tAx &gt; 0\) for every \(x \neq 0\). We shall also consider \(A\) to be symmetric in the definition. Every positive definite matrix is invertible, \(A_{ii} &gt; 0\) for each \(i\), \((A_{ij})^2 &lt; A_{ii}A_{jj}\), and \(\max_{1\leq k, j \leq n} \mid A_{kj} \mid \leq \max_{1 \leq i \leq n}\mid A_{ii}\mid\).</p> <p>A <strong>leading principal submatrix</strong> of matrix \(A\) is the top-left \(k \times k\) submatrix of \(A\). \(A\) <u>is positive definite iff each leading principal submatrix of</u> \(A\) <u> has a positive determinant.</u></p> <p>Gaussian Elimination on a symmetric matrix can be applied without interchanging columns iff the matrix is positive definite.</p> <p>A matrix \(A\) is positive definite iff \(A = LDL^t\) where \(L\) is lower triangular with 1’s on the diagonal and \(D\) is a diagonal matrix with positive diagonal entries. Alternatively, \(A\) is positive dfinite iff \(A = LL^t\) where \(L\) is a lower triangular matrix. <strong>Note.</strong> Positive definite is stronger than Cholesky decomposition.</p> </li> <li> <p>We have been seeing direct methods for solving \(Ax = b\). We shall see some iterative methods now. We need a distance metric to check the closeness of the approximation. We will consider \(l_2\) distance = \(\| x - y\|_2 = \left( \sum_{i = 1}^n (x_i - y_i)^2 \right)^{1/2}\) and \(l_\infty\) distance = \(\| x - y\|_2 = \max_{i = 1}^n \mid x_i - y_i \mid\). Also, \(\|x - y\|_\infty \leq \|x - y\|_2 \leq \sqrt n\|x - y\|_\infty\). We also need to consider distances in matrices.</p> </li> <li> <p><strong>Distances in Matrices</strong> - \(\|A\|_2 = \max_{\|x\|_2 = 1} \|Ax\|_2\) and \(\|A\|_\infty = \max_{\|x\|_\infty = 1} \|Ax\|_\infty\). The \(l_\infty\) can be directly calculated using \(\|A\|_\infty = ]max_{i} \sum_j \mid A_{ij} \mid\).</p> </li> <li> <p><strong>eigenvalues, eigenvectors</strong> - A <strong>non-zero</strong> vector \(v \in \mathbb R^n\) is an eigenvector for \(A\) if there is a \(\lambda \in \mathbb R\) such that \(Av = \lambda v\) and \(\lambda \in \mathbb R\) is the eigenvalue. The <strong>characteristic polynomial</strong> of \(A\) is \(\det(A - \lambda I)\). We do not consider the complex roots that are not real for these polynomials to calculate eigenvalues.</p> <p><strong>Spectral radius</strong> - \(\rho(A) = \max \mid \lambda \mid\). Then, we have the relation that \(\|A\|_2 = [\rho(A^tA)]^{1/2}\) and also \(\rho(A) \leq \|A\|_2\) and \(\rho(A) \leq \|A\|_\infty\).</p> <p><strong>Convergent matrices</strong> - It is of particular importance to know when powers of a matrix become small, that is, when all the entries approach zero. An \(n \times n\) matrix \(A\) is called convergent if for each \(1 \leq i, j \leq n\), \(\lim_{k \to \infty}(A^k)_{ij} = 0\).</p> <ul> <li>\(A\) is a convergent matrix</li> <li> \[\lim_{n \to \infty} \|A^n\|_2 = 0\] </li> <li> \[\lim_{n \to \infty} \|A^n\|_\infty = 0\] </li> <li> \[\rho(A) &lt; 1\] </li> <li>\(\lim_{n \to \infty} A^nv =0\) for every vector \(v\).</li> </ul> <p>The above statements are all equivalent.</p> </li> <li> <p>Iterative techniques are not often used for smaller dimensions. We will study <strong>Jacobi</strong> and the <strong>Gauss-Seidel</strong> method.</p> <p><strong>Jacobi Method</strong> - We assume that \(\det(A)\) being non-zero (as matrix must be invertible for solution) and the diagonal entries of \(A\) are also non-zero. We have</p> <p>Jacobi suggested that we start with an initial vector \(x^{(0)} = [x_1^{(0)}, \dots, x_n^{(0)}]\) and for \(k \geq 1\)</p> \[x_i^{(k)} = \frac{b_i - \sum_{j \neq i} a_{ij}x_j^{(k - 1)}}{a_{ii}}\] <p>The error in the iterations is given by</p> \[\mid x_i - x_i^{(k)} \mid \leq (\sum_{j \neq i} \frac{a_{ij}}{a_{ii}})\|x_j - x_j^{(k - 1)}\|_\infty\] <p>which gives</p> \[\| x - x^{(k)} \|_\infty \leq (\max_i \sum_{j \neq i} \frac{a_{ij}}{a_{ii}})\|x_j - x_j^{(k - 1)}\|_\infty\] <p>If \(\mu = \max{i}\sum_{j \neq i} \frac{a_{ij}}{a_{ii}} &lt; 1\), then convergence is guaranteed. If \(\mu &lt; 1\), then the condition is nothing but that of strictly diagonally dominant matrices.</p> <p><strong>Gauss-Seidel method</strong> - The idea is that once we have improved one component, we use it to improve the component of the next component and so on.</p> \[x_i^{(k)} = \frac{1}{a_{ii}} \left[b_i - \sum_{j = 1}^{i - 1}a_{ij}x_j^{(k)} - \sum_{j = i + 1}^n a_{ij}x_j^{(k - 1)}\right]\] <p>There are linear systems where Jacobi method converges but Gauss-Seidel method does not converge. If \(A\) is strictly diagonally dominant, then both methods converge to the true solution.</p> </li> <li> <p><strong>Residual vector</strong> - If \(\tilde x\) is an approximation to \(Ax = b\), then \(r = b - A\tilde x\) is the residual vector. However, it is not always true that when \(\|r \|\) is small then \(\|x - \tilde x\|\) is also small. This is because \(r = A(x - \tilde x)\), and that represents the affine transformation of space. This phenomenon is captured as follows</p> <p>For a non-singular \(A\), we have</p> \[\|x - \tilde x\|_\infty \leq \|r \|_\infty \cdot \|A^{-1}\|_\infty\] <p>If \(x \neq 0\) and \(b \neq 0\)</p> \[\frac{\|x - \tilde x \|_\infty}{\|x\|_\infty} \leq \|A\|_\infty\cdot \|A^{-1}\|_\infty \cdot \frac{\|r\|_\infty}{\|b\|_\infty}\] <p>These relations work for \(l_2\) norm too.</p> <blockquote> <p>I think \(\|A\|_\infty \|x\|_\infty \geq \|b\|_\infty\)</p> </blockquote> <p>The <strong>condition number</strong> of a non-singular matrix \(A\) is</p> \[K(A) = \|A\|_\infty \cdot \|A^{-1}\|_\infty\] <p>Also, \(\|AA^{-1}\|_\infty \leq \|A\|_\infty \|A^{-1}\|_\infty\). A non-singular matrix \(A\) is said to be <strong>well-conditioned</strong> if \(K(A)\) is close to 1.</p> <p>However, the condition number depends on the round-off errors too. The effects of finite-digit arithmetic show up in the calculation of the inverse. As the calculation of inverse is tedious, we try to calculate the condition number without the inverse. If we consider \(t\)-digit arithmetic, we approximately have</p> \[\|r\|_\infty \approx 10^{-t}\|A\|_\infty \cdot \|\tilde x \|_\infty\] <p>One drawback is that we would have to calculate \(r\) is double precision due to the above relation. The approximation for \(K(A)\) comes from \(Ay = r\). Now, \(\tilde y \approx A^{-1}r = x - \tilde x\). Then,</p> \[\|\tilde y\| \leq \|A^{-1}\| \cdot (10^{-t}\cdot \|A\| \|\tilde x\|)\] <p>Using the above expression, we get</p> \[K(A) \approx \frac{\|\tilde y\|}{\|\tilde x\|}10^t\] <p>The only catch in the above method is that we need to calculate \(r\) in \(2t\)-finite arithmetic.</p> <p><strong>Iterative refinement</strong> - As we had defined \(\tilde y = x - \tilde x\), in general, \(\tilde x + \tilde y\) is more accurate. This is called as iterative improvement. If the process is applied using \(t\)-digit arithmetic and if \(K(A) \approx 10^q\), then after \(k\) iterations, we have approximately \(\min(t, k(t - q))\) correct digits. When \(q&gt; t\), increased precision must be used.</p> </li> <li> <p><strong>Approximations for eigenvalues</strong></p> <p><strong>Gerschgorin theorem</strong> - We define discs \(D_i = \left\{ z \in \mathbb C: \mid z - a_{ii} \mid \leq \sum_{j \neq i} \mid A_{ij} \mid \right\}\). Then, all eigenvalues of \(A\) are contained in the union of all the disks \(D_i\). The union of any \(k\) of the disk that do not intersect the remaining \(n - k\) disks contains precisely \(k\) of the eigenvalues including the multiplicity. From this theorem, we get that strictly diagonally dominant matrices are invertible. This is also true for strictly diagonally column dominant matrices.</p> <p>The above theorem provides us the initial approximations for the eigenvalues. We shall see the Power method.</p> <p><strong>Power method</strong> - We assume \(\mid \lambda_1 \mid &gt; \mid \lambda_2 \mid \geq \dots \geq \mid \lambda_n \mid\), and that \(A\) has \(n\) linearly independent eigenvectors. Choose a non-zero \(z \in V\), and compute</p> \[\lim_{k \to \infty} A^k z\] <p>to get the eigenvalue! (Think of vector space transformations geometrically).</p> <p>If \(z = \sum \alpha_i v_i\), we get</p> \[A^k z = \lambda^k_1 \alpha_1 v_1 + \dots + \lambda^k_n \alpha_n v_n = \lambda_1^k \alpha_1 v_1\] <p>for high values of \(k\).</p> <p>Sometimes, \(z\) may not have the component of \(v_1\). We choose a vector such that this is not the case.</p> <p>Sometimes, it may also be the case that</p> <p>\(\mid \lambda_1 \mid \geq \mid \lambda_2 \mid \geq \dots &gt; \mid \lambda_n \mid &gt; 0\). \(A\) is invertible iff this holds. Then, we use the power method on \(A^{-1}\).</p> <p>Sometimes, \(\mid \lambda_1 \mid &lt; 1\) and we’ll converge to 0. On the other hand, if it is more than 1, the limit will shoot to infinity. To take care of these, we scale \(A^k(z)\), so that it is finite and non-zero.</p> <p>Firstly, we choose \(z\) such that \(\|z^{(0)}\| = 1\) and we choose a component \(p_0\) of \(z^{(0)}\) such that \(\mid z_{p_0}^{(0)} \mid = 1\). Following this, we scale each subsequent value as follows - Let \(w^{(1)} = Az^{(0)}\) and \(\mu^{(1)} = w_{p_0}^{(1)}\).</p> \[\mu^{(1)} = \lambda_1 \frac{\alpha_1(v_1)_{p_0} + \dots + (\lambda_n/ \lambda_1) \alpha_n (v_n)_{p_0}}{\alpha_1(v_1)_{p_0} + \dots + \alpha_n (v_n)_{p_0}}\] <p>Now, we choose \(p_1\) to be the least integer with \(\mid w_{p_1}^{(1)}\mid = \|w^{(1)}\|\) and define \(z^{(1)}\) by</p> \[z^{(1)} = \frac{1}{w_{p_1}^{(1)}} Az^{(0)}\] <p>Then, in general,</p> \[\mu^{(m)} = w_{p_{m - 1}}^{(m)} = \lambda_1 \frac{\alpha_1(v_1)_{p_0} + \dots + (\lambda_n/ \lambda_1)^m \alpha_n (v_n)_{p_0}}{\alpha_1(v_1)_{p_0} + \dots + (\lambda_n/ \lambda_1)^{m - 1}\alpha_n (v_n)_{p_0}}\] <p>Now, \(\lim_{m \to \infty} \mu^{(m)} = \lambda_1\).</p> <p>To find other eigenvalues, we use <strong>Gram-Schmidt orthonormalisation</strong>.</p> <p><img src="/assets/img/Numerical Analysis/image-20220415200748597.png" alt="image-20220415200748597"/></p> </li> </ul> <hr/> <h4 id="end-of-course">END OF COURSE</h4> <hr/>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[A course disucssing interpolation theory, numerical intergration, numerical solutions to ordinary differential equations, numerical solutions to system of linear equations and roots of non-linear equations.]]></summary></entry><entry><title type="html">Automata Notes</title><link href="https://sudhansh6.github.io/blog/automata/" rel="alternate" type="text/html" title="Automata Notes"/><published>2022-01-06T00:00:00+00:00</published><updated>2022-01-06T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/automata</id><content type="html" xml:base="https://sudhansh6.github.io/blog/automata/"><![CDATA[<h1 id="lecture-1">Lecture 1</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">03-01-22</code></p> </blockquote> <h2 id="overview">Overview</h2> <p>Consider the problem of determining whether a given multivariate polynomial with integer coefficients has integer roots. This problem is <em>undecidable</em> - we cannot write a deterministic algorithm which halts in finite time that always gives the correct answer for a given polynomial. We will explore various techniques and theorems through the course to answer questions like these.</p> <p><strong><em>Example.</em></strong> Consider the language \(L_1 = \{a^nb^m: n,m \geq 0\}\). Consider the problem of deciding whether a word is present in this language or not. We can write such a program using a DFA of the form</p> <p><strong><em>Example.</em></strong> Suppose we have \(L_2 = \{a^nb^n: n \geq 0\}\). Can we construct a DFA for the same?</p> <p>It can be shown that such a language cannot be represented using a DFA. Instead, we use an instrument known as <strong><em>pushdown automaton</em></strong>.</p> <p>A <em>pushdown automaton</em> has a stack associated with a DFA. Every transition in the automaton describes an operation such as “push” and “pop” on the stack. A string is accepted by the automaton if the stack is empty at the end of the string. The languages accepted by such automatons are known as <strong><em>context-free grammar</em></strong>.</p> <p><strong><em>Example.</em></strong> Extending the previous example, consider the language \(L_3 = \{a^nb^nc^n:n \geq 0\}\). Turns out, a pushdown automaton cannot represent this language.</p> <p>We have a <strong><em>Turing machine</em></strong> that represents the ultimate computer that can perform any computation (not all). This machine has a ‘tape’ associated with it along with different decisions at each section of the tape. The languages associated with these machines are known as <strong><em>unrestricted grammar</em></strong>.</p> <p>These machines and the associated languages can be represented using a diagram known as <a href="#chomsky-hierarchy">Chomsky hierarchy</a>. We will also prove that adding non-determinism affects the expressive power of PDAs but not of DFAs and TMs.</p> <h1 id="lecture-2">Lecture 2</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">04-01-22</code></p> </blockquote> <p><strong><em>Definition.</em></strong> A <em>finite state automaton</em> is defined as a tuple \((Q, \Sigma, \delta, q_0, F)\), where</p> <ul> <li>\(Q\) is a finite non-empty set of states,</li> <li>\(\Sigma\) is the alphabet with a set of symbols,</li> <li>\(\delta: Q \times \delta \to Q\) is the state-transition function,</li> <li>\(q_0 \in Q\) is the initial state, and</li> <li>\(F \subseteq Q\) is the set of accepting states.</li> </ul> <p><strong><em>Example.</em></strong> Let us consider the language \(L_2 = \{a^nb^n:n\geq 0\}\) we saw before. How do we write the <em>context-free grammar</em> for this language? We write a set of <u>base cases and inductive rules</u> as follows -</p> <div style="text-align:center;"> $$ \begin{align} S &amp;\to \epsilon \\ S &amp;\to aSb \end{align} $$ </div> <p>Typically, we use \(S \to \epsilon\) as the base case. We start out with the string \(S\), and then use the above rules to keep replacing the \(S\) until we obtain a string consisting only of <em>terminals</em>. Here, the symbols \(a, b,\) and \(\epsilon\) are terminals whereas \(S\) is a non-terminal.</p> <p><strong><em>Example.</em></strong> Consider the grammar of matched parentheses. This is given by -</p> <div style="text-align:center;"> $$ \begin{align} S &amp;\to ()\\ S &amp;\to (S)\\ S &amp;\to SS \end{align} $$ </div> <p><strong><em>Definition.</em></strong> A <em>context-free grammar</em> is defined as a tuple \((V, \Sigma, R, S)\), where</p> <ul> <li>\(V\) is a set of <em>non-terminals</em> or <em>variables</em>,</li> <li>\(\Sigma\) is the alphabet,</li> <li>\(R: V \to (V \cup \Sigma)^*\) is the finite set of <em>rules</em>, and</li> <li>\(S \in V\) is the <em>start</em> symbol.</li> </ul> <p>The language of a given grammar is the set of all strings derivable using the rules.</p> <p><strong><em>Example.</em></strong> Consider the language \(\{a^nb^nc^n: n &gt; 0\}\). This can be represented by the unrestricted grammar as -</p> <div style="text-align:center;"> $$ \begin{align} S &amp;\to abc \\ S &amp;\to aAbc \\ Ab &amp;\to bA \\ Ac &amp;\to Bbcc \\ bB &amp;\to Bb \\ aB &amp;\to aa \\ aB &amp;\to aaA \end{align} $$ </div> <p>These set of rules are very similar to CFG except for the fact that, now, we have strings on the LHS too. Notice how \(A, B\) are used to convey information across the string when new \(a\)’s or \(b\)’s are added.</p> <blockquote> <p><strong><em>Homework.</em></strong> Write the unrestricted grammar rules for the language \(L = \{a^{n^2} : n \in \Z^+\}\)</p> </blockquote> <p><strong><em>Definition.</em></strong> An <em>unrestricted grammar</em> is defined as a tuple \((V, \Sigma, R, S)\), where</p> <ul> <li>\(V\) is a set of <em>non-terminals</em> or <em>variables</em>,</li> <li>\(\Sigma\) is the alphabet,</li> <li>\(R: (V \cup \Sigma)^* \to (V \cup \Sigma)^*\) is the finite set of <em>rules</em>, and</li> <li>\(S \in V\) is the <em>start</em> symbol.</li> </ul> <p><strong><em>Definition.</em></strong> <em>Regular expressions</em> are defined by the following set of rules -</p> <ol> <li> <p>\(\phi, \{\epsilon\}, \{a\}\) (for any \(a \in \Sigma\)) are regular expressions.</p> </li> <li> <p>If \(E_1, E_2\) are regular expressions,</p> <ol> <li>\(E_1 + E_2\) (union),</li> <li>\(E_1E_2\) (concatenation)</li> <li>\(E_1^*\) (<strong>Kleene star</strong>), and</li> <li>\((E_1)\) (parenthesis)</li> </ol> <p>are all regular expressions.</p> </li> </ol> <p><strong><em>Example.</em></strong> Consider \(L = \{\text{strings with even number of a's}\}\). This can be represented using the regular expression \(b^*(ab^*ab^*)^*\).</p> <blockquote> <p><strong><em>Homework.</em></strong> Suppose \(L\) is restricted to have only an odd number of \(b\)’s. How do we write the regular expression for this language?</p> </blockquote> <p>In general, PDAs are represented as a Finite State Machine. That is, we have an action associated with each transition. An empty stack is denoted using the symbol \(Z_0\). That is, the stack begins with a single symbol \(Z_0\). Each transition is represented as \(l, A\), where \(l\) is a letter and \(A\) is an action such as</p> <ul> <li>\(X \vert aX\) - push</li> <li>\(aX \vert X\) - pop</li> <li>\(W\vert Wa\) - not sure what this is</li> </ul> <p>Now, a string is rejected by the FSM in two scenarios -</p> <ol> <li>There is no transition defined at the current state for the current symbol in the string, and</li> <li>The stack is not empty, i.e. popping the stack does not yield \(Z_0\) at the end of the string input.</li> </ol> <h1 id="lecture-3">Lecture 3</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">06-01-22</code></p> </blockquote> <h2 id="chomsky-hierarchy">Chomsky Hierarchy</h2> <p><img src="/assets/img/Automata\image-20220106090455841.png" alt="image-20220106090455841"/></p> <h2 id="regular-expressions">Regular Expressions</h2> <p>We define these languages using a base case and an inductive rule. <strong>Empty string</strong> is not the same as the <strong>empty set</strong>. The base case to be considered is \(L = \{\epsilon\}\)?</p> <h3 id="inductive-rules">Inductive Rules</h3> <p><strong>Lemma.</strong> If \(E_1, E_2\) are regular expressions, then so are</p> <ul> <li>\(E_1 + E_2\) - Union</li> <li>\(E_1.E_2\) - Concatenation</li> <li>\(E_1^*\) - Kleene star</li> <li>\((E_1)\) - Parentheses</li> </ul> <p>“Nothing else” is a regular expression. That is, we must use the four rules mentioned above to construct a regular expression.</p> <p>The fourth rule can be used as follows - \(L((ab)^*) = \{\epsilon, ab, abab, ababab, ...\}\). The parentheses help us group letters from the alphabet to define the language.</p> <p><strong><em>Example.</em></strong> Construct a language with <u>only even number of a's'</u>. Good strings include \(\{aba, baabaa\}\), and bad strings include \(\{abb, bbabaa\}\).</p> <p>The automata can be easily drawn as -</p> <p><img src="/assets/img/Automata\image-20220106085034344.png" alt="image-20220106085034344"/></p> <p>How do we write a regular expression for this? Consider the expression \(R = (ab^*ab^*)^*\). However this expression does not include strings that start with \(b\).</p> <blockquote> <p><strong><em>Homework.</em></strong> Try and fix this expression -</p> \[R = b^*.(ab^*ab^*)^*\] </blockquote> <p><strong><em>Example.</em></strong> Construct an expression for defining the language \(L = \{\text{all strings with even number of a's} \text{and odd number of b's}\}\)</p> <p>The following automaton would work for this language -</p> <p><img src="/assets/img/Automata\image-20220106085649521.png" alt="image-20220106085649521"/></p> <blockquote> <p><strong><em>Homework</em></strong>. Find a regular expression for the above language.</p> </blockquote> <p><strong><em>Example.</em></strong> What language does the regular expression \(b^*ab^*(ab^*ab^*)^*\)?</p> <p>It represents the language with an <u>odd</u> number of \(a\)’s. How do we check this? Start with the base cases - It has \(\epsilon\), and it also has \(\{ab, ba\}\). Try to check the pattern and use induction.</p> <p>We will soon learn how to derive relations such as “Is \(L = \phi\)?”, “Is \(\|L\| = \infty\)?”, “Is \(L_1 \subset L_2\)?”… All we are doing right now is exploring all the topics in the course using a BFS approach.</p> <h2 id="representation-of-push-down-automata">Representation of Push-Down Automata</h2> <p>The bottom of the stack contains a special character that indicates the bottom of the stack. We design a Finite State Machine which knows the special characters (for bottom of the stack or other purposes) and also the top element in the stack. This FSM can pop or push on the stack to go to the next state.</p> <p><strong><em>Example.</em></strong> Represent \(L = \{a^nb^n\}\) using a FSM.</p> <p><img src="/assets/img/Automata/image-20220106091221465.png" alt="image-20220106091221465"/></p> <p>This is how a FSM is represented. A string is accepted <strong>iff</strong> the stack is empty. Note the transition from \(q_0\) to \(q_1\). It says that the top of the stack must be \(a\). In case it isn’t the case, the string is rejected.</p> <p>In a FSM, the string is rejected due to one of the two reasons -</p> <ul> <li>No transition for the given input symbol or we reach the top stack symbol (in the case of finite length languages)</li> <li>Input is over, and the stack is not empty.</li> </ul> <p><strong><em>Example.</em></strong> Try the same for \(L = \{\text{equal \#}a's \text{ and } b's\}\).</p> <p><img src="/assets/img/Automata/image-20220106092231128.png" alt="image-20220106092231128"/></p> <p>Does this work?</p> <h2 id="non-determinism">Non-determinism</h2> <p><strong><em>Example.</em></strong> Represent \(L = \{ww^R\} \| w \in (a + b)^*\}\). Here, “R” represents reverse. That is, this language is the language of palindromes. Here, we keep pushing and then we keep popping after a decision point. <u>The decision point is a non-deterministic guess</u>. If there is a correct guess, then the algorithm will work.</p> <p><strong><em>Example.</em></strong> Is \(n\) composite? How do we design a non-deterministic algorithm for this problem?</p> <ol> <li>Guess for a factor \(p &lt; n\)</li> <li>Check if \(p\) divides \(n\). If the answer is “yes” then it is composite, else repeat.</li> </ol> <blockquote> <p>How do we reject empty strings in PDA?</p> <p>Does adding accepting states in the FSM increase the representation power? Does using the special symbol in between the stack increase the representation power?</p> </blockquote> <p>The reference textbook for this course is “Hopcroft Ullman Motwani.”</p> <h1 id="lecture-4">Lecture 4</h1> <p>This was self-reading about automatons (DFA and NFA).</p> <h1 id="lecture-5">Lecture 5</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">11-01-22</code></p> </blockquote> <blockquote> <p><strong><em>Homework.</em></strong> Find the number of binary strings of length 10 with no two consecutive 1’s.</p> <p><em>Answer.</em> \(f\) considers bit strings starting with \(0\) and \(g\) considers bit strings starting with \(1\).</p> \[\begin{align} f(1) &amp;= 1; f(2) = 2; \\ g(1) &amp;= 1; g(2) = 1 \\ f(n) &amp;= f(n - 1) + g(n - 1); \\ g(n) &amp;= f(n - 1); \end{align}\] <p>\(f: \{1, 2, 3, 5, 8, \dots\} \\ g: \{1, 1, 2, 3, 5, \dots \}\) Therefore, there are \(144\) such required strings.</p> </blockquote> <p><strong><em>Example.</em></strong> How do we construct an automata which captures the language of binary strings with no two consecutive 1’s? We use something known as a <strong>trap state</strong>. All the bad strings will be <em>trapped</em> in that state, and no transition from the trap state will lead to a final state. Consider the following automaton.</p> <p><img src="/assets/img/Automata/image-20220111114750638.png" alt="image-20220111114750638"/></p> <p>Here, the 3rd state is the trap state.</p> <h3 id="extended-transition-function">Extended transition function</h3> <p>\(\hat \delta : Q \times\Sigma^* \to 2^Q\) is defined as</p> \[\hat \delta(S, aW) = \begin{cases} \delta(S, a) &amp; w = \epsilon \\ \hat \delta(\delta(S, a), W) &amp; \text{otherwise} \end{cases}\] <p>For the sake of convenience we drop the hat and use \(\delta\) for the extended function (polymorphism).</p> <blockquote> <p><strong><em>Homework.</em></strong> Read the proof for showing the equivalence of language sets in DFA.</p> </blockquote> <h2 id="non-determinism-1">Non-determinism</h2> <p>Non-determinism basically refers to the procedures where we get the same output from the same input through <em>multiple runs</em>. In <strong>don’t care</strong> non-determinism, we get the same output with different algorithms/procedures. However, in <strong>don’t know</strong> non-determinism, a single stochastic algorithm goes through many runs (scenarios) to get the answer.</p> <h1 id="lecture-6">Lecture 6</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">13-01-22</code></p> </blockquote> <p>A deterministic automaton has a single choice of transition from a given state for a given symbol from the alphabet. However, in the case of a non-deterministic automata, such guarantee does not exist. The next state for a given symbol from a given state of the NFA is a set of states rather than a single state. There are two types of NFA as discussed previously.</p> <ul> <li>Don’t know NFA - Guess the right choice</li> <li>Don’t care NFA - All choices give the same result</li> </ul> <p>One might ponder if NFAs are more expressive than DFAs. The answer is surprisingly no. Non-determinism can be introduced to an automaton in two ways -</p> <ul> <li>Choice of subset of states for each transition</li> <li>\(\epsilon\)-transitions</li> </ul> <p><strong>Example.</strong> Construct an automaton to represent the language \(L = \{abc\} \cup \{\text{all strings ending in b}\}\) whose alphabet is \(\{a,b,c\}\)</p> <p><img src="/assets/img/Automata/image-20220113085632353.png" alt="image-20220113085632353"/></p> <p><strong>Note.</strong> We have introduced \(\epsilon\)-transitions in this automaton.</p> <p>As we shall see later in the course, the three models - DFA, NFA, and NFA with \(\epsilon\)-transitions; are all equivalent in terms of expressive power.</p> <h2 id="properties-of-languages">Properties of Languages</h2> <p>We mainly check two properties of languages - decision and closure.</p> <ul> <li> <p>Decision - Is \(w \in L\)? (Membership Problem) - decidable for regular languages.</p> <p>Decidable problems - Problems for which we can write a <strong>sound</strong> algorithm which <strong>halts in finite time</strong>.</p> <p>We can also consider problems like - Given \(DFA(M_1)\) and \(DFA(M_2)\), Is \(L(M_1) = L(M_2)\)? or \(L(M_1) \subset L(M_2)\)?</p> <p>Is \(L = \phi\)? Is \(L\) finite? Is \(L\) finite and has an even number of strings? We can also ask questions about the DFA - Can \(L(M)\) be accepted by a DFA with \(k &lt; n\) states (minimalism)?</p> <p><strong><em>Example.</em></strong> Can we build a DFA whose language is \(L = \{\text{bit strings divisible by } 7\}\) with less than 7 states? Turns out, the answer is no.</p> <p>Finally, we can ask a very difficult question such as - Show \(L\) cannot be accepted by any DFA. The technique for solving such a question is known as the <strong><em>Pumping Lemma</em></strong>. This lemma is based on the <em>Pigeonhole principle</em>.</p> </li> <li> <p>Closure - closure using Union, Intersection, Kleene Star, and Concatenation. The question we ask is if \(L_1, L_2\) belong to class \(C\), then does \(L_1 \texttt { op } L_2\) belong to \(C\)?</p> </li> </ul> <h1 id="lecture-7">Lecture 7</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">17-01-22</code></p> </blockquote> <p>We plan to show the equivalence of the following models -</p> <ul> <li>DFAs</li> <li>NFAs</li> <li>NFA-\(\epsilon\)</li> <li>Regular Expressions</li> </ul> <blockquote> <p><strong><em>Homework.</em></strong> Draw a DFA for the language \(L_1 \cup L_2\) where \(L_1\) is \(a\) followed by even number of \(b\)s and \(L_2\) is \(a\) followed by odd number of \(c\)s.</p> </blockquote> <h2 id="regular-expressions-1">Regular Expressions</h2> <p>They are defined using base cases and inductive cases. Before we discuss this, we define <strong>closed-world</strong> assumption. It is the presumption that a statement that is true is also known to be true. Therefore, conversely, what is not currently known to be true, is false. More info regarding this topic can be found <a href="https://en.wikipedia.org/wiki/Closed-world_assumption">here</a>. The base cases for regular expressions are given as follows.</p> \[\begin{align} L(a) &amp;= \{a\} \\ L(\epsilon) &amp;= \{\epsilon\} \\ L(\phi) &amp;= \phi \text{ (An empty set)} \end{align}\] <p>The inductive cases are</p> \[\begin{align} L_1, L_2 &amp;\to L_1 \cup L_2 \\ L_1, L_2 &amp;\to \{uv \vert u \in L_1, v \in L_2\} \\ L &amp;\to L_1^* \end{align}\] <h2 id="re-equiv-nfa-epsilon">RE \(\equiv\) NFA-\(\epsilon\)</h2> <p>To begin with, we convert the base cases of regular expressions to NFA-\(\epsilon\)s. Here are the automatons</p> <p><img src="/assets/img/Automata/image-20220131011047071.png" alt="image-20220131011047071"/></p> <p>Now, for the inductive cases -</p> <ul> <li> <p>For union of \(L_1\) and \(L_2\), consider the NFAs of both languages. Create a new start state \(s\) and final state \(f\). Connect \(s\) to the start states of both NFAs with \(\epsilon\) transitions and the final states of both NFAs to \(f\) with the same transitions.</p> <blockquote> <p>How do we prove that this construction represents the union?</p> </blockquote> </li> <li> <p>For concatenation of \(L_1\) and \(L_2\), connect all the final states of \(L_1\)’s NFA to all the start states of \(L_2\)’s NFA using \(\epsilon\) transitions.</p> </li> <li> <p>For \(L^*\), connect all the final states of the NFA to the start states of the NFA using \(\epsilon\) transitions. However, this does not accept \(\epsilon\). Fix it. Think.</p> </li> </ul> <h1 id="lecture-8">Lecture 8</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">18-01-22</code></p> </blockquote> <h2 id="decision-properties">Decision Properties</h2> <h3 id="membership-problem">Membership Problem</h3> <p>Does \(w \in L\)? How do we show that this problem is decidable? The algorithm is well-known.</p> <h3 id="emptiness-problem">Emptiness Problem</h3> <p>Is \(L(M) = \phi\)? This problem can be solved using <em>reachability</em> in graphs. One can check if the final states are reachable from the start state. It can be achieved using <strong>fixed point</strong>. This method is essentially propagating the frontier from the start state. We add neighbors of all the states in the current frontier and keep expanding it. We do this until our frontier does not change. This frontier is the fixed point.</p> <h3 id="infiniteness-problem">Infiniteness Problem</h3> <p>Is \(L(M)\) finite? We need to search for a loop in the graph to answer this question. Suppose our DFA has \(N\) states. If the DFA accepts a string whose length is greater than or equal to \(N\), then a state has to repeat while forming this string. This observation is a result of the <strong>Pigeonhole principle</strong>.</p> <p>This check is both necessary and sufficient. If a language is infinite, then it means that it has a string of length greater than \(N\). How are we going to use this property to check the infiniteness of the language? Is there a decidable algorithm?</p> <p><strong><em>Claim.</em></strong> If there is a string of length \(\geq N\) in \(L\), then there is a string of length between \(N\) and \(2N - 1\). Think about the proof. <em>Clue.</em> You can use at the most \(N - 1\) self loops.</p> <p>It is sufficient to test for the membership of all strings of length between \(N\) and \(2N - 1\).</p> <h2 id="pumping-lemma">Pumping Lemma</h2> <p><strong><em>Problem.</em></strong> Show that \(L = \{a^nb^n\}\) is not regular.</p> <p>We use proof by contradiction. Assume that a \(DFA(M)\) with \(N\) states accepts \(L\).</p> <p><strong><em>Theorem.</em></strong> <em>Pumping Lemma</em>. For every regular language \(L\), there exists an integer \(n\) , called the <em>pumping length</em>, where every string \(w \in L\) of length \(\geq n\) can be written as \(w = xyz\) where</p> \[\begin{align} \vert xy \vert &amp;\leq n \\ \vert y\vert &amp;&gt; 0\\ \forall i \geq 0,\ &amp; x(y)^iz \in L \end{align}\] <p>Let us understand this by proving \(L = \{a^nb^n \vert n &gt; 0\}\) is not regular using contradiction. Assume there is a \(DFA(M)\) with \(N\) states that accepts \(L\).</p> <p>Now, consider the string \(a^Nb^N\). Now, define \(a^n\) as \(xy\). Let \(x = a^j\) and \(y = a^k\) where \(n \leq N\). According to the pumping lemma, \(a^j(a^k)^ib^N \in L\). Therefore, there can’t exist a DFA that represents \(L\).</p> <blockquote> <p><strong><em>Homework.</em></strong> Prove that the language with equal number of \(a\)‘s and \(b\)’s is not regular.</p> <p>The idea is to use closure properties.</p> </blockquote> <blockquote> <p><strong><em>Homework.</em></strong> Prove the pumping lemma.</p> <p><em>Answer.</em> Consider a DFA\((Q, \Sigma, Q_0, \delta, F)\) with \(N\) states, and a word \(w\) such that \(\vert w \vert = T \geq N\) belongs to \(L\). Now, let \((S_i)_{i =0}^T\) (0 - indexed) be the sequence of states traversed by \(w\). The state \(S_L\) is an accepting state as \(w \in L\). Also, there exist \(0 \leq i &lt; j \leq N\) such that \(S_i = S_j\) as there are only \(N\) states in the DFA (Pigeonhole principle). Now, define the string formed by \(S_0, \dots, S_i\) as \(x\), the one formed by \(S_i, \dots, S_j\) as y, and that formed by \(S_j, \dots, S_T\) as \(z\). We have \(w = xyz\). Also, \(\vert y \vert &gt; 0\) as \(i &lt; j\) and \(\vert xy \vert \leq N\) as \(j \leq N\).</p> <p>Since \(S_i = S_j\), we can <em>pump</em> the string \(y\) as many times as we wish by traversing cycle \(S_i, \dots, S_j\). Now, since \(S_T \in F\), the string formed by the sequence \(S_0, \dots, \{S_i, \dots, S_j\}^t, \dots, S_T\) (\(t \geq 0\)) also belongs to \(L\). This word is equivalent to \(xy^iz\). \(\blacksquare\)</p> </blockquote> <h1 id="lecture-11">Lecture 11</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">25-01-22</code> There were no lecture 9 and 10</p> </blockquote> <h2 id="nfa-equiv-dfa">NFA \(\equiv\) DFA</h2> <p>We convert a NFA to DFA using <strong>subset construction</strong>. That is, if \(Q\) is the set of states in the NFA, then \(2^Q\) will the set of states in the DFA. Initially, we construct the start states using the subset of start states in the NFA. Then, we build the following states in the DFA by tracing all the states reached by each character in the NFA into a single subset. If any of the subset consists of a final state, then that subset state is made into a final state in the DFA.</p> <p>This equivalence can also be extended to show the equivalence of NFA-\(\epsilon\) and \(DFA\).</p> <h2 id="dfa-minimization">DFA Minimization</h2> <p>All DFAs with minimal number of states are <strong>isomorphic</strong>.</p> <h3 id="distinguishable-states">Distinguishable states</h3> <p>Consider a DFA \((Q, \Sigma, Q_0, \delta, F)\). States \(q_i,q_j \in Q\) are said to be distinguishable iff there exists a word \(w \in \Sigma^*\) such that \(\delta(q_1, w) \not \in F\) and \(\delta(q_2, w) \in F\) or vice versa.</p> <p>One can merge indistinguishable states to minimize a DFA.</p> <h2 id="context-free-grammar">Context-Free Grammar</h2> <p>Let us try to write the grammar for the language \(L = \{a^ib^jc^k \vert i = j \text{ or } j = k\}\). Consider the following rules</p> \[\begin{align} S &amp;\to S_1 \vert S_2 \\ S_1 &amp;\to aS_1bC \vert C \\ S_2 &amp;\to AbS_2c \vert \epsilon \\ C &amp;\to cC \vert \epsilon \\ A &amp;\to aA \vert \epsilon \end{align}\] <p>In general, union is straightforward to write in CFG due to rules like rule 1. However, there is an ambiguity in the above rules. For example, the word \(a^3b^3c^3\) can be generated using different derivations. We’ll discuss this later in the course.</p> <p>As we discussed before, a context free grammar \(G\) is defined by \((V, \Sigma, R, S)\). A string is accepted by the grammar if \(w \in \Sigma^*\) and \(R \xrightarrow{*} w\). That is, the rules must be able to <em>derive</em> \(w\).</p> <p>A one-step derivation is given by \(u_1Au_2 \to u_1\beta u_2\) if \(A \to \beta \in R\) and \(u_1, u_2 \in (V \cup \Sigma)^*\).</p> <p>Consider the set of rules for defining arithmetic expressions. The language is defined over \(\Sigma = \{+, -, \times, \div, x, y, z, \dots\}\).</p> \[\begin{align} S &amp;\to x \vert y \vert z \\ S &amp;\to S + S \\ S &amp;\to S - S \\ S &amp;\to S \times S \\ S &amp;\to S \div S \\ S &amp;\to (S) \end{align}\] <p>Now, for an expression such \(x + y \times z\), we can give a left-most derivation or a right-most derivation. A precedence order removes ambiguities in the derivation. In programming languages, a parser removes these ambiguities using some conventions.</p> <blockquote> <p><strong><em>Homework.</em></strong> Try and write the set of rules for the language which consists of \(a\)s and \(b\)s such that every string has twice the number of \(a\)s as that of \(b\)s.</p> <p><em>Answer.</em> We try to write the rules for equal number of a’s and b’s, and try to extend them.</p> \[\begin{align} S &amp;\to \epsilon \\ S &amp;\to aSb \ \vert \ Sab \ \vert \ abS \\ S &amp;\to bSa \ \vert \ Sba \ \vert \ baS \end{align}\] <p>Does this work?</p> </blockquote> <p><strong>Note.</strong> \(\vert R \vert\) is finite.</p> <h1 id="lecture-12">Lecture 12</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">27-01-22</code></p> </blockquote> <p>We had seen the CFG for the language \(L = \{a^nb^n\}\). How can one prove that the rules represent that language? That is, how do we know that the rules generate <u>all the strings</u> in the language and <u>no other strings</u>?</p> <p>Consider the language we saw in the previous lecture - strings with equal number of a’s and b’s. The possible set of rules are -</p> \[\begin{align} S &amp;\to aB \ \vert \ bA \\ B &amp;\to b \ \vert \ bS \ \vert \ aBB \\ A &amp;\to a \ \vert \ aS \ \vert \ bAA \\ \end{align}\] <p>The thinking is as follows -</p> <ul> <li>\(S\) generates strings with equal number of a’s and b’s. \(B\) generates strings with one more b than a’s, and similarly for \(A\).</li> <li>To write the inductive cases of \(S\), we consider strings starting with both a and b. Inductive cases for \(A, B\) are also constructed similarly.</li> </ul> <p>Another possible set of rules are -</p> \[\begin{align} S &amp;\to aSb \ \vert \ bSa \\ S &amp;\to \epsilon \\ S &amp;\to SS \\ \end{align}\] <p>These rules are the minimized set of the rules I wrote for the hw in the last lecture.</p> <h2 id="grammars">Grammars</h2> <p>We can write grammars in canonical forms. A grammar is said to be in <strong>Chomsky normal form</strong> if the rules are of the form</p> \[\begin{align} C \to DE &amp;\text{ - one non-terminal replaced by 2 non-terminals} \\ C \to a &amp;\text{ - one terminal replaced by a terminal} \end{align}\] <p><strong><em>Claim.</em></strong> Any grammar \(G\) (\(\epsilon \not\in L(G)\)) can be converted to an equivalent Chomsky Normal form.</p> <p>Another canonical form of grammar is <strong>Greibach normal form</strong>. The rules are of the form</p> \[\begin{align} C &amp;\to a\alpha \end{align}\] <p>Each non-terminal converts to a terminal followed by a symbol in \((V \cup \Sigma)^*\). Similar to the Chomsky form, every grammar can be converted to the Greibach normal form.</p> <h2 id="push-down-automatons">Push Down Automatons</h2> <p>Consider the language \(L = \{wcw^R \vert w \in (a + b)^*\}\).</p> <blockquote> <p><strong><em>Aside.</em></strong> Try writing the Chomsky normal form rules for the above language. \(\begin{align} S &amp;\to AA_0 \mid BB_0 \mid c \\ A_0 &amp;\to SA \\ B_0 &amp;\to SB \\ A &amp;\to a \\ B &amp;\to b \\ \end{align}\)</p> </blockquote> <p>The pushdown automaton for this language is given by -</p> <p><img src="/assets/img/Automata/image-20220131205500689.png" alt="image-20220131205500689"/></p> <h1 id="lecture-13">Lecture 13</h1> <p>How do we take a grammar and produce the normal form of the grammar? We shall learn techniques such as eliminating useless symbols, elimination epsilon and unit productions. These will be used to get the designable form of the grammar that can also be used to show languages which are context-free using the pumping lemma.</p> <p>The notation \(\alpha_1 \xrightarrow{n} \alpha_2\) is used to denote that \(\alpha_2\) can be derived from \(\alpha_1\) in \(n\) steps. The \(*\) over the arrow represents the derivative closure of the rules.</p> <p>Grammars \(G_1, G_2\) are equivalent iff \(L(G_1) = L(G_2)\). For example, we have seen two grammars for the language \(L = \text{ strings with } \#a = \#b\), and they are both equivalent.</p> <h3 id="derivation-trees">Derivation trees</h3> <ul> <li>Trees whose all roots are labeled.</li> <li>The root of the tree is labeled with the start symbol of the grammar \(S\).</li> <li>Suppose A is the label of some internal node, then A has a children \(X_1, \dots X_R\) iff \(A \to X_1\dots X_R \in R\).</li> <li>\(\epsilon\) can only be the label of leaf nodes.</li> </ul> <h3 id="derivations">Derivations</h3> <p>Grammars with multiple derivation trees are ambiguous. Are there languages for which any grammar is ambiguous? Such languages are called as <strong>inherently ambiguous</strong>.</p> <blockquote> <p><strong><em>Homework.</em></strong> Find examples of inherently ambiguous grammars.</p> </blockquote> <h2 id="simplifying-grammars">Simplifying grammars</h2> <p><strong><em>Definition.</em></strong> A variable \(B\) is productive in \(G\) iff \(B \xrightarrow[G]{*} w\) for some \(w \in \Sigma^*\).</p> <p><strong><em>Definition.</em></strong> A variable \(B\) is reachable from the start symbol \(S\) of \(G\) if \(S \xrightarrow[G]{*} \alpha B \beta\) for some \(\alpha, \beta \in (\Sigma \cup V)^*\).</p> <p>We useful variable \(X\) is productive and reachable. These are necessary conditions but not sufficient. For example, consider the following grammar.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>S -&gt; AB
A -&gt; c
</code></pre></div></div> <p>Here, \(A\) is useless even if it is productive and reachable.</p> <p><strong><em>Definition.</em></strong> A variable \(X\) is useless if there is no derivation tree that gives \(w \in \Sigma^*\) from \(S\) with \(X\) as the label of some internal node.</p> <h3 id="fixed-point-algorithm">Fixed point algorithm</h3> <p>The idea is to find a monotonic algorithm which gives us the set of useful symbols at the fixed point of the underlying function.</p> <p>Firstly, let us try this for productive variables. We propose the following algorithm.</p> \[\begin{align} P_0 &amp;= \phi \\ P_i &amp;\to P_{i + 1} \\ &amp;\triangleq P_i \cup \{N \mid N \to \alpha \in R \text{ and } \alpha \in (\Sigma \cup P_i)^* \} \end{align}\] <p>For example, consider the grammar</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>S -&gt; AB | AA
A -&gt; a
B -&gt; bB
-- The algorithm
P_o = \phi
P_1 = {A} // Rule 2
P_2 = {A, S} // Rule 1
P_3 = {A, S}
</code></pre></div></div> <p>Therefore, the symbols \(S, A\) are productive in the above grammar. We propose a similar algorithm for reachable states in the grammar.</p> \[\begin{align} R_0 &amp;= \phi \\ R_i &amp;\to R_{i + 1} \\ &amp;\triangleq R_i \cup \{P \mid Q \to \alpha P \beta; \alpha, \beta \in (\Sigma \cup P_i)^* \text{ and } Q \in R_i\} \end{align}\] <p><strong><em>Definition.</em></strong> A <strong>nullable</strong> symbol is a symbol which can derive \(\epsilon\). These symbols can be eliminated.</p> <p><strong><em>Definition.</em></strong> A unit production is a rule of the form \(A \to B\) where \(A, B \in V\).</p> <h1 id="lecture-14">Lecture 14</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">01-02-22</code></p> </blockquote> <p>We shall continue discussing simplifying grammars. As an initial step, we can get rid of symbols which are either unreachable or unproductive. However, the order in which we check productivity and reachability affects how the grammar is simplified. For example, consider the grammar</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>S -&gt; a
S -&gt; AB
A -&gt; a
</code></pre></div></div> <p>Now, if we eliminate unreachable symbols first (that is none), and then eliminate unproductive symbols (\(B\)), we get</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>S -&gt; a
A -&gt; a
</code></pre></div></div> <p>Notice that the above simplified grammar has a redundant rule. However, if we check for productivity (\(B\) is eliminated) and then reachability (\(A\) is unreachable), we get</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>S -&gt; a
</code></pre></div></div> <p><strong><em>Claim.</em></strong> <u>Eliminating unproductive symbols followed by unreachable symbols gives a grammar without any useless variables.</u></p> <blockquote> <p><strong><em>Homework.</em></strong> Prove the above claim.</p> </blockquote> <h3 id="epsilon-productions">\(\epsilon\)-productions</h3> <p>Suppose \(G\) is a CFG such that \(L(G)\) has \(\epsilon\), then \(L(G) - \{\epsilon\}\) can be generated by a CFG \(G_1\) which has no \(\epsilon\)-productions (rules of the form \(A \to \epsilon\)).</p> <h3 id="nullable-symbols">Nullable symbols</h3> <p>\(A\) is <strong>nullable</strong> if \(A \xrightarrow[G]{*} \epsilon\).</p> <p>How do we remove nullable symbols? We can’t just take out a rule of the form \(A \to \epsilon\) as it may make \(A\) unproductive. The way to go about this is to replace each rule of the form \(A \to X_1\dots X_n\) to \(A \to \alpha_1 \dots \alpha_n\) where</p> \[\alpha_ i = \begin{cases} X_i &amp; X_i\text{ is not nullable}\\ \epsilon &amp;\text{otherwise} \end{cases}\] <p>However, one must be cautious about replacing all the symbols by \(\epsilon\) and also about recursive rules. For instance, consider the following grammar</p> \[S \to \epsilon \mid aSb \mid bSa \mid SS\] <p>Now, \(S\) is nullable but we can’t replace \(S \to aSb\) by \(S \to ab\) simply. We also need to have the rule \(S \to aSb\) in place for recursion. Basically, for each rule of the form \(A \to X_1\dots X_n\), we need to have all the rules of the form \(A \to \alpha\) where \(\alpha\) spans over all subsets of \(X_1 \dots X_n\).</p> <h3 id="unit-productions">Unit Productions</h3> <p>To remove unit productions, let us assume that there are no \(\epsilon\)-productions and useless symbols. Now, we need to find all pairs \((X, Y)\) such that \(X \xrightarrow[G]{*} Y\). Once we find such pairs, we can try and eliminate the unit symbols themselves, or solely the rules.</p> <p>Let us take the following grammar for \(L \{w \mid n_a(w) = n_b(w)\}\)</p> \[S \to \epsilon \mid aSb \mid bSa \mid SS \\ \text{ simplified to } \\ S \to abS \mid bSa \mid SS \mid ab \mid ba\] <p>and</p> \[S \to \epsilon \\ S \to aB \mid bA \\ B \to b \mid bS \mid aBB \\ A \to a \mid aS \mid bAA \\ \text{ simplified by just removing } S \to \epsilon\\\] <p>To convert the grammar into Chomsky normal form, we do approximately the following - we replace each rule of the form \(A \to X_1 \dots X_n\) with \(A \to X_1B\) and \(B \to X_2 \dots X_n\). We keep repeating this until there is only a single terminal symbol on the rhs, or two non-terminals.</p> <h1 id="lecture-15">Lecture 15</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">03-02-22</code></p> </blockquote> <p>Consider a grammar in Chomsky form. The derivation tree will consist of nodes with either 2 children (\(A \to BC\)), 1 child (\(A \to a\)) or leaf nodes.</p> <p>Let the longest path in the derivation tree be \(k\). Then, the upper bound on the length of the word formed is \(2^{k - 1}\) and lower bound is \(k\). <strong>Note.</strong> The variables from \(V\) can repeat across a path.</p> <h2 id="pumping-lemma-for-cfg">Pumping Lemma for CFG</h2> <p>For any CFL \(L\), the language \(L - \{\epsilon \}\) has a Chomsky Normal form grammar.</p> <p><strong><em>Theorem.</em></strong> <em>Pumping Lemma</em> For every CFL \(L\), there is a \(n\) such that for all strings \(\vert z \vert \geq n\) with \(z \in L(G)\), there are strings \(u, v, w, x, y\) such that \(z = uvwxy\) and</p> <ul> <li> \[\vert vwx \vert \leq n\] </li> <li> \[\vert vx \vert \geq 1\] </li> <li>for all \(z_i = u v^iwx^iy\), \(i \geq 0\), \(z_i \in L(G)\).</li> </ul> <p>It is easy to show \(L = \{a^nb^nc^n \mid n \geq 1\}\) is not a CFL using the above lemma.</p> <h3 id="proof-of-the-lemma">Proof of the lemma</h3> <p>If there is a \(z \in L(G)\) such \(\vert z \vert \geq n = 2^k\), then there must be a path of length at least \(k + 1\). Now, due to pigeonhole principle, a variable must repeat in this path in the derivation tree of \(z\).</p> <p>Now, consider the path \(S, \dots, A, \dots, A, \dots, a\).</p> <ul> <li>It is easy to the see that the word formed by the subtree below the first \(A\) has length \(\leq n\).</li> <li>Also, as there are 2 \(A\)s in the path, the number of letters in the “left” subtree and “right” subtree would be greater than 1.</li> <li>These “left” and “right” subtrees can be derived an arbitrary number of times as \(A\) can be derived from \(A\).</li> </ul> <p><em>Note.</em> To draw some more intuition, call the subtree formed by the first \(A\) as \(T_1\), and the subtree formed by the second \(A\) as \(T_2\). We shall label the word formed by \(T_2\) as \(w\), and the word labeled by \(T_1\) as \(vwx\). Now, \(\vert vx \vert \geq 1\). <em>Think.</em> The “left” and “right” subtree in the above explanation together are formed by the set \(T_1 \setminus T_2\).</p> <blockquote> <p><strong><em>Homework.</em></strong> Show that the following languages cannot be represented by CFG.</p> <ul> <li> \[L = \{a^ib^ic^i \mid j \geq i\}\] </li> <li> \[L = \{a^ib^jc^k \mid i \leq i \leq k\}\] </li> <li> \[L = \{a^ib^jc^id^j \mid i, j \geq 1\}\] </li> </ul> </blockquote> <p>We shall see that 2 stacks PDA is the most expressive Turing machine.</p> <blockquote> <p>Can’t there be useless symbols in normal forms? yes -&gt; \(S \to AB\)</p> </blockquote> <h2 id="ogdens-lemma">Ogden’s Lemma</h2> <p>This lemma is a generalization of pumping lemma. Consider the pumping lemma for the FSA. It basically said that some state must repeat for a string of length \(\geq n\) in the language. Ogden’s lemma talks about a similar claim for CFGs. It will be formally discussed in the next lecture.</p> <h2 id="closure-properties">Closure properties</h2> <p>The union, concatenation and Kleene closure of CFGs are also CFGs. Proof is left as a homework.</p> <p><strong>Intersection</strong> is not context-free!</p> <h1 id="lecture-16">Lecture 16</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">07-02-22</code></p> </blockquote> <h2 id="pushdown-automata">Pushdown Automata</h2> <p>Consider the languages \(L_1 = \{wcw^R \mid w \in (0 + 1)^* \}\) and \(L_3 = \{ww^R \mid w \in (0 + 1)^* \}\). Let us build PDAs for these two models.</p> <p>The stack of the PDA uses different symbols that those from \(\Sigma\). We shall denote the superset of symbols of stack by \(\Tau\). Each transition in the PDA depends on the input symbol and the symbol on the top of the stack. These are similar to the FSA transition diagram. If both of these symbols are satisfied for the transition, we perform <em>an action</em> on the stack by pushing or popping symbols. Note that we can also choose to not perform any action.</p> <p>We shall also use the symbol \(X\) to denote ‘matching any character’ for the symbols on stack. We can also have \(\epsilon\) transitions in non-deterministic PDAs. \(\epsilon\) transitions basically denote ignoring input symbols.</p> <p><strong>Note.</strong> We can push multiple symbols in each transition but we can only pop one symbol in each transition.</p> <p>The PDA for \(L_1\) is given by</p> \[\begin{align} \delta(q_1, c, X) &amp;= (q_2, X) \\ \delta(q_1, 1, X) &amp;= (q_1, GX) \\ \delta(q_1, 0, X) &amp;= (q_1, BX)\\ \delta(q_2, 0, B) &amp;= (q_2, \epsilon) \\ \delta(q_2, 1, G) &amp;= (q_2, \epsilon) \\ \delta(q_2, \epsilon, Z_0) &amp;= (q_2, \epsilon) \\ \end{align}\] <p>The PDA for \(L_3\) is similar but we cannot express this language without non-determinism. We can’t guess where the “middle of the string” occurs without <em>guessing</em>. Therefore, non-deterministic PDAs are more expressive than deterministic ones.</p> <p>Remember that the non-deterministic PDA accepts a string if there is at least one run that accepts it.</p> <h3 id="formal-definition-of-pda">Formal definition of PDA</h3> <p>A Pushdown automata is formally given by the 7 element tuple \(M \lang Q, \Sigma, \Tau, \delta, q_0, Z_0, F \rang\) where</p> <ul> <li>\(\Tau\) is the set of stack symbols</li> <li>\(Z_0\) is a special symbol denoting the bottom of the stack</li> <li>\(F\) is the set of final states of the automata</li> <li>\(\delta: Q \times (\Sigma \cup \epsilon) \times \Tau \to (Q, \lambda)\) where \(\lambda \in \Tau^*\) is an action performed on the transition.</li> </ul> <p>A string is accepted if one of the runs ends with an empty stack or at a final state. That is, \(L(M) = \{w \mid (q_0, w, Z_0) \vdash^*_M (p, \epsilon, \epsilon)\} \cup \{ w \mid (q_0, w, Z_0) \vdash^*_M (q, \epsilon, \lambda), q \in F\}\). In the following lectures, we shall prove DPDA \(\neq\) NPDA, ‘accepting by empty stack’ \(\equiv\) ‘accepting by final state’ and PDA (NPDA) \(=\) CFG.</p> <h1 id="lecture-17">Lecture 17</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">08-02-22</code></p> </blockquote> <p>We shall continue the formal description of PDAs.</p> <p>We define a <strong>move</strong> or an <strong>arrow</strong> as a transition in the PDA.</p> <p><strong>Instantaneous Description</strong> refers to the property of defining a system using a <em>single number</em>. In case of a PDA, the description is given by a numerical representation of (Current string, Remaining input tape, Stack content). These values, that constitute the dynamic information of a PDA, keep changing during moves. The notation \(ID_1 \vdash_M ID_2\) denotes a single transition in the PDA. The notation \(ID_1 \vdash^*_M ID_2\) is a sequence of 0 or more moves.</p> <p>We have seen the <strong>accept criteria</strong> in the last lecture. We will show that a PDA built on the final state accept criteria has an equivalent PDA built on the empty state accept criteria.</p> <h2 id="greibach-normal-form">Greibach Normal Form</h2> <p>CFGs of the form \(A \to a\alpha\).</p> <p><strong>Note.</strong> \(\alpha\) in the above expression consists of non-terminals only!</p> <p>What are the challenges involved in converting a grammar to a Greibach normal form? It involves the following steps.</p> <h3 id="elimination-of-productions">Elimination of productions</h3> <p>Suppose we want to remove the rule \(A \to \alpha_1 B \alpha_2\) and keep the language of the CFG same. Then, we consider all rules of the form \(B \to \beta\) and replace each of them by \(A \to \alpha_1 \beta \alpha_2\). After doing this, we can delete the initial production involving \(A\).</p> <h3 id="left-recursion">Left Recursion</h3> <p>Suppose we have rules of the form \(A \to A\alpha_1 \mid A\alpha_2 \mid \dots \mid A\alpha_r\) which we wish to remove. Now, there would be other rules involving \(A\) of the form \(A \to \beta_1 \mid \dots \mid \beta_s\).</p> <p>Now, we define a new variable \(B \not\in V\). Then, we write rules of the form \(A \to \beta_1B \mid \dots \mid \ \beta_s B\) and \(\beta \to \alpha_1 B \mid \dots \mid \alpha_r B\). We can then remove all the left-recursive productions of \(A\). Basically, we have replaced left-recursion rules with right-recursion rules.</p> <h3 id="ordering-of-variables">Ordering of Variables</h3> <p>Simplify the variables in one order? <em>Wasn’t taught?</em></p> <h2 id="equivalence-of-pda-and-cfg">Equivalence of PDA and CFG</h2> <p>Given any CFG \(G\), we convert it into Greibach normal form. Then, we choose the non-terminals as the stack symbols. The initial symbol in the grammar is taken as the top symbol of the stack initially. For each rule \(A \to a\alpha\) we write the transition \(\delta(q, a, A) = (q’, \alpha)\). We start with a single state in the DFA, and write all the rules on this state. Through this intuition, we have shown that every CFG has a corresponding PDA. Then, we need to show that every PDA has a corresponding CFG.</p> <p>To show that CFG \(\subseteq\) PDA, we need to show the equivalence of a language in both directions. That is, for every string accepted by the CFG, we need to show that the PDA also accepts it and vice versa. Note that we also have to prove PDA \(\subseteq\) CFG.</p> <h1 id="lecture-18">Lecture 18</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">10-02-22</code></p> </blockquote> <p>We will continue the equivalence proof now.</p> <p>## Ogden’s Lemma revisited</p> <p>Suppose we had to prove the language \(L = \{a^ib^jc^kd^l \mid i = 0 \text{ or } j = k = l\}\) is not CFG. Can we do this using pumping lemma? No. <em>Think</em>.</p> <p>We do this using Ogden’s lemma which goes something like this. You can give a \(z = z_1 \dots z_m\) such that \(m \geq n\). Now, we can mark positions by choosing a subset of \(z_i\)’s. Then, the \(vwx\) from the pumping lemma must have at most \(n\) marked positions and \(vx\) has at least one marked position. This is a stronger condition as compared to the pumping lemma.</p> <blockquote> <p><strong><em>Homework.</em></strong> Show that \(L = \{a^ib^jc^k \mid i \neq j, j \neq k, i \neq k\}\) is not context-free using Ogden’s lemma.</p> </blockquote> <h2 id="closure-properties-of-cfls">Closure properties of CFLs</h2> <ul> <li><strong>Union</strong> - \(L(G_1) \cup L(G_2)\) regular? Yes. Add a rule \(S \to S_1 \mid S_2\).</li> <li><strong>Kleene closure</strong> - \((L(G_1))^*\) regular? Yes. Add a rule \(S \to S_1S \mid \epsilon\).</li> <li><strong>Concatenation</strong> - \(L(G_1). L(G_2)\) regular? Yes. Add a rule \(S \to S_1 S_2\).</li> <li><strong>Intersection</strong> - Consider the example \(L_1 = \{a^ib^ic^j \mid i, j \geq 1\}\) and \(L_2 = \{a^ib^jc^i \mid i, j \geq 1\}\). Both of them are context free but their intersection is not.</li> <li><strong>Complement</strong> - Easy to show from intersection.</li> </ul> <h2 id="decision-properties-of-cfls">Decision properties of CFLs</h2> <h3 id="is-the-language-empty">Is the language empty?</h3> <p>This is easy to check. Remove useless symbols and show that \(S\) is useful.</p> <h3 id="is-the-language-finite">Is the language finite?</h3> <p>Consider the Chomsky normal form of the grammar. <u>Draw a graph corresponding to the grammar such that every node has a single edge directed to a terminal or two edges directed to non-terminals</u>.</p> <p><strong><em>Claim.</em></strong> If the graph has a cycle, then the language formed by the grammar is infinite. This is intuitive.</p> <p>We can find cycles using DFS simply.We can derive more efficient methods which is not the goal of this course.</p> <h3 id="does-a-string-belong-to-the-language">Does a string belong to the language?</h3> <p>The first approach is the following. Get the Greibach form of the grammar. Now, for each letter \(c\) in the string check whether there are rules of the form \(A \to c\alpha\). We consider the values of all such \(\alpha\)s and continue parsing the rest of the input string. Therefore, at each letter in the string we limit the number of possible derivations. If we reach the end of the string without running out of possible derivations at each letter, then the string belongs to the language. Remember that we will start with the start symbol for the beginning of the string.</p> <p>However, we can see that this method is not very efficient. We shall see the CYK algorithm which works in \(\mathcal O(n^3)\).</p> <h1 id="lecture-19">Lecture 19</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">14-02-22</code></p> </blockquote> <h2 id="additional-closure-properties-of-cfls">Additional closure properties of CFLs</h2> <ul> <li> <p>\(L(G) \cap R\), where \(R\) is a regular language, is also context-free.</p> <p>How do we show that \(L = \{ww \mid w \in (a + b)^*\}\) is not context free? It can be shown via pumping lemma which is complicated. However, it is very easy to prove it using the above property.</p> <p>How do we prove the closure property? Consider a \(PDA(M_1)\) for \(L(G)\) and \(DFA(M_2)\) for \(R\). Now, we construct a ‘product’ PDA similar to what we did in NFAs. We use the ‘final state’ accept PDA for the purpose. The final states would be \(F_1 \times F_2\). It is easy to see that we can get the required property.</p> </li> </ul> <h2 id="cyk-algorithm">CYK algorithm</h2> <p>The Cocke-Younger-Kasami algorithm can be used to check if a string \(w\) belongs to \(L(G)\).</p> <p>Suppose there is a \(w = w_1 w_2 \dots w_i \dots w_l \dots w_n\) whose membership we need to check. We consider subproblems of the form: Is \(w_{ij} \triangleq w_i \dots w_{i + j - 1}\) derivable from any non-terminal in the grammar? Then, finally, we need to answer is \(w_{1n}\) can be derived from \(S\).</p> <p>We consider the Chomsky normal form of the grammar for this algorithm. All strings of the form \(w_{j1}\) can be checked easily. For each \(w_{i1}\) we store all the non-terminals that can derive it. Now, the recursion for all \(j \geq 2\) is given by</p> \[f(w_{in}) = \bigcup_{j = 1}^{n - 1}\bigcup_{A \in f(w_{ij})} \bigcup_{B \in f(w_{i(n - j)})} \{C \mid (C \to AB) \in G \}\] <p>See the following example using the CYK algorithm.</p> <p><img src="/assets/img/Automata/image-20220222011041265.png" alt="image-20220222011041265"/></p> <p>It is easy to see that this algorithm takes \(\mathcal O(n^3)\).</p> <h1 id="lecture-20">Lecture 20</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">15-02-22</code></p> </blockquote> <h3 id="epsilon-closure">\(\epsilon\) closure</h3> <p>We define \(\epsilon\) closure for a state \(q\) in a DFA as all the states that \(q\) can reach via \(\epsilon\) transitions. Then, to remove all the \(\epsilon\) transitions from \(q\), we add a transition corresponding to all transitions from \(q\) to each of the states in the \(\epsilon\) closure of \(q\). Also, if any state in the \(\epsilon\) closure of \(q\) is final, then \(q\) can also be made as a final state.</p> <p><strong>Note.</strong> Minimal DFAs need not be unique. They are isomorphic.</p> <h1 id="lecture-21">Lecture 21</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">28-02-22</code></p> </blockquote> <h2 id="satan-cantor-game">Satan-Cantor Game</h2> <p>The game goes like this. Satan chooses a random natural number and asks Cantor to guess the number. Cantor gets a chance to guess once on each day. If Cantor guesses the right number, then he can go to heaven. Does Cantor have a strategy that will guarantee him going to heaven? Yes, Cantor can choose numbers in the sequence \(1, 2, 3, \dots\) and eventually <em>it is guaranteed</em> that Cantor chooses the correct number.</p> <p>Suppose, Satan changes the game by choosing a random integer. Does Cantor have a strategy then? Yes, choose numbers of the form \(0, 1, -1, 2, -2, \dots\). Now, suppose Satan chooses a pair of integers, then how do we get a strategy? We can order the points based on the distance from the origin in the 2D Cartesian plane (<strong>Cantor’s pairing function</strong>). Similarly, a tuple of \(n\) integers is also enumerable.</p> <p>Can we give a set for Satan such that the elements are not enumerable? What if Satan chooses \((n_1, \dots, n_k)\) where \(k\) is not known to Cantor. Can Cantor win in this case? Yes. <em>Proof is homework</em>.</p> <p>Satan can choose the set of real numbers, and in that scenario Cantor would not be able to win. The key idea here is <a href="https://www.cse.iitb.ac.in/~mp/teach/ds/aut19/slides/21.pdf"><strong>Cantor’s diagonalization argument</strong></a>.</p> <p>Also, look up <a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmogorov complexity</a>. Enumerability \(\equiv\) Countability.</p> <h2 id="enumerability-in-languages">Enumerability in Languages</h2> <p>A language which is finite will be regular, and we can build an FSA for it. How about infinite languages? CFLs can represent a subset of these languages.</p> <p>If we categorise the infinite languages as languages which are <u>recursively enumerable</u> and <u>uncountable</u>. Can we build models for recursively enumerable languages?</p> <h2 id="turing-machines">Turing Machines</h2> <p>Turing wanted to build a machine that could recognize any enumerable language. The inspiration is drawn from the <em>recursive function theory</em> developed by Alonzo Church. There are other people who developed <strong>Type-0 grammars</strong> which are similar to CFGs but the left hand side of the rules can consist of a string composed of terminals and non-terminals making it context-dependent.</p> <p>Church and Turing came up with a thesis, and they proposed that Church’s theory and Turing’s machine can be used to compute any <em>effectively computable</em> function.</p> <blockquote> <p><strong><em>Aside.</em></strong> Aren’t all languages enumerable with alphabet being a finite set?</p> </blockquote> <p>“Is \(L(G)\) empty?” is a decidable problem. However, “Is the complement of \(L(G)\) empty?” is not decidable! This property was proven by Godel.</p> <p>We shall now introduce the notion of Turing machines. Instead of a stack, we have an <strong>input tape</strong> with a <em>fixed left end</em>. A string is written in the tape with each letter being in a <strong>cell</strong> of the tape. The machine also involves a control which is essentially a FSM that takes the input from the tape, and performs actions on it. Basically, in PDAs, we could only see the top of the stack, but here we are able to freely traverse over the tape.</p> <h2 id="definitions-from-set-theory">Definitions from set theory</h2> <p><strong><em>Definition.</em></strong> <strong>Enumerable</strong> of a set refers to the property of being able to define a one-to-one correspondence from the elements of the set to positive integers.</p> <p><strong><em>Definition.</em></strong> In computability theory, a set \(S\) of natural numbers is called computably enumerable, recursively enumerable, semi-decidable, partially decidable, listable, provable or Turing-recognisable if</p> <ul> <li>there is an algorithm such that the set of input numbers for which the algorithm halts is exactly \(S\).</li> <li>or equivalently, there is an algorithm that enumerates the members of \(S\). That means that its output is simply a list of all the members of \(S = \{s_1, s_2, s_3, \dots\}\) . If \(S\) is infinite, this algorithm will run forever.</li> </ul> <h1 id="lecture-22">Lecture 22</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">07-03-22</code></p> </blockquote> <h2 id="dovetailing">Dovetailing</h2> <p>In the Satan-Cantor game, the idea was to enumerate all possible answers “fairly”.</p> <p>Let us consider the scenario where Satan chooses a \(k\) tuple of integers where \(k\) is unknown. How do we enumerate this set? We use dovetailing for the same. Consider the tuple \((k, n)\). For one such given tuple, we will enumerate all \(k\)-tuples with distance from origin being less than \(n\). The idea is that we are balancing the enumeration across two dimensions, \(k\) and \(n\), together.</p> <h2 id="cantors-diagonalization">Cantor’s diagonalization</h2> <p>How is the above set different from the power set of natural numbers? Let us consider the question in terms of languages. Consider a language \(L \subseteq \Sigma^*\) with a finite \(\Sigma\). \(\Sigma^*\) is enumerable (\(\epsilon, a, b, c, aa, ab, ac, \dots\)). Now, is the set of all languages \(\mathcal L\) enumerable? We know that \(\mathcal L = 2^{\Sigma^*}\).</p> <p>We shall use contradiction to show that the set is not enumerable. Consider the following table -</p> <table> <thead> <tr> <th> </th> <th>\(\epsilon\)</th> <th>\(a\)</th> <th>\(\dots\)</th> </tr> </thead> <tbody> <tr> <td>\(L_1\)</td> <td>1</td> <td>0</td> <td> </td> </tr> <tr> <td>\(L_2\)</td> <td>1</td> <td>1</td> <td> </td> </tr> <tr> <td>\(\vdots\)</td> <td> </td> <td> </td> <td>\(\ddots\)</td> </tr> </tbody> </table> <p>The table consists of all the languages (assuming they are enumerable) with the respective strings in the language tagged as 1. Define the language \(L_{new} = \{w_i \mid w_i \not \in L_i\} \forall i \in \mathcal N\). We then have a contradiction that \(\exists L_{new}\) such that \(l_{new} \neq L_i\) for any \(i\).</p> <p><strong>Claim.</strong> The state of every computer can be encoded as a <em>single</em> number.</p> <h2 id="turing-machines-2">Turing Machines (2)</h2> <p>The machine can do the following things</p> <ul> <li>Replace the symbols on the tape by symbols in \(T\) where \(\Sigma \subseteq T - \{B\}\). \(B\) is a blank symbol to denote an empty cell</li> <li>Move left or right by one step</li> <li>Go to the next state</li> </ul> <p>So, the labels on the transitions of the FSM are of the form \(X \mid Y, D\) where \(X, Y \in T\) and \(D = L \mid R\). It means that the symbol \(X\) in the cell is replaced by the symbol \(Y\). The machine has to <em>definitely</em> move left or right in each transition. A “run” starts from the first cell and the state \(q_0\) in the FSM. As an example, we consider the following FSM for the language \(L = \{0^n1^n \mid n \geq 1\}\).</p> <p><img src="/assets/img/Automata/image-20220312155130493.png" alt="image-20220312155130493"/></p> <p><strong>TODO</strong> Draw the above diagram again.</p> <p>In a Turing machine, we halt when there is no move possible.</p> <p>Formally, a Turing machine is defined as \(T = (Q, \Sigma, T, \delta, q_0, B, F)\) where \(\delta: Q \times T \to Q \times T \times \{L, R\}\). There are many variations possible to this machine which we shall show are equivalent to this simple definition. For example, we can have multiple tapes, output tape, non-determinism, etc.</p> <h1 id="lecture-23">Lecture 23</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">08-03-22</code></p> </blockquote> <blockquote> <p>Is countable same as enumerable? Countable implies that there is a one-to-one mapping from the elements in the set to natural numbers. This definition is equivalent to that of enumerability.</p> <p>A Turing machine can enumerate a language. A language is <strong>recursively enumerable</strong> if a Turing machine can enumerate it.</p> <p>\(\Sigma^*\) is enumerable but there are languages in \(\Sigma^*\) which are not enumerable.</p> </blockquote> <p>The theory of Turing machines was developed in the 1930s.</p> <p>Consider the derivation \((q_0, w_{inp}) \vdash^*_M (q_k, x)\) where the machine halts. A machine halts when there is no move at any given state. If \(q_k \in F\) and the machine \(M\) halts, then \(M\) accepts \(w_{inp}\).</p> <h3 id="variations-of-turing-machines">Variations of Turing Machines</h3> <ul> <li>2-way infinite tapes</li> <li>Multi-tape heads</li> <li>Non-determinism</li> <li>Output type (write only, immutable)</li> </ul> <p>Let us try and build the Turing machine for \(L = \{a^nb^nc^n \mid n \geq 1\}\). We’ll use the following logic. In each pass, we will change 1 a, 1 b and 1 c to X, Y and Z respectively. If we have any extra a’s, b’s or c’s, then we will halt.</p> <p><img src="/assets/img/Automata/image-20220312183041093.png" alt="image-20220312183041093"/></p> <p>The machine is given in the above diagram.</p> <blockquote> <p><strong><em>Homework.</em></strong> Draw a Turing machine for the language \(L = \{ww \mid w \in (a + b)^+\}\). Hint - Think of subroutines. That is, tackle smaller problems like “Find the middle of the string”, and then match left and right.</p> <p><strong><em>Homework.</em></strong> Try \(L = \{a^n \mid n \text{ is prime}\}\)</p> </blockquote> <h2 id="type-0-grammars">Type-0 Grammars</h2> <p>These are also known as <strong>Unrestricted grammars</strong>. These are essentially same as CFGs but the LHS of rules can be any string. Consider the language \(L = \{a^nb^nc^n\mid n \geq 0\}\). The grammar can be as follows</p> \[\begin{align} S &amp;\to ABCS \mid T_c \\ CA &amp;\to AC \\ BA &amp;\to AB \\ CB &amp;\to BC \\ CT_c &amp;\to T_cc \\ BT_b &amp;\to T_bb \\ AT_a &amp;\to T_aa \\ T_c &amp;\to T_b \\ T_b &amp;\to T_a \\ T_a &amp;\to \epsilon \end{align}\] <p>If one notices closely, then there are a few issues with this grammar. However, it is correct considering non-determinism. That is, a terminal string derived from this grammar will be in \(L\).</p> <blockquote> <p><strong><em>Homework.</em></strong> Write the Type-0 grammar for \(L = \{ww \mid w \in (a + b)^+\}\)</p> </blockquote> <h1 id="lecture-24">Lecture 24</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">10-03-22</code></p> </blockquote> <p>Let us go back to the \(L = \{a^nb^nc^n\}\) language. How do we come up with a deterministic grammar? We use the idea of <strong>markers</strong>.</p> <ul> <li> <p>1-end marker</p> \[\begin{align} S &amp;\to S_1$ \\ S_1 &amp;\to ABC \mid ABCS_1 \\ CA &amp;\to AC \\ CB &amp;\to BC \\ BA &amp;\to AB \\ C$ &amp;\to c ,\; Cc \to cc \\ Bc &amp;\to bc ,\; Bc \to bb \\ Ab &amp;\to ab ,\; Aa \to aa \\ \end{align}\] <p>The problem with the above grammar is obvious. This issue is often prevented using priority rules in hierarchical grammars. Another fix for this is, adding the following set of rules</p> \[\begin{align} Ca &amp;\to aC ,\; ca \to ac\\ Cb &amp;\to bC ,\; cb \to bc\\ Ba &amp;\to aB ,\; ba \to ab \end{align}\] <blockquote> <p>Check if this is correct</p> </blockquote> </li> <li> <p>2-end marker</p> </li> </ul> <h3 id="tm-as-a-recogniser">TM as a recogniser</h3> <p>Are the set of moves on a TM a decision procedure? A decision procedure refers to a procedure that outputs yes or no for a given input. It involves <em>correctness</em> and <em>termination</em>. No, TM does not do that. For example, consider never halting TMs. Therefore, TM is a semi-decision procedure. In order to classify TM as a decision procedure, we need to show that it halts on every input. Also, we need to show that it gives the correct answer.</p> <h3 id="tm-as-a-computer">TM as a “computer”</h3> <p>Can TM replicate any function of the form \(y = f(X_1, \dots, X_n)\) where the arguments belong to $\mathbb N$. Yes, it can be done. What should a Turing machine do when we give invalid arguments to partial functions? It should never halt. On total functions, the TM must always halt and give the correct answer. For example, we have the following TM for \(f(x, y) = max(0, x - y)\). Numbers \(x\) and \(y\) are written consecutively on the tape in unary form separated by a \(1\).</p> <p><img src="/assets/img/Automata/image-20220328173509367.png" alt="image-20220328173509367"/></p> <h1 id="lecture-25">Lecture 25</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">14-03-22</code></p> </blockquote> <p>If the TM has to halt when \(w \not \in L\), then we shall see that this set of TMs will only recognise recursively enumerable languages. For TMs to accept all the enumerable languages, we must allow the TM to not halt for certain inputs.</p> <h3 id="tm-as-an-enumerator">TM as an enumerator</h3> <p>Such TMs do not take any input. It continuously writes input on the tape and never halts. The TM prints a sequence that looks like the following \(w_1 \# w_2 \# \dots\) for each \(w_i \in L\). The machine halts if \(L\) is finite, but doesn’t otherwise. \(L\) is recursively enumerable iff a TM can enumerate the language.</p> <p><strong>Semi-Decision Procedure</strong> - Given \(M_1\) for \(L\) as an enumerator, construct \(M_2\) as a recogniser for \(L\). In \(M_2\), we can just check if the given input is present in the list of words printed by \(M_1\). Now, we have come up with a semi-decision procedure based on an enumerator.</p> <p>Our goal is to determine if there are any functions that cannot be computed by TMs. Or rather, are there any languages that TM cannot enumerate or recognise. Enumeration and Recognition are equivalent (hint: dovetailing).</p> <h2 id="recursive-function-theory">Recursive Function Theory</h2> <p>Let \(N = \{0, 1, \dots\}\). Any function we consider will be of the form \(f: N^k \to N\). How do we define these functions? We start with base cases and recursion. To define these functions, we shall define some basic functions that will be used to define the others.</p> <ul> <li><strong>Constant function</strong> - \(C^k_n(x_1, \dots, x_k) = n\) for all \(x_1, \dots, x_k \in N\).</li> <li><strong>Successor functions</strong> - \(S(x) = x + 1\).</li> <li><strong>Projection function</strong> - \(P^k_i(x_1, \dots, x_i, \dots, x_k) = x_i\) for \(1 \leq i \leq k\).</li> </ul> <p>We move on to more primitive recursive functions.</p> <ul> <li><strong>Composition</strong> - \(f(x_1, \dots, x_k) = h(g_1(x_1, \dots, x_k), \dots, g_m(x_1, \dots, x_k))\)</li> </ul> <p>Primitive recursion is given by</p> <ul> <li><strong>Basecase</strong> - \(f(0, x_1, \dots, x_k) = g(x_1, \dots, x_k)\)</li> <li><strong>Recursion</strong> - \(f(S(y), x_1, \dots) = h(y, f(y, x_1, \dots, x_k), x_1, \dots, x_k)\)</li> </ul> <p>For example, the formal definition of \(+\) is given by</p> \[\begin{align} +(0, x) &amp;= P^1_1(x) \\ +(S(y), x) &amp;=h(y, +(y, x), x) \\ h(y, +(y, x), x) &amp;= S(+(y, x)) \end{align}\] <p>There are functions which are not primitive recursive.</p> <h1 id="lecture-26">Lecture 26</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">15-03-22</code></p> </blockquote> <p>We shall see that primitive recursion is not expressive enough, Adding “minimization” to our set of rules will help us define any total function. We shall also see <strong>Equational Programming</strong> that is Turing-complete.</p> <p>As we have seen earlier, we are using 3 base functions - constant, successor, and projection. Composition is depicted as \(f = h \circ (g_1, \dots, g_m)\).</p> <p>Primitive-Recursion in layman’s terms refers to recursion using for loops. We write the functions as \(f = \rho(g, h)\) where \(g, h\) are the same functions that are used in the recursive definition. For example, \(add \triangleq \rho(P_1^1, S \circ P_2^3)\). The recursion must be of this form. The formal definition of \(mult\) is given by \(mult \triangleq \rho(C_0^1, add \circ (P_1^3, P_2^3))\).</p> <p>How do we write functions such as \(div\). Do we need <code class="language-plaintext highlighter-rouge">if-then-else</code> notion in our theory? Before we define this, let us try to define \(prev\) functions. We will define that \(prev(0) = 0\) for totality of the function. Then, we can define the function as \(prev \triangleq \rho(C_0^0, P_1^2)\). Using, \(prev\) we can define \(monus\) as \(monus(y, x) = x \dot- y \triangleq \rho(P_1^1, prev \circ P_2^3)\).</p> <p>Now, to introduce the notion of booleans, we define \(isZero \triangleq \rho(C_1^0, C_0^2)\). We can use this function to utilise booleans. Similarly, \(leq \triangleq \rho(C_1^1, isZero \circ monus \circ (S \circ P_1^3, P_3^3))\). Using similar logic, we can implement looping and branching.</p> <h1 id="lecture-27">Lecture 27</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">17-03-22</code></p> </blockquote> <p>We will now see the limitations of primitive-recursion.</p> <h2 id="limitations-of-primitive-recursion">Limitations of primitive recursion</h2> <p>We have seen composition as \(h \circ (g_1, \dots, d_n)\). This concept is equivalent to passing parameters to functions in programming. To refresh the shorthand notion of primitive recursive functions, we define if-then-else as \(if\_then\_else(x, y, z)\) as \(y\) when \(x &gt; 0\) and \(z\) otherwise. Therefore, \(if\_then\_else(x, y, z) = \rho(P_2^2, P_3^4)\). Using this, we define</p> \[\begin{align} quotient(x, y) &amp;\triangleq \rho ( ite \circ (P_1^1, 0, \infty), \\ &amp; ite \circ (P_3^3, ite \circ ( \\ &amp;monus \circ (mult \circ (P_3^3, S\circ P_2^3) , \\ &amp;S \circ P_1^3) , P_2^3 , S \circ P_2^3), \infty) ) \end{align}\] <h3 id="ackermann-function">Ackermann function</h3> <p>Many mathematicians tried to show that primitive recursion is not enough to represent all functions. In this pursuit, Ackermann came up with the following function</p> \[\begin{align} A(0, x) &amp;= x + 1\\ A(y + 1, 0) &amp;= A(y, 1) \\ A(y + 1, x + 1) &amp;= A(y, A(y + 1, x)) \\ \end{align}\] <p>Similarly, there is a ‘91-function’ that Mc Carthy came up with. It is given as</p> \[M_c (x) \begin{cases} x - 10 &amp; x&gt; 100 \\ M_c(M_c(x + 11)) &amp; \text{otherwise} \end{cases}\] <p>The above function always returns \(91\) for \(x &lt; 100\). However, it requires a lot of recursive calls for evaluating its value.</p> <h3 id="all-primitive-functions-are-total">All primitive functions are total</h3> <p>The converse of the above statement is false. For example, the Ackermann function is not primitive. The idea is to show that any \(f\) defined using primitive recursion grows slower than \(A(n_f, y)\) for some \(n_f\). Using <em>Godel numbering</em> we can count all the possible primitive recursive functions.</p> <h2 id="partial-recursive-functions">Partial Recursive functions</h2> <p>We use the idea of <strong>minimisation</strong> to define partial functions and also increase the expressive power of our definitions. We have the following definition</p> \[\mu(f)(x_1, \dots, x_k) \triangleq \begin{cases} z &amp; \forall z_1 &lt; z \; f(z_1, x_1, \dots, x_k) &gt; 0 \land f(z, x_1, \dots, z_k) = 0\\ \end{cases}\] <p>Notice that the first case in the above definition behaves like a while loop, as it gives the smallest value of \(z\) that renders the function zero. The ‘partiality’ in the function definition comes from the fact that \(f\) may never be zero.</p> <p><u>The Church-Turing thesis states that all definable functions that can be defined using primitive recursion, minimisation, and substitution can be computed by a Turing machine.</u> There is no proof for this yet. The set of these functions is the set of ‘all effectively computable functions’. However, there are undecidable functions that are not computable by a TM.</p> <h2 id="equational-logic">Equational Logic</h2> <p>This paradigm has the same expressive power as primitive recursion but is easier to express. To start off, we consider the function \(leq\). We will define \(N\) as \(\{0, S(0), S(S(0)), \dots\}\). We write the rules as</p> \[\begin{align} leq(0, x) &amp;= S(0) \\ leq(S(x), 0) &amp;= 0 \\ leq(S(x), S(y)) &amp;= leq(x, y) \end{align}\] <p>This way of writing rules is known as <strong>Pattern matching</strong>. Similarly, \(gcd\) is given by</p> \[\begin{align} gcd(0, x) &amp;= x \\ gcd(add(x, y), x) &amp;= gcd(y, x) \\ gcd(y, add(x, y)) &amp;= gcd(y, x) \\ \end{align}\] <p>In the above definition, the pattern matching is <em>semantic</em> and not <em>syntactic</em>. The <em>syntactic</em> pattern matching definition uses \(if\_then\_else\). However, note that \(if\_then\_else\) is always assumed to be calculated in a <strong>lazy manner</strong>. That is, the condition is evaluated first and the corresponding branch is then evaluated. If the expressions in the two branches are evaluated along with the condition, then there is a chance that the computation never ends.</p> <p>Also, the factorial function \(fact\) is given by</p> \[\begin{align} fact(0) &amp;= S(0) \\ fact(S(x)) &amp;= mult(S(x), fact(x)) \end{align}\] <p>Another way of writing the factorial function using <strong>tail recursion</strong> is</p> \[\begin{align} f(x) &amp;= f_a(1, x) \\ f(y, 1) &amp;= y \\ f_a(y, S(x)) &amp;= f_a(y\times S(x), x) \end{align}\] <p>Notice that the first definition of \(fact\) is very inefficient compared to the latter. The second definition does inline multiplication with the arguments carrying the answer. However, the values are pushed on to the stack in the case of the first definition. These concepts of optimisation come to great use in building compilers.</p> <h1 id="lecture-28">Lecture 28</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">21-03-22</code></p> </blockquote> <h2 id="term-rewriting">Term-rewriting</h2> <p>We have seen 3 computation paradigms</p> <ul> <li>Turing machine</li> <li>Type-0 grammars</li> <li>Partial recursive functions</li> </ul> <p>Now, we shall see a 4th paradigm called as term rewriting.</p> <p>We have <strong>terms</strong> and <strong>domains</strong>. There a few constructors involved with a domain. For example, we have \(0, S\) for \(N\). Also, for lists we have \(nl\) (empty list) and \(\bullet\) (cons/pipe). We use them as follows</p> <p><img src="/assets/img/Automata/image-20220330010921594.png" alt="image-20220330010921594"/></p> <p>There are also constants (\(0\)) and function symbols/constructors (\(S\)). Termination is guaranteed in normal forms. We also want unique normal form. This will ensure that a term gives a single answer with different recursions (<strong>confluence</strong>). If we get two answers, we call it ill-defined systems.</p> <p>To understand this better, we shall define \(exp\).</p> \[\begin{align} exp(x, 0) &amp;= s(0) \\ exp(x, S(y)) &amp;= mult(x, exp(x, y)) \end{align}\] <p>\(max\) is defined as</p> \[\begin{align} max(0, y) &amp;= y \\ max(S(x), S(y)) &amp;= max(x, y) \\ max(x, 0) &amp;= x \\ \end{align}\] <h3 id="lists">Lists</h3> <p>Suppose we want to define \(app\). We then have</p> \[\begin{align} app(nl, y) &amp;= y \\ app(\bullet(x, y), z) &amp;= \bullet(x, app(y, z)) \end{align}\] <p>Similarly,</p> \[\begin{align} rev(nl) &amp;= nl \\ rev(\bullet(x, y)) &amp;= app(rev(y), \bullet(x, nl)) \end{align}\] <blockquote> <p><strong><em>Homework.</em></strong> Show that \(len(x) = len(rev(x))\)</p> </blockquote> <p>How do we sort a list of numbers?</p> \[\begin{align} sort(nl) &amp;= nl \\ sort(\bullet(x, y)) &amp;= ins(x, sort(y)) \\ ins(x, nl) &amp;= \bullet(x, nl) \\ ins(x, \bullet(y, z)) &amp;= \bullet(\min(x, y), ins(\max(x, y), z)) \end{align}\] <blockquote> <p><strong><em>Homework.</em></strong> Try quick sort and merge sort.</p> </blockquote> <h3 id="termination">Termination</h3> <p>We have termination on \(N\) due to \(&gt;\). How about \(N \times N\)? We use <strong>lexicographic ordering</strong>. One might think it’s not well-founded. This is because \((9, 1)\) is greater than \((5, 1000)\). However, it is well-founded for finite length tuples. In case of strings, the length need not be finite in lexicographic ordering. Therefore, the ordered list need not be finite.</p> <blockquote> <p><strong><em>Homework.</em></strong> Give an ordering rule for multi-sets</p> </blockquote> <h3 id="predicates-and-branching">Predicates and Branching</h3> <p>It is easier to express these things in the term-rewriting paradigm. I’m skipping these for brevity.</p> <p>Can we write \(isPrime\), \(gcd\), and \(nth-prime\)? This paradigm naturally develops to Functional Programming. It’s essentially writing everything in terms of functions as we’ve been doing so far.</p> <h3 id="logic-programming">Logic Programming</h3> <p>Suppose we give the black-box the input \(x + 3 = 10\). Can we determine \(x\)? Can we ask multiple answers in case of \(u + v = 10\)? It is possible to do so in logic programming. We do this by <u>passing parameters by unification</u>. That is, we try to convert the parameters \(u, v\) to look like the parameters given in the rules of \(+\). This is known as <strong>backtracking</strong>.</p> <h1 id="lecture-29">Lecture 29</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">22-3-22</code></p> </blockquote> <p>Term matching is don’t care non-determinism. There might be multiple possible reductions at any given situation, and all will lead to the same answer in case of well-formed normal forms. Also, all the paths should terminate.</p> <p>What are the strategies for computing normal forms?</p> <ul> <li><strong>Hybrid</strong> - Check if sub-term matches rule-by-rule</li> <li><strong>Innermost</strong> - Many programming languages adopt this. Evaluate the “lowest” redex first.</li> <li><strong>Outermost</strong> - This is lazy evaluation. We evaluate topmost redex first.</li> </ul> <p>Redex is a sub-term where a rule can be applied.</p> <p>And then, sir lost his connection :D</p> <blockquote> <p><strong><em>Aside.</em></strong> Write primality test function using primitive recursion. \(\begin{align} prime(0) &amp;= 0 \\ prime(S(x)) &amp;= g(x, S(x)) \\ \\ g(0, y) &amp;= 0 \\ g(S(x), y) &amp;= ite(isZero(y), 1,\\ &amp;ite(isZero(monus(S(x), gcd(S(x), y))), 0, g(x, y))) \\ \\ gcd(0, y) &amp;= y \\ gcd(S(x), y) &amp;= ite(gte(S(x), y), \\ &amp;gcd(monus(S(x), y), y), gcd(y, x)) \end{align}\)</p> </blockquote> <h1 id="lecture-30">Lecture 30</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">24-03-22</code></p> </blockquote> <p>We were discussing the definition of \(quotient\) in the previous lecture. In terms of minimisation, we can write the definition as</p> \[\begin{align} quotient(x, y) &amp;= \mu(f)(x, y) \\ f(k, x, y) &amp;= y*k &gt; x \end{align}\] <h2 id="encoding-turing-machines">Encoding Turing machines</h2> <p>Let us consider the example of enumerating all the finite subsets of \(N\). A subset \((n_1, n_2, \dots, n_k)\) is represented as \(m = p_1^{n_1} \times \dots \times p_k^{n_k}\) where \(p_1, \dots, p_k\) are the first \(k\) prime numbers. Note that we need to ignore trailing zeros for this.</p> <p>Can we enumerate Turing Machines? Let us try to encode a Turing Machine as a bit string. The core idea is that a Turing Machine has a finite set of states. We can encode a transition in the following way - Suppose we have the transition \(\delta(q_3, X_1) = (q_7, X_2, D_0)\). Then, the bit string corresponding to this is \(11\; 000 \;1 \; 0000000 \; 1\; 0 \; 1 \; 00 \; 1 \; 0 \; 11\) Here, a \(1\) just acts as a separator, and every state, tape symbol and direction are encoded in unary format. A substring \(11\) represents the start of a transition, followed by \(q_3, q_7, X_1, X_2, D_0\) for the above example. This way, we can list all the transitions of the Turing Machine. All the transitions can be enclosed between a pair of substrings \((111, 111)\). Now, we just need to add the set of final states.</p> <p>Instead of listing out all the final states, we can convert the Turing Machine to an equivalent TM which has a single finite state.</p> <blockquote> <p><strong><em>Homework.</em></strong> Prove that the above conversion can be done for any Turing Machine.</p> </blockquote> <p>Now that we have encoded a TM, how do we enumerate the set of all TMs? We know how to enumerate bit strings based on the value and the length (ignore leading \(0\)‘s for non-ambiguity). We need out to weed out the bit strings that do not represent a valid TM.</p> <p>What features are present in a bit string that represents a TM? It needs to start with \(111\), and end with \(111\). If there is a \(11\) in between the above substrings, there need to be at least 4 \(1\)‘s in between with appropriate number of \(0\)’s in between. This concludes our discussion on encoding TMs.</p> <h2 id="variants-of-tm">Variants of TM</h2> <p>Let us consider the time of execution with a single computer and \(n\) computers. Any task will have at most \(n\) times speedup when done on \(n\)-computers in comparison to a single computer (generally). For example, matrix multiplication can be heavily parallelised. However, tasks such as gcd calculation of \(k\) numbers is <em>inherently sequential</em> and it is not easy to speed it up using parallel computation.</p> <p>Now, we shall consider variants of TMs and show equivalence of each with the 1-way infinite tape variant.</p> <h3 id="2-way-infinite-tape">2-way infinite tape</h3> <p>It is easy to see that a 2-way infinite tape can be restricted of the left movement beyond a point to simulate all the 1-way infinite tape machines.</p> <p>To prove the other direction, we will consider a multi-track 1-way TM. That is, every cell will now contain 2 elements. The basic idea is that a 2-way machine of the form \(\dots \mid B_1 \mid B_0 \mid A_0 \mid A_1 \mid \dots\) will be converted to \((A_0, C) \mid (A_1, B_0) \mid \dots\).</p> <p>The states in the 2-way machine will be separated based on whether the transition is on the left side of the tape or the right side. Then, based on the side, we will work on the first element in the tuple of each cell or the second element. The formal proof is left to the reader.</p> <h3 id="multi-tapes">Multi tapes</h3> <p>There are multiple tapes under a single control. Therefore, each transition is represented as \(( X_1, \dots, X_k )\mid ( Y_1, \dots, Y_k) , (D_1, \dots, D_k)\). One can see that all the logic boils down to the Satan-Cantor puzzle.</p> <p>Again, one direction of the equivalence is straightforward. The other half involves converting a \(k\)-tuple to a natural number.</p> <h3 id="non-determinism-2">Non-Determinism</h3> <p>The equivalence can be shown in a similar way as that of regular languages.</p> <h3 id="k-head-machine">k-head machine</h3> <p>We have a single tape which has multiple heads that move independently.</p> <h3 id="offline-tm">Offline TM</h3> <p>The input tape never changes. That is, the actions are read-only.</p> <h1 id="lecture-31">Lecture 31</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">28-03-22</code></p> </blockquote> <h2 id="universal-turing-machine">Universal Turing Machine</h2> <p>The intention is to build a general purpose computer. Recollect that a Turing Machine is represented as \(M = (Q, \Sigma, T, \delta, q, B, F)\).</p> <p>Every Turing Machine has an equivalent TM</p> <ul> <li>whose alphabet is \(\{0, 1\}\) and the set of tape symbols is \(\{0, 1, B\}\), and</li> <li>has a single final state.</li> </ul> <p>\(L\) is</p> <ul> <li><strong>recursively enumerable</strong> - there exists a TM (recogniser) that accepts \(L\) (need not halt on wrong input, think of recogniser built from enumerators).</li> <li><strong>recursive</strong> - there is a TM that accepts \(L\) <u>which halts on all inputs</u>.</li> </ul> <p>For example, consider the language that accepts a pair of CFGs \((G_1, G_2)\) if \(L(G_1) \cap L(G_2) \neq \phi\). To show that this language is recursively enumerable, we will give a <em>high level</em> algorithm in terms of primitive steps that we know can be converted to a TM. We consider the following algorithm.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Enumerate L(G_1) // enumerate w, check G_1 =&gt;* w (CYK)
- For each w in L(G_1) check if G_2 =&gt;* w
</code></pre></div></div> <p>Now, if we generate a word and check for the word in the other grammar alternately, then the algorithm will halt if the input is acceptable. However, it is not guaranteed to halt in case of unacceptable input for this algorithm. Does that mean the language is recursively enumerable?</p> <p>We need to show that we can’t construct an algorithm that halts in case of an unacceptable input. Then, we can conclusively state that the language is recursively enumerable.</p> <p>In conclusion, there are language that are recursively enumerable, and languages that are not recursively enumerable. The set of recursive languages are a strict subset of the set of recursively enumerable languages.</p> <h3 id="non-recursive-language">Non-recursive language</h3> <p>We know that we can enumerate all TMs and all \(w \in \Sigma^*\). Then, we draw a 2D infinite bit matrix A where \(A_{ij}\) tells whether \(w_i \in M_j\). Now, we use the Cantor’s diagonalization argument to conclude that \(L_d = \{i \mid A_{ii} = 0\}\) is not recursively enumerable. <strong>Genius proof</strong> based off Barber’s paradox.</p> <p>If \(L\) is not recursively enumerable, then \(\bar L\) is also not recursively enumerable. That is, show that if TM accepts \(L\), then it must also accept \(\bar L\).</p> <p>One might ponder on how we construct the table \(A\) when some TM may not halt for a few inputs. The important distinction is that, we are “defining” this table conceptually and not “computing” it. The concept of dovetailing also comes to use here.</p> <h3 id="recursively-enumerable-but-not-recursive">Recursively enumerable but not Recursive</h3> <p>The language of the <strong>Universal Turing Machine</strong> is the required example. We formally define this language as \(L_u = \{( M, w) \mid\ M \text{ halts on } w\}\).</p> <p>Equivalently, we are trying to write a Python script that takes another Python script as input along with some arguments. This script should halt when the input python script halts on the input argument, and need not halt otherwise. Basically, we are trying to build a simulator TM.</p> <blockquote> <p>No lectures 32 and 33.</p> </blockquote> <h1 id="lecture-34">Lecture 34</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">04-04-22</code></p> </blockquote> <h2 id="undecidability">Undecidability</h2> <p>A language \(L \subseteq \Sigma^*\) is <strong>recursive</strong> if there is a Turing Machine \(M\) that halts in an accept state if \(w \in L\) and in a reject state if \(w \not\in L\). It <em><u>need</u></em> not halt in reject states for <strong>recursively enumerable</strong> languages. The algorithms similar to the latter TMs are <strong>semi-decidable</strong>.</p> <p>We were looking for languages that are not recursively enumerable \(L_d\) and languages that are recursively enumerable but not recursive \(L_u\). For the latter set, we had seen the universal TM. We also have the language of “polynomials that have integer roots”. How do we show that this is recursively enumerable? The key idea lies in encoding polynomials as a number. The motivation for this language comes from <strong>Hilbert’s 10th Problem</strong>.</p> <p>Another example for an undecidable problem is the <strong>Halting Problem</strong>.</p> <p>Let us continue the discussion on \(L_u\) and \(L_d\) from the last lecture. We had constructed a matrix \(A\) and showed using the diagonalization argument that there are languages that are not recursively enumerable. However, we did not address two issues</p> <ul> <li>There are multiple TMs that accept the same language - It does not matter.</li> <li>How do we fill the table? - Computation vs Definition.</li> </ul> <p>Also, all the languages defined by each row \(A_i\) in the matrix form the set of recursively enumerable languages.</p> <p>We had defined \(L_u\) as \(\{(M_i, w_j) \mid M_i \text{ accepts } w_j \}\). We will show that this language is recursively enumerable by constructing a Turing Machine \(M\) that accepts this language. The input tape will initially have the encoding of \(M_i\) followed by the encoding of \(w_j\). Now, we use two more tapes</p> <ul> <li>One for copying \(w_j\)</li> <li>Another tape for keeping track of the state in \(M\), starting with state 0.</li> </ul> <p>Now, we give the higher level logic of \(M\).</p> <ul> <li>Validate \(M_i\). This can be done using a TM.</li> <li>Run \(M_i\) step-by-step. Pick the top state from the 3rd tape, find the corresponding move from the first tape, and update the 2nd and 3rd tapes.</li> </ul> <p>The last two tapes help simulate any input TM. In conclusion, we have constructed a universal TM. Therefore, the TM paradigm is powerful enough to perform <strong>self-introspection</strong>.</p> <p>Note that we have still not shown \(L_u\) is not recursive. We will use the properly of \(L_d\) not being recursively enumerable to show this.</p> <h1 id="lecture-35">Lecture 35</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">05-04-22</code></p> </blockquote> <p>Let us continue the discussion on the Universal Turing Machine. The encoding of a move \(\delta(q_i, X_j) = (q_k, Xl, D_m)\) in a Turing Machine is \(0^i10^j10^k10^l10^m\). The universal TM works in 3 steps</p> <ul> <li>Decode/Validate</li> <li>Set up the tapes</li> <li>Simulate</li> </ul> <p>Also, we had shown that \(L_u\) is recursively enumerable by the construction of the universal TM. Suppose, there is a TM that halts every time for \(L_u\). Now, we will show that \(L_u\) is recursive then \(L_d\) is recursive. To do that, we will convert \(L_d\) into an instance of \(L_u\).</p> <p><img src="/assets/img/Automata/image-20220413194329791.png" alt="image-20220413194329791"/></p> <p>Here, we can see that we are able to answer the question \(w_i \in L_d\) by constructing a TM using the Universal TM. That is, if the Universal TM is recursive, then \(L_d\) is also recursive. <em>Think</em>.</p> <h2 id="reduction-technique">Reduction Technique</h2> <p>To understand this technique, consider the following two problems</p> <ul> <li>\(L_{ne} = \{( M ) \mid L(M) \neq \phi\}\) - Machine accepts at least one string</li> <li>\(L_{rec} = \{( M ) \mid L(M) \text { is recursive}\}\) - Machine halts on all inputs</li> </ul> <p>We can show that both are recursively enumerable. The first language can be shown as recursively enumerable using dovetailing (words and moves being the dimensions). To show non-recursivity, we convert \(L_d\) or \(L_u\) to an instance of this language. We need to try and come up with an algorithm that halts for machines with empty languages.</p> <blockquote> <p>How do we do this?</p> </blockquote> <p>The second problem is a little bit more difficult.</p> <p>In conclusion, given a language</p> <ul> <li>To prove that it is recursive, we need to construct a TM that halts on all inputs.</li> <li>To prove that it is recursively enumerable, we just need to construct a TM that halts on good inputs. To show that it is not recursive, we need to use reductions probably.</li> <li>To prove non recursively enumerable, we need to show that we can’t construct any TM.</li> </ul> <p>TM is a powerful model as it is universal (self-referential), and it has many undecidable problems. They can represent Type-0 Grammar, Partial Recursive functions, and RAM (random access mechanism).</p> <h2 id="post-correspondence-problem">Post Correspondence Problem</h2> <p>Post is a mathematician. Consider the following table</p> <table> <thead> <tr> <th>w</th> <th>x</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>111</td> </tr> <tr> <td>1011</td> <td>10</td> </tr> <tr> <td>10</td> <td>0</td> </tr> </tbody> </table> <p>Now, take the string \(101111110\). Can we decompose this string in terms of \(w_i\) or \(x_i\)? For example, we can decompose the string as \(2113\) in terms of \(w's\) and \(2113\) again in terms of \(x’s\). These decompositions need not be unique.</p> <p>So, PCP is stated as follows.</p> <p>Given \(w = \{w_i \mid w_i \in \Sigma^* \}, x = \{x_i \mid x_i \in \Sigma^*\}\) and a string \(s \in \Sigma^*\), we need to be able to come up with decompositions \(s = w_{i1} \dots w_{il}\) and \(s = x_{j1}\dots x_{jl}\) such that \(ik = jk\) for \(k \in \{1, \dots, l\}\).</p> <p>Consider the following table.</p> <table> <thead> <tr> <th>w</th> <th>x</th> </tr> </thead> <tbody> <tr> <td>10</td> <td>101</td> </tr> <tr> <td>011</td> <td>11</td> </tr> <tr> <td>101</td> <td>011</td> </tr> </tbody> </table> <p>Now, for any string \(s\), we will not be able to come up with decompositions that are equal for the above sets.</p> <p>The reason we came up with this problem, is to show that PCP is undecidable (not recursive). That is, no TM can halt correctly on all PCP instances. However, it is recursively enumerable as we can enumerate all sequences fairly. This can be shown by a reduction of \(L_u\) to PCP.</p> <p>Let us now consider another problem - Ambiguity of CFGs. Can we build a TM that halts with a ‘yes’ if the input CFG has an ambiguity and with a ‘no’ is it does not. In an ambiguous grammar, a single word has multiple derivations in the grammar. That is, we have multiple structurally different derivation trees. Using this problem we are trying to show the importance of <strong>direction of reduction</strong>. We know that PCP is undecidable. Therefore, we need to reduce PCP to an instance of the CFG problem. That is, we’ll essentially show that is the CFG problem is decidable, then so is PCP.</p> <p>PCP is essentially a pair of sets of strings. What is the grammar corresponding to these sets such that when PCP has a solution, the grammar is ambiguous and unambiguous otherwise? We introduce \(k\) new symbols \(a_1, \dots, a_k\). We construct the following grammar</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>S -&gt; Sa | Sb
Sa -&gt; w_i Sa a_i | w_i a_i
Sb -&gt; x_i Sb a_i | x_i a_i
</code></pre></div></div> <p>It is easy to see that this grammar meets our requirements. To prove this, we need to show the proof in two directions</p> <ul> <li>If PCP has a solution, then G is ambiguous</li> <li>If G is ambiguous, then PCP has a solution. Here, we need to show that ambiguity happens only due to <code class="language-plaintext highlighter-rouge">Sa</code> and <code class="language-plaintext highlighter-rouge">Sb</code>.</li> </ul> <blockquote> <p>Why did we introduce <code class="language-plaintext highlighter-rouge">a</code>?</p> </blockquote> <p><strong>Note.</strong> To show problem A is undecidable knowing that problem B is undecidable, we essentially show that A is a subroutine of B.</p> <h2 id="properties-of-recursive-languages">Properties of Recursive Languages</h2> <p>If \(L_1\) and \(L_2\) are both recursive, then so are</p> <ul> <li> \[L_1 \cup L_2\] </li> <li> \[L_1 \cap L_2\] </li> <li> \[L_1 - L_2\] </li> <li> \[L_1^c\] </li> <li> \[L_1L_2\] </li> </ul> <p>The intersection and complement closure break in case of recursively enumerable languages <strong>(?)</strong></p> <h1 id="lecture-36">Lecture 36</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">07-04-22</code></p> </blockquote> <p><strong>Note.</strong> Don’t get confused into thinking there are three (disjoint) kinds of sets: decidable, semi-decidable, and undecidable. There are two kinds: decidable and undecidable. Semi-decidable falls under undecidable.</p> <h2 id="np-completeness">NP-Completeness</h2> <h3 id="decision-problems">Decision Problems</h3> <p>These are a class of problems that deal with problem instances like Fermat’s last theorem - \(\forall n &gt; 2\exists a, b, c, a^n + b^n = c^n\)? We also have the law of excluded middle in decision problems - The answer is either ‘yes’ or ‘no’.</p> <p>Optimisation problems can be converted to a decision problem.</p> <h3 id="decidable-problems">Decidable Problems</h3> <p>We solve these problems by building a brute-force TM - <strong>generate and test method</strong>. For example, consider the problem of finding a vertex cover in a graph of size \(k\). Now, the TM will enumerate all subsets of size \(k\) and check each one for vertex cover conditions.</p> <p>We come up with “algorithms” for these problems and check their efficiency. Apart from time and space complexity, we also consider <strong>descriptive (Kolmogorov) complexity</strong>. It refers to the program size. In our case, it refers to the size of the TM.</p> <h3 id="complexity-classes">Complexity Classes</h3> <p>Given two problems, which problem is “more difficult”? The difficulty criterion we consider is time. We define two classes of problems \(P\) and \(NP\).</p> <ul> <li><strong>P</strong> is the class of (decision) problems that can be solved in “polynomial time” by a deterministic Turing Machine \(M\).</li> <li><strong>PSPACE</strong> refers to the class of problems that use polynomial number of cells in the tape.</li> <li><strong>NP</strong> is the class of (decision) problems that can be solved in <strong>polynomial time</strong> by <u>non-deterministic TM</u>. This is equivalent to guessing a solution in polynomial time followed by checking using a deterministic TM in polynomial time. The claim is that this is same as a non-deterministic TM. We shall use this technique to show that problems are in NP. That is, we’ll show that we can guess in P-time and verify in P-time.</li> </ul> <p>For example, let us try to show vertex cover problem is in \(NP\). Aside - A polynomial algorithm was developed in the 1980s for the problem “Is \(n\) a prime number” in IITK (Manindra Agarwal).</p> <h3 id="more-difficult-problem">More difficult problem</h3> <p>Given \(L_1\) and \(L_2\), which one is more difficult? We use reduction to answer this question.</p> \[L_1 \leq_{P} L_2\] <p>If \(L_1\) can be converted to an instance of \(L_2\) in polynomial time, then we have the above defined relation. That is, \(L_2\) is at least as difficult as \(L_1\).</p> <h3 id="completeness">Completeness</h3> <p>We try to determine the most difficult problem amongst all the problems. To start off, we define that \(L_{sat}\) belongs to NP-complete set. This problem refers to the satisfiability of propositional logic expressions.</p> <p>Every problem in \(NP\) can be reduced to \(L_{sat}\). In some sense, \(L_{sat}\) is the most difficult problem. But how did we come to this conclusion without knowing all the problems in \(NP\)? We can show that <u>any non-deterministic TM can be reduced to</u> \(L_{sat}\).</p> <h1 id="lecture-37">Lecture 37</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">11-04-22</code></p> </blockquote> <p>Let us continue the topic of NP-completeness.</p> <p>We define hard problems as anything not in \(P\). There are problems that are more difficult than the ones in \(NP\)!</p> <p>We classify decidable problems as the set of recursive languages. These problems can be further classified as</p> <ul> <li>Deterministic TM takes only polynomial number of steps in terms of the input size to halt.</li> <li> <p>A non-deterministic TM takes polynomial number of steps.</p> </li> <li>A non-deterministic TM needs \(\Omega (2^n)\) number of steps.</li> </ul> <h3 id="sat-problem-p_sat">SAT Problem \(P_{sat}\)</h3> <p>Given a well formed formula \(F\), is \(F\) satisfiable? A well formed formula is a formula that is formed via the base case (propositional variables) and inductive rules (using connectors like negation, ‘and’ and ‘or’).</p> <p>This problem can be solved via brute force but that is exponential. So we now ask the following questions</p> <ul> <li>Is \(P_{sat} \in P\)?</li> <li>Is \(P_{sat} \in NP\)?</li> </ul> <p>We’ll answer the second question first. It is easy to construct a non-deterministic TM that solves the problem in polynomial steps. We can generate a random guess (bit string) and verify (check satisfiability) in polynomial steps. This procedure is the way to show that any problem is in \(NP\).</p> <h3 id="comparison-of-problems">Comparison of Problems</h3> <p>Let us revisit the topic of \(\leq_{P}\). \(P_{sat} \leq_{P} 3\text{-}SAT\), and \(3\text{-}SAT \leq_P VC\) where \(VC\) is the vertex cover problem.</p> <p>To do perform the latter reduction, we construct nodes for \(p\) and \(\neg p\) for each propositional variable in the clause. Then, we construct cliques out of each disjunction in the CNF. Finally, we connect each \(p\) with \(\neg p\). Now, if we can find a vertex cover of this graph with size \(2k\), we get the satisfiability of the 3-CNF clause with \(k\) disjunctive clauses.</p> <ul> <li>If \(F\) has a satisfying assignment, take out exactly one literal with value \(1\) out of each disjunctive clause and construct a VC out of the remaining literals. It can be shown that the vertex cover will have size \(2k\).</li> <li>If there is a vertex cover, then we can drop at most 1 node from each disjunctive clause. Also, as the size if \(2k\) we can drop exactly 1 node from each clause. Assign these dropped nodes with a value of \(1\). Now, since we have a true value in each clause, the assignment satisfies the expression.</li> </ul> <h3 id="np-completeness-1">NP-Completeness</h3> <p>As we mentioned before, we consider \(P_{sat}\) or \(3\text{-}SAT\) as the hardest problem. That is, for every \(Q \in NP\), \(Np \leq_P P_{sat}\).</p> <p>We show this by reducing non-deterministic Turing Machines to \(P_{sat}\). To prove that a problem is NP-complete, we show</p> <ul> <li>That the problem lies in \(NP\). This step is very important as there are problems that are more difficult than \(NP\).</li> <li>Convert the 3-SAT problem (or any other known NP-complete problem) to an instance of the given problem.</li> </ul> <p>We list out some problems in NP-complete set.</p> <ul> <li>3-SAT</li> <li>Vertex Cover</li> <li>Clique in a graph</li> <li>\(k\)-coloring</li> <li>Min-cut and Max-flow</li> <li>Travelling Salesman Problem</li> <li>Hamiltonian Cycle in a graph</li> <li>Partitioning of a set of numbers such that the partitions have the same sum</li> <li>Knapsack problem</li> </ul> <p>The good news about \(NP\)-complete problems is that there are many heuristics and approximations such as LAs Vegas and Monte Carlo that work well for most of the above problems.</p> <hr/> <h4 id="end-of-course">END OF COURSE</h4> <hr/>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[An introductory course to Automata theory. The first half covers DFAs, NFAs, and their various properties. Relations of regularity of languages and DFAs/NFAs and proofs of non-regularity of languages. The second half of the notes covers pushdown automata, context free grammar and their relation with deterministic PDAs. Briefly touches upon Turing machines.]]></summary></entry><entry><title type="html">DiBS Notes</title><link href="https://sudhansh6.github.io/blog/dbms/" rel="alternate" type="text/html" title="DiBS Notes"/><published>2022-01-06T00:00:00+00:00</published><updated>2022-01-06T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/dbms</id><content type="html" xml:base="https://sudhansh6.github.io/blog/dbms/"><![CDATA[<h1 id="lecture-1">Lecture 1</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">03/01/2022</code></p> </blockquote> <h1 id="chapter-1-introduction">~Chapter 1: Introduction</h1> <ul> <li><strong>Embedded databases</strong> - Databases which don’t have high amount of concurrent access, but is mostly used by a single user. These databases are implemented by SQLite in general.</li> <li> <p>Motivation for Database systems -</p> <ul> <li>Atomicity</li> <li>Concurrency</li> <li>Security</li> <li>…</li> </ul> </li> </ul> <h2 id="data-models">Data Models</h2> <p>A collection tools for describing the data, relationships in the data, semantics and other constraints.</p> <p>We start using the relational models that are implemented using SQL. Then, we shall study Entity-Relationship data model. These are used for database design. We will briefly touch upon Object-based data models. Finally, we shall see Semi-structured data model as a part of XML.</p> <p>There are also other models like Network model and Hierarchical model (old) etc.</p> <h2 id="relational-model">Relational Model</h2> <p>All the data is stored in various tables. A <strong>relation</strong> is nothing but a table. Back in the 70’s, Ted Codd (Turing Award 1981) formalized the Relational Model. According to his terminology, a table is same as a <em>relation</em>, a column is also called an <em>attribute</em>, and the rows of the table are called as <em>tuples</em>. The second major contribution he made was introducing the notion of operations. Finally, he also established a low-level database engine that could execute these operations.</p> <p>Some more terminology - <strong>Logical Schema</strong> is the overall logical structure of the database. It is analogous to the type information of a variable in a program. A <strong>physical schema</strong> is the overall physical structure of the database. An <strong>instance</strong> is the actual content of the database at a particular point in time. It is analogous to the value of a variable. The notion of <strong>physical data independence</strong> is the ability to modify the physical schema without changing the logical schema.</p> <h3 id="data-definition-language-ddl">Data Definition Language (DDL)</h3> <p>It is the specification notation for defining the database schema. DDL compiler generates a set of table templates stored in a <em>data dictionary</em>. A data dictionary contains metadata (data about data) such as database schema, integrity constraints (primary key) and authorization.</p> <h3 id="data-manipulation-language-dml">Data Manipulation Language (DML)</h3> <p>It is the language for accessing and updating the data organized by the appropriate data model. It is also known as a <em>query language</em>. There are basically two types of data-manipulation languages - <strong>Procedural DML</strong> requires a user to specify what data is needed and how to get that data; <strong>Declarative DML</strong> requires a user to specify what data is need without specifying how to get those data. Declarative/non-procedural DMLs are usually easier to learn.</p> <h3 id="sql-query-language">SQL Query Language</h3> <p>SQL query language is <strong>non-procedural</strong>! It is declarative. SQL is <strong>not</strong> a Turing machine equivalent language. There are extensions which make it so. SQL does not support actions such as input from the users, typesetting, communication over the network and output to the display. A query takes as input several tables and always returns a single table. SQL is often used embedded within a higher-level language.</p> <p><strong>Database Design</strong> involves coming up with a Logical design and Physical design.</p> <p>A <strong>Database Engine</strong> accepts these queries and parses them. It is partitioned into modules that deal with each of the responsibilities of the overall system. The functional components of a database system can be divided into</p> <ul> <li> <p>Storage manager - Actually stores the data. It takes the logical view and maps it to the physical view. It is also responsible to interact with the OS file manager for efficient storing, retrieving and updating of data. It has various components such as authorization and integrity manager, transaction manager, file manager and buffer manager. It implements several data structures as a part of the physical system implementation - data files, data dictionary and indices.</p> </li> <li> <p>Query processor - It includes DDL interpreter, DML compiler (query to low-level instructions along with query optimization) and the query evaluation engine.</p> <p>Query processing involves parsing and translation, optimization, and evaluation. Statistics of the data are also used in optimization.</p> </li> <li> <p>Transaction management - A <strong>transaction</strong> is a collection of operations that performs a single logical function in a database application. The <strong>transaction management component</strong> ensures that the database remains in a consistent state despite system failure. The <strong>concurrency control manager</strong> controls the interaction among the concurrent transactions to ensure the consistency of the data.</p> </li> </ul> <h1 id="lecture-2">Lecture 2</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">04-01-22</code></p> </blockquote> <h3 id="database-architecture">Database Architecture</h3> <p>There are various architectures such as centralized databases, client-server, parallel databases and distributed databases.</p> <h1 id="chapter-2-intro-to-relational-model">~Chapter 2: Intro to Relational Model</h1> <h3 id="attributes">Attributes</h3> <p>The set of allowed values for each attribute is called the <strong>domain</strong> of the attribute. Attribute values are required to be <em>atomic</em> - indivisible. Todd realized that having sets or divisible attributes complicates the algebra. The special value <strong><em>null</em></strong> is a member of every domain that indicates the value is unknown. The null values causes complications in the definition of many operations.</p> <p>Relations are <strong>unordered</strong>. The order of tuples is irrelevant for the operations logically.</p> <p><strong>Database Schema</strong> - The logical structure of the database.</p> <h3 id="keys">Keys</h3> <p><em>K</em> is a <strong>superkey</strong> of the relation <em>R</em> if values for <em>K</em> are sufficient to identify a unique tuple of each possible relation \(r(R)\). Superkey \(K\) is a <strong>candidate key</strong> if \(K\) is minimal. One of the candidate keys is selected to be the <strong>primary key</strong>. A <strong>foreign key</strong> constraint ensures that the value in one relation must appear in another. There is a notion of <em>referencing</em> relation and a <em>referenced</em> relation.</p> <h3 id="relational-query-languages">Relational Query Languages</h3> <p>Pure languages include Relational algebra, Tuple relational calculus and Domain relational calculus. These three languages are equivalent in computing power.</p> <h1 id="lecture-3">Lecture 3</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">06/01/2022</code></p> </blockquote> <h2 id="relational-algebra">Relational Algebra</h2> <p>An algebraic language consisting of a set of operations that take one or two relations as input and produces a new relation as their result. It consists of six basic operators -</p> <ul> <li>select \(\sigma\)</li> <li>project \(\Pi\)</li> <li>union \(\cup\)</li> <li>set difference \(-\)</li> <li>Cartesian product \(\times\)</li> <li>rename \(\rho\)</li> </ul> <p>We shall discuss each of them in detail now.</p> <h3 id="select-operation">Select Operation</h3> <p>The <strong>select</strong> operation selects tuples that <u>satisfy a given predicate</u>. So, it’s more like <code class="language-plaintext highlighter-rouge">where</code> rather than <code class="language-plaintext highlighter-rouge">select</code> in SQL. The notation is given by \(\sigma_p(r)\).</p> <h3 id="project-operation">Project Operation</h3> <p>A unary operation that returns its argument relation, with certain attributes left out. That is, it gives a subset of attributes of tuples. By definition, it should only return the attributes. However, in most cases we can return modified attributes. The notation is given by \(\Pi_{attr_1, attr_2, ...}(r)\)</p> <h3 id="composition-of-relation-operations">Composition of Relation Operations</h3> <p>The result of a relational-algebra is a relation and therefore we different relational-algebra operations can be grouped together to form <u>relational-algebra expressions</u>.</p> <h3 id="cartesian-product-operation">Cartesian-product Operation</h3> <p>It simply takes the cartesian product of the two tables. Then, we can use the select condition to select the relevant (rational) tuples.</p> <p>The <strong>join</strong> operation allows us to combine a select operation and a Cartesian-Product operation into a single operation. The join operation \(r \bowtie_\theta s = \sigma_\theta (r \times s)\). Here \(\theta\) represents the predicate over which join is performed.</p> <h3 id="union-operation">Union operation</h3> <p>This operation allows us to combine two relations. The notation is \(r \cup s\). For this operation to be vald, we need the following two conditions -</p> <ul> <li>\(r, s\) must have the same <strong>arity</strong> (the same number of attributes in a tuple).</li> <li>The attribute domains must be <u>compatible</u>.</li> </ul> <blockquote> <p>Why second?</p> </blockquote> <h3 id="set-intersection-operation">Set-Intersection Operation</h3> <p>This operator allows us to find tuples that are in both the input relations. The notations is \(r \cap s\).</p> <h3 id="set-difference-operation">Set Difference Operation</h3> <p>It allows us to find the tuples that are in one relation but not in the other.</p> <h3 id="the-assignment-operation">The assignment Operation</h3> <p>The assignment operation is denoted by \(\leftarrow\) and works like the assignment in programming languages. It is used to define temporary relation variables for convenience. With the assignment operation, a query can be written as a sequential program consisting of a series of assignments followed by an expression whose value is displayed as the result of the query.</p> <h3 id="the-rename-operation">The rename Operation</h3> <p>The expression \(\rho_x(E)\) is used to rename the expression \(E\) under the name \(x\). Another form of the rename operator is given by \(\rho_{x(A1, A2, ...)}(E)\).</p> <blockquote> <p>Difference between rename and assignment? Is assignment used to edit tuples in a relation?</p> </blockquote> <p>Are these set of relational operators enough for Turing completeness? No! Check <a href="https://www.quora.com/Turing-Completeness/Why-is-relational-algebra-not-Turing-complete#:~:text=Relational%20algebra%20clearly%20doesn't,analysis%20such%20as%20query%20optimizers.">this</a> link for more info.</p> <h3 id="aggregate-functions">Aggregate Functions</h3> <p>We need functions such as <code class="language-plaintext highlighter-rouge">avg</code>, <code class="language-plaintext highlighter-rouge">min</code>, <code class="language-plaintext highlighter-rouge">max</code>, <code class="language-plaintext highlighter-rouge">sum</code> and <code class="language-plaintext highlighter-rouge">count</code> to operate on the multiset of values of a column of a relation to return a value. Functions such as <code class="language-plaintext highlighter-rouge">avg</code> and <code class="language-plaintext highlighter-rouge">sum</code> cannot be written using FOL or the relations we defined above. Functions such as <code class="language-plaintext highlighter-rouge">min</code> and <code class="language-plaintext highlighter-rouge">max</code> can be written using a series of queries but it is impractical. The other way of implementing this is to use the following</p> <div style="text-align:center;"> $$ \Pi_{mark}(marks) - \Pi_{m1.mark}(\sigma_{m1.mark &gt; m2.mark} \\ (\rho_{m1}(marks) \times \rho_{m2}(marks))) $$ </div> <p>However, this definitive expression is very inefficient as it turns a linear operation to a quadratic operation.</p> <p><strong>Note.</strong> The aggregates <strong>do not</strong> filter out the duplicates! For instance, consider $\gamma_{count(course_id)}(\sigma_{year = 2018}(section))$. What if a course has two sections? It is counted twice.</p> <h3 id="group-by-operation">Group By Operation</h3> <p>This operation is used to group tuples based on a certain attribute value.</p> <h2 id="equivalent-queries">Equivalent Queries</h2> <p>There are more ways to write a query in relation algebra. Queries which are <u>equivalent</u> need not be <u>identical</u>.</p> <p>In case of SQL, the database optimizer takes care of optimizing equivalent queries.</p> <h1 id="chapter-3-basic-sql">~Chapter 3: Basic SQL</h1> <h2 id="domain-types-in-sql">Domain Types in SQL</h2> <ul> <li><code class="language-plaintext highlighter-rouge">char(n)</code> - Fixed length character string, with user-specified length \(n\). We might need to use the extra spaces in the end in the queries too!</li> <li><code class="language-plaintext highlighter-rouge">varchar(n)</code> - Variable length strings</li> <li>…</li> </ul> <h2 id="create-table-construct">Create Table Construct</h2> <p>An SQL relation is defined using the create table command -</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">table</span> <span class="n">r</span>
	<span class="p">(</span><span class="n">A_1</span><span class="p">,</span> <span class="n">D_1</span><span class="p">,</span> <span class="n">A_2</span><span class="p">,</span> <span class="n">D_2</span><span class="p">,</span> <span class="p">...,</span> <span class="n">A_n</span><span class="p">,</span> <span class="n">D_n</span><span class="p">)</span>
</code></pre></div></div> <h2 id="integrity-constraints-in-create-table">Integrity Constraints in Create Table</h2> <p>Types of integrity constraints</p> <ul> <li>primary key \((A_1, A_2, A_3, ...)\)</li> <li>Foreign key \((A_m, ..., A_n)\) references r</li> <li>not <code class="language-plaintext highlighter-rouge">null</code></li> </ul> <p>SQL prevents any update to the database that violates an integrity constraint.</p> <h1 id="lecture-4">Lecture 4</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">10-01-22</code></p> </blockquote> <h2 id="basic-query-structure">Basic Query Structure</h2> <ul> <li>A typical SQL query has the form:</li> </ul> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">select</span> <span class="n">A1</span><span class="p">,</span> <span class="n">A2</span><span class="p">,</span> <span class="p">...,</span> <span class="n">An</span>
<span class="k">from</span> <span class="n">r1</span><span class="p">,</span> <span class="n">r2</span><span class="p">,</span> <span class="p">...,</span> <span class="n">rm</span>
<span class="k">where</span> <span class="n">P</span>
</code></pre></div></div> <p>where, \(A_i\) are attributes, \(r_i\) are relations, and \(P\) has conditions/predicates. The result of an SQL query is a relation. SQL is case-insensitive in general.</p> <ul> <li>We shall be using PostgreSQL for the rest of the course.</li> <li>SQL names are usually case insensitive. Some databases are case insensitive even in string comparison!</li> </ul> <h3 id="select-clause">select clause</h3> <ul> <li>To force the elimination of duplicates, insert the keyword <code class="language-plaintext highlighter-rouge">distinct</code> after select. Duplicates come from <ol> <li>Input itself is a multiset</li> <li>Joining tables</li> </ol> <p>Removing duplicates imposes an additional overhead to the database engine. Therefore, it was ubiquitously decides to exclude duplicates removal in SQL.</p> </li> <li> <p>The keyword <code class="language-plaintext highlighter-rouge">all</code> specifies that duplicates should not be removed.</p> </li> <li>SQL allows renaming relations and attributes using the <code class="language-plaintext highlighter-rouge">as</code> clause. We can skip <code class="language-plaintext highlighter-rouge">as</code> in some databases like Oracle. Also, some databases allow queries with no <code class="language-plaintext highlighter-rouge">from</code> clause.</li> </ul> <h3 id="from-clause">from clause</h3> <p>If we write <code class="language-plaintext highlighter-rouge">select * from A, B</code>, then the Cartesian product of <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> is considered. This usage has some corner cases but are rare.</p> <p>### as clause</p> <p>It can be used to rename attributes as well as relations.</p> <h3 id="self-join">Self Join</h3> <p>How do we implement various levels of recursion without loops and only imperative statements? Usually, <code class="language-plaintext highlighter-rouge">union</code> is sufficient for our purposes. However, this is infeasible in case of large tables or higher levels of hierarchy.</p> <h3 id="string-operations">String operations</h3> <p>SQL also includes string operations. The operator <code class="language-plaintext highlighter-rouge">like</code> uses patterns that are describes using two special character - <code class="language-plaintext highlighter-rouge">percent %</code> - Matches any substring and <code class="language-plaintext highlighter-rouge">underscore _</code> matches any character (Use <code class="language-plaintext highlighter-rouge">\</code> as the escape character). Some databases even fully support regular expressions. Most databases also support <code class="language-plaintext highlighter-rouge">ilike</code> which is case-insensitive.</p> <h3 id="set-operations">Set operations</h3> <p>These include <code class="language-plaintext highlighter-rouge">union</code>, <code class="language-plaintext highlighter-rouge">intersect</code> and <code class="language-plaintext highlighter-rouge">except</code> (set difference). To retain the duplicates we use <code class="language-plaintext highlighter-rouge">all</code> keyword after the operators.</p> <h3 id="null-values">null values</h3> <p>It signifies an unknown value or that a value does not exist. The result of any arithmetic expression involving <code class="language-plaintext highlighter-rouge">null</code> is <code class="language-plaintext highlighter-rouge">null</code>. The predicate <code class="language-plaintext highlighter-rouge">is null</code> can be used to check for null values.</p> <h1 id="lecture-5">Lecture 5</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">11-01-22</code></p> </blockquote> <h3 id="aggregate-functions-1">Aggregate Functions</h3> <p>The <code class="language-plaintext highlighter-rouge">having</code> clause can be used to select groups which satisfies certain conditions. Predicates in the <code class="language-plaintext highlighter-rouge">having</code> clause are applied after the formation of groups whereas predicates in the <code class="language-plaintext highlighter-rouge">where</code> clause are applied before forming the groups.</p> <h2 id="nested-subqueries">Nested subqueries</h2> <p>SQL provides a mechanism for the nesting of subqueries. A subquery is a select-from-where expression that is nested within another query. The nesting can be done in the following ways -</p> <ul> <li><code class="language-plaintext highlighter-rouge">from</code> clause - The relation can be replaced by any valid subquery</li> <li><code class="language-plaintext highlighter-rouge">where</code> clause - The predicate can be replaced with an expression of the form <code class="language-plaintext highlighter-rouge">B &lt;operation&gt; (subquery)</code> where <code class="language-plaintext highlighter-rouge">B</code> is an attribute and <code class="language-plaintext highlighter-rouge">operation</code> will be defined later.</li> <li><strong>Scalar subqueries</strong> - The attributes in the <code class="language-plaintext highlighter-rouge">select</code> clause can be replaced by a subquery that generates a single value!</li> </ul> <h3 id="subqueries-in-the-from-clause">subqueries in the <code class="language-plaintext highlighter-rouge">from</code> clause</h3> <p>the <code class="language-plaintext highlighter-rouge">with</code> clause provides a way of defining a temporary relation whose definition is available only to the query in which the <code class="language-plaintext highlighter-rouge">with</code> clause occurs. For example, consider the following</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">max_budget</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">as</span> 
	<span class="p">(</span><span class="k">select</span> <span class="k">max</span><span class="p">(</span><span class="n">budget</span><span class="p">))</span>
	<span class="k">from</span> <span class="n">department</span><span class="p">)</span>
<span class="k">select</span> <span class="n">department</span><span class="p">.</span><span class="n">name</span> <span class="k">from</span> <span class="n">department</span><span class="p">,</span> <span class="n">max_budget</span>
<span class="k">where</span> <span class="n">department</span><span class="p">.</span><span class="n">budget</span> <span class="o">=</span> <span class="n">max_budget</span><span class="p">.</span><span class="n">value</span>
</code></pre></div></div> <p>We can write more complicated queries. For example, if we want all departments where the total salary is greater than the average of the total salary at all departments.</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">dept_total</span><span class="p">(</span><span class="n">dept_name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span> <span class="k">as</span>
	<span class="p">(</span><span class="k">select</span> <span class="n">dept_name</span><span class="p">,</span> <span class="k">sum</span><span class="p">(</span><span class="n">salary</span><span class="p">)</span> <span class="k">from</span> <span class="n">instructor</span> <span class="k">group</span> <span class="k">by</span> <span class="n">dept_name</span><span class="p">)</span>
		<span class="n">dept_total_avg</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">as</span>
		<span class="p">(</span><span class="k">select</span> <span class="k">avg</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="k">from</span> <span class="n">dept_total</span><span class="p">)</span>
    <span class="k">select</span> <span class="n">dept_name</span>
    <span class="k">from</span> <span class="n">dept_total</span><span class="p">,</span> <span class="n">dept_total_avg</span>
    <span class="k">where</span> <span class="n">dept_total</span><span class="p">.</span><span class="n">value</span> <span class="o">&gt;</span> <span class="n">dept_total_avg</span><span class="p">.</span><span class="n">value</span>
</code></pre></div></div> <h3 id="subqueries-in-the-where-clause">subqueries in the <code class="language-plaintext highlighter-rouge">where</code> clause</h3> <p>We use operations such as <code class="language-plaintext highlighter-rouge">in</code> and <code class="language-plaintext highlighter-rouge">not in</code>. We can also check the set membership of a subset of attributes in the same order. There is also a <code class="language-plaintext highlighter-rouge">some</code> keyword that returns a True if at least one tuple exists in the subquery that satisfies the condition. Similarly we have the <code class="language-plaintext highlighter-rouge">all</code> keyword. There is also the <code class="language-plaintext highlighter-rouge">exists</code> clause which returns True if the tuple exists in the subquery relation. For example, if we want to find all courses taught in both the Fall 2017 semester and in the spring 2018 semester. We can use the following</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">select</span> <span class="n">course_id</span> <span class="k">from</span> <span class="n">section</span> <span class="k">as</span> <span class="n">S</span>
<span class="k">where</span> <span class="n">semester</span> <span class="o">=</span> <span class="s1">'Fall'</span> <span class="k">and</span> <span class="nb">year</span> <span class="o">=</span> <span class="mi">2017</span> <span class="k">and</span>
	<span class="k">exists</span> <span class="p">(</span><span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">section</span> <span class="k">as</span> <span class="n">T</span>
			<span class="k">where</span> <span class="n">semester</span> <span class="o">=</span> <span class="s1">'Spring'</span> <span class="k">and</span> <span class="nb">year</span> <span class="o">=</span> <span class="mi">2018</span>
			<span class="k">and</span> <span class="n">S</span><span class="p">.</span><span class="n">course_id</span> <span class="o">=</span> <span class="n">T</span><span class="p">.</span><span class="n">course_id</span><span class="p">)</span>
</code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">S</code> is the correlation name and the inner query is the correlated subquery. Correspondingly, there also is a <code class="language-plaintext highlighter-rouge">not exists</code> clause.</p> <p>The <code class="language-plaintext highlighter-rouge">unique</code> construct tests whether a subquery has any duplicate tuples in its result. It evaluates to True if there are no duplicates.</p> <h3 id="scalar-subquery">Scalar Subquery</h3> <p>Suppose we have to list all the departments along with the number of instructors in each department. Then, we can do the following</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">select</span> <span class="n">dept_name</span><span class="p">,</span> 
	<span class="p">(</span><span class="k">select</span> <span class="k">count</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">from</span> <span class="n">instructor</span>
	<span class="k">where</span> <span class="n">department</span><span class="p">.</span><span class="n">dept_name</span> <span class="o">=</span> <span class="n">instructir</span><span class="p">.</span><span class="n">dept_name</span> <span class="k">as</span> <span class="n">num_instructors</span>
	<span class="p">)</span> <span class="k">from</span> <span class="n">department</span><span class="p">;</span>
</code></pre></div></div> <p>There would be a <strong>runtime error</strong> if the subquery returns more than one result tuple.</p> <h2 id="modification-of-the-database">Modification of the database</h2> <p>We can</p> <ul> <li> <p>delete tuples from a given relation using <code class="language-plaintext highlighter-rouge">delete from</code>. It deletes all tuples without a <code class="language-plaintext highlighter-rouge">where</code> clause. We need to be careful while using delete. For example, if we want to delete all instructors whose salary is less than the average salary of instructors. We can implement this using a subquery in the <code class="language-plaintext highlighter-rouge">where</code> clause. The problem here is that the average salary changes as we delete tuples from instructor. The solution for this problem is - we can compute average first and then delete without recomputation. This modification is usually implemented.</p> </li> <li> <p>insert new tuples into a give relation using <code class="language-plaintext highlighter-rouge">insert into &lt;table&gt; values &lt;A1, A2, ..., An&gt;</code>. The <code class="language-plaintext highlighter-rouge">select from where</code> statement is evaluated fully before any of its results are inserted into the relation. This is done to prevent the problem mentioned in <code class="language-plaintext highlighter-rouge">delete</code>.</p> </li> <li> <p>update values in some tuples in a given relation using <code class="language-plaintext highlighter-rouge">update &lt;table&gt; set A1 = ... where ...</code>. We can also use a <code class="language-plaintext highlighter-rouge">case</code> statement to make non-problematic sequential updates. For example,</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">update</span> <span class="n">instructor</span>
	<span class="k">set</span> <span class="n">salary</span> <span class="o">=</span> <span class="k">case</span>
					<span class="k">when</span> <span class="n">salary</span> <span class="o">&lt;=</span> <span class="mi">1000</span> <span class="k">then</span> <span class="n">salary</span> <span class="o">*</span><span class="mi">1</span><span class="p">.</span><span class="mi">05</span>
					<span class="k">else</span> <span class="n">salary</span><span class="o">*</span><span class="mi">1</span><span class="p">.</span><span class="mi">03</span>
                <span class="k">end</span>
</code></pre></div> </div> </li> </ul> <p><strong><code class="language-plaintext highlighter-rouge">coalesce</code></strong> takes a series of arguments and returns the first non-null value.</p> <h1 id="lecture-6">Lecture 6</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">13-01-22</code></p> </blockquote> <h1 id="chapter-4-intermediate-sql">~Chapter 4: Intermediate SQL</h1> <p><strong>Join operations</strong> take two relations and return as a result another relation. There are three types of joins which are described below.</p> <h3 id="natural-join">Natural Join</h3> <p>Natural join matches tuples with the same values for <strong>all common attributes</strong>, and retains only one copy of each common column.</p> <blockquote> <p>Can’t do self-join using this?</p> </blockquote> <p>However, one must be beware of natural join because it produces unexpected results. For example, consider the following queries</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- Correct version</span>
<span class="k">select</span> <span class="n">name</span><span class="p">,</span> <span class="n">title</span> <span class="k">from</span> <span class="n">student</span> <span class="k">natural</span> <span class="k">join</span> <span class="n">takes</span><span class="p">,</span> <span class="n">course</span>
<span class="k">where</span> <span class="n">takes</span><span class="p">.</span><span class="n">course_id</span> <span class="o">=</span> <span class="n">course</span><span class="p">.</span><span class="n">course_id</span>
<span class="c1">-- Incorrect version</span>
<span class="k">select</span> <span class="n">name</span><span class="p">,</span> <span class="n">title</span>
<span class="k">from</span> <span class="n">student</span> <span class="k">natural</span> <span class="k">join</span> <span class="n">takes</span> <span class="k">natural</span> <span class="k">join</span> <span class="n">course</span>
</code></pre></div></div> <p>The second query omits all pairs where the student takes a course in a department other than the student’s own department due to the attribute department name. Sometimes, we don’t realize some attributes are being equated because all the common attributes are equated.</p> <h3 id="outer-join">Outer join</h3> <p>One can lose information with inner join and natural join. Outer join is an extension of the join operation that avoids loss of information. It computes the join and then adds tuples from one relation that do not match tuples in the other relation to the result of the join. Outer join uses <code class="language-plaintext highlighter-rouge">null</code> to fill the incomplete tuples. We have variations of outer join such as left-outer join, right-outer join, and full outer join. Can outer join be expressed using relational algebra? Yes, think about it. In general, \((r ⟖ s) ⟖ t \neq r ⟖ (s ⟖t)\).</p> <p><strong>Note.</strong> \((r ⟖ s) ⟕ t \neq r ⟖ (s⟕t)\). Why? Consider the following</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>r | X | Y | s | Y | Z | t | Z | X | P |
  | 1 | 2 |   | 2 | 3 |   | 3 | 4 | 7 |
  | 1 | 3 |   | 3 | 4 |   | 4 | 1 | 8 |
-- LHS				-- RHS
| X | Y | Z | P |	| X | Y | Z | P |
| 1 | 2 | 3 | - |	| 4 | 2 | 3 | 7 |
| 1 | 3 | 4 | 8 |	| 1 | 3 | 4 | 8 |
</code></pre></div></div> <h2 id="views">Views</h2> <p>In some cases, it is not desirable for all users to see the entire logical model. For example, if a person wants to know the name and department of instructors without the salary, then they can use</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">view</span> <span class="n">v</span> <span class="k">as</span> <span class="k">select</span> <span class="n">name</span><span class="p">,</span> <span class="n">dept</span> <span class="k">from</span> <span class="n">instructor</span>
</code></pre></div></div> <p>A view provides a mechanism to hide certain data from the view of certain users. The view definition is not the same as creating a new relation by evaluating the query expression. Rather, a view definition causes the saving of an expression; the expression is substituted into queries using the view.</p> <p>One view may be used in the expression defining another view. A view relation \(v_1\) is said to <em>depend directly</em> on a view relation \(v_2\) if \(v_2\) is used in the expression defining \(v_1\). It is said to <em>depend on</em> \(v_2\) if there is a path of dependency. A <em>recursive</em> view depends on itself.</p> <h3 id="materialized-views">Materialized views</h3> <p>Certain database systems allow view relations to be physically stored. If relations used in the query are updated, the materialized view result becomes out of date. We need to maintain the view, by updating the view whenever the underlying relations are updated. Most SQL implementations allow updates only on simple views.</p> <h2 id="transactions">Transactions</h2> <p>A transaction consists of a sequence of query and/or update statements and is atomic. The transaction must end with one of the following statements -</p> <ul> <li><strong>Commit work</strong> - Updates become permanent</li> <li><strong>Rollback work</strong> - Updates are undone</li> </ul> <h2 id="integrity-constraints">Integrity Constraints</h2> <ul> <li><code class="language-plaintext highlighter-rouge">not null</code></li> <li><code class="language-plaintext highlighter-rouge">primary key (A1, A2, ..., Am)</code></li> <li><code class="language-plaintext highlighter-rouge">unique (A1, A2, ..., Am)</code></li> <li><code class="language-plaintext highlighter-rouge">check (P)</code></li> </ul> <h3 id="check-clause"><code class="language-plaintext highlighter-rouge">check</code> clause</h3> <p>The <strong>check(P)</strong> clause specifies a predicate P that must be satisfied by every tuple in a relation.</p> <h3 id="cascading-actions">Cascading actions</h3> <p>When a referential-integrity constraint is violated, the normal procedure is to reject the action that caused the violation. We can use <code class="language-plaintext highlighter-rouge">on delete cascade</code> or <code class="language-plaintext highlighter-rouge">on update cascade</code>. Other than cascade, we can use <code class="language-plaintext highlighter-rouge">set null</code> or <code class="language-plaintext highlighter-rouge">set default</code>.</p> <h1 id="lecture-7">Lecture 7</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">17-01-22</code></p> </blockquote> <p>Continuing the previous referential-integrity constraints. Suppose we have the command</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">table</span> <span class="n">person</span><span class="p">(</span>
	<span class="n">ID</span> <span class="nb">char</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
	<span class="n">name</span> <span class="nb">char</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
	<span class="n">mother</span> <span class="nb">char</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
	<span class="n">father</span> <span class="nb">char</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span>
	<span class="k">primary</span> <span class="k">key</span> <span class="n">ID</span><span class="p">,</span>
	<span class="k">foreign</span> <span class="k">key</span> <span class="n">father</span> <span class="k">references</span> <span class="n">person</span><span class="p">,</span>
	<span class="k">foreign</span> <span class="k">key</span> <span class="n">mother</span> <span class="k">references</span> <span class="n">person</span>
<span class="p">)</span>
</code></pre></div></div> <p>How do we insert tuples here without violating the foreign key constraints? We can initially insert the name attributes and then the father and mother attributes. This can be done by inserting <code class="language-plaintext highlighter-rouge">null</code> in the mother/father attributes. Is there any other way of doing this? We can insert tuples by using the acyclicity among the tuples using topological sorting. There is also a third way which is supported by SQL. In this method, we can ask the database to defer the foreign key checking till the end of the transaction.</p> <h3 id="complex-check-conditions">Complex check conditions</h3> <p>The predicate in the check clause can be an arbitrary predicate that can include a subquery <strong>(?)</strong> For example,</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">check</span> <span class="p">(</span><span class="n">time_slot_id</span> <span class="k">in</span> <span class="p">(</span><span class="k">select</span> <span class="n">time_slot_id</span> <span class="k">from</span> <span class="n">time_slot</span><span class="p">))</span>
</code></pre></div></div> <p>This condition is similar to a foreign key constraint. We have to check this condition not only when the ‘section’ relation is updated but also when the ‘time_slot’ relation is updated. Therefore, <u>it is not currently supported by any database!</u></p> <h3 id="built-in-data-types-in-sql">Built-in Data Types in SQL</h3> <p>In addition to the previously mentioned datatypes, SQL supports <code class="language-plaintext highlighter-rouge">date</code>s, <code class="language-plaintext highlighter-rouge">interval</code>s, <code class="language-plaintext highlighter-rouge">timestamp</code>s, and <code class="language-plaintext highlighter-rouge">time</code>. Whenever we subtract <code class="language-plaintext highlighter-rouge">date</code> from a <code class="language-plaintext highlighter-rouge">date</code> or <code class="language-plaintext highlighter-rouge">time</code> from <code class="language-plaintext highlighter-rouge">time</code>, we get an <code class="language-plaintext highlighter-rouge">interval</code>.</p> <p>Can we store files in our database? Yes! We can store large objects as</p> <ul> <li><code class="language-plaintext highlighter-rouge">blob</code> - Binary large object - Large collection of uninterpreted binary data (whose interpretation is left to the application outside of the database system).</li> <li><code class="language-plaintext highlighter-rouge">clob</code> character large object - Large collection of</li> </ul> <p>Every database has its own limit for the maximum file size.</p> <h3 id="index-creation">Index Creation</h3> <p>An <strong>index</strong> on an attribute of a relation is a data structure that allows the database system to find those tuples in the relation that have a specified value for that attribute efficiently, without scanning through all the tuples of the relation. We create an index with the <code class="language-plaintext highlighter-rouge">create index</code> command</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">index</span> <span class="o">&lt;</span><span class="n">name</span><span class="o">&gt;</span> <span class="k">on</span> <span class="o">&lt;</span><span class="n">relation</span><span class="o">-</span><span class="n">name</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">attribute</span><span class="p">):</span>
</code></pre></div></div> <p>Every database automatically creates an index on the primary key.</p> <h3 id="authorization">Authorization</h3> <p>We can assign several forms of authorization to a database</p> <ul> <li><strong>Read</strong> - allows reading, but no modification of data</li> <li><strong>Insert</strong> - allows insertion of new data, but not modification of existing data</li> <li><strong>Update</strong> - allows modification, but not deletion of data</li> <li><strong>Delete</strong> - allows deletion of data</li> </ul> <p>We have more forms on the schema level</p> <ul> <li><strong>Index</strong> - allows creation and deletion of indices</li> <li><strong>Resources</strong>, <strong>Alteration</strong></li> <li><strong>Drop</strong> - allows deleting relations</li> </ul> <p>Each of these types of authorizations is called a <strong>privilege</strong>. These privileges are assigned on specified parts of a database, such as a relation, view or the whole schema.</p> <p>The <code class="language-plaintext highlighter-rouge">grant</code> statement is used to confer authorization.</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">grant</span> <span class="o">&lt;</span><span class="n">privilege</span> <span class="n">list</span><span class="o">&gt;</span> <span class="k">on</span> <span class="o">&lt;</span><span class="n">relation</span> <span class="k">or</span> <span class="k">view</span><span class="o">&gt;</span> <span class="k">to</span> <span class="o">&lt;</span><span class="k">user</span> <span class="n">list</span><span class="o">&gt;</span>
<span class="c1">-- Revoke statement to revoke authorization</span>
<span class="k">revoke</span> <span class="k">select</span> <span class="k">on</span> <span class="n">student</span> <span class="k">from</span> <span class="n">U1</span><span class="p">,</span> <span class="n">U2</span><span class="p">,</span> <span class="n">U3</span>
</code></pre></div></div> <ul> <li><code class="language-plaintext highlighter-rouge">&lt;user list&gt;</code> can be a user-id, <strong>public</strong> or a <em>role</em>. Granting a privilege on a view does no imply granting any privileges on the underlying relations.</li> <li><code class="language-plaintext highlighter-rouge">&lt;privilege-list&gt;</code> may be <strong>all</strong> to revoke all privileges the revokee may hold.</li> <li>If <code class="language-plaintext highlighter-rouge">&lt;revoke-list&gt;</code> includes <strong>public</strong>, all users lost the privilege except those granted it explicitly.</li> <li>If the same privilege was granted twice to the same user by different grantees, the user may retain the privilege after the revocation. All privileges that depend on the privilege being revoked are also revoked.</li> </ul> <p>One of the major selling points of Java was a <em>garbage collector</em> that got rid of <code class="language-plaintext highlighter-rouge">delete</code>/<code class="language-plaintext highlighter-rouge">free</code> and automatically freed up unreferenced memory. This action is done via a <em>counter</em> which keeps a count of the variables that are referencing a memory cell. SQL uses a similar counter for keeping track of permissions of various objects. However, this approach fails in case of cycles in the dependency graph. For instance, consider the following situation</p> <p><img src="/assets/img/Databases/image-20220117101819769.png" alt="image-20220117101819769"/></p> <p>This problem does not occur in case of programming languages. The solution to this problem is <strong><em><code class="language-plaintext highlighter-rouge">TODO</code></em></strong>.</p> <p>What about garbage collection when the program is huge? Is it efficient? Currently, many optimizations, like <em>incremental garbage collection</em>, have been implemented to prevent freezing of a program for garbage collection. Even after this, Java is not preferred for real-time applications. However, programmers prefer Java because of the ease of debugging and writing programs.</p> <h3 id="roles">Roles</h3> <p>A <strong>role</strong> is a way to distinguish among various users as far as what these users can access/update in the database.</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="n">a</span> <span class="k">role</span> <span class="o">&lt;</span><span class="n">name</span><span class="o">&gt;</span>
<span class="k">grant</span> <span class="o">&lt;</span><span class="k">role</span><span class="o">&gt;</span> <span class="k">to</span> <span class="o">&lt;</span><span class="n">users</span><span class="o">&gt;</span>
</code></pre></div></div> <p>There are a couple more features in authorization which can be looked up in the textbook.</p> <p>We can also give authorization on views.</p> <h3 id="other-authorization-features">Other authorization features</h3> <ul> <li> <p>references privilege to create foreign key</p> <p><code class="language-plaintext highlighter-rouge">grant reference (dept_name) on department to Mariano</code></p> </li> <li> <p>transfer of privileges</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">grant</span> <span class="k">select</span> <span class="k">on</span> <span class="n">department</span> <span class="k">to</span> <span class="n">Amit</span> <span class="k">with</span> <span class="k">grant</span> <span class="k">option</span><span class="p">;</span>
<span class="k">revoke</span> <span class="k">select</span> <span class="k">on</span> <span class="n">department</span> <span class="k">from</span> <span class="n">Amit</span><span class="p">,</span> <span class="n">Satoshi</span> <span class="k">cascade</span><span class="p">;</span>
<span class="k">revoke</span> <span class="k">select</span> <span class="k">on</span> <span class="n">deparment</span> <span class="k">from</span> <span class="n">Amit</span><span class="p">,</span> <span class="n">Satoshi</span> <span class="k">restrict</span><span class="p">;</span>
</code></pre></div> </div> </li> </ul> <h1 id="lecture-8">Lecture 8</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">18-01-22</code></p> </blockquote> <h1 id="-chapter-5-advanced-sql">~ Chapter 5: Advanced SQL</h1> <p>Programming languages with automatic garbage collection cannot clean the data in databases. That is, if you try using large databases, then your system may hang.</p> <h2 id="jdbc-code">JDBC code</h2> <p><code class="language-plaintext highlighter-rouge">DriverManager.getConnection("jdbc:oracle:thin:@db_name")</code> is used to connect to the database. We need to close the connection after the work since each connection is a process on the server, and the server can have limited number of processes. In Java we check the <code class="language-plaintext highlighter-rouge">null</code> value using <code class="language-plaintext highlighter-rouge">wasNull()</code> function (not intuitive).</p> <p>Prepared statements are used to take inputs from the user without SQL injection. We can also extract metadata using JDBC.</p> <h2 id="sql-injection">SQL injection</h2> <p>The method where hackers insert SQL commands into the database using SQL queries. This problem is prevented by using <code class="language-plaintext highlighter-rouge">prepared statement</code>s.</p> <p>This lecture was cut-short, and hence has less notes.</p> <h1 id="lecture-9">Lecture 9</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">20-01-22</code></p> </blockquote> <h2 id="functions-and-procedures">Functions and Procedures</h2> <p>Functions and procedures allow ‘business logic’ to be stored in the database and executed from SQL statements.</p> <p>We can define a function using the following syntax</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">function</span> <span class="o">&lt;</span><span class="n">name</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">params</span><span class="p">)</span>
	<span class="k">returns</span> <span class="o">&lt;</span><span class="n">datatype</span><span class="o">&gt;</span>
	<span class="k">begin</span>
		<span class="p">...</span>
	<span class="k">end</span>
</code></pre></div></div> <p>You can return scalars or relations. We can also define external language routines in other programming languages. These procedures can be more efficient than the ones defined in SQL. We can declare external language procedures and functions using the following.</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">procedure</span> <span class="o">&lt;</span><span class="n">name</span><span class="o">&gt;</span><span class="p">(</span><span class="k">in</span> <span class="n">params</span><span class="p">,</span> <span class="k">out</span> <span class="n">params</span> <span class="p">(</span><span class="o">?</span><span class="p">))</span>
<span class="k">language</span> <span class="o">&lt;</span><span class="n">programming</span><span class="o">-</span><span class="k">language</span><span class="o">&gt;</span>
<span class="k">external</span> <span class="n">name</span> <span class="o">&lt;</span><span class="n">file_path</span><span class="o">&gt;</span>
</code></pre></div></div> <p>However, there are security issues with such routines. To deal with security problems, we can</p> <ul> <li><strong>sandbox techniques</strong> - using a safe language like Java which cannot access/damage other parts of the database code.</li> <li>run external language functions/procedures in a separate process, with no access to the database process’ memory.</li> </ul> <h2 id="triggers">Triggers</h2> <p>When certain actions happen, we would like the database to react and do something as a response. A <strong>trigger</strong> is a statement that is executed automatically by the system as a side effect of a modification to the database. To design a trigger mechanism, we must specify the conditions under which the trigger is to be executed and the actions to be taken when the trigger executes. The syntax varies from database to database and the user must be wary of it.</p> <p>The SQL:1999 syntax is</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">create</span> <span class="k">trigger</span> <span class="o">&lt;</span><span class="n">name</span><span class="o">&gt;</span> <span class="k">after</span> <span class="p">[</span><span class="k">update</span><span class="p">,</span> <span class="k">insert</span><span class="p">,</span> <span class="k">delete</span><span class="p">]</span> <span class="k">of</span> <span class="o">&lt;</span><span class="n">relation</span><span class="o">&gt;</span> <span class="k">on</span> <span class="o">&lt;</span><span class="n">attributes</span><span class="o">&gt;</span>
<span class="k">referencing</span> <span class="k">new</span> <span class="k">row</span> <span class="k">as</span> <span class="n">nrow</span>
<span class="k">referencing</span> <span class="k">old</span> <span class="k">row</span> <span class="k">as</span> <span class="n">orow</span>
<span class="p">[</span><span class="k">for</span> <span class="k">each</span> <span class="k">row</span><span class="p">]</span>
	<span class="p">...</span>
</code></pre></div></div> <p>If we do not want the trigger to be executed for every row update, then we can use statement level triggers. This ensures that the actions is executed for all rows affected by a transaction. We use <code class="language-plaintext highlighter-rouge">for each</code> instead of <code class="language-plaintext highlighter-rouge">for each row</code> and we reference tables instead of rows.</p> <p>Triggers need not be used to update materialized views, logging, and many other typical use cases. Use of triggers is not encouraged as they have a risk of unintended execution.</p> <h2 id="recursion-in-sql">Recursion in SQL</h2> <p>SQL:1999 permits recursive view definition. Why do we need recursion? For example, if we want to find which courses are a prerequisite (direct/indirect) for a specific course, we can use</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="k">recursive</span> <span class="n">rec_prereq</span><span class="p">(</span><span class="n">course_id</span><span class="p">,</span> <span class="n">prereq_id</span><span class="p">)</span> <span class="k">as</span> <span class="p">(</span>
	<span class="k">select</span> <span class="n">course_id</span><span class="p">,</span> <span class="n">prereq_id</span> <span class="k">from</span> <span class="n">prereq</span>
	<span class="k">union</span>
	<span class="k">select</span> <span class="n">rec_prereq</span><span class="p">.</span><span class="n">course_id</span><span class="p">,</span> <span class="n">prereq</span><span class="p">.</span><span class="n">prereq_id</span><span class="p">,</span>
	<span class="k">from</span> <span class="n">rec_prereq</span><span class="p">,</span> <span class="n">prereq</span>
	<span class="k">where</span> <span class="n">rec_prereq</span><span class="p">.</span><span class="n">prereq_id</span> <span class="o">=</span> <span class="n">prereq</span><span class="p">.</span><span class="n">course_id</span>
<span class="p">)</span> <span class="k">select</span> <span class="o">*</span> <span class="k">from</span> <span class="n">rec_prereq</span><span class="p">;</span>
</code></pre></div></div> <p>This example view, <code class="language-plaintext highlighter-rouge">rec_prereq</code> is called the <em>transitive closure</em> of the <code class="language-plaintext highlighter-rouge">prereq</code> relation. Recursive views make it possible to write queries, such as transitive closure queries, that cannot be written without recursion or iteration. The alternative to recursion is to write a procedure to iterate as many times as required.</p> <p>The final result of recursion is called the <strong>fixed point</strong> of the recursive view. Recursive views are required to be <strong>monotonic</strong>. This is usually achieved using <code class="language-plaintext highlighter-rouge">union</code> without <code class="language-plaintext highlighter-rouge">except</code> and <code class="language-plaintext highlighter-rouge">not in</code>.</p> <h2 id="advanced-aggregation-features">Advanced Aggregation Features</h2> <h3 id="ranking">Ranking</h3> <p>Ranking is done in conjunction with an order by specification. Can we implement ranking with the knowledge we have currently? Yes, we can use count() to check how many tuples are ahead of the current tuple.</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">select</span> <span class="o">*</span><span class="p">,</span>  <span class="p">(</span><span class="k">select</span> <span class="k">count</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">from</span> <span class="n">r</span> <span class="k">as</span> <span class="n">r2</span> <span class="k">where</span> <span class="n">r2</span><span class="p">.</span><span class="n">t</span> <span class="o">&gt;</span> <span class="n">r1</span><span class="p">.</span><span class="n">t</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">from</span> <span class="n">r</span> <span class="k">as</span> <span class="n">r1</span>
</code></pre></div></div> <p>However, this is \(\mathcal O(n^2)\). Also, note that the above query implements <em>sparse rank</em>. <em>Dense rank</em> can be implemented using the <code class="language-plaintext highlighter-rouge">unique</code> keyword. Rank in SQL can be implemented using <code class="language-plaintext highlighter-rouge">rank() over ([order by A desc])</code>.</p> <p>Ranking can be done within partitions within the dataset. This is done using <code class="language-plaintext highlighter-rouge">partition by</code>. The whole query is given by</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">select</span> <span class="n">ID</span><span class="p">,</span> <span class="n">dept_name</span><span class="p">,</span> <span class="n">rank</span><span class="p">()</span> <span class="n">over</span>
	<span class="p">(</span><span class="k">partition</span> <span class="k">by</span> <span class="n">dept_name</span> <span class="k">order</span> <span class="k">by</span> <span class="n">GPA</span> <span class="k">desc</span><span class="p">)</span>
<span class="k">from</span> <span class="n">dept_grades</span>
<span class="k">order</span> <span class="k">by</span> <span class="n">dept_name</span><span class="p">,</span> <span class="n">dept_rank</span>
</code></pre></div></div> <p><u>Multiple rank clauses can occur in a single select clause!</u> Ranking is done after applying <code class="language-plaintext highlighter-rouge">group by</code> clause/aggregation. Finally, if we want only the top few ranks, we can use <code class="language-plaintext highlighter-rouge">limit</code>. However, this method is restrictive as we can’t select top-n in each partition and it is inherently non-deterministic. This is because ties are broken arbitrarily. It is usually better to select directly using the rank attribute by embedding the relation in an outer query.</p> <p>Ranking has other function such as</p> <ul> <li><code class="language-plaintext highlighter-rouge">percent_rank</code> gives percentile</li> <li><code class="language-plaintext highlighter-rouge">cume_dist</code> gives fraction</li> <li><code class="language-plaintext highlighter-rouge">row_number</code> (non-deterministic)</li> </ul> <p>SQL:1999 permits the user to specify <code class="language-plaintext highlighter-rouge">nulls first</code> or <code class="language-plaintext highlighter-rouge">nulls last</code>.</p> <p>For a given constant \(n\), the function <code class="language-plaintext highlighter-rouge">ntile(n)</code> takes the tuples in each partition in the specified order, and divides them into \(n\) buckets with equal number of tuples.</p> <h2 id="windowing">Windowing</h2> <p>Here are the examples of window specifications</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">between</span> <span class="k">rows</span> <span class="n">unbounded</span> <span class="k">preceding</span> <span class="k">and</span> <span class="k">current</span>
<span class="k">rows</span> <span class="n">nbounded</span> <span class="k">preceding</span>
<span class="k">range</span> <span class="k">between</span> <span class="mi">10</span> <span class="k">preceding</span> <span class="k">and</span> <span class="k">current</span> <span class="k">row</span>
<span class="k">range</span> <span class="n">interval</span> <span class="mi">10</span> <span class="k">day</span> <span class="k">preceding</span>
<span class="c1">-- Given a relation transaction</span>
<span class="c1">-- where value is positive for a deposite and </span>
<span class="c1">-- negative for a withdrawal, find total balance</span>
<span class="c1">-- of each account after each transaction on it</span>
<span class="k">select</span> <span class="n">account_number</span><span class="p">,</span> <span class="n">date_time</span><span class="p">,</span> 
	<span class="k">sum</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="n">over</span> 
		<span class="p">(</span><span class="k">partition</span> <span class="k">by</span> <span class="n">account_number</span>
        <span class="k">order</span> <span class="k">by</span> <span class="n">date_time</span>
        <span class="k">rows</span> <span class="n">unbounded</span> <span class="k">preceding</span><span class="p">)</span>
   	<span class="k">as</span> <span class="n">balance</span>
<span class="k">from</span> <span class="n">transaction</span>
<span class="k">order</span> <span class="k">by</span> <span class="n">account_number</span><span class="p">,</span> <span class="n">date_time</span>
</code></pre></div></div> <p>We can perform windowing within partitions.</p> <h1 id="lecture-10">Lecture 10</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">24-01-22</code></p> </blockquote> <p>We will cover one last concept in SQL and then move on to ER models.</p> <h2 id="olap">OLAP</h2> <p>OLAP stands for Online Analytical Processing. It allows interactive analysis of data, allowing data to be summarized and viewed in different ways in an online fashion. Data that can be modeled as dimension attributes and measure attributes are called <strong>multidimensional data</strong>. <strong>Measure attributes</strong> measure some value that can be aggregated upon. <strong>Dimension attributes</strong> define the dimension on which measure attributes are viewed.</p> <p>Items are often represented using <strong>cross-tabulation</strong> (cross-tab), also referred to as a <strong>pivot table</strong>. The dimension attributes form the row and column headers. The measure attributes are mentioned in each individual cell. Similarly, we can create a <strong>data cube</strong> which is a multidimensional generalization of a cross-tab. We can represent cross-tabs using relations. These can be used in SQL with <code class="language-plaintext highlighter-rouge">null</code> representing the total aggregates (despite the confusion).</p> <p>The <code class="language-plaintext highlighter-rouge">cube</code> operation in SQL computes the union of <code class="language-plaintext highlighter-rouge">group by</code>’s on every subset of the specified attributes. The function <code class="language-plaintext highlighter-rouge">grouping()</code> can be applied on an attribute to check if the <code class="language-plaintext highlighter-rouge">null</code> value represents ‘all’ or not. It returns 1 if the value is a null value representing all. The <code class="language-plaintext highlighter-rouge">rollup</code> construct generates union on every prefix of a specified list of attributes. It can be used to generate aggregates at multiple levels of a hierarchy.</p> <h3 id="olap-operations">OLAP Operations</h3> <ul> <li><strong>Pivoting</strong> - Changing the dimensions used in a cross-tab</li> <li><strong>Slicing</strong> - Creating a cross-tab for fixed values only. Sometimes called <strong>dicing</strong> when values for multiple dimensions are fixed.</li> <li><strong>Rollup</strong> - Moving from finer-granularity data to a coarser granularity.</li> <li><strong>Drill down</strong> - Opposite of rollup.</li> </ul> <p>Early OLAP systems precomputed all possible aggregates in order to provide online response. Since this is infeasible, it suffices to precompute some aggregates and compute others on demand from pre-computed aggregates.</p> <h1 id="chapter-6-database-design-using-the-er-model">~Chapter 6: Database Design using the ER Model</h1> <p>How do we design schemas for a database? Is there any systematic way? We shall study this in the following chapter. The entity-relationship model proves useful in modelling the data.</p> <p>When we design a database, we initially characterize the data needs of the database users. Then, we choose a data model to translate the requirements into a conceptual schema. The conceptual schema is designed using the ER model, and the implementation can be done in different ways such as the relation model. We do this in the final step where we move from an abstract data model to the implementation in the database.</p> <p>Why do we care about good design? A bad design can have <em>redundancy</em> - repeats information which might cause data inconsistency and <em>incompleteness</em> which might make certain aspects of the enterprise difficult or impossible to model.</p> <h2 id="er-model">ER Model</h2> <p><strong>Entity</strong> is a thing or an object in the enterprise that is distinguishable from other objects. It is described by a set of attributes. A <strong>relationship</strong> is an association among several entities. These models are represented graphically using the ER diagram.</p> <h3 id="entity-sets">Entity sets</h3> <p>An <strong>entity set</strong> is a set of entities of the same type that share the same properties. For example, it can be a set of all persons (each of which is an entity). A subset of the attributes form a primary key of the entity set.</p> <p>Entity sets can be represented graphically using rectangles and attributes listed inside it. The primary keys are underlined.</p> <h3 id="relationship-sets">Relationship sets</h3> <p>A <strong>relationship set</strong> is a mathematical relation among \(n \geq 2\) entities, each taken from entity sets.</p> \[\{(e_1, e_2, \dots, e_n \vert e_1 \in E_1, e_2 \in E_2, \dots, e_n \in E_n\}\] <p>where \((e_1, e_2, \dots, e_n)\) is a relationship. We draw a line between the related entities to represent relationships. Relationship sets are represented using diamonds.</p> <p>An attribute can also be associated with a relationship set. This is shown using a rectangle with the attribute name inside connected to the relationship set diamond with a dotted line.</p> <h3 id="roles-1">Roles</h3> <p>The entity sets of a relationship need not be distinct. In such a case, we assign ‘roles’ to the entity sets.</p> <h3 id="degree-of-a-relationship-set">Degree of a relationship set</h3> <p>The degree of a relationship set is defined as the number of entities associated with the relationship set. Most relationship sets are binary. We can’t represent all ternary relations as a set of binary relations.</p> <p>There are no null values in the ER model. This becomes an issue in case of ternary relationships. Such problems are prevented in binary relationships. For example, think about the ER model of people and their parents. If someone has only one parent, then it is difficult to represent this using a ternary relationship between people, fathers and mothers. Instead, we could have two separate father and mother relationships. Binary relationships also provide the flexibility of mapping multiple entities to the same entity between two entity sets. While this is also possible in ternary relationships, we have more options in case of binary relationships.</p> <p>Does ternary relationship convey more information than binary relationship in any case? Yes, that is why we can’t represent all ternary relations as a set of binary relations. For instance, think about the instructor, project and student mapping. There are many combinations possible here which can’t be covered using binary relationships.</p> <h3 id="complex-attributes">Complex attributes</h3> <p>So far, we have considered atomic attributes in the relation model. The ER model does not impose any such requirement. We have <strong>simple</strong> and <strong>composite</strong> attributes. A composite attribute can be broken down into more attributes. For instance, we can have first and last name in name. We can have <em>single-valued</em> and <em>multi-valued</em> attributes. We can also have <em>derived</em> attributes. Multivalued attributes are represented inside curly braces <code class="language-plaintext highlighter-rouge">{}</code>.</p> <h3 id="mapping-cardinality-constraints">Mapping cardinality constraints</h3> <p>A mapping cardinality can be one-to-one, one-to-many, many-to-one or many-to-many.</p> <h1 id="lecture-11">Lecture 11</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">25-01-22</code></p> </blockquote> <p>We express cardinality constraints by drawing either a directed line \((\to)\), signifying ‘one’, or an undirected line \((-)\), signifying ‘many’, between the relationship set and the entity set.</p> <p>Let us now see the notion of participation.</p> <p><strong>Total participation</strong> - every entity in the entity set participates in at least one relationship in the relationship set. This is indicated using a double line in the ER diagram.</p> <p><strong>Partial participation</strong> - some entities may not participate in any relationship in the relationship set.</p> <p>We can represent more complex constraints using the following notation. A line may have an associated minimum and maximum cardinality, shown in the form ‘l..h’. A minimum value of 1 indicates total participation. A maximum value of 1 indicates that the entity participates in at most one relationship. A maximum value of * indicates no limit.</p> <p>How do we represent cardinality constraints in Ternary relationships? We allow at most one arrow out of a ternary (or greater degree) relationship to indicate a cardinality constraint. For instance, consider a ternary relationship R between A, B and C with arrows to B, then it indicates that each entity in A is associated with at most one entity in B for an entity in C.</p> <p>Now, if there is more than one arrow, the understanding is ambiguous. For example, consider the same setup from the previous example. If there are arrows to B and C, it could mean</p> <ul> <li>Each entity in A is associated with a unique entity from B and C, or</li> <li>Each pair of entities from (A, B) is associated with a unique C entity, and each pair (A, C) is associated with a unique entity in B.</li> </ul> <p>Due to such ambiguities, more than one arrows are typically not used.</p> <h3 id="primary-key-for-entity-sets">Primary key for Entity Sets</h3> <p>By definition, individual entities are distinct. From the database perspective, the differences are expressed in terms of their attributes. A key for an entity is a set of attributes that suffice to distinguish entities from each other.</p> <h3 id="primary-key-for-relationship-sets">Primary key for Relationship Sets</h3> <p>To distinguish among the various relationships of a relationship set we use the individual primary keys of the entities in the relationship set. That is, for a relationship set \(R\) involving entity sets \(E_1, E_2, \dots, E_n\), the primary key is given by the union of the primary keys of \(E_1, E_2, \dots, E_n\). If \(R\) is associated with any attributes, then the primary key includes those too. The choice of the primary key for a relationship set depends on the mapping cardinality of the relationship set.</p> <p><strong>Note.</strong> In one-to-many relationship sets, the primary key of the <strong>many</strong> side acts as the primary key of the relationship set.</p> <h3 id="weak-entity-sets">Weak Entity Sets</h3> <p>Weak entity set is an entity set whose existence depends on some other entity set. For instance, consider the section and course entity set. We cannot have a section without a course - an existence dependency. What if we use a relationship set to represent this? This is sort of redundant as both section and course have the course ID as an attribute. Instead of doing this, we can say that section is a weak entity set identified by course.</p> <p>In ER diagrams, a weak entity set is depicted via a double rectangle. We underline the discriminator of a weak entity set with a dashed line, and the relationship set connecting the weak entity set (using a double line) to the identifying strong entity set is depicted by a double diamond. The primary key of the strong entity set along with the discriminators of the weak entity set act as a primary key for the weak entity set.</p> <p>Every weak entity set must be associated with an <strong>identifying entity set</strong>. The relationships associating the weak entity set with the identifying entity set is called the <strong>identifying relationship</strong>. Note that the relational schema we eventually create from the weak entity set will have the primary key of the identifying entity set.</p> <h3 id="redundant-attributes">Redundant Attributes</h3> <p>Sometimes we often include redundant attributes while associating two entity sets. For example, the attribute <code class="language-plaintext highlighter-rouge">course_id</code> was redundant in the entity set section. However, when converting back to tables, some attributes get reintroduced.</p> <h2 id="reduction-to-relation-schemas">Reduction to Relation Schemas</h2> <p>Entity sets and relationship sets can be expressed uniformly as <em>relation schemas</em> that represent the content of the database. For each entity set and relationship set there is a unique schema that is assigned the name of the corresponding entity set or relationship set. Each schema has a number of columns which have unique names.</p> <ul> <li>A strong entity set reduces to a schema with the same attributes.</li> <li>A weak entity set becomes a table that includes a column for the primary key of the identifying strong entity set.</li> <li>Composite attributes are flattened out by creating separate attribute for each component attribute.</li> <li>A multivalued attribute \(M\) of an entity \(E\) is represented by a separate schema \(EM\). Schema \(EM\) has attributes corresponding to the primary key of \(E\) and an attribute corresponding to multivalued attribute \(M\).</li> <li>A many-to-many relationship set is represented as a schema with attributes for the primary keys of the two participating entity sets, and any descriptive attributes of the relationships set.</li> <li>Many-to-one and one-to-many relationship sets that are total on the many-side can be represented by adding an extra attribute to the ‘many’ side. If it were not total, null values would creep up. It is better to model such relationships as many-to-many relationships so that the model needn’t be changed when the cardinality of the relationship is changed in the future.</li> </ul> <h3 id="extended-er-features">Extended ER Features</h3> <p><strong>Specialization</strong> - Overlapping and Disjoint; Total and partial.</p> <p>How do we represent this in the schema? Form a schema for the higher-level entity and for the lower-level entity set. Include the primary key of the higher level entity set and local attributes in that of the local one. However, the drawback of such a construction is that we need to access two relations (higher and then lower) to get information.</p> <p><strong>Generalization</strong> - Combine a number of entity sets that share the same features into a higher-level entity set.</p> <p><strong>Completeness constraint</strong> specifies whether or not an entity in the higher-level entity set must belong to at least on of the lower-level entity sets within a generalization (total and partial concept). Partial generalization is the default.</p> <p><strong>Aggregation</strong> can also be represented in the ER diagrams.</p> <ul> <li>To represent aggregation, create a schema containing - primary key of the aggregated relationship, primary key of the associated entity set, and any descriptive attributes.</li> </ul> <blockquote> <p>I don’t understand aggregation</p> </blockquote> <h3 id="design-issues">Design issues</h3> <p><img src="/assets/img/Databases/image-20220126214411806.png" alt="image-20220126214411806"/></p> <h1 id="lecture-12">Lecture 12</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">27-01-22</code></p> </blockquote> <h3 id="binary-vs-non-binary-relationships">Binary vs. Non-Binary Relationships</h3> <p>We had discussed this previously in <a href="#degree-of-a-relationship-set">this</a> section. In general, any non-binary relationship can be represented using binary relationships by creating an artificial entity set. We do this by replacing \(R\) between entity sets \(A, B, C\) by an entity set \(E\), and three relationship sets \(R_i\) relating \(E\) and \(i \in \{A, B, C\}\). We create an identifying attribute for E and add any attributes of \(R\) to \(E\). For each relationship \((a_i, b_i, c_i)\) in \(R\), we</p> <ul> <li>create a new entity \(e_i\) in the entity set \(E\)</li> <li>add \((e_i, j_i)\) to \(R_j\) for \(j \in \{A, B, C\}\)</li> </ul> <p><img src="/assets/img/Databases/image-20220215211541849.png" alt="image-20220215211541849"/></p> <p>We also need to translate constraints (which may not be always possible). There may be instances in the translated schema that cannot correspond to any instance of \(R\). We can avoid creating an identifying attribute for \(E\) by making it a weak entity set identified by the three relationship sets.</p> <h3 id="er-design-decisions">ER Design Decisions</h3> <p><strong>Important Points</strong></p> <ul> <li>A weak entity set can be identified by multiple entity sets.</li> <li>A weak entity set can be identified by another weak entity set (indirect identification).</li> <li>In SQL, the value in a foreign key attribute can be null (the attribute of the relation having the fks constraint).</li> </ul> <h3 id="uml">UML</h3> <p>The <strong>Unified Modeling Language</strong> has many components to graphically model different aspects of an entire software system. The ER diagram notation we studied was inspired from the UML notation.</p> <h1 id="chapter-7-functional-dependencies">~Chapter 7: Functional Dependencies</h1> <p>When programmers usually skip the design phase, they run into problems with their relational database. We shall briefly mention these problems and see the motivation for this chapter.</p> <p>A good relational design does not have repetition of information, and no unnecessary null values. The only way to avoid the repetition of information is to decompose a relation to different schemas. However, all decompositions are not <strong>lossless</strong>.</p> <h1 id="lecture-13">Lecture 13</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">31-01-22</code></p> </blockquote> <p>The term <strong>lossy decomposition</strong> does not imply loss of tuples but rather the loss of information (relation) among the tuples. How do we formalise this idea?</p> <h2 id="lossless-decomposition---1">Lossless Decomposition - 1</h2> <p>Let \(R\) be a relations schema and let \(R_1\) and \(R_2\) form a decomposition of \(R = R_1 \cup R_2\). A decomposition if lossless if there is no loss of information by replacing \(R\) with the two relation schemas \(R_1, R_2\). That is,</p> \[\pi_{R_1}(r) \bowtie \pi_{R_2}(r) = r\] <p>Note that this relations must hold for all <strong>instances</strong> to call the decomposition lossless. And, conversely a decomposition is lossy if</p> \[r \subset \pi_{R_1}(r) \bowtie \pi_{R_2}(r)\] <p>We shall see the sufficient condition in a <a href="#lossless-decomposition---2">later section</a>.</p> <h2 id="normalization-theory">Normalization theory</h2> <p>We build the theory of functional/multivalued dependencies to decide whther a particular relation is in a “good” form.</p> <h2 id="functional-dependencies">Functional Dependencies</h2> <p>An instance of a relations that satisfies all such real-world constriants is called a <strong>legal instance</strong> of the relation. <u>A functional dependency is a generalization of the notion of a key</u>.</p> <p>Let \(R\) be a relation schema and \(\alpha, \beta \subseteq R\). The functional dependency \(\alpha \to \beta\) holds on \(R\) iff for any legal relations \(r(R)\) whenever two tuples \(t_1, t_2\) of \(r\) agree on the attributes \(\alpha\), they also agree on the attributes \(\beta\). That is,</p> \[\alpha \to \beta \triangleq t_1[\alpha] = t_2[\alpha] \implies t_1[\beta] = t_1[\beta]\] <h3 id="closure-properties">Closure properties</h3> <p>If \(A \to B\) and \(B \to C\) Then \(A \to C\). The set of <strong>all</strong> functional dependencies logically implied by a functional dependency set \(F\) is the <strong>closure</strong> of \(F\) denoted by \(F^+\).</p> <h3 id="keys-and-functional-dependencies">Keys and Functional Dependencies</h3> <p>\(K\) is a superket for relation schema \(R\) iff \(K \to R\). \(K\) is a candidate key for \(R\) iff</p> <ul> <li>\(K \to R\) and</li> <li>for no \(A \subset K\), \(A \to R\)</li> </ul> <p>Functional dependencies allow us to express constraints that cannot be expressed using super keys.</p> <h3 id="use-of-functional-dependencies">Use of functional dependencies</h3> <p>We use functional dependencies to test relations to see if they are legal and to specify constraints on the set of legal relations.</p> <p><strong>Note.</strong> A specific instance of a relation schema may satisfy a functional dependency even if that particular functional dependency does not hold across all legal instances.</p> <h2 id="lossless-decomposition---2">Lossless Decomposition - 2</h2> <p>A decomposition of \(R\) into \(R_1\) and \(R_2\) is lossless decomposition if at least one of the following dependecnies is in \(F^+\)</p> <ul> <li> \[R_1 \cap R_2 \to R_1\] </li> <li> \[R_1 \cap R_2 \to R_2\] </li> </ul> <p>The above functional dependencies are a necessary condition only if all constraints are functional dependencies.</p> <h2 id="dependency-preservation">Dependency Preservation</h2> <p>Testing functional dependency constrinats each time the database is updated can be costly. If testing a functional dependency can be done by considering just one relation, then the cost of testing this constraint is low. A decomposition that makes it computaitonally hard to enforce functional dependencies is sat to be <strong>not dependency preserving</strong>.</p> <h1 id="lecture-14">Lecture 14</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">01-02-22</code></p> </blockquote> <h2 id="boyce-codd-normal-form">Boyce-Codd Normal Form</h2> <p>There are a few designs of relational schema which prevent redundancies and have preferable properties. One such design format is BCNF.</p> <p>A relation schema \(R\) is in BCNF with respect to a set \(R\) of functional dependencies if <strong>for all</strong> functional dependencies in \(F^+\) of the form \(\alpha \to \beta\) where \(\alpha, \beta \subseteq R\), at least one of the following holds</p> <ul> <li>\(\alpha \to \beta\) is trivial (\(\beta \subseteq \alpha\))</li> <li>\(\alpha\) is a superkey for \(R\).</li> </ul> <p>Let \(R\) Be a schema \(R\) That is not in BCNF. Let \(\alpha \to \beta\) Be the FD that causes a violation of BCNF. Then, to convert \(R\) to BCNF we decompose it to</p> <ul> <li> \[\alpha \cup \beta\] </li> <li> \[R - (\beta - \alpha)\] </li> </ul> <p><strong><em>Example.</em></strong> Consider the relation \(R= (A, B, C)\) with \(F \{A \to B, B \to C\}\). Suppose we have the following decompositions</p> <ul> <li> \[R_1 = (A, B), R_2 = (B, C)\] <p>This decompositions is lossless-join and also dependency preserving. Notice that it is dependency preserving even though we have the \(A \to C\) constraint. This is because \(A \to C\) is implied from the other two constraints.</p> </li> <li> \[R = (A, B), R_2 = (A, C)\] <p>This decomposition is lossless but is not dependency preserving.</p> </li> </ul> <h3 id="bcnf-and-dependency-preservation">BCNF and Dependency Preservation</h3> <p><u>It is not always possible to achieve both BCNF and dependency preservation</u>.</p> <h2 id="third-normal-form">Third Normal Form</h2> <p>This form is useful when you are willing to allow a small amount of data redundancy in exchange for dependency preservation.</p> <p>A relations \(R\) Is in third normal form (3NF) if <strong>for all</strong> \(\alpha \to \beta \in F^+\) <strong>at least</strong> one of the following holds</p> <ul> <li>\(\alpha \to \beta\) is trivial</li> <li>\(\alpha\) is a super key for \(R\)</li> <li>Each attribute \(A\) In \(\beta - \alpha\) is contained in a candidate key for \(R\).</li> </ul> <p>There are 1NF and 2NF forms but they are not very important. <u>If a relation is in BCNF, then it is in 3NF.</u></p> <h3 id="redundancy-in-3nf">Redundancy in 3NF</h3> <p>Consider \(R\) which is in 3NF \(R = (J, K, L)\) and \(F = \{JK \to L, L \to K \}\). Then, we can have the following instance</p> <table> <thead> <tr> <th>J</th> <th>K</th> <th>L</th> </tr> </thead> <tbody> <tr> <td>p1</td> <td>q1</td> <td>k1</td> </tr> <tr> <td>p2</td> <td>q1</td> <td>k1</td> </tr> <tr> <td>p3</td> <td>q1</td> <td>k1</td> </tr> <tr> <td>null</td> <td>q2</td> <td>k2</td> </tr> </tbody> </table> <h3 id="3nf-and-dependency-preservation">3NF and Dependency Preservation</h3> <p>It is always possible to obtain a 3NF design without sacrificing losslessness or dependency preservation. However, we may have to use null values (like above) to represent some of the possible meaningful relationships among data items.</p> <h3 id="goals-of-normalisation">Goals of Normalisation</h3> <p>A “good” schema consists of lossless decompositions and preserved dependencies. We can use 3NF and BCNF (preferable) for such purpose.</p> <p>There are database schemas in BCNF that do not seem to be sufficiently normalised. <em>Multivalued dependencies, Insertion anomaly, …</em></p> <h2 id="functional-dependency-theory">Functional Dependency Theory</h2> <h3 id="closure-of-a-set-of-functional-dependencies">Closure of a set of functional dependencies</h3> <p>We can compute \(F^+\) from \(F\) by repeatedly applying <strong>Armstrong’s Axioms</strong></p> <ul> <li><strong>Reflexive rule</strong> - If \(\beta \subseteq \alpha\), then \(\alpha \to \beta\)</li> <li><strong>Augmentation rule</strong> - If \(\alpha \to \beta\), then \(\gamma\alpha \to \gamma\beta\)</li> <li><strong>Transitivity rule</strong> - If \(\alpha \to \beta\) and \(\beta \to \gamma\) then \(\alpha \to \gamma\)</li> </ul> <p>It is trivial to see that these rules are <strong>sound</strong>. However, showing that these rules are <strong>complete</strong> is much more difficult.</p> <p>Additional rules include (can be derived from above)-</p> <ul> <li><strong>Union rule</strong> - If \(\alpha \to \beta\) and \(\alpha \to \gamma\), then \(\alpha \to \beta\gamma\)</li> <li><strong>Decomposition rule</strong> - If \(\alpha \to \beta\gamma\), then \(\alpha \to \beta\)</li> <li><strong>Pseudo-transitivity rule</strong> - If \(\alpha \to \beta\) and \(\gamma\beta \to \delta\), then \(\alpha \gamma \to \delta\)</li> </ul> <h3 id="closure-of-attribute-sets">Closure of Attribute Sets</h3> <p>Given a set of attributes \(\alpha\), define the <strong>closure</strong> of \(\alpha\) <strong>under</strong> \(F\) (denoted by \(\alpha^+\)) as the set of attributes that are functionally determined by \(\alpha\) under \(F\). We use the following procedure to compute the closure of A</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>result := A
while (change) do
	for each beta to gamma in F do
		begin
			if beta in result then result = result union gamma
    end
</code></pre></div></div> <p>The time complexity of this algorithm is \(\mathcal O(n^3)\) where \(n\) is the number of attributes.</p> <p>There are several uses of the attribute closure algorithm</p> <ul> <li>To test if \(\alpha\) is a superkey, we compute \(\alpha\) and check if it contains all attributes of the relation</li> <li>To check if a functional dependency \(\alpha \to \beta\) holds, see if \(\beta \subseteq \alpha^+\)</li> <li>For computing the closure of \(F\). For each \(\gamma \subseteq R\), we find \(\gamma\) and for each \(S \subset \gamma^+\), we output a functional dependency \(\gamma \to S\).</li> </ul> <h3 id="canonical-cover">Canonical Cover</h3> <p>A <strong>Canonical cover</strong> of a functional dependency set \(F\) is the minimal set of functional dependencies such that its closure is \(F^+\).</p> <p>An attribute of a functional dependency in \(F\) is <strong>extraneous</strong> if we can remove it without changing \(F^+\). Removing an attribute from the left side of a functional dependency will make it a stronger constraint.</p> <h1 id="lecture-15">Lecture 15</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">05-02-22</code></p> </blockquote> <h3 id="extraneous-attributes">Extraneous attributes</h3> <p>Removing an attribute from the left side of a functional dependency makes it a stronger constraint. Attribute A is extraneous in \(\alpha\) if</p> <ul> <li> \[A \in \alpha\] </li> <li>\(F\) logically implies \((F - \{\alpha \to \beta\}) \cup \{(\alpha - A) \to \beta\}\)</li> </ul> <p>To test this, consider \(\gamma = \alpha - \{A\}\). Check if \(\gamma \to \beta\) can be inferred from \(F\). We do this by computing \(\gamma^+\) using the dependencies in \(F\), and if it includes all attributes in \(\beta\) then \(A\) is extraneous in \(\alpha\).</p> <p>On the other hand, removing an attribute from the right side of a functional dependency could make it a weaker constraint. Attribute A is extraneous in \(\beta\) if</p> <ul> <li> \[A \in \beta\] </li> <li> <p>The set of functional dependencies</p> <p>\((F - \{\alpha \to \beta\}) \cup \{\alpha \to ( \beta - A)\}\) logically implies \(F\).</p> </li> </ul> <p>To test this, consider \(F’ = (F - \{\alpha \to \beta\}) \cup \{\alpha \to ( \beta - A)\}\) and check if \(\alpha^+\) contains A; if it does, then \(A\) is extraneous in \(\beta\).</p> <h2 id="canonical-cover-revisited">Canonical Cover Revisited</h2> <p>A <strong>canonical cover</strong> for \(F\) is a set of dependencies \(F_c\) such that</p> <ul> <li>\(F\)(\(F_c\)) logically implies all dependencies in \(F_c\)(\(F\))</li> <li>No functional dependency in \(F_c\) contains an extraneous attribute</li> <li><u>Each left side of functional dependency in $$F_c$$ is unique</u></li> </ul> <p>To compute a canonical cover for \(F\) do the following</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fc = F
repeat
	Use the union rule to replace any dependencies in Fc of the form
		alpha -&gt; beta and alpha -&gt; gamma with
    alpha -&gt; beta gamma
	Find a functional dependency alpha to beta 
	in Fc with an extraneous attribute 
	in alpha or in beta
	Delete extraneous attribute if found
until Fc does not change
</code></pre></div></div> <h2 id="dependency-preservation---3">Dependency Preservation - 3</h2> <p>Let \(F_i\) be the set of dependencies \(F^+\) that include only the attribtues in \(R_i\). A decomposition is <strong>dependency preserving</strong> if \((F_1 \cup \dots \cup F_n)^+ = F^+\). However, we can’t use this definition to test for dependency preserving as it takes exponential time.</p> <p>We test for dependency preservation in the following way. To check if a dependency \(\alpha \to \beta\) is preserved in a decomposition, apply the following test</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>result = alpha
repeat
	for each Ri in the decomposition
		t = (result cap Ri)^+ cap Ri
		result = result \cup t
until result does not change
</code></pre></div></div> <p>If the result contains all attributes in \(\beta\), then the functional dependency \(\alpha \to \beta\) is preserved. This procedure takes polynomial time.</p> <h2 id="testing-for-bcnf">Testing for BCNF</h2> <p>To check if a non-trivial dependency \(\alpha \to \beta\) cause a violation of BCNF, compute \(\alpha^+\) and verify that it includes all attributes of \(R\). Another simpler method is to check only the dependencies in the given set \(F\) for violations of BCNF rather than checking all dependencies in \(F^+\). If none of the dependencies in \(F\) cause a violation, then none of the dependencies in \(F^+\) will cause a violation. <em>Think.</em></p> <p>However, the simplified test using only \(F\) is incorrect when testing a relation in a decomposition of \(R\). For example, consider \(R = (A, B, C, D, E)\), with \(F = \{ A \to B, BC \to D\}\) and the decomposition \(R_1 = (A, B), R_2 = (A, C, D, E)\). Now, neither of the dependencies in \(F\) contain only attributes from \((A, C, D, E)\) so we might be misled into thinking \(R_2\) satisfies BCNF.</p> <p>Therefore, testing decomposition requires the restriction of \(R^+\) to that particular set of tables. If one wants to the use the original set of dependencies \(F\), then they must check that \(\alpha^+\) either includes no attributes of \(R_i - \alpha\) or includes all attributes of \(R_i\) for every set of attributes \(\alpha \subseteq R\). If a condition \(\alpha \to \beta \in F^+\) violates BCNF, then the dependency \(\alpha \to (\alpha^+ - \alpha) \cap R_i\) can be shown to hold on \(R_i\) and violate BCNF.</p> <p>In conclusion, the BCNF decomposition algorithm is given by</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>result = R
done = false
compute F+
while (not done) do
	if (there is a schema Ri in result that is not in BCNF)
		let alpha to beta be a nontrivial functional dependency
      that holds on Ri such that alpha to Ri
      is not in F+
      and alpha cap beta is null
 		result = {(result - Ri),(Ri - beta),(alpha, beta)}
 	else done = true
		
</code></pre></div></div> <p>Note that each \(R_i\) is in BCNF and decomposition is lossless-join.</p> <h2 id="3nf">3NF</h2> <p>The main drawback of BCNF is that is may not be dependency preserving. Through 3NF, we allow some redundancy to acquire dependency preserving along with lossless join.</p> <p>To test for 3NF, we only have to check the FDs in \(F\) and not all the FDs in \(F^+\).We use attribute closure to check for each dependency \(\alpha \to \beta\), if \(\alpha\) is a super key. If \(\alpha\) is not a super key, we need to check if each attribute in \(\beta\) is contained in a candidate key of \(R\).</p> <p>However, this test is shown to be NP-hard, but the decomposition into third normal form can be done in polynomial time.</p> <blockquote> <p>Doesn’t decomposition imply testing? No, one relation can have many 3NF decompositions.</p> </blockquote> <h3 id="3nf-decomposition-algorithm">3NF Decomposition Algorithm</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let Fc be a canconical cover for F;
i = 0;
/* initial schema is empty */
for each FD alpha to beta in Fc do
	if none of the schemas Rj (j &lt;= i) contains alpha beta
		then begin
			i = i + 1
			Ri = alpha beta
		end
/* Here, each of the FDs will be contained in one of the Rjs */
if none of the schemas Rj (j &lt;= i) contains a candidate key for R
  then begin
    i = i + 1
    Ri = any candidate key for R
  end
/* Here, there is a relation contianing the candidate key of R */
/* Optionally remove redundant relations */
repeat
  if any schema Rj is contained in another schema Rk
    then delete Rj
      Rj = Ri;
      i = i - 1
  return (R1, ..., Ri)
</code></pre></div></div> <p>Guaranteeing that the above set of relations are in 3NF is the easy part. However, proving that the decomposition is lossless is difficult.</p> <h3 id="comparison-of-bcnf-and-3nf">Comparison of BCNF and 3NF</h3> <p>3NF has redundancy whereas BCNF may not be dependency preserving. The bigger problem is 3Nf allows certain function dependencies which are not super key dependencies. However, none of the SQL implementations today support such FDs.</p> <h2 id="multivalued-dependencies">Multivalued Dependencies</h2> <p>Let \(R\) be a relation schema and let \(\alpha, \beta \subseteq R\) . The multivalued dependency</p> \[\alpha \to\to \beta\] <p>holds on \(R\) if in any legal relation \(r(R)\), for all pairs for tuples \(t_1, t_2\) in \(r\) such that \(t_1[\alpha] = t_2[\alpha]\), there exists tuples \(t_3\) and \(t_4\) in \(r\) such that</p> \[t_1[\alpha] = t_2[\alpha] = t_3[\alpha] = t_4[\alpha] \\ t_3[\beta] = t_1[\beta] \\ t_3[R - \beta] = t_2[R - \beta] \\ t_4[\beta] = t_2[\beta] \\ t_4[R - \beta] = t_1[R - \beta]\] <p>Intuitively, it means that the relationship between \(\alpha\) and \(\beta\) is independent of the remaining attributes in the relation. The tabular representation of these conditions is given by</p> <p><img src="/assets/img/Databases/image-20220217224504065.png" alt="image-20220217224504065"/></p> <p>The definition can also be mentioned in a more intuitive manner. Consider the attributes of \(R\) that are partitioned into 3 nonempty subsets \(W, Y, Z\). We say that \(Y \to\to Z\) iff for all possible relational instances \(r(R)\),</p> \[&lt;y_1, z_1, w_1&gt; \in r \text{ and } &lt; y_1, z_2, w_2 &gt; \in r \\ \implies \\ &lt;y_1, z_1, w_2 &gt; \in r \text{ and } &lt;y_1, z_2, w_1 &gt; \in r\] <p><strong>Important Points</strong>-</p> <ul> <li>If \(Y \to\to Z\) then \(Y \to\to W\)</li> <li>If \(Y \to Z\) then \(Y \to\to Z\)</li> </ul> <blockquote> <p>why?</p> </blockquote> <p>The closure \(D^+\) of \(D\) is the set of all functional and multivalued dependencies logically implied by \(D\). We are not covering the reasoning here.</p> <h2 id="fourth-normal-form">Fourth Normal Form</h2> <p>A relation schema \(R\) is in 4NF rt a set \(D\) of functional and multivalued dependencies if for all multivalued dependencies in \(D^+\) in the form \(\alpha \to\to \beta\), where \(\alpha, \beta \subseteq R\), at least one of the following hold</p> <ul> <li>\(\alpha \to\to \beta\) is trivial</li> <li>\(\alpha\) is a super key for schema \(R\)</li> </ul> <p><u>If a relation is in 4NF, then it is in BCNF.</u> That is, 4NF is stronger than BCNF. Also, 4NF is the generalisation of BCNF for multivalued dependencies.</p> <h3 id="restriction-of-mvds">Restriction of MVDs</h3> <p>The restriction of \(D\) to \(R_i\) is the set \(D_i\) consisting of</p> <ul> <li>All functional dependencies in \(D^+\) that include only attributes of \(R_i\)</li> <li>All multivalued dependencies of the form \(\alpha \to \to (\beta \cap R_i)\) where \(\alpha \in R_i\) and \(\alpha \to\to \beta \in D^+\).</li> </ul> <h3 id="4nf-decomposition-algorithm">4NF Decomposition Algorithm</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>result = {R};
done = false;
compute D+
Let Di denote the restriction of D+ to Ri
while(not done)
	if (there is a schema Ri in result not in 4NF)
		let alpha to to beta be a nontrivial MVD that holds
		on Ri such that  alpha to Ri is not in Di and 
		alpha cap beta is null
		result = {result - Ri, Ri - beta, (alpha, beta)}
	else done = true
</code></pre></div></div> <p>This algorithm is very similar to that of BCNF decomposition.</p> <h1 id="lecture-16">Lecture 16</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">07-02-22</code></p> </blockquote> <h2 id="further-normal-forms">Further Normal Forms</h2> <p><strong>Join dependencies</strong> generalise multivalued dependencies and lead to <strong>project-join normal form (PJNF)</strong> also known as <strong>5NF</strong>. A class of even more general constraints leads to a normal form called <strong>domain-key normal form</strong>. There are hard to reason with and no set of sound and complete set of inference rules exist.</p> <h2 id="overall-database-design-process">Overall Database Design Process</h2> <p>We have assumed \(R\) is given. In real life, we can get it based on applications through ER diagrams. However, one can consider \(R\) to be generated from a single relations containing all attributes that are of interest (called <strong>universal relation</strong>). Normalisation breaks this \(R\) into smaller relations.</p> <p>Some aspects of database design are not caught by normalisation. For example, a <strong>crosstab</strong>, where values for on attribute become column names, is not captured by normalisation forms.</p> <h2 id="modeling-temporal-data">Modeling Temporal Data</h2> <p><strong>Temporal data</strong> have an associated time interval during which the data is valid. A <strong>snapshot</strong> is the value of the data at a particular point in time. Adding a temporal component results in functional dependencies being invalidated because the attribute values vary over time. A <strong>temporal functional dependency</strong> \(X \xrightarrow{\tau} Y\) holds on schema \(R\) if the functional dependency \(X \to Y\) holds on all snapshots for all legal instances \(r(R)\).</p> <p>In practice, database designers may add start and end time attributes to relations. SQL standard [start, end). In modern SQL, we can write</p> <div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">period</span> <span class="k">for</span> <span class="n">validtime</span> <span class="p">(</span><span class="k">start</span><span class="p">,</span> <span class="k">end</span><span class="p">)</span>
<span class="k">primary</span> <span class="k">key</span> <span class="p">(</span><span class="n">course_id</span><span class="p">,</span> <span class="n">validtime</span> <span class="k">without</span> <span class="k">overlaps</span><span class="p">)</span>
</code></pre></div></div> <h1 id="chapter-8-complex-data-types">~Chapter 8: Complex Data Types</h1> <p>Expected to read from the textbook.</p> <ul> <li>Semi-Structured Data</li> <li>Object Orientation</li> <li>Textual Data</li> <li>Spatial Data</li> </ul> <h2 id="semi-structured-data">Semi-Structured Data</h2> <p>Many applications require storage of complex data, whose schema changes often. The relational model’s requirement of atomic data types may be an overkill. JSON (JavaScript Object Notation) and XML (Extensible Markup Language) are widely used semi-structured data models.</p> <p><strong>Flexible schema</strong></p> <ul> <li><strong>Wide column</strong> representation allow each tuple to have a different set of attributes and can add new attributes at any time</li> <li><strong>Sparse column</strong> representation has a fixed but large set of attributes but each tuple may store only a subset.</li> </ul> <p><strong>Multivalued data types</strong></p> <ul> <li>Sets, multi-sets</li> <li>Key-value map</li> <li>Arrays</li> <li>Array database</li> </ul> <h3 id="json">JSON</h3> <p>It is a verbose data type widely used in data exchange today, There are efficient data storage variants like BSON</p> <h3 id="knowledge-representation">Knowledge Representation</h3> <p>Representation of human knowledge is a long-standing goal of AI. <strong>RDF: Resource Description Format</strong> is a simplified representation for facts as triples of the form (subject, predicate, object). For example, (India, Population, 1.7B) is one such form. This form has a natural graph representation. There is a query language called SparQL for this representation. <strong>Linked open data</strong> project aims to connect different knowledge graphs to allow queries to span databases.</p> <p>To represent n-ary relationships, we can</p> <ul> <li>Create an artificial entity and link to each of the n entities</li> <li>Use <strong>quads</strong> instead of triples with context entity</li> </ul> <h2 id="object-orientation">Object Orientation</h2> <p><strong>Object-relational data model</strong> provides richer type system with complex data types and object orientation. Applications are often written in OOP languages. However, the type system does not match relational type system and switching between imperative language and SQL is cumbersome.</p> <p>To use object-orientation with databases, we could build an <strong>object-relational database</strong>, adding object-oriented features to a relational database. Otherwise, we could automatically convert data between OOP and relational model specified by <strong>object-relational mapping</strong>. <strong>Object-oriented database</strong> is another option that natively supports object-oriented data and direct access from OOP. The second method is widely used now.</p> <h3 id="object-relational-mapping">Object-Relational Mapping</h3> <p>ORM systems allow</p> <ul> <li>specification of mapping between OOP objects and database tuples</li> <li>Automatic modification of database</li> <li>Interface to retrieve objects satisfying specified conditions</li> </ul> <p>ORM systems can be used for websites but not for data-analytics applications!</p> <h2 id="textual-data">Textual Data</h2> <p><strong>Information retrieval</strong> basically refers to querying of unstructured data. Simple model of keyword queries consists of fetching all documents containing all the input keywords. More advanced models rank the relevance of documents.</p> <h3 id="ranking-using-tf-idf">Ranking using TF-IDF</h3> <p>Term is a keyword occurring in a document query. The <strong>term frequency</strong> \(TF(d, t)\) is the relevance of a term \(t\) to a document \(d\). It is defined by</p> \[TF(d, t) = \log( 1+ n(d, t)/n(d))\] <p>where \(n(d, t)\) is the number of occurrences of term \(t\) in document \(d\) and \(n(d)\) is the number of terms in the document \(d\).</p> <p>The <strong>inverse document frequency</strong> \(IDF(t)\) is given by</p> \[IDF(t) = 1/n(t)\] <p>This is used to give importance to terms that are rare. <strong>Relevance</strong> of a document \(d\) to a set of terms \(Q\) can be defined as</p> \[r(d, Q) = \sum_{t \in Q} TF(d, t)*IDF(t)\] <p>There are other definitions that take <strong>proximity</strong> of words into account and <strong>stop words</strong> are often ignored.</p> <h1 id="lecture-17">Lecture 17</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">08-02-22</code></p> </blockquote> <p>The TF-IDF method in search engine was did not work out as web designers added repeated occurrences of words on their website to increase the relevance. There were plenty of shady things web designers could do in order to increase the page relevance. To prevent this problem, Google introduced the model of <strong>PageRank</strong>.</p> <h3 id="ranking-using-hyperlinks">Ranking using Hyperlinks</h3> <p>Hyperlinks provide very important clues to importance. Google’s PageRank measures the popularity/importance based on hyperlinks to pages.</p> <ul> <li>Pages hyperlinked from many pages should have higher PageRank</li> <li>Pages hyperlinked from pages with higher PageRank should have higher PageRank</li> </ul> <p>This model is formalised by a <strong>random walk</strong> model. Let \(T[i, j]\) be the probability that a random walker who is on page \(i\) will click on the link to page \(j\). Then, PageRank[j] for each page \(j\) is defined as</p> \[P[j] = \delta/N + (1 - \delta)*\sum_{i = 1}^n(T(i, j)*P(j))\] <p>where \(N\) is the total number of pages and \(\delta\) is a constant usually set to \(0.15\). As the number of pages are really high, some sort of bootstrapping method (Monte Carlo simulation) is used to approximate the PageRank. PageRank also can be fooled using mutual link spams.</p> <h3 id="retrieval-effectiveness">Retrieval Effectiveness</h3> <p>Measures of effectiveness</p> <ul> <li><strong>Precision</strong> - what % of returned results are actually relevant.</li> <li><strong>Recall</strong> - what percentage of relevant results were returned</li> </ul> <h2 id="spatial-data">Spatial Data</h2> <p>Not covered</p> <h1 id="chapter-9-application-development">~Chapter 9: Application Development</h1> <h2 id="http-and-sessions">HTTP and Sessions</h2> <p>The HTTP protocol is <strong>connectionless</strong>. That is, once the server replied to a request, the server closes the connection with the client, and forgets all about the request. The motivation to this convention is that it reduces the load on the server. The problem however is authentication for every connection. Information services need session information to acquire user authentication only once per session. This problem is solved by <strong>cookies</strong>.</p> <p>A <strong>cookie</strong> is a small piece of text containing identifying information</p> <ul> <li>sent by server to browser on first interaction to identify session</li> <li>sent by browser to the server that created the cookie on further interactions (part of the HTTP protocol)</li> <li>Server saved information about cookies it issued, and can use it when serving a request. E.g, authentication information, and user preferences</li> </ul> <p>Cookies can be stored permanently or for a limited time.</p> <p><strong>Java Servlet</strong> defines an API for communication between the server and app to spawn threads that can work concurrently.</p> <h2 id="web-services">Web Services</h2> <p>Web services are basically URLs on which we make a request to obtain results.</p> <p>Till HTML4, local storage was restricted to cookies. However, this was expanded to any data type in HTML5.</p> <h2 id="http-and-https">HTTP and HTTPS</h2> <p>The application server authenticates the user by the means of user credentials. What if a hacker scans all the packets going to the server to obtain a user’s credentials? So, HTTPS was developed to encrypt the data sent between the browser and the server. How is this encryption done? The server and the browser need to have a common key. Turns out, there are crypto techniques that can achieve the same.</p> <p>What if someone creates a middleware that simulates a website a user uses? This is known as <strong>man in the middle attack</strong>. How do we know we are connected to the authentic website? The basic idea is to have a <strong>public key</strong> of the website and send data encrypted via this public key. Then, the website uses its own <strong>private key</strong> to decrypt the data. This conversion is reversible. As in, the website encrypts the data using its own private key which can be decoded by the user using the public key.</p> <p>How do we get public keys for millions of websites out there? We use <strong>digital certificates</strong>. Let’s say we have a website’s public key and that website has the public key of the user (via their website). The website then encrypts the user’s public key using its private key to generate a digital certificate. This digital certificate can be advertised on the user’s website to allow other users to check the authenticity of the user’s website. Now, another user can obtain this certificate, decrypt it using the first website’s private key, and verify the authenticity of the user’s webpage. These verifications are maintained as a hierarchical structure to maintain digital certificates of millions of websites.</p> <h2 id="cross-site-scripting">Cross Site Scripting</h2> <p>In cross site scripting, the user’s session for one website is used in another website to execute actions at the server of the first website. For example, suppose a bank’s website, when logged in, allows the user to transfer money by visiting the link <code class="language-plaintext highlighter-rouge">xyz.com/?amt=1&amp;to=123</code>. If another website has a similar link (probably for displaying an image), then it can succeed in transferring the amount if the user is still logged into the bank. This vulnerability is called called <strong>cross-site scripting (XSS)</strong> or <strong>cross-site request forgery (XSRF/CSRF)</strong>. <strong>XSRF</strong> tokens are a form of cookies that are used to check these cross-site attacks (CORS from Django).</p> <h1 id="lecture-18">Lecture 18</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">14-02-22</code></p> </blockquote> <h2 id="application-level-authorisation">Application Level Authorisation</h2> <p>Current SQL standard does not allow fine-grained authorisation such as students seeing their own grades but not others. <strong>Fine grained (row-level) authorisation</strong> schemes such as Oracle Virtual Private Database (VPD) allows predicates to be added transparently to all SQL queries.</p> <h1 id="chapter-10-big-data">~Chapter 10: Big Data</h1> <p>Data grew in terms of volume (large amounts of data), velocity (higher rates of insertions) and variety (many types of data) in the recent times. This new generation of data is known as <strong>Big Data</strong>.</p> <p>Transaction processing systems (ACID properties) and query processing systems needed to be made scalable.</p> <h2 id="distributed-file-systems">Distributed File Systems</h2> <p>A distributed file system stores data across a large collection of machines, but provides a single file-system view. Files are replicated to handle hardware failure, and failures were to be detected and recovered from.</p> <h3 id="hadoop-file-system-architecture">Hadoop File System Architecture</h3> <p>A single namespace is used for an entire cluster. Files are broken up into blocks (64 MB) and replicated on multiple <em>DataNodes</em>. A client finds the location of blocks from <em>NameNode</em> and accesses the data from <em>DataNodes</em>.</p> <p>The key idea of this architecture is using large block sizes for the actual file data. This way, the metadata would be reduced and the <em>NameNode</em> can store the <em>DataNodes</em> info in a much more scalable manner.</p> <p>Distributed file systems are good for <u>millions of large files</u>. However, distributed file systems have very high overheads and poor performance with billions of smaller tuples. Data coherency also needs to be ensured (write-once-read-many access model).</p> <h2 id="sharding">Sharding</h2> <p>It refers to partitioning data across multiple databases. Partitioning is usually done on some <strong>partitioning attributes</strong> known as <strong>partitioning keys</strong> or <strong>shard keys</strong>. The advantage to this is that it scales well and is easy to implement. However, it is not transparent (manually writing all routes and queries across multiple databases), removing load from an overloaded database is not easy, and there is a higher change of failure. Sharding is used extensively by banks today.</p> <h2 id="key-value-storage-systems">Key Value Storage Systems</h2> <p>These systems store large numbers of small sized records. Records are partitioned across multiple machines, and queries are routed by the system to appropriate machine. Also, the records are replicated across multiple machines to ensure availability. Key-value stores ensure that updates are applied to all replicas to ensure consistency.</p> <p>Key-value stores may have</p> <ul> <li><u>uninterpreted bytes</u> with an associated key</li> <li><u>Wide-column</u> with associated key</li> <li>JSON</li> </ul> <p><strong>Document stores</strong> store semi-structured data, typically JSON. Key-value stores support <code class="language-plaintext highlighter-rouge">put</code>, <code class="language-plaintext highlighter-rouge">get</code> and <code class="language-plaintext highlighter-rouge">delete</code>. Some systems also support <strong>range queries</strong> on key values. Document stores also support queries on non-key attributes. <u>Key value stores are not full database systems</u>. They have no/limited support for transactional updates and applications must manage query processing on their own. These systems are therefore known as <strong>NoSQL</strong> systems.</p> <h2 id="parallel-and-distributed-databases">Parallel and Distributed Databases</h2> <p><strong>Replication</strong> to ensure availability. <strong>Consistency</strong> implemented using majority protocols. <strong>Network partitions</strong> involve a network can break into two or more parts, each with active systems that can’t talk to other parts. In presence of partitions, cannot guarantee both availability and consistency - <strong>Brewer’s CAP</strong> theorem. Traditional database systems choose consistency, and most web applications choose availability.</p> <h1 id="lecture-19">Lecture 19</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">15-02-22</code></p> </blockquote> <h2 id="mapreduce-paradigm">MapReduce Paradigm</h2> <p>The goal here is to be able to run many queries/scripts across a large number of machines. <code class="language-plaintext highlighter-rouge">Map</code> and <code class="language-plaintext highlighter-rouge">Reduce</code> have similar functionalities as seen in Python. Programmers realised many operations can be reduced to a sequence of map and reduce actions (popular in functional programming).</p> <p>Google formalised the notion of map-reduce for web-crawling and other web-development needs as map-reduce workers. This paradigm was used along with distributed file systems.</p> <p>The default input for the map operations is a line.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>map(k, v) -&gt; list(k1, v1)
reduce(k1, list(v1)) -&gt; v2
</code></pre></div></div> <p>However, map-reduce code for database queries was large. So, the developers at Facebook came up with Hive which converts SQL queries to map-reduce queries.</p> <h2 id="algebraic-operations">Algebraic Operations</h2> <p>We shall study these as a part of <strong>Spark</strong>.</p> <h1 id="lecture-20">Lecture 20</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">17-02-22</code></p> </blockquote> <h2 id="algebraic-operations-in-spark">Algebraic Operations in Spark</h2> <p><strong>Resilient Distributed Dataset (RDD)</strong> abstraction is a collection of records that can be stored across multiple machines. RDDs can be created by applying algebraic operations on other RDDs. This is a generalisation to RA where the operators can be any piece of code. <u>RDDs can be lazily computed when needed.</u> As in, the tree is executed only on specific functions such as <code class="language-plaintext highlighter-rouge">saveAsTextFile()</code> or <code class="language-plaintext highlighter-rouge">collect()</code>.</p> <p>Spark makes use of Java Lambda expressions with the following syntax.</p> <div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">-&gt;</span> <span class="nc">ArrayasList</span><span class="o">(</span><span class="n">s</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">" "</span><span class="o">)).</span><span class="na">iterate</span><span class="o">()</span>
</code></pre></div></div> <p>RDDs in Spark can be typed in programs, but not dynamically.</p> <h2 id="streaming-data">Streaming Data</h2> <p>Streaming data refers to data that arrives in a continuous fashion in contrast to <strong>data-at-rest</strong>.</p> <p>Approaches to querying streams-</p> <ul> <li><strong>Windowing</strong> - Break up stream into windows and queries are run on windows.</li> <li><strong>Continuous Queries</strong> - Queries written e.g. in SQL, output partial result based on stream seen so far; query results are updated continuously.</li> <li><strong>Algebraic operators on streams</strong> - Operators are written in an imperative language.</li> <li><strong>Pattern Matching</strong> - <em>Complex Even Processing (CEP)</em> systems. Queries specify patterns, system detects occurrences of patterns and triggers actions.</li> <li><strong>Lambda architecture</strong> - Split the stream into two, one output goes to stream processing system and the other to a database for storage.</li> </ul> <p>There are stream extensions to SQL - Tumbling window, Hopping window, Sliding window and Sessions windows.</p> <h3 id="publish-subscribe-systems">Publish Subscribe Systems</h3> <p><strong>Public-subscribe (pub-sub)</strong> systems provide convenient abstraction for processing streams. For example, Apache Kafka</p> <h2 id="graph-databases">Graph Databases</h2> <p>A <strong>graph data model</strong> can be seen as a generalisation of the ER model. Every entity can be seen as a node, and every binary relationship is an edge. Higher degree relationships can be expressed as multiple binary relationships.</p> <p>Check out <em>Neo4J</em>. Query languages for graph databases make it easy for graph traversal.</p> <h1 id="lecture-21">Lecture 21</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">28-02-22</code></p> </blockquote> <h3 id="parallel-graph-processing">Parallel graph processing</h3> <p>Two popular approaches have been devised for parallel processing on very large graphs</p> <ul> <li>Map-reduce and algebraic frameworks</li> <li><strong>Bulk synchronous processing (BSP)</strong></li> </ul> <h3 id="bulk-synchronous-processing">Bulk Synchronous Processing</h3> <p>Each vertex of a graph has data associated with it. The vertices are partitioned across multiple machines, and state of the nodes are kept in-memory. Now, in each step (<em>superstep</em>)</p> <ul> <li>Nodes process received messages</li> <li>Update their state</li> <li>Send further messages or vote to halt</li> <li>Computation ends when all nodes vote to halt, and there are no pending messages</li> </ul> <p>The method is synchronous as the computation is done in steps. However, this method is not fault tolerant as all the computations need to be recomputed in case of a failure. Checkpoints can be created for restoration.</p> <h1 id="chapter-11-data-analytics">~Chapter 11: Data Analytics</h1> <ul> <li><strong>Data Analytics</strong> - The processing of data to infer patterns, correlations, or models for prediction.</li> <li>Data often needs to be <strong>extracted</strong> from various source formats, <strong>transformed</strong> to a common schema, and <strong>loaded</strong> into the <u>data warehouse</u>. (ETL)</li> <li><strong>Data mining</strong> extends techniques developed by ML onto large datasets</li> <li>A <strong>data warehouse</strong> is a repository of information gathered from multiple sources, stored under a unified schema at a single site. It also permits the study of historical trends. The common schema is <u>optimised for querying and not transactions</u>. The schema is most often <strong>denormalized</strong> (faster query time).</li> <li>Data in warehouses can be stored as <strong>fact tables</strong> or <strong>dimension tables</strong>. The attributes of fact tables can be usually viewed as <strong>measure attributes</strong> (aggregated upon) or <strong>dimension attributes</strong> (small ids that are foreign keys to dimension tables).</li> <li>A fact table branching out to multiple dimension schema is a <strong>star schema</strong>. A <strong>snowflake schema</strong> has multiple levels of dimension tables (can have multiple fact tables).</li> <li>A <strong>data lake</strong> refers to repositories which allow data to be stored in multiple formats without schema integration. Basically, data is just dumped for future use.</li> <li>Data warehouses often use <strong>column-oriented storage</strong>.</li> </ul> <h1 id="chapter-12-physical-storage-systems">~Chapter 12: Physical Storage Systems</h1> <p>The performance of a database engine depends on the way data is stored underneath. The storage hierarchy typically used is as follows</p> <p><img src="/assets/img/Databases/image-20220312223422611.png" alt="image-20220312223422611"/></p> <p>Tertiary storage is used for data archives in today’s world. Data is read as a cache-line from the main memory (lookahead sorta). Similarly, to account for even higher latency of the flash memory, we read one page at a time.</p> <h2 id="storage-interfaces">Storage Interfaces</h2> <p>The way we interface with the storage also has a great impact on the performance. We have the following standards</p> <ul> <li>SATA (Serial ATA) - Supports upto 6 Gbps (v3)</li> <li>SAS (Serial Attached SCSI) - Supports upto 12 Gbps (v3)</li> <li>NVMe (Non-Volatile Memory Express) works with PCIe connecters and gives upto 24 Gbps.</li> </ul> <h1 id="lecture-22">Lecture 22</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">03-03-22</code></p> </blockquote> <p>A large number of disks are connected by a high-speed network to a number of servers in a <strong>Storage Area Network (SAN)</strong>. The <strong>Network Attached Storage (NAS)</strong> provides a file system interface using a network file system protocol like FTP. SAN can be connected to multiple computers and gives a view of a local disk. It has fault tolerance and replication. A NAS pretends to be a file system unlike SAN.</p> <h2 id="magnetic-disks">Magnetic Disks</h2> <p>The surface of the platter is divided into 50k-100k circular <strong>tracks</strong>. Each track is divided into 500-1000 (on inner tracks) or 1000-2000 (on outer tracks) <strong>sectors</strong>. A <strong>cylinder</strong> is a stack of platters.</p> <p>The performance of a disk is measured via its <strong>access time</strong> which involves seek time and rotational latency, <strong>I/O operations per second (IOPS)</strong>, <strong>Mean/Annualized time to failure (MTTF)</strong> and <strong>data-transfer rate</strong>. We can tune the performance by changing parameters such as <strong>Disk block size</strong> and <strong>Sequential/Random access pattern</strong>. MTTF decreases as disk ages.</p> <p><strong><em>Note.</em></strong> Suppose the MTTF of a single disk is \(t\). How do we calculate the average failing time of a disk in a set of \(n\) disks? The probability that a disk fails in a given hour is \(1/t\). The probability that one of the disks in \(n\) fails is \(1 - (1 - 1/t)^n\). However, if \(t\) is large, it is simply \(n/t\). That is, on an average a disk fails in every \(t/n\) hours in a set of \(n\) disks.</p> <p>In a random access pattern, every request requires a <strong>seek</strong>. This method results in lower transfer rates. Current disks allow up to 50-200 IOPS.</p> <h2 id="flash-storage">Flash storage</h2> <p>A <strong>NAND flash</strong> is widely used for storage in contrast to a <strong>NOR flash</strong>. A page can only be written once, and it must be erased to allow rewriting. Flash storage does page-at-a-time read. If we try for a byte read, then the control lines take up a lot of storage, and the capacity goes down.</p> <p>A <strong>solid state disk</strong> uses standard block-oriented disk interfaces, but store data on multiple flash storage devices internally. We can use SSD using the SATA interface. An erase in flash storage happens in unit of <strong>erase block</strong>, and <strong>remapping</strong> of logical page addresses to physical page addresses avoids waiting for erase. The remapping is carried out by <strong>flash translation layer</strong>. After 100000 to 1000000 erases, the erase block becomes unreliable and cannot be use due to <strong>wear leveling</strong>.</p> <p>A SLC tolerates about \(10^6\) erases. A QLC has 4 voltage levels (2 bits can be stored in 1 physical bit). These are much less tolerant to erases (about \(10^3\) erases). <strong>Wear leveling</strong> normalises the erases in a region of the flash storage by storing cold data in the part where a lot of erases have been done.</p> <p>The performance of an SSD is measured through the data transfer rates. SSDs also support parallel reads. <strong>Hybrid disks</strong> combine small amount of flash cache with large magnetic disks.</p> <p>Recently, Intel has come up with the 3D-XPoint memory technology which is shipped as Intel Optane. It allows lower latencies than flash SSDs.</p> <h3 id="raid">RAID</h3> <p><strong>Redundant Arrays of Independent Disks</strong> is a set of disk organization techniques that manage a large number of disks <u>providing a view of a single disk.</u> The idea is that some disk out of a set of <em>N</em> disks will fails much higher than the chance that a specific single disk will fail. We expect <u>high capacity, high speed, and high reliability</u> from this system.</p> <p>In a way, we improve the reliability of the storage system using redundancy. For example, the simplest way to do this is <strong>mirroring</strong> (or shadowing) where we just duplicate all disks. <u>The **mean time to data loss** depends on the mean time to failure and the mean time to repair</u>. For example, if the MTTF is 100000 hours and the mean time to repair is 10 hours, then we get the mean time to data loss as \(500\times 10^6\) hours. How do we get this? The probability that one of the disk fails is \(2*10^{-5}\). Now, what is the probability that the other disk fails within the repair time? It is \(2* 10^{-4}\). Now, at this point we have data loss. Therefore, the mean time to data loss would be \(2.5 *10^8\) for one disk. As we have two disks, we get \(5 * 10^8\). Data loss occurs when both disks fail.</p> <p>The two main goals of parallelism in a disk system are to load balance multiple small accesses to increase throughput and parallelise large accesses to reduce the response time. We do this via bit-level stripping or <strong>block-level striping</strong>. In block level striping, with n disks, block \(i\) of a file goes to disk to disk(\(i\%n\)) + 1. Now, requests for the same file can run in parallel increasing the transfer rate.</p> <ul> <li>RAID level 0 : Block-striping; non-redundant. Used in high-performance applications where data loss is not critical.</li> <li>RAID level 1: Mirrored disks with block striping. Popular for best write performance and applications such as storing log files in a database system.</li> </ul> <p>RAID also <strong>parity blocks</strong> that stores the XOR of bits from the block of each disk. Parity block \(j\) stores XOR of bits from block \(j\) of each disk. This helps in recovery of data in case of a single disk failure (XOR the parity bit with the remaining blocks on various disks). Parity blocks are often spread across various disks for obvious reasons.</p> <ul> <li> <p>RAID level 5 - Block-interleaved Distributed Parity. This is nice but writes are slower. The cost of recovery is also high. <em>Think</em>.</p> </li> <li> <p>RAID level 6 - It has a P + Q redundancy scheme where 2 error correction blocks are stored instead of a single parity block. Two parity blocks guard against multiple(2) disk failures.</p> </li> </ul> <p>There are other RAID levels which are not used in practice.</p> <h1 id="lecture-23">Lecture 23</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">07-03-22</code></p> </blockquote> <p><strong>Software RAID</strong> vs. <strong>Hardware RAID</strong>. Copies are written sequentially to guard against corruption in case of power failure. There are couple of hardware issues</p> <ul> <li><strong>Latent sector failures</strong> - Data successfully written earlier gets damaged which can result in data loss even if only one disk fails.</li> <li><strong>Data scrubbing</strong> - Continuous scans for latent failures, and recover from copy/parity.</li> <li><strong>Hot swapping</strong> - Replacement of disk while the system is running without powering it down. This reduces the time to recovery, and some hardware RAID systems support this.</li> </ul> <h1 id="chapter-13-data-storage-structures">~Chapter 13: Data Storage Structures</h1> <h2 id="file-organization">File Organization</h2> <p>The database is stored as a collection of <em>files</em>. Each file is a sequence of <em>records</em>, and each record is a sequence of fields. We will assume the following</p> <ul> <li>Fixed record size</li> <li>Each file has records of one particular type only</li> <li>Different files are used for different relations</li> <li>Records are smaller than a disk block</li> </ul> <h3 id="fixed-length-records">Fixed length records</h3> <p>We store the records contiguously, and access a record based on the index and the offset. There might be fragmentation at the end of the block. How do we handle deleted records? We shall link all the free records on a free list.</p> <h3 id="variable-length-records">Variable length records</h3> <p>Strings are typically variable sized. Each record has variable length attributes represented by fixed size (offset, length) with the actual data stored after all fixed length attributes. Null values are represented by null-value bitmap. How do we structure these records in a block?</p> <h3 id="slotted-page-structure">Slotted Page Structure</h3> <p>A slotted page header contains -</p> <ul> <li>number of record entries</li> <li>end of free space in the block</li> <li>location and size of each record</li> </ul> <p>The records are stored contiguously after the header. Disk pointers point to the header and not directly to the record.</p> <h3 id="storing-large-objects">Storing Large Objects</h3> <p>Records were assumed to be smaller than pages. Otherwise, we store the records as files. In Postgres, the large attribute is automatically broken up into smaller parts.</p> <h2 id="organisation-of-records-in-files">Organisation of Records in Files</h2> <p>Records can be stored as</p> <ul> <li><strong>Heap</strong> - Record can be placed anywhere in the file where there is space. We maintain a hierarchical free space map of two levels usually.</li> <li><strong>Sequential</strong> - Store records in sorted sequential order, based on the value of the search key of each record.</li> <li><strong>B+ tree file organization</strong></li> <li><strong>Hashing</strong></li> </ul> <p>Some databases also support <strong>multi-table clustering file organisation</strong> that allows records of different relations to be stored in the same file.</p> <h2 id="metadata-and-partitioning">Metadata and Partitioning</h2> <p>The <strong>data dictionary</strong> or <strong>system catalog</strong> stored <strong>metadata</strong> such as</p> <ul> <li>names of relations</li> <li>names, types and lengths of attributes of each relation</li> <li>names and definitions of views</li> <li>integrity constraints</li> </ul> <h1 id="lecture-24">Lecture 24</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">08-03-22</code></p> </blockquote> <h3 id="partitioning">Partitioning</h3> <p><strong>Table partitioning</strong> - Records in a relation can be partitioned into smalled relations that are stored separately - <strong>Horizontal partitioning</strong>. Store each attribute of a relation separately - <strong>vertical partitioning</strong>. Also known as <strong>columnar representation</strong> or <strong>column oriented storage</strong>. This is a good idea for data analytics but not for transaction processing. The benefits of this representation include</p> <ul> <li>Reduced IO if only some attributes are accessed</li> <li>Improved CPU cache performance</li> <li>Improved Compression</li> <li>Vector Processing on modern CPU architectures</li> </ul> <p>The disadvantages are</p> <ul> <li>Tuple reconstruction is difficult</li> <li>Tuple deletion and updates are difficult</li> <li>Cost of decompression</li> </ul> <p>Some databases support a hybrid model which has both row and column representation.</p> <p><strong>Note.</strong> ORC and Parquet use file formats with columnar storage inside file. These are log file formats.</p> <h2 id="storage-access">Storage Access</h2> <p>Blocks are units of both storage allocation and data transfer. At the disk layer, a page is the physical unit. <strong>Buffer</strong> - The portion of the main memory to store copies of the disk blocks.</p> <h3 id="buffer-manager">Buffer Manager</h3> <p><strong>Pinned block</strong> - A memory block that is not allowed to be written back to the disk. A <strong>pin</strong> is done before reading/writing data from a block. An <strong>unpin</strong> done when read/write is complete. Multiple concurrent pin/unpin operations are possible. There are also <strong>shared and exclusive locks</strong> on buffer.</p> <h3 id="buffer-replacement-policies">Buffer Replacement Policies</h3> <p>Most OS replace the block using the LRU strategy. However, this is not suitable in many database operations. Therefore, a database system can query plan to predict future references. There are <strong>toss-immediate</strong> and <strong>MRU</strong> strategies too.</p> <h1 id="chapter-14-indexing">~Chapter 14: Indexing</h1> <p>A <strong>search key</strong> is a set of attributes used to look up records in a file. An <strong>index file</strong> consists of records (called <strong>index entries</strong>) of the form \(search-key \mid pointer\). These files are usually much smaller than the original file. We have two basic kinds of indices</p> <ul> <li><strong>Ordered indices</strong> - SEarch keys are stored in a sorted order</li> <li><strong>Hash indices</strong> - search keys are distributed uniformly across “buckets” using a “hash function”.</li> </ul> <h2 id="index-evaluation-metrics">Index Evaluation Metrics</h2> <ul> <li>Access types supported by the indices. These include searching for records with a specified value or records falling in a specified range of values.</li> <li>Access time</li> <li>Insertion time</li> <li>Deletion time</li> <li>Space overhead</li> </ul> <h2 id="ordered-indices">Ordered Indices</h2> <p>A <strong>clustering index</strong> in a sequentially ordered file, is the index whose search key specifies the sequential order of the file. It is also called as the <strong>primary index</strong> that is not to be confused with primary key. A <strong>secondary index/nonclustering</strong> is an index whose search key specifies an order different from the sequential order of the file.</p> <p>An index sequential file is a sequential file ordered on a search key, with a clustering index on the search key.</p> <h3 id="dense-index-files">Dense index Files</h3> <p>A <strong>dense index</strong> is an index for which there is a record in the index-file for every search-key value in the file. Also, every index-record made with a dense index need not be mapped to an index as we use the following structure.</p> <p><img src="/assets/img/Databases/image-20220407153333197.png" alt="image-20220407153333197"/></p> <p>A <strong>sparse index</strong> on the other hand contains index records for only some search-key values. To locate a record with a search-key value \(K\), we first find an index record with largest search-key value \(&lt; K\). Then, we search the file sequentially starting at the record to which the index record points. For unclustered index, we create a sparse index on top of a dense index (multilevel index).</p> <h1 id="lecture-25">Lecture 25</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">10-03-22</code></p> </blockquote> <p>Sparse indices take less space and have less maintenance overhead in comparison to dense indices. However, they are generally slower than dense indices.</p> <p><strong>Note.</strong> Secondary indices have to be dense.</p> <p>We use <strong>lexicographic ordering</strong> for composite search keys.</p> <h2 id="b-tree">B\(^+\)-Tree</h2> <p>We will ignore duplicate keys for now. The number of children for every node lies within a certain specified range for that tree. In a \(B^+\)-Tree we have \(n\) pointers and \(n-1\) values separating them. A pointer between values \(a\) and \(b\) will point to values \(c\) that satisfy \(a \leq c &lt; b\). It is not necessary for the internal nodes to be full.</p> <p>Formally, a \(B^+\)-tree is a rotted tree satisfying the following properties</p> <ul> <li>All paths from the root to a leaf are of the same length.</li> <li>Each node that is not a root or a leaf has between \(\lceil{n/2}\rceil\) and \(n\) children.</li> <li>A lead node has between \(\lceil (n - 1)/2 \rceil\) and \(n - 1\) values.</li> <li>If a root is not a leaf, it has at least 2 children, and if a root is a lead, it can have between \(0\) and \(n - 1\) values.</li> </ul> <p>A typical node looks like \(P_! \mid K_1 \mid \dots \mid K_{n - 1} \mid P_n\). Here \(K_i\) are the search-key values and \(P_i\) are pointers to children or records (buckets of records). Also, \(K_1 &lt; \dots &lt; K_{n - 1}\).</p> <h3 id="leaf-nodes">Leaf nodes</h3> <p>For \(i = 1, \dots, n - 1\), pointer \(P_i\) points to a file record with search-key value \(K_i\).</p> <p>Pointers help us keep the nodes logically close but they need not be physically close. The non-lead levels of the \(B^+\) tree form a hierarchy of sparse indices. The level below root has at least \(2* \lceil n/2 \rceil\) values, the next level has \(2* \lceil n/2 \rceil* \lceil n/2 \rceil\), and so on. So if there are \(K\) search key values in the file, the tree height is no more than \(\lceil \log_{\lceil n/1 \rceil} K\rceil\).</p> <h3 id="queries-on-b-trees">Queries on \(B^+\)-trees</h3> <p><strong>Range queries</strong> finds all records with search key values in a given range. These are implemented as iterators.</p> <p>To handle non-unique keys, create a composite key that indexes into the duplicate values. Search for an index can be implemented as a range query. If the index is clustering, then all accesses are sequential. However, if the index if non-clustering, each record access may need an I/O operation.</p> <h3 id="insertion-on-b-trees">Insertion on \(B^+\)-trees</h3> <p>Insertion is easy when the nodes are not full. However, when nodes are full, we would have to split the nodes. We split a node through the parent, by adding a splitting value in the parent node. We do this recursively, and if the root gets full, we create a new root. We insert from leaves because the leaves hold the pointers to records.</p> <p><img src="/assets/img/Databases/image-20220407164819276.png" alt="image-20220407164819276"/></p> <p>The above image gives the formal algorithm.</p> <h3 id="deletion-on-b-trees">Deletion on \(B^+\)-trees</h3> <p>We need to ensure that there are at least a minimum number of values in each node. The complexity of the updates is of the order \(\mathcal O( \log_{\lceil n/2 \rceil}K)\). The height of the tree decreases when a node has very few children. Note that a deleted value can still appear as a separator in the tree after the deletion. Also, the average node occupancy depends on the insertion order (2/3rds with random and 1/2 with insertion in sorted order).</p> <h1 id="lecture-26">Lecture 26</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">14-03-22</code></p> </blockquote> <p>If we allow non-unique keys, we can store a key with multiple pointers. However, the complexity comes in terms of deletion. Worst case complexity may be linear.</p> <h3 id="b-tree-file-organisation">\(B^+\)-Tree file Organisation</h3> <p>Leaf nodes in a \(B^+\) tree file organisation store records, instead of pointers. As records are larger than pointers, the maximum number of records that can be stored in a lead node is less than the number of pointers in a non-leaf node. To improve space utilisation, we can involve more sibling nodes in redistribution during splits and merges. Involving 2 siblings in redistribution results in each node having at least \(\lfloor 2n/3 \rfloor\) entries.</p> <p>Record relocation and secondary indices - If a record moves, all secondary indices that store record pointers have to be updated. Therefore, node splits in \(B^+\) tree file organisation become very expensive. The solution to this is use a search key of the \(B^+\) tree file organisation instead o record pointer in a secondary index. For example, consider students database sorted using roll numbers with names as a secondary index. If the records move, we would need to update all the “name” index pointers. So what we do is, make the “name” index pointers point to the “roll number” index pointers instead of the records directly. Since “roll number” is a clustered index, no relocation of secondary indices is required.</p> <h3 id="indexing-strings">Indexing Strings</h3> <p>How do we use variable length strings as keys? We use <strong>prefix compression</strong> along with variable fanout (?). In the internal nodes, we can use simplified separators.</p> <h2 id="bulk-loading-and-bottom-up-build">Bulk Loading and Bottom-Up Build</h2> <p>Inserting entries one-at-a-time into a \(B^+\)-tree requires \(\geq\) 1 I/O per entry assuming leaves don’t fit in the memory.</p> <ul> <li>Sort entries first, and insert in a sorted order. This will have much improved I/O performance.</li> <li>Build a \(B^+\)tree <strong>bottom-up</strong>. AS before sort the entries, and then create tree layer-by-layer starting with the leaf level.</li> </ul> <p>However, the above two methods expect a bulk insertion. What do we do if we have a sudden burst of inserts? We will look at alternatives later.</p> <h3 id="b-tree-index-files">B-Tree Index Files</h3> <p>Similar to \(B^+\)-tree but B-tree allows search-key values to appear only once and eliminates redundant storage of search keys. The pointers to the records are stored in the internal nodes too! The problem with this approach is that the tree becomes taller. There is minimal advantage too.</p> <p>Indexing on flash has a few issues, as writes are no in-place and it eventually requires a more expensive erase.</p> <p>A key idea is to use large node size to optimise disk access, but structure data within a node using a tree with small node size, instead of using an array for faster cache access (so that all nodes fit inside a single cache line).</p> <h2 id="hashing">Hashing</h2> <h3 id="handling-bucket-overflows">Handling bucket overflows</h3> <p><strong>Overflow chaining</strong> - The overflow buckets of a given bucket are chained together in a linked list. The above scheme is called <strong>closed addressing</strong> or <strong>open/closed hashing</strong>.</p> <p>Overflow can happen due to insufficient buckets or skewness in the data.</p> <p>Hashing is not used widely on disks but is used in-memory.</p> <p><strong>Covering indices</strong> - Attributes that are added to index to prevent the control from fetching the entire record.</p> <p>Some databases allow creation of indices on foreign keys.</p> <p>Indices over tuples can be problematic for a few queries due to lexicographic ordering.</p> <h1 id="lecture-27">Lecture 27</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">15-03-22</code></p> </blockquote> <h2 id="write-optimised-indices">Write Optimised Indices</h2> <p>Performance of \(B^+\) trees can be poor for write intensive workloads. This is because we require one I/O per leaf, assuming all internal nodes are in memory. There are two approaches to reducing cost of writes</p> <ul> <li>Log-structured merge tree</li> <li>Buffer tree</li> </ul> <h3 id="log-structured-merge-lsm-tree">Log Structured Merge (LSM) Tree</h3> <p>Consider only insert queries for now. Records are first inserted into in-memory L0 tree. When the in-memory tree is full, we move the records to the disk in the L1 tree. \(B^+\)-tree is constructed using bottom-up by merging the existing L1 tree with records from L0 tree. The goal is to minimise random I/O.</p> <p>The benefits are that inserts are done using sequential I/O operations and the leaves are full avoiding space wastage. The drawbacks are that queries have to search multiple trees and the entire context of each level is copied multiple times. <strong>Bloom filters</strong> avoid lookups in most trees. The idea is to use hash functions and bitmaps.</p> <p>How about deletes? They will now incur a lot of I/O. We do a logical delete by inserting a new delete entry. Updates are handled as inserts followed by deletes.</p> <p>LSM trees were introduced for disk-based indices. These are useful to minimise erases with flash-based indices.</p> <h3 id="buffer-tree">Buffer Tree</h3> <p>Each internal node of \(B^+\)-tree has a buffer to store inserts. The inserts are moved to lower levels when the buffer is full. With a large buffer, many records are moved to lower level at each time. Therefore, per record I/O decreases.</p> <p>The benefits are less overhead on queries, and it can be used with any tree index structure. However, they have more random I/O than LSM trees.</p> <h2 id="spatial-and-temporal-indices">Spatial and Temporal Indices</h2> <p>A <strong>k-d tree</strong> is a structure used for indexing multiple dimensions. Each level of k-d tree partitions the space into two, and we cycle through the dimensions at each level. Range queries do not have \(\log\) complexity bounds in this index structure.</p> <p>Queries can mix spatial (contains, overlaps) and non-spatial conditions.</p> <p>The <strong>k-d-B</strong> tree extends the k-d tree to allow multiple child nodes for each internal node. This is well suited for secondary storage.</p> <p>Each node in a <strong>quadtree</strong> is associated with a rectangular region of space. Similarly, we can cut across more dimensions in each level.</p> <h3 id="r-tree">R-tree</h3> <p>The motivation behind this structure was to store objects in the spatial domain in a single leaf. We try to create minimally overlapping bounding boxes for all the objects and create a structure similar to a \(B^+\)-tree. Multiple paths may need to be searched, but the performance is good in practice.</p> <p>Suppose we want to insert a new object that overlaps with many bounding boxes. We choose the box which overlaps the least, or the box which has the lowest change in size on the addition. Now, insertion is done via the <strong>clustering algorithm</strong>. The clustering is also done via some heuristics such as minimum overlap of bounding boxes. Greedy heuristics are often used.</p> <h1 id="lecture-28">Lecture 28</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">17-03-22</code></p> </blockquote> <h3 id="indexing-temporal-data">Indexing Temporal Data</h3> <p>A time interval has a start and an end time. A query may ask for all tuples that are valid at a point in time or during a time interval. We can use a spatial index called an <strong>R-tree</strong> for indexing.</p> <h1 id="chapter-15-query-processing">~Chapter 15: Query Processing</h1> <p>Database engines often apply optimisations based on statistics over data which are approximate. An annotated expression specifying a detailed execution strategy is called an <strong>evaluation plan</strong>.</p> <p><strong>Query optimisation</strong> chooses an evaluation plan with the lowest cost (a metric based on approximated statistics).</p> <h2 id="measures-of-query-cost">Measures of Query Cost</h2> <p>Many factors contribute to time cost such as disk access, CPU, and network communication. Cost can be measured on <strong>response time</strong> or <strong>total resource consumption</strong>. As estimating the time is more difficult, we often resort to resource consumption for optimisation. This metric is also useful is shared databases. For our purposes, we will just consider costs related to I/O time.</p> <p>Now, the disk cost is estimated as the sum of average seeks, blocks read, and blocks written. For simplicity we just use the <strong>number of block transfers from disk</strong> and the <strong>number of seeks</strong>. Then, we get \(b \times t_T + S\times t_S\). On a high end magnetic disk, \(t_S = 4ms, t_T = 0.1ms\) and on a SSD, \(t_S = 20-90\mu s, t_T = 2-10 \mu s\) for 4KB blocks.</p> <p>We assume no data is available in the buffer.</p> <h2 id="selection-operation">Selection Operation</h2> <p><code class="language-plaintext highlighter-rouge">A1</code>- <strong>Linear Search</strong> - Assume that file is stored sequentially.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = b/2*t_T + 1*t_S
</code></pre></div></div> <p>We do not consider binary search as it requires a lot more (random) accesses and access time is high in disks.</p> <h3 id="selection-using-indices">Selection using Indices</h3> <p><code class="language-plaintext highlighter-rouge">A2</code> - <strong>Clustering index, Equality on key</strong> - Retrieve a single record that satisfied the corresponding equality condition.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = (h_i + 1)*(t_T + t_S)
</code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">h_i</code> is the height of the index (in \(B^+\)-tree?), and since we are doing random I/O for each case, we need to add both seek and transfer time.</p> <p><code class="language-plaintext highlighter-rouge">A3</code>-<strong>Clustering index, equality on non-key</strong> - Retrieve multiple records. Records will be on consecutive blocks. Let \(b\) be the number of blocks containing matching records.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = h_i*(t_T + t_S) + t_S + t_T*b
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">A4</code>-<strong>Secondary index, equality on key/non-key</strong></p> <p>If the search-key is a candidate key, then</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = (h_i + 1)*(t_T + t_S)
</code></pre></div></div> <blockquote> <p>Why?</p> </blockquote> <p>Otherwise, each of <code class="language-plaintext highlighter-rouge">n</code> matching records may b on a different block</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = (h_i + n)*(t_T + t_S)
</code></pre></div></div> <p>It might be cheaper to scan the whole relation as sequential access is easier than random I/O.</p> <p><code class="language-plaintext highlighter-rouge">A5</code>-<strong>Clustering index, comparison</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = linear_cost for &lt;
		 = index_equality_cost + linear_cost
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">A6</code>-<strong>Non-clustering index, comparison</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cost = cost + cost_records (I/O)
</code></pre></div></div> <p>The difference between clustering and non-clustering indices is that we would have to fetch records in case of non-clustering indices in order to read the non-clustering index attribute. This is not the case in case of a clustering index.</p> <p>Let me write my understanding of all this</p> <blockquote> <p>Indices are just files with pointers. Basically, scanning through indices is faster than scanning through a sequence of entire records, so we use indices. Instead of storing indices sequentially, we use \(B^+\) trees so that its faster. We can’t do this with records directly because records are big and they may not reside in a single file (I think).</p> <p>Now, clustering indices are indices whose order is same as the order of the records. So, once we fetch a record for an index, all records corresponding to the next indices will be in a sequence. Therefore, we won’t have additional seek time.</p> <p>However, this is not the case for non-clustering indices. If we want records corresponding to the next indices of the current index, we’d have additional seek time as the records may lie in different blocks.</p> </blockquote> <h3 id="implementation-of-complex-selections">Implementation of Complex Selections</h3> <p>How do we implement conjunctions? If all the attributes in the conjunction are indexed, then it is straightforward. We will just take the intersection of all results. Otherwise, test all the other conditions after fetching the records into the memory buffer.</p> <p>Also, as we discussed before, we can use a composite index.</p> <p>Disjunctions are a slightly different. If we have all indexed attributes, we just take the union. Otherwise, we just have to do a linear scan. Linear scan is also the best way in most cases for negation.</p> <h2 id="bitmap-index-scan">Bitmap Index Scan</h2> <p>We have seen that index scans are useful when less number of records match in the case of secondary indices. If more records match, we should prefer a linear scan. How do we decide the method beforehand? The <strong>bitmap index scan</strong> algorithm is used in PostgreSQL.</p> <p>We create a bitmap in memory with a bit for each page in the relation. A record ID is just the page ID and the entry number. We initially do an index scan to find the relevant pages, and mark these pages as 1 in the bitmap. After doing this, we just do a linear scan fetching only pages with bit set to 1.</p> <p>How is the performance better? It is same as index scan when only a few bits are set, and it is same as a linear scan when most bits are set. Random I/O is avoided in both cases.</p> <h2 id="sorting">Sorting</h2> <p>For relations that fit in memory, we can use quicksort. Otherwise, we use <strong>external sort merge</strong>.</p> <p><img src="/assets/img/Databases/image-20220407235807034.png" alt="image-20220407235807034"/></p> <p><img src="/assets/img/Databases/image-20220407235757873.png" alt="image-20220407235757873"/></p> <h1 id="lecture-29">Lecture 29</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">21-03-22</code></p> </blockquote> <h2 id="join-operation">Join Operation</h2> <h3 id="nested-loop-join">Nested Loop Join</h3> <p>Requires no indices and can be used with any kind of join condition. It is very expensive though, as it is quadratic in nature. Most joins can be done in linear time in one of the relations, as most joins are foreign key joins. In the worst case, there would memory enough to hold only one block of each relation. The estimated cost then is, <code class="language-plaintext highlighter-rouge">n_r*b_s + b_r</code> block transfers and <code class="language-plaintext highlighter-rouge">n_r + b_r</code> seeks.</p> <h3 id="block-nested-loop-join">Block Nested-Loop Join</h3> <p>We first do block matching and then tuple matching. Asymptotically, this looks same as the above method but it decreases the I/O cost as the number of seeks come down.</p> <h3 id="indexed-nested-loop-join">Indexed Nested-Loop Join</h3> <p>An index is useful in equi-join or natural join. For each tuple \(t_r\) in the outer relation \(r\), we use the index to look up tuples in \(s\) satisfy the join condition with tuple \(t_r\). In the worst case, the buffer has space for only one page of \(r\), and for each tuple in \(r\), we perform an index lookup on \(s\). Therefore, the cost of the join is <code class="language-plaintext highlighter-rouge">b_r(t_T + t_S) + n_r*C</code>, where \(c\) is the cost of traversing index and fetching all matching \(s\) tuples for one tuple of \(r\). The second term is the dominating term. We should use the fewer tuples relations as the outer relation.</p> <h3 id="merge-join">Merge Join</h3> <p>Sort both relations on their join attribute, and merge the sorted relations. Cost is <code class="language-plaintext highlighter-rouge">(b_r + b_s)*t_T + (ceil(b_r/b_b) + ceil(b_S/b_b))*t_S</code> along with the cost of sorting.</p> <p><strong>Hybrid merge-join</strong> - If one relation is sorted, an the other has a secondary \(B^+\)-tree on the join attribute, then we can merge the sorted relation with the leaf entries of the \(B^+\)-tree. Then we sort the result on the addresses of the unsorted relation’s tuples. Finally, we scan the unsorted relation in physical address order and merge with the previous result, to replace addresses by actual tuples.</p> <h3 id="hash-join">Hash Join</h3> <p>The goal in the previous methods was to simplify the relations so that they fit in the memory. Along with this, we can also parallelise our tasks.</p> <p>In this method, we hash on the join attributes and then merge each of the partitions. It is applicable for equi-joins and natural joins.</p> <p><img src="/assets/img/Databases/image-20220408003107650.png" alt="image-20220408003107650"/></p> <p>The value \(n\) and the hash function \(h\) are chosen such that each \(s_i\) fits in the memory. Typically, \(n\) is chosen as \(\lceil b_s/M \rceil *f\) where \(f\) is a <strong>fudge factor</strong>. The probe relation need not fit in memory. We use <strong>recursive partitioning</strong> if number of partitions is greater than number of pages in the memory.</p> <p><strong>Overflow resolution</strong> can be done in the build phase. Partition \(s_i\) is further partitioned using a different hash function. <strong>Overflow avoidance</strong> performs partitioning carefully to avoid overflows during the build phase. Both methods fail with a high number of duplicates.</p> <p>Cost of hash join is <code class="language-plaintext highlighter-rouge">(3(b_r + b_s) + 4n_h)*t_T + 2t_T(ceil(b_r/b_b) + ceil(b_s/b_b))</code>. Recursive partitioning adds a factor of \(\log_{\lfloor M/bb\rfloor - 1}(b_s/M)\). If the entire build can be kept in the memory, then no partitioning is required and cost estimate goes down to \(b_r + b_s\).</p> <p>Can we not build an entire index on \(s\) instead of hash join? Building an on-disk index is very expensive on disk. Indices have to be maintained which is an overhead.</p> <p><strong>Hybrid Hash-join</strong> keeps the first partition of the build relation in memory. This method is most useful when \(M \gg \sqrt b_s\).</p> <h3 id="complex-joins">Complex Joins</h3> <p>Similar methods to that of selection can be used here. That is conjunction of \(n\) conditions requires intersections of the result of \(n\) joins. In disjunction, we take the union of the join results. This method works for sets but not for multi-sets! For multi-sets, we can make sets out of the records.</p> <h3 id="joins-on-spatial-data">Joins on Spatial Data</h3> <p>There is no simple sort order for spatial joins. Indexed nested loops join with spatial indices such as R-trees, quad-trees and k-d-B-trees. Nearest neighbour joins can be done with tiling.</p> <h2 id="other-operations">Other operations</h2> <p><strong>Duplicate elimination</strong> can be implemented via hashing or sorting. An optimisation is to delete duplicates during run generation as well as at intermediate merge steps. <strong>Projection</strong> can be done by performing projection on each tuple. <strong>Aggregation</strong> can implemented similar to duplicate elimination. Sorting and hashing can be used to bring tuples in the same group together, and then the aggregate functions can be applied on each group.</p> <h1 id="lecture-30">Lecture 30</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">22-03-22</code></p> </blockquote> <h2 id="set-operations-1">Set Operations</h2> <p>These are fairly straightforward using merge-join after sorting or hash-join.</p> <h2 id="outer-join-1">Outer Join</h2> <p>During merging, for every tuple \(t_r\) from \(r\) that do not match any tuple in \(s\), output \(t_r\) padded with nulls.</p> <h2 id="evaluation-of-expressions">Evaluation of Expressions</h2> <p>We have two method to evaluate an entire expression tree</p> <ul> <li><strong>Materialisation</strong> - Generate results of an expression whose inputs are relations or are already computed, materialize (store) it on disk</li> <li><strong>Pipelining</strong> - Pass on tuples to parent operations even as an operation is being executed</li> </ul> <h3 id="materialisation">Materialisation</h3> <p>We evaluate one operation at a time, and store each temporary result on the disk. this method is always applicable, but the cost is high. The overall cost is the sum of costs of individual operations and the cost of writing intermediate results to the disk.</p> <p><strong>Double buffering</strong> - Use two output buffers for each operation, when one is full write it to disk while the other is getting filled.</p> <h3 id="pipelining">Pipelining</h3> <p>We evaluate several operations simultaneously, passing the results of one operation on to the next. However, this is not always possible in case of aggregation, sorts and hash-joins. It is executed in two ways -</p> <ul> <li><strong>Demand driven</strong> - In lazy evaluation, the system repeatedly requests next tuple from the top level operation. The operation has to maintain states. Pull model.</li> <li><strong>Producer driven</strong> - In eager pipelining the operators produce tuples eagerly and pass them up to their parents. Push model.</li> </ul> <h2 id="blocking-operations">Blocking Operations</h2> <p>They cannot generate any output until all the input is consumed. For example, sorting, aggregation, etc. They often have two sub-operations, and we can treat them as separate operations. All operations in a <strong>pipeline</strong> stage run concurrently.</p> <h2 id="query-processing-in-memory">Query Processing in Memory</h2> <p>In early days, memory was the bottleneck. So, engineers had to reduce the I/O. Query was compiled to machine code, and compilation usually avoids many overheads of interpretations to speed up query processing. This was often done via generation of Java byte code with JIT compilation. Column oriented storage was preferred as it allowed vector operations, and cache conscious algorithms were used.</p> <h1 id="lecture-31">Lecture 31</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">24-03-22</code></p> </blockquote> <h3 id="cache-conscious-algorithms">Cache conscious algorithms</h3> <p>The goal is to minimise the cache misses.</p> <ul> <li><strong>Sorting</strong> - We can use runs that are as large as L3 cache to avoid cache misses during sorting of a run. Then merge runs as usual in merge sort.</li> <li><strong>Hash-join</strong> - We first create partitions such that build + probe partitions fit in memory. Then, we sub partition further such that sub partition and index fit in L3 cache. This speeds up probe phase.</li> <li>Lay out attributes of tuples to maximise cache usage. Store often accessed attributes adjacent to each other.</li> <li>Use multiple threads for parallel query processing. Cache miss leads to stall of one thread, but others can proceed.</li> </ul> <h1 id="chapter-16-query-optimisation">~Chapter 16: Query Optimisation</h1> <p>As we have seen before, there are multiple ways to evaluate a given query. The cost difference can be magnanimous in some cases. A plan is evaluated on cost formulae, statistical information and statistical estimation of intermediate results. Most databases support <code class="language-plaintext highlighter-rouge">explain &lt;query&gt;</code> that gives the details of the evaluation plan.</p> <h2 id="generating-equivalent-expressions">Generating Equivalent Expressions</h2> <p>Two queries are equivalent in the (multi)set version if both of them generate the same (multi)set of tuples on <strong>every legal database instance</strong>. Note that we ignore the order of tuples in relational algebra.</p> <ul> <li> <p>Conjunctive selection operations can be deconstructed into a sequence of individual selections</p> \[\sigma_{\theta_1 \land \theta_2}(E) \equiv \sigma_{\theta_1} (\sigma_{\theta_2}(E))\] </li> <li> <p>Selection operations are commutative</p> \[\sigma_{\theta_1} (\sigma_{\theta_2}(E)) \equiv \sigma_{\theta_2} (\sigma_{\theta_1}(E))\] </li> <li> <p>Only the last in a sequence of project operations is needed, the others can be omitted.</p> \[\Pi_{L_1}(\dots(\Pi_{L_n}(E))\dots) \equiv \Pi_{L_1}(E)\] </li> <li> <p>Selections can be combined with Cartesian products and theta joins.</p> \[\begin{align} \sigma_{\theta}(E_1 \times E_2) &amp;\equiv E_1 \bowtie_\theta E_2 \\ \sigma_{\theta_1}(E_1 \bowtie_{\theta_2} E_2) &amp; \equiv E_1 \bowtie_{\theta_1 \land \theta_2} E_2 \end{align}\] </li> <li> <p><strong>Theta-join and natural joins</strong> operations are <u>commutative</u> as well as <u>associative</u>. However, order will not be the same in SQL.</p> <p>sc \((E_1 \bowtie_{\theta_1} E_2)\bowtie_{\theta_2 \land \theta_3} E_3 \equiv(E_1 \bowtie_{\theta_1 \land \theta_3} E_2)\bowtie_{\theta_2 } E_3\)</p> <p>where \(\theta_2\) contains attributes only from \(E_2\) and \(E_3\).</p> </li> <li> \[\sigma_{\theta_1 \land \theta_2} (E_1 \bowtie_\theta E_2) \equiv (\sigma_{\theta_1}(E_1)) \bowtie_\theta (\sigma_{\theta_2}(E_2))\] </li> <li> <p>Projection distributes over join. Throw out useless attributes before joining.</p> <p><img src="/assets/img/Databases/image-20220416151913983.png" alt="image-20220416151913983"/></p> </li> <li> <p>We also have the usual set operations equivalences. Selection operation distributes over \(\cup, \cap, -\).</p> </li> <li> <p>We can also come up with rules involving left outer join (⟖), aggregations and group by’s.</p> \[\sigma_\theta(E_1 ⟕ E_2) \equiv (\sigma_\theta(E_1) ⟕ E_2)\] <p>where \(\theta\) does not involve attributes from \(E_2\) that are not in \(E_1\). If it involves only the attributes from \(E_2\) and is <u>null rejecting</u>, we can convert the left outer join to inner join.</p> </li> <li></li> <li> \[_A\gamma_{count(A)}(s_1 \bowtie_{s_1.A = s_2.A}s_2) \equiv \Pi_{A, c_1 \times c_2}(_A\gamma_{count(A)}(s_1) \bowtie_{s_1.A = s_2.A} {_A}\gamma_{count(A)}(s_2))\] </li> <li> \[\sigma_\theta({_A}\gamma_{agg(B)}(E)) \equiv {_A}\gamma_{agg(B)}(\sigma_\theta(E))\] <p>where \(\theta\) uses only attributes from the grouping attributes.</p> </li> </ul> <p>There were 300 rules in SQL server in 2008!</p> <h1 id="lecture-32">Lecture 32</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">28-03-22</code></p> </blockquote> <p>Note that left/right outer join is not commutative! An optimiser has to consider the cost not just the size. Sometimes, more tuples might be faster due to indices. Associativity is some times helpful in join when the join result of, say, \(r2, r3\) is much larger than that of \(r1, r2\). In that case, we compute the smaller join first. One must also beware about the overhead of applying all these transformations.</p> <p>There are other optimisations such as detecting duplicate sub-expressions and replacing them by one copy. Dynamic Programming is also put to use. The algorithms for transformation of evaluation plans must also be taken into account. Practical query optimisers either enumerate all plans and choose the best plan using cost, or they use heuristics to choose a plan.</p> <h3 id="cost-based-optimisation">Cost based optimisation</h3> <p>If we have \(r_1 \bowtie \dots \bowtie r_n\), we have \((2(n - 1))!/(n - 1)!\). We use dynamic programming to store the least-cost join order. Using dynamic programming, we are bringing down factorial order to an exponential order \(3^n\). The cost of each join is evaluated by interchanging selection and join operations based on indices. Further optimisation is done by only considering <strong>left-deep join trees</strong> where the rhs of a join is a relation and not an intermediate join. After this, the time complexity is \(\mathcal O(n2^n)\) and space complexity is \(\mathcal O(2^n)\).</p> <p>How about sort orders? Certain sort orders can make subsequent operations cheaper. However, we don’t consider this much. The Volcano project also considers physical equivalence rules.</p> <h3 id="heuristic-optimisation">Heuristic Optimisation</h3> <p>Heuristic optimisation transforms the query-tree by using a set of rules that typically improve execution performance. Nested subqueries hinder optimisation techniques.</p> <p>System-R used heuristics for aggregates. We also need to check <u>optimisation cost budget</u> and <u>plan caching</u>. As some applications use the same query repeatedly, we can try and use the same evaluation plan based on a heuristic on statistics.</p> <h2 id="statistics-for-cost-estimation">Statistics for Cost Estimation</h2> <p>We consider \(n_r\) (no. of tuples), \(b_r\) (no. of blocks), \(I_r\) (size of a tuple), \(f_r\) (blocking factor \(b_r = \lceil n_r/f_r\rceil\)) and \(V(A, r)\) (no. of distinct values). Histograms are used to compute statistics.</p> <h1 id="lecture-33">Lecture 33</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">29-03-22</code></p> </blockquote> <h3 id="selection-size-estimation">Selection size estimation</h3> <ul> <li> \[\sigma_{A = v}(r) \approx n_r/V(A, r)\] </li> <li>Assuming, \(\min\) and \(\max\) are available - \(\sigma_{A \leq v}(r) = \begin{cases} 0 &amp;&amp; v &lt; \min(A, r) \\ n_r \cdot \frac{v - \min(A, r)}{\max(A, r) - \min{A, r}} \end{cases}\)</li> </ul> <p>These estimates are refined using updates in the histograms. Similarly, we can derive size estimates for complex selections.</p> <h3 id="join-size-estimation">Join size estimation</h3> <ul> <li>If \(R \cap S = \phi\), then \(r \bowtie s = r \times s\).</li> <li>If \(R \cap S\) is a key in \(R\), then a tuple of \(s\) will join with at most one tuples from \(r\) -&gt; \(r \bowtie s \leq s\).</li> <li>If \(R \cap S\) in \(S\) is a foreign key in \(S\) referencing \(R\), then \(r \bowtie s = s\).</li> <li>If the common attribute is not a key, then the size is \((n_r*n_s)/V(A, s)\) if every tuple in \(R\) produces a tuple in the join.</li> </ul> <p>Similarly, we have other size estimations.</p> <p>For projection, we have \(\Pi_A(r) = V(A, r)\), and for aggregation we have \({_G}\gamma_A(r) = V(G, r)\). There are estimates for set operations too!</p> <p>In summary, these estimates work well in practice, but the errors are multiplied across multiple queries. In worst cases, they might hamper the performance.</p> <h2 id="additional-optimisations">Additional Optimisations</h2> <h3 id="optimising-nested-subqueries">Optimising Nested Subqueries</h3> <p>SQL treats the nested subquery as a function with a few parameters - This evaluation is known as <strong>correlated evaluation</strong>. The parameters to the function are known as <strong>correlation variables</strong>. This method is inefficient because a large number of call may be made for the nested query that results in unnecessary random I/O.</p> <p>However, every nested subquery in SQL can be written in terms of joins. SQL optimisers try to do this. One must be beware of duplicates during this conversion. The (left)<strong>semijoin</strong> operator ⋉ is defined as - A tuple \(r_i\) appears \(n\) times in \(r ⋉_\theta s\) if it appears \(n\) times in \(r\), and there is atleast on matching tuple \(s_i\) in \(s\). This operator is often used by optimisers to maintain the duplicate count. Similarly, for <code class="language-plaintext highlighter-rouge">not exists</code>, we have <strong>anti semijoin</strong> \(\bar ⋉\).</p> <p><strong>Decorrelation</strong> is the process of replacing a nested query by a query with a join/semi-join. This process is a bit non-trivial in case of scalar subqueries. Note that relational algebra can’t deal with exceptions.</p> <h3 id="materialised-views">Materialised views</h3> <p>The values of the view are computed and stored. The re-computation during updates is expensive. Therefore, we adopt <u>incremental view maintenance</u>. The changes to a relation or expressions are referred to as its <strong>differential</strong>.</p> <p>To explain the above, consider a materialised view of a join. For a new insert, we find the corresponding matching tuples for join and add them. Similarly for deletes. We can do this due to distributivity of \(\bowtie\) and \(\cup\).</p> <p>Project is a more difficult operation due to duplicates. Therefore, we maintain a count for how many times the set of attributes occur. Aggregates can also be done in a similar way.</p> <p>To handle expressions, the optimiser might have to change the evaluation plan. For example, the tree structure in join order may not be efficient if indices are present during insertions.</p> <h1 id="lecture-34">Lecture 34</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">28-03-22</code></p> </blockquote> <p>We had discussed about view maintenance, but how do we use materialised views? A query optimiser can replace sub-expressions with appropriate views if the user writes the queries only in terms of relations. Sometimes, the opposite can also be useful.</p> <p>Materialised view selection and index selection are done based on typical system <strong>workload</strong>. Commercial database systems provide <u>tuning</u> tools that automate this process.</p> <h3 id="top-k-queries">Top-K queries</h3> <p>The query optimiser should consider that only the top-k results are required from a huge relation. This can be done via indexed nested loops join with the relation that is being used for sorting as the outer relation. There are other alternatives too.</p> <h3 id="multi-query-optimisation">Multi-query Optimisation</h3> <p>Multiple queries with common sub-routines can be done in parallel.</p> <h3 id="parametric-query-optimisation">Parametric Query Optimisation</h3> <p>The evaluation plan can change based on the input parameters to the queries. To optimise this, we divide the range of the parameter into different partitions and choose a good evaluation plan for each partition.</p> <h1 id="chapter-17-transactions">~Chapter 17: Transactions</h1> <p>A <strong>transaction</strong> is a <em>unit</em> of program execution. It access and possible updates various data items. It also guarantees some well-defined robustness properties. To discuss transactions, we move to a level below queries where each <em>atomic</em> instruction is performed. We need to ensure correctness during failures and concurrency.</p> <p>In OS, we have seen that mutexes are used for concurrency. However, we need a higher level of concurrency in databases.</p> <p><strong>ACID guarantees</strong> refer to Atomicity (Failures), Consistency (Correctness), Isolation (Concurrency) and Durability (Failures). There is a notion of <u>consistent state</u> and <u>consistent transaction</u>. Durability refers to persistence in case of failures. Atomicity refers to all-or-nothing for each update. Partial updates are reversed using logs. Two concurrent transactions must execute as if they are unaware of the other in isolation. In conclusion, ACID transactions are a general systems abstraction.</p> <p>Concurrency increases processor and disk utilisation. It also reduces the average response time. Isolated refers to concurrently executing actions but showing as if they were occurring serially/sequentially.</p> <h2 id="serialisability">Serialisability</h2> <p>A schedule is <strong>serialisable</strong> if it is equivalent to a serial schedule. We are assuming that transactions that are run in isolation are atomic, durable and preserve consistency.</p> <p><strong>Conflict serialisable</strong> schedules are a subset of serialisable schedules that detect or prevent conflict and avoid any ill effects. To understand these, we will consider <code class="language-plaintext highlighter-rouge">read</code>s and <code class="language-plaintext highlighter-rouge">write</code>s.</p> <p><strong>Conflicting instructions</strong> - Instructions form two transactions <strong>conflict</strong> only if one or both <em>update</em> the <em>same shared</em> item. For example, <code class="language-plaintext highlighter-rouge">write A</code> in T1 conflicts with <code class="language-plaintext highlighter-rouge">read A</code> in T2. These are similar to RAW, WAW, etc conflicts seen in Architecture course.</p> <p>We can swap the schedules of non-conflicting schedules and obtain a serial schedule. Such schedules are conflict serialisable. Conflict equivalence refers to the equivalence between the intermediate schedules obtained while swapping. More formally,</p> <p><img src="/assets/img/Databases/image-20220416193446900.png" alt="image-20220416193446900"/></p> <p>We are skipping <strong>view equivalence</strong>. There are other notions of serialisability (considering a group of operations).</p> <h3 id="testing-for-conflict-serialisability">Testing for Conflict Serialisability</h3> <p>A <strong>precedence graph</strong> is a directed graph where vertices are transaction IDs and edges represent conflicting instructions with arrows showing the temporal order. Then, we can perform topological sorting to check for serialisability. A schedule is serialisability iff its precedence graph is acyclic.</p> <h1 id="lecture-35">Lecture 35</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">04-04-22</code></p> </blockquote> <p>We shall discuss a series of concepts in isolation now.</p> <h3 id="recoverable-schedules">Recoverable Schedules</h3> <p>If a transaction \(T_i\) reads a data item previously written by a transaction \(T_j\), then the commit operation of \(T_j\) appears before the commit operation of \(T_i\). These schedules are recoverable.</p> <h3 id="cascading-rollbacks">Cascading rollbacks</h3> <p>A single transaction failure leads to a series of transaction rollbacks. That is, uncommitted transactions must be rolled back.</p> <h3 id="cascadeless-schedules">Cascadeless Schedules</h3> <p>Cascading rollbacks cannot occur in these schedules. For each pair of transactions \(T_i\) and \(T_j\) such that \(T_j\) reads a data item previously written by \(T_i\), the commit operation \(T_i\) appears before the read operation of \(T_j\). That is, they allow reading of committed values only and disallow <em>dirty reads</em>. Every cascadeless schedule is recoverable.</p> <h3 id="weak-levels-of-isolation">Weak levels of Isolation</h3> <p>Some applications can tolerate schedules that are not serialisable. For example, in statistics we only want approximations. Ideally, we’d like serialisable, recoverable and preferably cascadable.</p> <p>There are levels of isolation defined in SQL-92 like - read committed, read uncommitted, repeatable read, and serialisable. Most often, databases run in read committed mode.</p> <h3 id="concurrency-control-mechanisms">Concurrency Control Mechanisms</h3> <p>We have a ton of theory for implementing whatever we have discussed. There are locking schemes, timestamp schemes, optimistic/lazy schemes and multi-versions (similar to master-slave mechanism in parallel cores of architecture).</p> <h2 id="transactions-in-sql">Transactions in SQL</h2> <p>We have predicate operations in SQL. A tuple might fail a predicate before a transaction, but passes it after the transaction has completed. Some databases lock the matched tuples for consistency, but that does not always work. That is, we need “predicate locking”, not just key-based locks that locks all <em>possible</em> matching tuples. Phantom reads refer to not matching tuples that are just added.</p> <p>In SQL, a transaction begins implicitly. <strong>commit work</strong> commits current transaction and begins a new one. <strong>rollback work</strong> causes current transaction to abort. Isolation levels can be set at database level or at the start of a transaction.</p> <h1 id="chapter-18-concurrency-control">~Chapter 18: Concurrency Control</h1> <h2 id="lock-based-protocols">Lock based protocols</h2> <p>A lock is a mechanism to control concurrent access to a data item. Items can be locked in two modes</p> <ul> <li><strong>exclusive</strong> (X) mode - Data item can be both read as well as written. <code class="language-plaintext highlighter-rouge">lock-X</code></li> <li><strong>shared</strong> (S) mode - Data item can only be read. <code class="language-plaintext highlighter-rouge">lock-S</code></li> </ul> <p>These requests are made implicitly using a concurrency control manager. Not that X lock can’t be obtained when a S lock is held on an item.</p> <p>A <strong>locking protocol</strong> is a set of rules followed by all transactions while requesting and releasing locks. They enforce serialisability by restricting the set of possible schedules. To handle <strong>deadlocks</strong>, we need to roll back some transactions. <strong>Starvation</strong> is also possible if concurrency control manager is badly designed. This occurs because X-locks can’t be granted when S-lock is being used.</p> <h3 id="two-phase-locking-protocol">Two-Phase Locking Protocol</h3> <p>This protocol ensures conflict-serialisable schedules. We have two phases</p> <ul> <li>Phase 1: Growing phase. A transaction can obtain locks but not release them</li> <li>Phase 2: Shrinking phase. It’s the opposite of the above.</li> </ul> <p>The transactions can be serialised in the order of their lock points (the point where a transaction has acquired its final lock). However, this protocol does not ensure protection from deadlocks. It also does not ensure recoverability. There are extensions that ensure recoverability from cascading roll-back.</p> <ul> <li><strong>Strict two-phase locking</strong> - A transaction must hold all its exclusive locks till it commits/aborts.</li> <li><strong>Rigorous two-phase locking</strong> - A transaction must hold all locks till commit/abort.</li> </ul> <p>We mostly use the second protocol.</p> <p>Two-phase locking is not necessary condition for serialisability. That is, there are conflict serialisable schedules that cannot be obtained if the two-phase locking protocol is used.</p> <p>Given a locking protocol, a schedule \(S\) is <strong>legal</strong> under a locking protocol if it can be generated by a set of transactions that follow the protocol. A protocol <strong>ensures</strong> serialisability if all legal schedules under that protocol are serialisable.</p> <p>In the two-phase locking protocol. we can upgrade (S to X) in the growing phase and downgrade (X to S) in the shrinking phase.</p> <h1 id="lecture-36">Lecture 36</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">05-04-22</code></p> </blockquote> <h2 id="implementation-of-locking">Implementation of Locking</h2> <p>A <strong>lock manager</strong> can be implemented as a separate process. The lock manager maintains an in-memory data-structure called a <strong>lock table</strong> to record granted locks and pending requests. A lock table is implemented as a hash table with queues.</p> <h2 id="graph-based-protocols">Graph-Based Protocols</h2> <p>They impose a partial ordering \(\to\) on the set \(D = \{d_1, \dots, d_n\}\) of all the data items. If \(d_i \to d_j\), then any transaction accessing both \(f_i\) and \(f_j\) must access \(d_i\) before accessing \(d_j\).</p> <p>A tree protocol ensures conflict serialisability as well as freedom from deadlock. However, it does not guarantee recoverability and it has other issues.</p> <h2 id="deadlocks">Deadlocks</h2> <p>There are other deadlock prevention strategies like</p> <ul> <li><strong>wait-die</strong> - non-preemptive. Older transaction may wait for younger one to release the data item, and the younger transaction never wait for older ones. They are rolled back instead.</li> <li><strong>wound-wait</strong> - preemptive. Older transaction <em>wounds</em> (forces rollback) of a younger transaction instead of waiting for it.</li> </ul> <p>In both the schemes, a rolled back transaction restarts with its original timestamp.</p> <ul> <li>Time-out based schemes</li> </ul> <h3 id="deadlock-recovery">Deadlock Recovery</h3> <p>There are two ways for rollback</p> <ul> <li>Total rollback</li> <li>Partial rollback - Roll back victim transaction only as far as necessary to release locks that another transaction in cycle is waiting for.</li> </ul> <p>A solution for starvation is that the oldest transaction in the deadlock set is never chosen as victim of rollback.</p> <h2 id="multiple-granularity">Multiple Granularity</h2> <p>A lock table might be flooded with entries when a transaction requires coarser granularity. The levels of granularity we use are database, area, file and record. What if we only use coarse granularities? The problem is that it’s not effective in terms of concurrency.</p> <p>We use <strong>intention locks</strong> to take care of hierarchy of granularities.</p> <h3 id="intention-lock-modes">Intention Lock Modes</h3> <p>In addition to S and X, we have</p> <ul> <li><strong>Intention-shared</strong> IS - Indicates explicit locking at a lower level of the tree but only with shared locks.</li> <li><strong>Intention-exclusive</strong> IX - Indicates explicit locking at a lower level with exclusive or shared locks.</li> <li><strong>Shared and intention exclusive</strong> SIX - The subtree rooted by that node is locked explicitly in shared mode and explicit locking is being done at a lower level with exclusive-mode locks.</li> </ul> <p>Intention locks allow a higher level node to be locked in S or X mode without having to check all descendent nodes. That is, to get a lock at the bottom level, we need to start taking intention locks from the root. Also, we have the following <strong>compatibility matrix</strong>.</p> <p><img src="/assets/img/Databases/image-20220417153625206.png" alt="image-20220417153625206"/></p> <p>The query engine decides all the locks based on the input queries. It follows adaptive lock granularity when it can’t decide. There is also a notion of <strong>lock granularity escalation</strong> in adaptive locking where the query engine shifts to a coarser granularity in case there are too many locks at a particular level.</p> <p>Now, we discuss locking in the case of predicate reads. Consider an insert. You can lock the entire relation to ensure consistency, but when someone inserts a new record there won’t be any lock on it. So, we use the following rules.</p> <p><img src="/assets/img/Databases/image-20220417155825429.png" alt="image-20220417155825429"/></p> <p>Note that one also locks the metadata in two-phase locking scheme that helps it ensure serialisability.</p> <h2 id="phantom-phenomenon">Phantom Phenomenon</h2> <p>A transaction \(T_1\) performs a predicate read, and a transaction \(T_2\) inserts a matching tuple while \(T_1\) is active but after predicate read. As a result, some of these schedules are not serialisable.</p> <h3 id="handling-phantoms">Handling Phantoms</h3> <p>If the conflict is at the data level, locking the metadata will prevent insertion and deletion in the relation. However, this provides very low concurrency for insertions and deletions.</p> <h3 id="index-locking-to-prevent-phantoms">Index Locking to prevent Phantoms</h3> <p>Every relation must have at least one index. A transaction \(T_i\) that performs a lookup must lock all the index leaf nodes that it accesses in \(S\)-mode. That is, it locks a range. A transaction \(T_j\) that inserts, updates or deletes a tuple \(t_i\) in a relation \(r\) must update all indices to \(r\). It can’t acquire X-lock in the locked range of \(T_i\). So phantom reads won’t occur when the rules of two-phase locking protocol must be observed. The key idea here is that, tuples in the matching range will be sequential do to the index.</p> <h3 id="next-key-locking-to-prevent-phantoms">Next-Key Locking to prevent Phantoms</h3> <p>This method provides higher concurrency. It locks all values that satisfy index lookup, and also lock the next key value in the index.</p> <p>Note that the above locks are done in the \(B^+\) trees.</p> <h1 id="lecture-37">Lecture 37</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">07-04-22</code></p> </blockquote> <h2 id="timestamp-ordering-protocol">Timestamp Ordering Protocol</h2> <p>The timestamp-ordering protocol guarantees serialisability since all the arcs in the precedence graph are of the form having nodes with edges from smaller timestamp to larger timestamp. This protocol ensures freedom from deadlock as no transaction ever waits. However, the schedule may not be cascade free and may not even be recoverable.</p> <p>To make it recoverable, we have the following solutions</p> <ul> <li>All the writes are done at the end of the timestamp. A transaction that aborts is restarted with a new timestamp.</li> <li>Limited form of locking - wait for data to be committed before reading it</li> <li>Use commit dependencies to ensure responsibility.</li> </ul> <h2 id="thomas-write-rule">Thomas’ Write Rule</h2> <p>Modified version of the TSO in which obsolete <strong>write</strong> operations may be ignored under certain circumstances. When a transaction items to write to a data item Q which will be rewritten by another transaction, then the original write is considered as obsolete. \(TS(T) &lt; W_\text{timestamp}(Q)\). So, rather than rolling back the original transaction as the TSO does, we ignore the write operation in the original transaction. In this way, Thomas’ Write Rules allows greater potential concurrency. It allows some view serialisable schedules that are not conflict-serialisable.</p> <p>The rule comes into the picture in case of <strong>blind writes</strong> - A write is done without a preceding read.</p> <h2 id="validation-based-protocol">Validation Based Protocol</h2> <p>It uses timestamps, but they are not pre-decided. The validation is performed at commit time to detect any out-of-serialisation order reads/writes. It is also known as <strong>optimistic concurrency control</strong> since transaction executes fully in the hope that all will go well during validation. This is done in three phases</p> <ul> <li>Read and execution phase. Writes are done to temporary variables.</li> <li>Validation phase</li> <li>Write phase</li> </ul> <p>Each transaction has 3 timestamps corresponding to start of execution, validation phase, and write phase. The validation time stamps are used in the protocol. In validation, we check that for all \(T_i, T_j\) such that \(TS(T_i) &lt; TS(T_j)\), one of the following must hold</p> <ul> <li> \[finishTS(T_i) &lt; startTS(T_j)\] </li> <li>\(startTS(T_j) &lt; finishTS(T_i) &lt; validationTS(T_j)\) and the set of data items written by \(T_i\) does not intersect with the set of data items read by \(T_j\).</li> </ul> <p>This is when the validation succeeds.</p> <h2 id="multiversion-concurrency-control">Multiversion Concurrency Control</h2> <p>Multiversion schemes keep old versions of data item to increase concurrency. There are variants such as</p> <ul> <li>Multiversion Timestamp Ordering</li> <li>Multiversion Two-Phase Locking</li> <li>Snapshot isolation</li> </ul> <p>The key ideas are that</p> <ul> <li>Each successful write results in the creation of a new version that labeled using timestamps.</li> <li>When a read operation is issued, we select the appropriate timestamp based on the timestamp of transaction issuing read and return the value of the selected version.</li> </ul> <h3 id="multiversion-timestamp-ordering">Multiversion Timestamp ordering</h3> <p>Each data item \(Q\) has a sequence of versions \(&lt; Q_1, \dots, Q_n&gt;\) each of which have</p> <ul> <li>Content</li> <li>Write timestamp</li> <li>Read timestamp</li> </ul> <p>If \(T\) issues a read or write, let \(Q_k\) be the version with the highest <u>write</u> timestamp that has a value less than the timestamp of \(T\). Then for a read, we return the value from \(Q_k\), and for a write we overwrite if both the timestamps are equal. We roll back \(T\) if \(TS(T) &lt; R\_timestamp\). Otherwise, we simply create a new entry.</p> <p>Like the basic TSP, recoverability is not ensured.</p> <h3 id="multiversion-two-phase-locking">Multiversion Two-Phase Locking</h3> <p>Differentiates between read-only transactions and update transactions. Update transactions follow rigorous two-phase locking. Read of a data item returns the latest version of the item. The first write of \(Q\) by \(T\) results in creation of a new version \(Q_i\) and the timestamp is updated after the completion of the transaction. After the transaction \(T\) completes, \(TS(T_i) = \texttt{ts-counter} + 1\) and \(W\_timestamp(Q) = TS(T_i)\) for all versions of \(Q\) that it creates. Then, the <code class="language-plaintext highlighter-rouge">ts-counter</code> is incremented. All of this must be done atomically.</p> <p>In read only transactions, <code class="language-plaintext highlighter-rouge">ts-counter</code> is assigned to the timestamp. As a result, only serialisable schedules are produced.</p> <p>The issues with multiversion schemes are they increase storage overhead and there are issues with keys constraint checking and indexing with multiple versions.</p> <h2 id="snapshot-isolation">Snapshot Isolation</h2> <p>A transaction \(T\) executing with snapshot isolation</p> <ul> <li>Takes snapshot of the committed data at start</li> <li>Always reads/modified data in its own snapshot</li> <li>Updates of concurrent transactions are not visible to \(T\)</li> <li>Writes of \(T\) are complete when it commits</li> </ul> <p>So, <strong>first committer wins</strong> rule is being used. <strong>Serialisable snapshot isolation (SSI)</strong> is an extension that ensures serialisability. However, there are some anomalies in this.</p> <h1 id="lecture-38">Lecture 38</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">11-04-22</code></p> </blockquote> <h1 id="chapter-19">~Chapter 19:</h1> <p>A transaction failure can occur as a result of</p> <ul> <li>logical errors - internal error condition in the transaction. These can be somewhat prevented by <code class="language-plaintext highlighter-rouge">assert</code> statements.</li> <li>system error - database must terminate the transaction due to errors like deadlock.</li> </ul> <p><strong>System crash</strong> - Occurs due to a power failure, hardware or software failure. A <strong>failstop assumption</strong> refers to the assumptions that non-volatile storage contents are not corrupted by a system crash. A <strong>disk failure</strong> destroys disk storage.</p> <p>Logging helps in recovery. To recover from failure, we first find inconsistent blocks by</p> <ul> <li>We compare two copies of every disk block which is expensive, or</li> <li>Use logs sort of mechanism</li> </ul> <p>and then overwrite the inconsistent blocks. We also need to ensure atomicity despite failures.</p> <h2 id="log-based-recovery">Log-Based Recovery</h2> <p>A <strong>log</strong> is a sequence of log records that keep information of update activities on the database. When transaction \(T\) starts, it registers itself by writing \((T start)\) log record. Before \(T\) executes \(write(X)\), it writes a log record \((T, X, V_1, V_2)\). Upon finishing the last statement, the log record \((T commit)\) is written. There are two approaches using log</p> <ul> <li>Immediate database modification - write to buffer which will write to disk when before the transaction commits. Log record is written before database item is written.</li> <li>Deferred database modification - writes are done only after commit. Not used frequently.</li> </ul> <p><u>A transaction is said to have committed when its commit log record is output to stable storage.</u> Also, the log records are interleaved for concurrent transactions.</p> <h2 id="concurrency-control-and-recovery">Concurrency Control and Recovery</h2> <p>We assume that if a transaction \(T\) has modified an item, no other transaction can modify the same item until \(T\) has committed or aborted. This is equivalent to <em>strict two phase locking</em>.</p> <ul> <li>\(undo(T_i)\) restores the value of all data items updated by \(T_i\) to their old values, going backwards from the last log record for \(T_i\). For each restoration, we add a log record of \((T_i, X, V)\). When undo of a transaction is complete, a log record \((T_i abort)\) is written out.</li> <li>\(redo(T_i)\) sets the value of all data items updated by \(T_i\) to the new values, going forward from the first log record of \(T_i\). The log is unchanged here.</li> </ul> <h2 id="recovering-from-failure">Recovering from Failure</h2> <p>When recovering from failure,</p> <ul> <li>Transaction \(T_i\) needs to be undone if the log contains the record \((T_i start)\) but does not contain \((T_i commit)\) or \((T_i abort)\)</li> <li>It needs to be redone if it contains \((T_i start)\) and \((T_i commit)\) or \((T_i abort)\).</li> </ul> <p>Note that the second step is wasteful in some cases. We recovered before, and we are doing it again. This is known as <strong>repeating history</strong>.</p> <h3 id="checkpoints">Checkpoints</h3> <p>Processing the entire log can be slow. We streamline recovery procedures by periodically performing checkpointing.</p> <ul> <li>Output all log records currently residing in the main memory onto stable storage</li> <li>Output all modified buffer blocks to the disk</li> <li>Write a log record \((checkpoint L)\) onto stable storage where \(L\) is a list of all transactions active at the time of checkpoint</li> <li>All updates are stopped while doing checkpointing.</li> </ul> <p>The log records before a checkpoint are not needed!</p> <h1 id="lecture-39">Lecture 39</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">12-04-22</code></p> </blockquote> <p>## Recovery Algorithm</p> <p><img src="/assets/img/Databases/image-20220417183018051.png" alt="image-20220417183018051"/></p> <p><img src="/assets/img/Databases/image-20220417183027590.png" alt="image-20220417183027590"/></p> <p><img src="/assets/img/Databases/image-20220417183036363.png" alt="image-20220417183036363"/></p> <h2 id="log-record-buffering">Log Record Buffering</h2> <p>Log records are buffered in main memory, instead of being output directly to stable storage. When the buffer is full, a <strong>log force</strong> operation is performed.</p> <p>As we had said, before a block of data in the main memory is output to the database, all log records pertaining to data in that block must have been output to stable storage. This rule is called as <strong>write-ahead logging</strong> or WAL rule. Strictly speaking, this is only required for undo transactions.</p> <h2 id="database-buffering">Database Buffering</h2> <p>The recovery algorithm supports the <strong>no-force policy</strong> - updated blocks need not be written to disk when transaction commits. However, <strong>force policy</strong> is a more expensive commit. The recovery algorithm also supports the <strong>steal policy</strong> - blocks containing updates of uncommitted transactions can be written to disk, even before the transaction commits.</p> <p>No updates should be in progress on a block when it is output to disk. It can be ensured by acquiring exclusive locks on the block before writing a data item. Locks can be released once the write is completed. Such locks held for short durations are called <strong>latches</strong>.</p> <p>In summary, we acquire latch, do log flush, write block to disk and finally release the latch.</p> <p>The <strong>dual paging</strong> problem refers to the extra I/O that is done when fetching a page from swap to buffer and then moving it to the disk. This can be prevented by letting the OS pass the control to the database when it needs to evict a page from the buffer. The database outputs the page to the database instead of the swap space, and release the page from the buffer.</p> <h2 id="fuzzy-checkpointing">Fuzzy Checkpointing</h2> <p>to avoid long interruption of normal processing during checkpointing, we allow updates to happen during checkpointing. We permit transactions to proceed with their actions once we note the list \(M\) of modified buffer blocks. We store a pointer to the <strong>checkpoint</strong> record in a fixed position <strong>last_checkpoint</strong> on disk once the all modified buffer blocks in \(M\) are output to disk. During recovery, we use this last checkpoint.</p> <h2 id="failure-with-loss-of-nonvolatile-storage">Failure with Loss of Nonvolatile Storage</h2> <p>Technique similar to checkpointing used to deal with the loss of non-volatile storage. That is, we periodically <strong>dump</strong> the entire content of the database to stable storage. No transaction may be active during the dump procedure. We perform the following -</p> <ul> <li>output all log records from main memory to storage</li> <li>output all buffer blocks onto the disk</li> <li>copy the contents of the database to stable storage</li> <li>output a record \((dump)\) to lon on stable storage.</li> </ul> <p>There are versions of <strong>fuzzy dump</strong> and <strong>online dump</strong>.</p> <h2 id="remote-backup-systems">Remote Backup Systems</h2> <p>We need to <strong>detect failure</strong> at the backup site using heartbeat messages and perform <strong>transfer of control</strong> to take control at the backup site. The log records are copied at the backup before, and recovery can be initiated. Once the primary site goes back up, we give back the control.</p> <p>The <strong>time to recover</strong> is very important to reduce delay in takeover. Therefore, the backup sire periodically processed the redo log records, performs a checkpoint, and can then delete earlier parts of the log.</p> <p>A <strong>hot-spare</strong> configuration permits very fast takeover - backup continually processes redo logs as they arrive and when the failure is detected, the backup rolls back incomplete transactions and is ready to process new transactions. An alternative to remote system backup is to use distributed systems.</p> <p>We have the following levels of durability</p> <ul> <li><strong>One safe</strong> - Commit as soon as transaction’s commit log record is written at primary.</li> <li><strong>Two-very safe</strong> - Commit when transaction’s commit log record is written at primary and backup.</li> <li><strong>Two-safe</strong> - Proceed as in two-very-safe is both primary and backup are active.</li> </ul> <p>We also need to reduce latency for communication. We add a <strong>near site</strong> backup close to the primary site. Log records are replicated both at near site and remote backup site. If primary fails, remote backup site gets latest log records, which it may have missed, from near site.</p> <hr/> <h4 id="end-of-course">END OF COURSE</h4> <hr/>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[An introductory course for design and programming of database systems. Covers the entity-relationship (ER) approach to data modelling, the relational model of database management systems (DBMSs) and the use of query languages such as SQL. Briefly discusses query processing and the role of transaction management.]]></summary></entry><entry><title type="html">IPL Notes</title><link href="https://sudhansh6.github.io/blog/ipl/" rel="alternate" type="text/html" title="IPL Notes"/><published>2022-01-05T00:00:00+00:00</published><updated>2022-01-05T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/ipl</id><content type="html" xml:base="https://sudhansh6.github.io/blog/ipl/"><![CDATA[<h1 id="lecture-1">Lecture 1</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">05-01-22</code></p> </blockquote> <h2 id="binding">Binding</h2> <p>Binding refers to finding certain attributes of certain objects. For example, when we consider the example of developing a program, we have the following steps.</p> <p><img src="/assets/img/IPL\image-20220105111530405.png" alt="image-20220105111530405"/></p> <p>At each step, the number of unbound objects decrease along with time.</p> <h2 id="implementation-mechanisms">Implementation Mechanisms</h2> <p>The important point to note is that the “execution” is done after “translation” in the case of <strong>compilers</strong> (Analysis + Synthesis). However, <strong>interpreters</strong> (Analysis + Execution) themselves are programs which take the source code as input and translate/execute the user-program.</p> <p>Compilers/Translators lower the level of the program specification, whereas the interpreters do the opposite. That is why interpreters are able to directly execute the source code.</p> <p><img src="/assets/img/IPL\image-20220105112702885.png" alt="image-20220105112702885"/></p> <blockquote> <p>Why is the arrow in the reverse direction? Think of python. The python program itself is a low-level code which is able to execute a high-level user code.</p> </blockquote> <table> <thead> <tr> <th>High Level</th> <th>Low Level</th> </tr> </thead> <tbody> <tr> <td>C statements</td> <td>Assembly statement</td> </tr> </tbody> </table> <p><strong>Note.</strong> A practical interpreter does partial translation/compilation but not completely as it is redundant.</p> <blockquote> <p>Compilers performs bindings <strong>before</strong> runtime, and an interpreter performs bindings <strong>at</strong> runtime.</p> </blockquote> <blockquote> <p>Why interpreters if compilers are fast? Compilers are used for programs which need to be executed multiple times with speed. However, we must also consider the time needed to compiler the codes in case of compilers. Considering this, the turnaround time in both cases is almost the same. The user needs to think whether they want to reduce the execution time or reduce the turnaround time.</p> </blockquote> <table> <thead> <tr> <th>Language</th> <th>Mode of execution</th> </tr> </thead> <tbody> <tr> <td>C++</td> <td>Compiler</td> </tr> <tr> <td>Java/ C#</td> <td>Compiler + Interpreter</td> </tr> <tr> <td>Python</td> <td>Interpreter</td> </tr> </tbody> </table> <p>Java programs run on a <em>virtual machine</em> - JIT (just in time) compilation. the virtual machine invokes a native compiler for instances of code which are repeated often inside the code. This speeds up the execution in case of Java.</p> <p>Compilers like GCC can be used for multiple machines and languages. <img src="/assets/img/IPL/image-20220105120200389.png" alt="image-20220105120200389"/></p> <h2 id="simple-compiler">Simple compiler</h2> <p><img src="/assets/img/IPL\image-20220105120528314.png" alt="image-20220105120528314"/></p> <p>The first step in the translation sequence of the compiler is <strong>scanning and parsing</strong>. A parse-tree is built from the <u>grammar rules, terminals, and non-terminals</u>. Scanning involves finding special characters to create the parse tree, whereas parsing is essentially creating the parse tree itself.</p> <p>The next step involves <strong>semantic analysis</strong>. The semantic analyzer performs <strong>type checking</strong> to check the validity of the expressions. Therefore, we create an <u>abstract syntax tree</u> (<strong>AST</strong>) is created from the parse tree.</p> <blockquote> <p>Only for type-checking? Yes, that is the primary purpose, but it has other uses too. Sometimes, type checking is done on the fly while generating the parse tree itself. The advantage for generating the AST is that we can write a generic code (interpreter) to parse the abstract trees (since it involves only tokens).</p> </blockquote> <p>Then, we generate a <strong>TAC list</strong> (three address code) in the <strong>IR generation</strong> step. This step helps in separating data and control flow, and hence simplifies optimization. The control flow is linearized by flattening nested control constructs.</p> <p>In the <strong>Instruction Selection</strong> step, we generate the <strong>RTL list</strong> which is like pseudo-assembly code with all the assembly instruction names. We try to generate as few instructions as possible using temporaries and local registers.</p> <blockquote> <p>What is the purpose of this step? We like to divide the problem into many steps to leave room for optimization.</p> </blockquote> <p>In the final step of <strong>Emitting Instructions</strong>, we generate the assembly code by converting the names of assembly instructions to the actual code.</p> <p>The whole process is summarized by the following image.</p> <p><img src="/assets/img/IPL\image-20220105123323838.png" alt="image-20220105123323838"/></p> <h2 id="observations">Observations</h2> <p>A compiler bridges the gap between source program and target program. Compilation involves gradual lowering of levels of the IR of an input program. The design of the IRs is the most critical part of a compiler design. Practical compilers are desired to be retargetable.</p> <h1 id="lecture-2">Lecture 2</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">07-01-22</code></p> </blockquote> <p>Write about retargetable compilers. A <strong>retargetable compiler</strong> does not require manual changes, the backend is generated according to the specifications.</p> <h2 id="why-are-compilers-relevant-today">Why are compilers relevant today?</h2> <p>In the present age, very few people write compilers. However, translation and interpretation are fundamental to CS at a conceptual level. It helps us understand stepwise refinement - building layers of abstractions and bridging the gaps between successive layers without performance penalties. Also, knowing compilers internals makes a person a much better programmer.</p> <h2 id="modularity-of-compilation">Modularity of Compilation</h2> <p>A compiler works in phases to map source features to target features. We divide each feature into multiple levels for convenience. This will be clear through the assignments (language increments).</p> <h2 id="course-plan">Course Plan</h2> <p>We shall cover scanning, parsing, static semantics, runtime support, and code generations in the theory course. In the lab course, we shall do incremental construction of SCLP.</p> <p><img src="/assets/img/IPL/image-20220107114750855.png" alt="image-20220107114750855"/></p> <h1 id="lecture-3">Lecture 3</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">12-01-22</code></p> </blockquote> <h2 id="compilation-models">Compilation Models</h2> <p><img src="/assets/img/IPL/image-20220112111235980.png" alt="image-20220112111235980"/></p> <p>Registers are introduced earlier in the Davidson Fraser model. Therefore, it becomes machine dependent. The target independent IR in <code class="language-plaintext highlighter-rouge">gcc</code> is called ‘gimple’.</p> <h3 id="typical-front-ends">Typical Front ends</h3> <p>How does a front end work? Usually, the <em>parser</em> calls the <em>scanner</em> that reads the source program to extract <strong>tokens</strong>. These tokens are then used by the parser to create the parse tree. We can say that the scanner is a subordinate routine of the parser. On the other side, the parser sends the parse tree to the <em>semantic analyzer</em> which generates the AST. Finally, we get the AST/ Linear IR along with a symbol table.</p> <h3 id="typical-back-ends-in-aho-ullman-model">Typical Back ends in Aho Ullman Model</h3> <p>The backend receives the machine independent IR from the front end. It then proceeds to optimize the instructions via compile time evaluations and elimination of redundant computations. There are techniques such as constant propagation and dead-code elimination that are used.</p> <p>After optimization, the code generator converts the machine independent IR to machine dependent IR. The essential operations done by the code generator are instruction selection, local register allocation, and the choice of order of evaluation.</p> <p>Finally, we have the machine dependent optimizer which outputs assembly code. It involves register allocators, instruction schedulers and peephole optimizer.</p> <h3 id="gnu-tool-chain-for-c">GNU Tool Chain for C</h3> <p>We know that the <code class="language-plaintext highlighter-rouge">gcc</code> compiler take source program as input and produces a target program. Internally, this is done via a compiler called ‘cc1’ (C compiler 1) which generates the assembly program. This assembly code is taken by the assembly component of the compiler to generate an object file. Finally, the loader takes this object file along with <code class="language-plaintext highlighter-rouge">glibc/newlib</code> to generate the target program. The assembly program and loader together are known as <strong><code class="language-plaintext highlighter-rouge">binutils</code></strong>.</p> <h3 id="llvm-tool-chain-for-c">LLVM Tool Chain for C</h3> <p>The LLVM tool chain looks similar to the GNU tool chain. We have <code class="language-plaintext highlighter-rouge">clang -cc1</code>, <code class="language-plaintext highlighter-rouge">clang -E</code>, <code class="language-plaintext highlighter-rouge">llvm-as</code> (assembly program) and <code class="language-plaintext highlighter-rouge">lld</code>(loader). The re-targetability mechanism is done via table generation code.</p> <h2 id="modern-challenges">Modern Challenges</h2> <p>Currently, most of the implementation of compilers has been more or less done. What is left? Some applications and architectures demand special programming languages and compilers that are <em>non-standard</em>. Also, there are some design issues with compilers. It is the IR that breaks or makes a compiler. Including these there are many places of optimization such as</p> <ul> <li>Scaling analysis to large programs without losing precision</li> <li>Increasing the precision of analysis</li> <li>Combining static and dynamic analysis.</li> </ul> <p><em>Full Employment Guarantee Theorem for Compiler Writers</em>.</p> <ul> <li>Software view is stable, hardware view is disruptive.</li> <li>Correctness of optimizations</li> <li>Interference with security</li> <li>Compiler verification and Translation Validation</li> <li>Machine Learning?</li> <li> \[\dots\] </li> </ul> <h2 id="-lexical-analysis">~ Lexical Analysis</h2> <h2 id="scanners">Scanners</h2> <p>To discover the structure of the program, we first get a sequence of <strong>lexemes</strong> or <strong>tokens</strong> using the smallest meaningful units. Once this is done, we remove spaces and other whitespace characters in the <strong>lexical analysis</strong> or <strong>scanning</strong> step.</p> <p>In the second step, we group the lexemes to form larger structures (parse tree). This is called as <strong>syntax analysis</strong> or <strong>parsing</strong>. There are tools like <code class="language-plaintext highlighter-rouge">Antlr</code> that combine scanning with parsing.</p> <h3 id="lexemes-tokens-and-patterns">Lexemes, Tokens, and Patterns</h3> <p><strong><em>Definition.</em></strong> <em>Lexical Analysis</em> is the operation of dividing the input program into a sequence of lexemes (tokens). <strong>Lexemes</strong> are the smallest logical units (words) of a program. Whereas, <strong>tokens</strong> are sets of similar lexemes, i.e. lexemes which have a common syntactic description.</p> <p>How are tokens and lexemes decided? What is the basis for grouping lexemes into tokens? Generally, lexemes which play similar roles during syntax analysis are grouped into a common token. Each keyword plays a different role and are therefore a token by themselves. Each punctuation symbol and each delimiter is a token by itself All comments are uniformly ignored and hence are grouped under the same token. All identifiers (names) are grouped in a common token.</p> <p>Lexemes such as comments and white spaces are not passed to the later stages of a compiler. These have to be detected and ignored. Apart from the token itself, the lexical analyzer also passes other information regarding the token. These items of information are called <strong>token attributes</strong>.</p> <p>In conclusion, the lexical analyzer</p> <ul> <li>Detects the next lexeme</li> <li>Categorizes it into the right oken</li> <li>Passes to the syntax analyzer. The token name for further syntax analysis and the lexeme itself.</li> </ul> <h1 id="lecture-4">Lecture 4</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">19-01-22</code></p> </blockquote> <p>Creating a lexical analyzer</p> <ul> <li>Hand code - possible more efficient but seldom used nowadays.</li> <li>Use a generator - Generates from formal description. Less prone to errors.</li> </ul> <p>A formal description of the tokens of the source language will consist of</p> <ul> <li>A regular expression describing each token, and</li> <li>A code fragment called an <strong>action routine</strong> describing the action to be performed, on identifying each token.</li> </ul> <p>Some regex notation -</p> <ul> <li>Alternation is represented using <code class="language-plaintext highlighter-rouge">|</code></li> <li>Grouping is done via parentheses</li> <li><code class="language-plaintext highlighter-rouge">+</code> is positive closure and <code class="language-plaintext highlighter-rouge">*</code> is Kleene closure</li> </ul> <p>The generator, such as Lex, puts together</p> <ul> <li>A <strong>DFA</strong> constructed from the token specification</li> <li>A code fragment called a <strong>driver routine</strong> which can traverse any DFA</li> <li><strong>Action routines</strong> associated with each regular expression.</li> </ul> <p>Usually, the lexeme with the longest matching pattern is taken into consideration.</p> <h2 id="regular-expressions">Regular Expressions</h2> <p>A regular expression is a set of strings (<em>a language</em>) that belongs to set formed the following rules</p> <ul> <li>\(\epsilon\) and single letters are regular expressions.</li> <li>If \(r, s\) are regular expressions, then \(r\vert s\) is a regular expression. That is, \(L(r\vert s) = L(r) \cup L(s)\)</li> <li>If \(r,s\) are regular expressions, then \(rs\) is a regular expression. That is, \(L(rs) =\) concatenation of strings in \(L(r)\) and \(L(s)\).</li> <li>If \(r\) is a regular expression then \(r^*\) is a regular expression. That is, \(L(r^*)\) is the concatenation of zero or more strings from \(L(r)\). Similarly, for \(r^+\)</li> <li>If \(r\) is a regular expression, then so is \((r)\).</li> </ul> <p>The syntax of regular expressions according to Lex is given as</p> <p><img src="/assets/img/IPL/image-20220129164112391.png" alt="image-20220129164112391"/></p> <p>The return statements in the action routines are useful when a lexical analyzer is used in conjunction with a parser. Some conventions followed by lexical analyzers are</p> <ul> <li>Starting from an input position, detect the longest lexeme that could match a pattern.</li> <li>If a lexeme matches more than one pattern, declare the lexeme to have matched the earliest pattern.</li> </ul> <h3 id="lexical-errors">Lexical errors</h3> <p>There are primarily three kinds of errors.</p> <ul> <li>Lexemes whose length exceeds the bound specified by the language. Most languages have a bound on the precision of numeric constants. This problem is due to the finite size of parse tables.</li> <li>Illegal characters in the program.</li> <li>Unterminated strings or comments.</li> </ul> <p>We could issue an appropriate error message to handle errors.</p> <h1 id="lecture-5">Lecture 5</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">21-01-22</code></p> </blockquote> <h2 id="tokenizing-the-input-using-dfas">Tokenizing the input using DFAs</h2> <p>The format to show a trace of scanning is given by</p> <p>| Step No | State | Matched String | Buffer | <code class="language-plaintext highlighter-rouge">NextChar</code> | <code class="language-plaintext highlighter-rouge">LastFinalState</code> | <code class="language-plaintext highlighter-rouge">MarkedPos</code> | Action | | ——- | —– | ————– | —— | ———- | —————- | ———– | —— |</p> <p>Here, <code class="language-plaintext highlighter-rouge">MatchedString</code> is the prefix of the buffer matched to identify a lexeme. <code class="language-plaintext highlighter-rouge">NextChar</code> is the next character in the input; it will be shifted to the buffer if there is a valid transition in the DFA. <code class="language-plaintext highlighter-rouge">MarkedPos</code> is the position of the character (in the buffer) just after the last seen lexeme. The important point to note is that when there is no transition on <code class="language-plaintext highlighter-rouge">Nextchar</code>,</p> <ul> <li>if <code class="language-plaintext highlighter-rouge">MarkedPos</code> is -1, no final state was seen, the first character in the buffer is discarded, and the second character becomes <code class="language-plaintext highlighter-rouge">NextChar</code>.</li> <li>otherwise, the lexeme up to <code class="language-plaintext highlighter-rouge">MarkedPos</code> (excluding it) is returned, the character at <code class="language-plaintext highlighter-rouge">MarkedPos</code> becomes <code class="language-plaintext highlighter-rouge">NextChar</code>.</li> </ul> <p>In either case, the <code class="language-plaintext highlighter-rouge">LastFinalState</code> is set to -1 and the state is set to 0. See the following example for clarity.</p> <p><img src="/assets/img/IPL/image-20220129172907863.png" alt="image-20220129172907863"/></p> <p>This sort of parsing has quadratic behavior in the worst case. Consider the following two questions.</p> <ul> <li>Is \(S\) always equal to <code class="language-plaintext highlighter-rouge">LastFinalState</code>?</li> <li>Is <code class="language-plaintext highlighter-rouge">MP</code> always equal to the length of the last lexeme?</li> </ul> <p>The answer to both questions is “No”. Think.</p> <h2 id="-introduction-to-parsing-using-lexflex-and-yaccbison">~ Introduction to Parsing using Lex/Flex and Yacc/Bison</h2> <p>Lex is a program that takes regular expressions as input and generates a scanner code. A Lex script can be decoded as follows. It has three parts separated by “%%”.</p> <ul> <li>The first part consists of declarations. These are copied to <code class="language-plaintext highlighter-rouge">lex.yy.c</code> and are contained in the pair “%{” and “%}”. Declarations outside of this pair are directives to lex or macros to be used in regular expressions.</li> <li>The second part consists of rules and actions. These are basically a sequence of “Regex SPACE Action” lines.</li> <li>Finally, the third part consists of auxiliary code.</li> </ul> <p>Yacc is a program that takes tokens from the scanner and parses them. A yacc script is very similar to that of lex. It has three parts which are separated by “%%”.</p> <ul> <li>The first part has declarations that are to be copied to <code class="language-plaintext highlighter-rouge">y.tab.c</code> contained in the pair “%{” and “%}”. Declarations outside of this pair are directives to yacc or specifications of tokens, types of values of grammar symbol, precedence, associativity etc.</li> <li>The second part has rules and actions. That is, a sequence of grammar productions and actions.</li> <li>The last part has the auxiliary code.</li> </ul> <p>The yacc and lex scripts are combined together using gcc. The typical workflow is</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yacc <span class="nt">-dv</span> &lt;file-name&gt;.y
gcc <span class="nt">-c</span> y.tab.c
lex &lt;file-name&gt;.l
gcc <span class="nt">-c</span> lex.yy.c
gcc <span class="nt">-o</span> &lt;obj-file-name&gt; lex.yy.o y.tab.o <span class="nt">-ly</span> <span class="nt">-ll</span>
</code></pre></div></div> <h1 id="lecture-6">Lecture 6</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">29-01-22</code></p> </blockquote> <p>A parser identifies the relationship between tokens.</p> <h2 id="constructing-dfas-for-multiple-patterns">Constructing DFAs for Multiple Patterns</h2> <ul> <li>Firstly, join multiple DFAs/NFAs using \(\epsilon\) transitions.</li> <li>This creates an NFA. This NFA can be converted to a DFA by subset construction. Each state in the resulting DFA is a set of “similar” states of the NFA. The start state of the DFA is a union of all original start states (of multiple patterns). The subsequent states are identified by finding out the sets of states of the NFA for each possible input symbol.</li> </ul> <h2 id="constructing-nfa-for-a-regular-expression">Constructing NFA for a Regular Expression</h2> <p>Consider a regular expression \(R\).</p> <ol> <li>If \(R\) is a letter in the alphabet \(\Sigma\), create a two state NFA that accepts the letter.</li> <li>If \(R\) is \(R_1.R_2\), create an NFA by joining the two NFAs \(N_1\) and \(N_2\) by adding an epsilon transition from every final state of \(N_1\) to the start state of \(N_2\).</li> <li>If \(R\) is \(R_1\vert R_2\), create an NFA by creating a new start state \(s_0\) and a new final state \(s_f\). Add an epsilon transition from \(s_0\) to the start state of \(R_1\) and similarly for \(R_2\). Also, add an epsilon transition from every final state of \(N_1\) to \(s_f\) and similarly for \(N_2\).</li> <li>If \(R\) is \(R_1^*\), create an NFA by adding an epsilon transition from every final state of \(R_1\) to the start state of \(R_1\).</li> </ol> <blockquote> <ul> <li>In the 2nd rule, all the final states in \(N_1\) must be made into normal states?</li> <li>In the 4th rule, \(R\) must accept \(S = \epsilon\) too.</li> <li>Where is the rule for \(R = (R_1)\).</li> </ul> </blockquote> <p>Recall the following rules</p> <ul> <li> <table> <tbody> <tr> <td><em>First matching rule preferred</em>. For example, if we write the rule $$L(L</td> <td>D)^*\(- ID before\)int\(- INT, then the lexeme\)int$$ will be taken as ID token and not INT token.</td> </tr> </tbody> </table> </li> <li><em>Longest match preferred.</em> For example, consider identifiers and int and the lexeme \(integer\). Then the lexeme will be treated as a single identifier token, and not an INT followed by ID.</li> </ul> <p>These rules are implicitly a part of DFAs in a way. The construction ensures the longest match is preferred. The accepted pattern is chosen from the possible patterns based on the first matching rule. <u>To ensure our grammar works as intended, special patterns must be written before general patterns.</u></p> <h2 id="representing-dfas-using-four-arrays">Representing DFAs using Four Arrays</h2> <p>A parsing table is also represented in a similar way. This is a general efficient representation for sparse data. The representation is explained through an example. Consider the following DFA</p> <table> <thead> <tr> <th> </th> <th>a</th> <th>b</th> <th>c</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>1</td> <td>0</td> <td>3</td> </tr> <tr> <td>1</td> <td>1</td> <td>2</td> <td>3</td> </tr> <tr> <td>2</td> <td>1</td> <td>3</td> <td>3</td> </tr> <tr> <td>3</td> <td>1</td> <td>0</td> <td>3</td> </tr> </tbody> </table> <p>with \(2\) being the final state. We use the following character codes</p> <table> <thead> <tr> <th>Char</th> <th>Code</th> </tr> </thead> <tbody> <tr> <td>a</td> <td>0</td> </tr> <tr> <td>b</td> <td>1</td> </tr> <tr> <td>c</td> <td>2</td> </tr> </tbody> </table> <p>Notice that states 0 and 3 have identical transitions. States 1 and 2 differ only on the ‘b’ transition. We shall use these similarities to exploit compact representation. The four arrays we consider are <strong>default</strong>, <strong>base</strong>, <strong>next</strong> and <strong>check</strong>. We follow the given steps.</p> <ol> <li>We choose to fill the entries for state 0 first.</li> <li>The ‘check’ array contains 0 to confirm that the corresponding entries in the ‘next’ array are for state 0. ‘Base’ is the location from which the transitions of state are stored in the ‘next’ array.</li> <li>For state 1, we reuse the transition on a and c from state 0 but we need to enter transition on b explicitly. We do this using the next free entry in the next array and back calculating the base of state 1.</li> <li>State 2 is filled in the same way.</li> <li>State 3 is identical to state 0. We keep its base same as that of state 0.</li> </ol> <p>The transition function in pseudocode is given by</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">nextState</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="kt">char</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">Check</span><span class="p">[</span><span class="n">Base</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+</span> <span class="kt">char</span><span class="p">]</span> <span class="o">==</span> <span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Next</span><span class="p">[</span><span class="n">Base</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+</span> <span class="kt">char</span><span class="p">];</span>
    <span class="k">else</span>
    	<span class="k">return</span> <span class="n">nextState</span><span class="p">(</span><span class="n">Default</span><span class="p">[</span><span class="n">state</span><span class="p">],</span> <span class="kt">char</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <blockquote> <p>How do we prevent clashes in the next array?</p> </blockquote> <p>We can further compress the tables using <strong>equivalence class</strong> reduction. In the above code, instead of <code class="language-plaintext highlighter-rouge">char</code>, we can have <code class="language-plaintext highlighter-rouge">class</code>. So for example, instead of defining transitions separately for 26 characters, we can define a single transition for all the letters. Further optimization can be done via <em>meta-equivalence classes</em>.</p> <h1 id="lecture-7">Lecture 7</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">02-02-22</code></p> </blockquote> <h2 id="syntax-analysis">Syntax Analysis</h2> <p>We had seen Lexical analysis so far. Syntax analysis discovers the larger structures in a program.</p> <p>A syntax analyzer or parser</p> <ul> <li>Ensures that the input program is well-formed by attempting to group tokens according to certain rules. This is <strong>syntax checking</strong>.</li> <li>Creates a hierarchical structure that arises out of such grouping - <strong>parse-tree</strong>.</li> </ul> <p>Sometimes, parser itself implements <strong>semantic-analysis</strong>. This way of compiler organization is known as parser driven front-end. On the other hand, if there is a separate semantic analyzer, then the organization is known as <code class="language-plaintext highlighter-rouge">parser driven back-end</code>?. Also, if the the entire compilation is interleaved along with parsing, then it is known as parser driven compilation. However, this organization is ineffective as optimizations are not possible.</p> <p>Till the early 70s, parsers were written manually. Now, plenty of parser generating tools exist such as</p> <ul> <li>Yacc/Bison - Bottom-up (LALR) parser generator</li> <li>Antlr - Top-down (LL) scanner cum parser generate.</li> <li>PCCTS, COCO, JavaCC, …</li> </ul> <p>To check whether a program is well-formed requires the specification to be unambiguous, correct and complete, convenient</p> <p>A <a href="https://sudhansh6.github.io/posts/automata#context-free-grammar"><strong>context free grammar</strong></a> meets these requirements. Each rule in the grammar is called as a <em>production</em>. The language defined by the grammar is done via the notion of a derivation. That is, the set of all possible <em>terminal strings</em> that can be derived from the start symbol of a CFG is the language of the CFG.</p> <p>For example, consider the grammar to define the syntax for variable declaration</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>declaration -&gt; type idlist;
type -&gt; integer | float | string
idlist -&gt; idlist, id | id
</code></pre></div></div> <p>Now, the parser can check if a sentence belongs to the grammar to check the correctness of the syntax. However, sometimes our algorithms to check whether a word is present in a grammar don’t work. When the derivations are unambiguous, most of the algorithms work in all cases.</p> <p>Human language is context-sensitive not context-free. For example, “Kite flies boy” and “boy files kite” are both syntactically correct. Such problems also arise in case of programming languages, but these are easier to deal with. We shall see this issue in Semantic Analysis.</p> <p>Equivalence of grammars in NP complete.</p> <h3 id="why-context-free">Why “Context Free”?</h3> <p>The only kind of productions permitted are of the form <code class="language-plaintext highlighter-rouge">non-terminal -&gt; sequence of terminals and non-terminals</code>. In a derivation, <u>the replacement is made regardless of the context</u> (symbols surrounding the non-terminal).</p> <h3 id="derivation-as-a-relation">Derivation as a relation</h3> <p>A string \(\alpha, \alpha \in (N \cup T)^*\), such that \(S \xrightarrow{*} \alpha\), is called a <strong>sentential form</strong> of \(G\).</p> <p>During a derivation, there is a choice of non-terminals to expand at each sentential form. We can arrive at leftmost or rightmost derivations.</p> <h1 id="lecture-8">Lecture 8</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">04-02-22</code></p> </blockquote> <h3 id="parse-trees">Parse Trees</h3> <p>If a non-terminal \(A\) is replaced using a production \(A \to \alpha\) in a left-sentential form, then \(A\) is also replaced by the same rule in a right-sentential form. The previous statement is only true when there is no ambiguity in derivations. The commonality of the two derivations is expressed as a <em>parse tree</em>.</p> <p>A <em>parse tree</em> is a pictorial form of depicting a derivation. The root of the tree is labeled with \(S\), and each leaf node is labeled b a token on by \(\epsilon\). An internal node of the tree is labeled by a non-terminal. If an internal node has \(A\) as its label and the children of this node from left to right are labeled with \(X_1, X_2, \dots, X_n\), then there must be a production \(A \to X_1X_2\dots X_n\) where \(X_i\) is a grammar symbol.</p> <h3 id="ambiguous-grammars">Ambiguous Grammars</h3> <p>Consider the grammar</p> \[E \to E + E \ \vert \ E*E \ \vert \ id\] <p>and the sentence <code class="language-plaintext highlighter-rouge">id + id * id</code>. We can have more than one leftmost derivation for this sentence.</p> \[\begin{align} E &amp;\implies E + E \\ &amp;\implies id + E \\ &amp;\implies id + E * E \\ &amp;\implies id + id * id \end{align}\] <p>The other leftmost derivation is -</p> \[\begin{align} E &amp;\implies E * E \\ &amp;\implies E + E * E \\ &amp;\implies id + E * E \\ &amp;\implies id + id * id \end{align}\] <p>A grammar is <strong>ambiguous</strong>, if there is a sentence for which there are</p> <ul> <li>more than one parse trees, or equivalently</li> <li>more than one leftmost/right most derivations.</li> </ul> <p>We can disambiguate the grammar while parsing (easier choice) or the grammar itself.</p> <p><strong>Grammar rewriting</strong></p> <p>Ambiguities can be eradicated via</p> <ul> <li>Precedence</li> <li>Associativity</li> </ul> <h3 id="parsing-strategies">Parsing Strategies</h3> <p>We have top-down and bottom-up parsing techniques. We shall only see bottom-up parsing in this course.</p> <p>Consider the grammar</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>D -&gt; var list : type;
tpye -&gt; integer | float;
list -&gt; list, id | id
</code></pre></div></div> <p>The bottom-up parse and the sentential forms produced for the string <code class="language-plaintext highlighter-rouge">var id, id : integer ;</code> is -</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var id, id : integer ;
var list, id : integer ;
var list : integer ;
var list : type ;
D
</code></pre></div></div> <p>The sentential forms happen to be a right most derivation reverse order. The basic steps of a bottom-up parser are</p> <ul> <li>to identify a <em>substring</em> within a rightmost sentential form which matches the rhs of a rules</li> <li>when this substring is replaced by the lhs of the matching rule, it must produce the previous rm-sentential form.</li> </ul> <p>Such a substring is called <strong>handle</strong>.</p> <p><strong>Handle</strong> - A handle of a right sentential form \(\gamma\), is</p> <ul> <li>a production rule \(A \to \beta\), and</li> <li>an occurrence of substring \(\beta\) in \(\gamma\).</li> </ul> <p>Bottom-up parsing is an LR parsing as it amounts to reading the input from left to right, and placing the right most derivation in reverse.</p> <p><strong>Note.</strong> Only terminal symbols can appear to the right of a handle in a right most sentential form.</p> <h2 id="-syntax-analysis">~ Syntax Analysis</h2> <p>We shall assume that we know how to detect handles in a string, and proceed with parsing algorithms.</p> <h2 id="shift-reduce-parsing">Shift Reduce Parsing</h2> <p>Basic actions of the shift-reduce parser are -</p> <ul> <li><strong>Shift</strong> - Moving a single token form the input buffer onto the stack till a handle appears on the stack.</li> <li><strong>Reduce</strong> - When a handle appears on the stack, it is popped and replaced by the lhs of the corresponding production rule.</li> <li><strong>Accept</strong> - When the stack contains only the start symbol and input buffer is empty, then we accept declaring a successful parse.</li> <li><strong>Error</strong> - When neither shift, reduce or accept are possible, we throw an error (syntax).</li> </ul> <h1 id="lecture-9">Lecture 9</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">09-02-22</code></p> </blockquote> <h3 id="properties-of-shift-reduce-parsers">Properties of shift-reduce parsers</h3> <p>Is the following situation possible?</p> <ul> <li>\(\alpha \beta \gamma\) is the stack content, and \(A\to \gamma\) is the handle.</li> <li>The stack content reduces to \(\alpha \beta A\)</li> <li>Now, \(B \to \beta\) in the next handle.</li> </ul> <p>Notice that the handle is buried in the stack. The search for the handle can be expensive. If this is true, then there is a sequence of rightmost derivations</p> \[S \xrightarrow{*rm} \alpha BAxyz \xrightarrow{rm} \alpha \beta Axyz \xrightarrow{rm} \alpha \beta \gamma xyz\] <p>However, this is not a valid rightmost derivation. Therefore, the above scenario is not possible.</p> <p>This property does not ensure unique reductions with SR parser. For example, we can have the following.</p> <p><img src="/assets/img/IPL/image-20220209113043082.png" alt="image-20220209113043082"/></p> <p>These problems are collectively grouped as shift-reduce conflicts and reduce-reduce conflicts. Given a parsing table, each (state, char) pair can have two possible valid actions. These conflicts are resolved by conventionally choosing one action over the other.</p> <h2 id="simple-right-to-left-parsing-slr1">Simple Right to Left Parsing (SLR(1))</h2> <p>Shift reduce parsing is a precursor to this parsing method. As in, shift reduce parsing does not have a definitive algorithm as such, and it is formalized using SLR parsing.</p> <h3 id="shift-reduce-parsing-formal-algorithms">Shift Reduce Parsing: Formal Algorithms</h3> <p>The first step involves disambiguating the grammar. Shift reduce conflicts are resolved using <strong>precedence</strong> and <strong>associativity</strong>. As we have seen, we trace right most derivations in reverse by identifying handles in right sentential forms and <em>pruning</em> them for constructing the previous right sentential form.</p> <p>How do we identify handles? <u>We need to discover a prefix of right sentential form that ends with a handle.</u></p> <p>A <strong>viable prefix</strong> of a right sentential form that does not extend beyond the handle. It is either a string with no handle, or it is a string that ends with a handle. By suffixing terminal symbols to the viable prefix of the second kind, we can create a right sentential form. <u>The set of viable prefixes forms a regular language (as they are prefixes), thus they can be recognised by a DFA</u>. We keep pushing prefixes on the stack until the handle appears on the top of the stack.</p> <p>The occurrence of a potential handle does not mean it should be reduced, the next terminal symbol decides whether it is an actual handle. In general, the set of viable prefixes need not be finite and the DFA to recognise them may have cycles.</p> <p>An item is a grammar production with a dot (\(\bullet\)) in it somewhere in the RHS. The dot separates what has been seen from what may be seen in the input . It can be used to identify a set of items for a viable prefix. A terminal to the left of the dot indicates a <em>complete item</em>.</p> <p>Now, we shall see how to compute <strong>LR(0) item sets</strong>. <code class="language-plaintext highlighter-rouge">L</code> means that it reads the input left to right, <code class="language-plaintext highlighter-rouge">R</code> denotes that it traces the rightmost derivation in reverse, and <code class="language-plaintext highlighter-rouge">0</code> tells that an item does no contain any lookahead symbol. Consider the grammar</p> \[\begin{align} E \to E + E \mid E*E \mid id \end{align}\] <p>We augment the grammar by adding a synthetic start symbol. Then we construct the start state by putting a dot at the start of the start symbol and taking a closure. We then identify the transitions on every symbol that has a dot before it to construct new states. For every state identified in this manner, we take a closure and identify the transitions on every symbol that has a dot before it.</p> <h1 id="lecture-10">Lecture 10</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">11-02-22</code></p> </blockquote> <blockquote> <p>Does a rule of the form \(T \to \epsilon\) always lead to a shift-reduce conflict?</p> </blockquote> <p>Consider \(\beta Aw \xrightarrow{rm} \beta \alpha w\). When do we reduce occurrence of \(\alpha\) in \(\beta\alpha\) using \(A \to \alpha\) using LR(\(k\))? (When do we decide that \(\alpha\) and \(A \to \alpha\) form a handle in \(\beta \alpha\)).</p> <ul> <li>As soon as we find \(\alpha\) in \(\beta \alpha\) - LR(0) items and no lookahead in the input - <strong>SLR(0) Parser</strong></li> <li>As soon as we find \(\alpha\) in \(\beta \alpha\) and the next input token can follow \(A\) in some right sentential form - LR(0) items and 1 lookahead in the input - <strong>SLR(1) Parser</strong></li> <li>As soon as we find \(\alpha\) in \(\beta \alpha\) and the next input token can follow \(A\) in \(\beta \alpha\) - LR(1) items and 1 lookahead in the input - <strong>CLR(1) Parser</strong></li> </ul> <p>To formalise the notion of follow, we defined <code class="language-plaintext highlighter-rouge">FIRST</code> and <code class="language-plaintext highlighter-rouge">FOLLOW</code> sets.</p> <p><code class="language-plaintext highlighter-rouge">FIRST</code>\((\beta)\) contains the terminals that may begin a string derivable from \(\beta\). If \(\beta\) derives \(\epsilon\) then \(\epsilon \in FIRST(\beta)\).</p> <h1 id="lecture-12">Lecture 12</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">18-02-22</code></p> </blockquote> <h2 id="role-of-semantic-analysis">Role of Semantic Analysis</h2> <ul> <li> <p>We have seen <strong>lexical errors</strong> and <strong>syntax errors</strong>. Name and scope analysis cannot be done using scanner and parser (as our grammar is context free). These are detected by semantic analyser and are known as <strong>semantic errors</strong>.</p> <p>Compilers usually try to list all the errors in the program by trying to recover from each error, and continue compiling the remaining code.</p> </li> <li>C++ requires references to be initialised.</li> <li> <p>Overflow shows up as a warning and not as an error. Why? This is because type conversions are allowed in C++. These sort of errors are <strong>runtime errors</strong>.</p> </li> <li> <p>Suppose we have the following program</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">40</span><span class="p">;</span>
  
<span class="n">cout</span><span class="o">&lt;&lt;</span> <span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="mi">5</span> <span class="o">&lt;&lt;</span><span class="n">endl</span><span class="p">;</span>
</code></pre></div> </div> <p>The output of this code is <code class="language-plaintext highlighter-rouge">1</code> and not <code class="language-plaintext highlighter-rouge">0</code>. This is because <code class="language-plaintext highlighter-rouge">1 &lt;= i</code> is evaluated as <code class="language-plaintext highlighter-rouge">True</code>(<code class="language-plaintext highlighter-rouge">1</code>) and <code class="language-plaintext highlighter-rouge">1 &lt;= 5</code> is <code class="language-plaintext highlighter-rouge">True</code>.</p> <p>This is a runtime activity and the error cannot be identified by a compiler (unless constant propagation optimisation is performed). This error is known as a <strong>logical error</strong>.</p> <p>Runtime and Logical errors are usually not detected by the compiler.</p> </li> <li> <p>Does <code class="language-plaintext highlighter-rouge">int a[3] = {1, 2, 3, 4};</code> give an error? Yes! (Too many initialisers) It’s a semantic error. What if we had <code class="language-plaintext highlighter-rouge">int a[4] = {1, 2, 3};cout&lt;&lt;a[3];</code>? It does not give an error or a warning. It gives a <em>runtime error</em> in the form of a <strong>segmentation fault</strong>.</p> </li> <li> <p>Suppose we had the following code</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">f</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">a</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">)</span> <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div> </div> <p>Does this give an error? No, the compiler gives a warning that says “control reaches end of non-void function.” Such functions can be used to generate pseudorandom numbers. However, does <code class="language-plaintext highlighter-rouge">f(-5)</code> give a runtime error? No, it returns <code class="language-plaintext highlighter-rouge">-5</code>. Why is that so?</p> <p>Also, if the definition of <code class="language-plaintext highlighter-rouge">f</code> was modified to the following, then we get random numbers</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">f</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">a</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">)</span> <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">a</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div> </div> <p>But again, if <code class="language-plaintext highlighter-rouge">f</code> is modified to below, we start getting <code class="language-plaintext highlighter-rouge">-5</code> everytime.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">f</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">)</span>
<span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">a</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">)</span> <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
  <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">a</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="mi">5</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Or the following f</span>
<span class="kt">int</span> <span class="nf">f</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">a</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">)</span> <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">g</span><span class="p">(</span><span class="n">a</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div> </div> <p>Many such variations are possible with different behaviours. Also, the output of the program depends on the compiler flags too!</p> <p>The language specifications say that a variable must be declared before its use but may not be defined before its use.</p> </li> <li> <p>Consider the following code</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">float</span> <span class="n">inc</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">while</span> <span class="p">(</span><span class="n">inc</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">sum</span> <span class="o">+=</span> <span class="n">inc</span><span class="p">;</span>
	<span class="n">inc</span> <span class="o">+=</span> <span class="mf">0.1</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div> </div> <p>This program can go into an infinite loop due to <strong>floating imprecision</strong>. This is a runtime activity and cannot be detected by a compiler. We will classify this as a logical error and not a runtime error. Remember that <code class="language-plaintext highlighter-rouge">0</code> does not cause any imprecision!</p> </li> <li> <p>Consider the following code</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">f</span><span class="p">(</span><span class="kt">short</span> <span class="n">a</span><span class="p">)</span> <span class="p">{</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">"short"</span><span class="p">;}</span>
<span class="kt">void</span> <span class="nf">f</span><span class="p">(</span><span class="kt">long</span> <span class="n">a</span><span class="p">)</span> <span class="p">{</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">"long"</span><span class="p">;}</span>
<span class="kt">void</span> <span class="nf">f</span><span class="p">(</span><span class="kt">char</span> <span class="n">a</span><span class="p">)</span> <span class="p">{</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">"char"</span><span class="p">;}</span>
<span class="kt">int</span> <span class="nf">main</span> <span class="p">()</span>
<span class="p">{</span>
	<span class="n">f</span><span class="p">(</span><span class="mi">100</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div> </div> <p>How does the compiler decide which function to use? There is a difficulty in resolving function overloading. This is a <strong>semantic error</strong>. If we had a definition for <code class="language-plaintext highlighter-rouge">int</code>, then there would not be any issue, as integer is a default type.</p> <p>Also, comparison of string and integer will give a <strong>syntax error</strong>.</p> </li> <li><code class="language-plaintext highlighter-rouge">main</code> in C can be recursive!</li> </ul> <h1 id="lecture-13">Lecture 13</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">02-03-22</code></p> </blockquote> <h2 id="why-separate-semantic-analysis-from-syntax-analysis">Why separate semantic analysis from syntax analysis?</h2> <p>The constraints that define semantic validity cannot be described by context free grammars. Also, using context sensitive grammars for parsing is expensive. Practical compilers use CFGs to admit a superset of valid sentences and prune out invalid sentences by imposing context sensitive restrictions. For example, \(\{wcw \mid w \in \Sigma^*\}\) is not a CFG. We accept all sentences in \(\{xcy \mid x, y \in \Sigma^*\}\), enter \(x\) in a symbol table during declaration processing, and when ‘variable uses’ are processed, lookup the symbol table and check if \(y = x\).</p> <p>We identify some attributes of the context-free grammar, and apply some constraints on them to simulate context-sensitivity.</p> <h3 id="terminology">Terminology</h3> <ul> <li><strong>Undefined behaviour</strong> - Unchecked prohibited behaviour flagged by the language. These are not a responsibility of the compiler or its run time support. They have unpredictable outcomes, and the compiler is legally free to do anything. Practical compilers try to detect these and issue warnings (not errors).</li> <li><strong>Unspecified behaviour</strong> (aka implementation-defined behaviour) - These refer to a valid feature whose implementation is left to the compiler. The available choices do no affect the result by mat influence the efficiency. For example, the order of evaluation of subexpressions is chosen by the compiler. Practical compilers make choices based on well defined criteria,</li> <li><strong>Exceptions</strong> - Prohibited behaviour checked by the runtime support. Practical compilers try to detect these at compile time.</li> </ul> <h2 id="different-forms-of-semantic-analysis">Different forms of Semantic Analysis</h2> <p><img src="/assets/img/IPL/image-20220302113554943.png" alt="image-20220302113554943"/></p> <h2 id="syntax-directed-definitions-sdds">Syntax Directed Definitions (SDDs)</h2> <p>The augmented CFG with attributes is given by</p> <p>| \(A \to \alpha\) | \(b = f(c_1, c_2, \dots, c_k)\) | | —————– | ——————————- |</p> <p>where \(b\) is an attribute of \(A\) and \(c_i\), \(1 \leq i \leq k\) are attributes of \(\alpha\). These semantic rules are evaluated when the corresponding grammar tules is use for derivation/reduction.</p> <p><strong>Note.</strong> The associativity decision is not decided through the order of execution, but through the order in which variables are ‘seen’.</p> <h1 id="lecture-14">Lecture 14</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">04-03-22</code></p> </blockquote> <h2 id="sdd-for-generating-ir-for-expression">SDD for generating IR for expression</h2> <p>For example, consider the input statement \(x = (a - b) * ( c+ d)\). The expected IR output is</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>t0 = a - b
t1 = c + d
t2 = t0 * t1
x = t2
</code></pre></div></div> <p>To achieve this, we can have a “code” attribute for each <code class="language-plaintext highlighter-rouge">id</code>, and then concatenate the codes to obtain the IR.</p> <p>Let us consider the example for generating an IR for a ternary expression. We have \(E_1 \to E_2\ ?\ E_3 : E_4\). Then, we have the following template for the code of \(E_1\).</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>E_2.code
t_1 = !E_2.place
if t_1 goto I_1
E_3.code
t_2 = E_3.place
goto I_2
I_1 : E_4.code
			t_2 = E_4.place
I_2 :
--------------------------
E_1.place = t_2
</code></pre></div></div> <blockquote> <p>Why are we using <code class="language-plaintext highlighter-rouge">!t1</code> instead of <code class="language-plaintext highlighter-rouge">t1</code>? It is just a design decision.</p> </blockquote> <p>Now, we shall use the following SDD for generating the above code.</p> <p><img src="/assets/img/IPL/image-20220304112033935.png" alt="image-20220304112033935"/></p> <p>There are two representations of a 2D array - row major representation and column major representation. FORTRAN uses column major representation for arrays.</p> <p>Should the compiler decide the representation or should the language do it? Languages specify the representation!</p> <p>In general, we use row-major representation where the address of the cell \((i, j)\) is given by <code class="language-plaintext highlighter-rouge">base + i1*n2 + i2</code>. For a general \(k\)-D array, the offset is given by the recurrence</p> \[\begin{align} O_1 &amp;= i_1 \\ O_{j + 1} &amp;= O_j \times n_j + i_j \end{align}\] <p>We add the attributes <code class="language-plaintext highlighter-rouge">name, offset, ndim</code> etc for IR of array accesses.</p> <p>Check the slides for the pseudocode to generate IR for unary/binary expressions, while loop etc.</p> <h1 id="lecture-15">Lecture 15</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">09-03-22</code></p> </blockquote> <h3 id="generating-ir-for-field-accesses-in-a-structure">Generating IR for Field Accesses in a Structure</h3> <p>The ‘access’ expressions become a little more non-trivial in case of structures as compared to variables. The memory offsets of a structure can be computed at compile time unlike an array offset. For example, the offset in <code class="language-plaintext highlighter-rouge">A[x]</code> cannot be calculated at compile time. Hence, it is easier to work on structures. A float is considered to occupy 8 bytes.</p> <p>The general IR code will have the following steps</p> <ul> <li>Obtain the base address</li> <li>Add the offset to the base</li> <li>Dereference the resulting address</li> </ul> <p>We will assume <code class="language-plaintext highlighter-rouge">offset(t, f)</code> gives the offset of field <code class="language-plaintext highlighter-rouge">f</code> in structure <code class="language-plaintext highlighter-rouge">t</code>. Similarly, <code class="language-plaintext highlighter-rouge">type(t, f)</code> gives the type of <code class="language-plaintext highlighter-rouge">f</code> in <code class="language-plaintext highlighter-rouge">t</code>. The TAC generation code is given by</p> <p><img src="/assets/img/IPL/image-20220323084445565.png" alt="image-20220323084445565"/></p> <p>We need to add the <code class="language-plaintext highlighter-rouge">code</code> and <code class="language-plaintext highlighter-rouge">place</code> attributes to the above rules. We shall assume that the members of a class are stored sequentially based on the declaration.</p> <p><strong>Note.</strong> We cannot have a circular dependancy among classes without pointers.</p> <p>We shall introduce pointers in our grammar now.</p> <p><img src="/assets/img/IPL/image-20220323085151934.png" alt="image-20220323085151934"/></p> <p><img src="/assets/img/IPL/image-20220323085134456.png" alt="image-20220323085134456"/></p> <p><strong>Note.</strong> The new code for \(F \to id_1.id_2\) generates different code as compared to our previous implementation.</p> <h2 id="syntax-directed-translation-schemes">Syntax Directed Translation Schemes</h2> <p>Given a production \(X \to Y_1Y_2 \dots Y_k\),</p> <ul> <li>If an attribute \(X.a\) is computed from those of \(Y_i\), \(1 \leq i \leq k\), the \(X.a\) is a <strong>synthesised attribute</strong>.</li> <li>If an attribute \(Y_i.a\), \(1 \leq i \leq k\) is computed from those of \(X\) or \(Y_j\), \(1 \leq j \leq i\), then \(Y_i.a\) is an <strong>inherited attribute</strong>.</li> </ul> <p>Using these definitions, we will come up with an alternate approach for generating the IR for structures that is more expressive.</p> <h1 id="lecture-16">Lecture 16</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">11-03-22</code></p> </blockquote> <p>Previously, we had calculated the cumulative offset for a structure access in the IR. As in, we computed the final offsets at compile time and consequently used <code class="language-plaintext highlighter-rouge">F.offset</code> attribute instead of the <code class="language-plaintext highlighter-rouge">F.code</code> attribute. Alternately, we did not do ‘compile time offset calculation’ for the pointer IR code.</p> <p>Let us now continue the discussion on inherited and synthesised attributes. A synthesised attribute is an attribute derived for a non-terminal on the lhs of a production from the non-terminals on the rhs of the same production. Inherited attributes are derived for non-terminals on the rhs of a production from the non-terminal on the rhs or other non-terminals on the rhs. So, why do we require inherited attributes?</p> <p>For example, consider type analysis</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Decl</span> <span class="o">-&gt;</span> <span class="n">Type</span> <span class="n">VarList</span> <span class="p">{</span><span class="err">$</span><span class="mi">2</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="o">=</span> <span class="err">$</span><span class="mi">1</span> <span class="o">-&gt;</span> <span class="n">name</span><span class="p">}</span>
<span class="n">Type</span> <span class="o">-&gt;</span> <span class="kt">int</span> <span class="o">|</span> <span class="kt">float</span> <span class="p">{</span><span class="err">$$</span> <span class="o">-&gt;</span> <span class="n">name</span> <span class="o">=</span> <span class="err">$</span><span class="mi">2</span><span class="p">;}</span>
<span class="n">VarList</span> <span class="o">-&gt;</span> <span class="n">VarList</span><span class="p">,</span> <span class="n">id</span> <span class="p">{</span><span class="err">$</span><span class="mi">1</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="o">=</span> <span class="err">$$</span> <span class="o">-&gt;</span> <span class="n">type</span><span class="p">;</span> <span class="err">$</span><span class="mi">3</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="o">=</span> <span class="err">$$</span> <span class="o">-&gt;</span> <span class="n">type</span><span class="p">;}</span>
<span class="n">Varlist</span> <span class="o">-&gt;</span> <span class="n">id</span> <span class="p">{</span><span class="err">$</span><span class="mi">1</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="o">=</span> <span class="err">$$</span> <span class="o">-&gt;</span> <span class="n">type</span><span class="p">;}</span>
</code></pre></div></div> <p>Here, the attribute <code class="language-plaintext highlighter-rouge">type</code> is inherited.</p> <p>However, how do we allow flow of attributes concurrently with parsing? Clearly, there would be non-terminals that are not generated at a given step during parsing, and inherited attributes cannot be computed if they depend on a symbol not yet seen. All these problems can be avoided by no allowing inherited attributes to be derived from the symbols to the right of the current symbol. So once the left non-terminals are derived, we can derive the attribute for the current non-terminal.</p> <p>In summary, given a production \(X \to Y_1, \dots, Y_k\)</p> <ul> <li>\(Y_i.a\) is computed only from the attributes of \(X\) or \(Y_j\), \(j &lt; i\).</li> <li>\(X.a\) would have been computed from the grammar symbols that have already been seen (i.e, in some production of the form \(Z \to \alpha X\beta\))</li> </ul> <h3 id="more-definitions">More definitions</h3> <p>An SDD is <strong>S-attributed</strong> if it uses only synthesised attributes, and an SDD is <strong>L-attributed</strong> if it uses synthesised attributes or inherited attributes that depend on some symbol to the left. That is, given a production \(X \to Y_1\dots Y_k\) attribute \(Y_i.a\), of some \(Y_i\) is computed only from the attributes of \(X\) or\(Y_j\), \(j &lt; i\).</p> <h2 id="syntax-directed-translation-schemes-sdts">Syntax Directed Translation Schemes (SDTS)</h2> <p>We generalise the notion of SDDs using SDTS. A Syntax Directed Translation Scheme is an SDD with the following modifications</p> <ul> <li>Semantic rules are replaced by actions possibly with side effects.</li> <li>The exact time of the action is specified; an action computing an inherited attribute of a non-terminal appears just before the non-terminal.</li> </ul> <p>For example, for the previous type analysis rules, we have</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">decl</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="p">{</span><span class="err">$</span><span class="mi">2</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="o">=</span> <span class="err">$</span><span class="mi">1</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="p">}</span> <span class="n">VarList</span>
</code></pre></div></div> <p>We similarly define <strong>S-Attributed SDTS</strong> as SDTS that only use synthesised attributes and all actions appear at the end of the RHS of a production. <strong>L-Attributed SDTS</strong> use only synthesises attributes or attributes that depend on a symbol towards the left. The actions may appear in the middle of the rules end and at the end of the RHS of a production.</p> <h2 id="type-analysis">Type Analysis</h2> <h3 id="type-expressions-and-representation">Type Expressions and Representation</h3> <p>A type expression describes types of all entities (variable, functions) in a program - basic types, user-defined types, and derived types.</p> <p><strong>Note.</strong> The size of an array is not a part of the type in C for validation. It is just used for memory allocation. \(\tau_1 \times \tau_2\) describes the product of two types, and \(\tau_1 \to \tau_2\) describes a function that takes arguments described by \(\tau_1\) and returns the result described by \(\tau_2\). Product is left-associative and has a higher precedence than \(\to\).</p> <h1 id="lecture-17">Lecture 17</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">16-03-22</code></p> </blockquote> <h3 id="marker-non-terminals">Marker Non-terminals</h3> <p>If we have a rule of the form \(X \to Y_1 \{\dots\} Y_2\), then we convert it to the following set of rules \(X \to Y_1 M Y_2 \\ M \to \epsilon \{\dots\}\) \(M\) is a marker non-terminal for \(Y_2\) in the grammar. \(Y_1.s\) and \(Y_2.s\) denote the synthesised attributes of \(Y_1\) and \(Y_2\) whereas \(Y_2.i\) denotes the inherited attribute of \(Y_2\).</p> <p>When \(M \to \epsilon \{\dots\}\) is about to be reduced, the parsing stack contains \(Y_1\), and the value stack contains \(Y_1.s\). Once the reduction is done, we add \(M\) to the parsing stack, and \(Y_2.i\) is added to the value stack.</p> <p>Marker non-terminals may cause reduce-reduce conflicts. It is possible to rewrite the yacc scripts to prevent these conflicts.</p> <h2 id="type-analysis-2">Type Analysis (2)</h2> <h3 id="type-equivalence">Type Equivalence</h3> <p>Consider two different with identical member variables and functions. How do we distinguish the structures themselves and pointers to these structures?</p> <p><strong>Name Equivalence</strong> - Same basic types are name equivalent. Derived type are name equivalent if they have the same name. every occurrence of a derived type in declarations is given a unique name.</p> <p><strong>Structure Equivalence</strong> - Same basic types are structurally equivalent. Derived types are structurally equivalent if they are obtained by applying the same type constructors to structurally equivalent types, or <em>one is type name that denotes the other type expressions?</em>.</p> <p>Name equivalence implies structural equivalence and not the other way around. C uses structural equivalence for everything except structures. For structures, it uses name equivalence.</p> <h3 id="type-inferencing">Type Inferencing</h3> <p>Functional languages do not require separate declarations for variables and types. Some sort of type inferencing is done for class during runtime in C.</p> <h2 id="name-and-scope-analysis">Name and Scope Analysis</h2> <p>We maintain a stack of symbol tables. At the start of a new scope, we push a new symbol table on the stack. We start with the “global” scope symbol table. At the end of every scope, we pop the top symbol table from the stack. For use of a name, we look it up in the symbol table starting from the stack top</p> <ul> <li>If the name is not found in a symbol table, search in the symbol table below</li> <li>If the same name appears in two symbol tables, the one closer to the top hides the one below</li> </ul> <p>However, with this setup, how do we access variables in the outer scope?</p> <h3 id="static-and-dynamic-scoping">Static and Dynamic Scoping</h3> <p>Under <em>static scoping</em>/ lexical scoping, the names visible at line \(i\) in procedure \(X\) are</p> <ul> <li>Names declared locally within \(X\) before line \(i\)</li> <li>Name declared in procedures enclosing \(X\) upto the declaration of \(X\) in the program.</li> </ul> <p>Under <em>dynamic scoping</em>, the names visible at line \(i\) in a procedure \(X\) are</p> <ul> <li>Names declared locally within \(X\) before line \(i\)</li> <li>Name declared in procedures enclosing \(X\) in a call chain reaching \(X\).</li> </ul> <p>Dynamic scoping is difficult to comprehend is seldom used.</p> <h1 id="lecture-18">Lecture 18</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">23-03-22</code></p> </blockquote> <h2 id="declaration-processing">Declaration Processing</h2> <p>How do we process <code class="language-plaintext highlighter-rouge">int **a[20][10]</code>? Do we take it as double pointer to an array of 20 rows where each row consists of 10 integers, or do we take it as a 2D-array of double arrays to integers with size \(20 \times 10\)?</p> <p>In order to do the former, we can use the following grammar</p> \[\begin{align} decl &amp;\to T \ item; \\ T &amp;\to int \mid double \\ item &amp;\to id \mid item [num] \end{align}\] <p>However, this is an inconvenient layout for 20 arrays of arrays of 10 ints. Suppose we correct it to the following</p> \[\begin{align} decl &amp;\to T \ item; \\ T &amp;\to int \mid double \\ item &amp;\to id \mid Array \\ Array &amp;\to [num] \mid [num]Array \end{align}\] <p>So basically, we have changed left recursive rule to a right recursive rule.</p> <p>We introduce the notion of base types and derived types for We similarly add the size and width attributes in the above rules.</p> <p>Addition of pointers is easy. We just add the following set of rules to the above set.</p> \[\begin{align} item &amp;\to * \{item2.bt = item.bt\} \\ &amp;\to item2 \\ &amp;\to \{item.dt = pointer(item2.dt); item.s = 4\} \end{align}x\] <h1 id="lecture-19">Lecture 19</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">25-03-22</code></p> </blockquote> <h2 id="runtime-support">Runtime support</h2> <p>The data objects come into existence during the execution of the program. The generated code must refer to the data objects using their addresses, which must be decided during compilation.</p> <p>Runtime support also helps in dynamic memory allocation, garbage collection, exception handling, and virtual function resolution.</p> <p>Virtual function resolution falls under the category of runtime support as pointer declarations are static but the objects pointed are dynamic.</p> <p>A programmer specifies the type of a data item, and also its role and allocation. Unnamed data resides on heap. We have the following properties of data</p> <p><img src="/assets/img/IPL/image-20220405185340593.png" alt="image-20220405185340593"/></p> <blockquote> <p>Local and global static variables?</p> </blockquote> <p>A sequential language may allow procedures to be</p> <ul> <li>Invoked as subroutines - Stack and static memory suffices for organising data for procedure invocations.</li> <li>Invoked recursively - Stack memory is required for organising data for procedure invocations.</li> <li>Invoked indirectly through a function pointer or passed as a parameter. Access to non-local data of the procedure needs to be provided.</li> </ul> <h2 id="compiling-procedure-calls">Compiling Procedure Calls</h2> <h3 id="activation-records">Activation Records</h3> <p>Every invocation of a procedure requires creating an <strong>activation record</strong>. An activation record provides space for</p> <ul> <li>Local variables</li> <li>Parameters</li> <li>Saved registers</li> <li>Return value</li> <li>Return address</li> <li>Pointers to activation records of the calling procedures</li> </ul> <p>We maintain two parameters known as <strong>frame pointer</strong> and <strong>stack pointer</strong>. Activation record of the callee is partially generated by the caller. We store the return parameter in one of the registers itself. Function prologue and function epilogue are generated by the compiler. We reduce the value of stack pointer when we push something on the stack. The stack pointer <code class="language-plaintext highlighter-rouge">$sp</code> points to the lower address of the next free location. Unlike the stack pointer, the frame pointer <code class="language-plaintext highlighter-rouge">$fp</code> holds the address of an occupied word.</p> <p><strong>Reminder.</strong> We are talking about runtime activity.</p> <p>As discussed earlier, the compiler generates a function prologue and epilogue. It also generates a code before the call and after the call. This is all the boring assembly code we typically see for calling functions. We shall this code changes when the parameter passing mechanism changes. We will also see implicit sharing through scoping.</p> <h1 id="lecture-20">Lecture 20</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">30-03-22</code></p> </blockquote> <h2 id="using-staticaccess-link">Using Static/Access Link</h2> <p>Let us reiterate the difference between static and dynamic scoping. In static scoping, we can use variables that are declared in the outer scope of the current scope. We use static/access link for this purpose. On the other hand, dynamic scoping follows the call chain for variables. We use control/dynamic link for this purpose.</p> <p>To access the activation record of a procedure at level \(l_1\) from within a procedure at level \(l_2\), we traverse the access link \(l_2 - l_1\) times. Note that \(d_{callee} \leq d_{caller}+ 1\).</p> <h2 id="parameter-passing-mechanisms">Parameter Passing Mechanisms</h2> <ul> <li><strong>Call by value</strong> - Copy the value of the actual parameter into the formal parameter. We use eager evaluation here. We do not know if the parameter is being used in the procedure, but we still evaluate it.</li> <li><strong>Call by reference</strong> - Copy the address of the actual parameter into the formal parameter.</li> <li><strong>Call by value-result (copy-restore)</strong> - Copy the value of the actual parameter and copy the final value of the formal parameter into the actual parameter. This method starts off with call by value. That is, the value of the actual parameter is copied into the formal parameter. However, in the end the value of the formal parameter that is evaluated inside the procedure is copied to the actual parameter. In the end we do <code class="language-plaintext highlighter-rouge">*(&amp;e) = x</code> and not <code class="language-plaintext highlighter-rouge">e = x</code> because <code class="language-plaintext highlighter-rouge">e</code> is an expression.</li> <li><strong>Call by name</strong> - Textural substitution of formal parameter by the actual parameter. Evaluation of the actual parameter is delayed until the value of formal parameter is used. The evaluation is done every time where the formal parameter is used in the procedure. So, this is equivalent to textual substitution or using a macro. All the modifications to the formal parameter is written to the address of the actual parameter. This is implemented by a <strong>thunk</strong> which is a parameterless procedure per actual argument to evaluate the expression and return the address of the evaluation.ccc</li> <li><strong>Call by need</strong> - Textual substitution of formal parameter by the actual parameter but evaluation only once. This is similar to call by name but the evaluation of the actual parameter is done only at the first use. For any later use, we use the precomputed values. This is known as <strong>lazy evaluation</strong>.</li> </ul> <p><img src="/assets/img/IPL/image-20220405205123349.png" alt="image-20220405205123349"/></p> <p>We have the above to make the distinction between the different methods.</p> <h3 id="parameter-passing-mechanisms-for-procedure-as-parameters">Parameter Passing Mechanisms for Procedure as Parameters</h3> <p>Pass a closure of the procedure to be passed as parameter. A data structure containing a pair consisting of</p> <ul> <li> <p>A pointer to the procedure body</p> </li> <li> <p>A pointer to the external environment (i.e. the declarations of the non-local variables visible in the procedure)</p> <p>Depends on the scope rules (i.e., static or dynamic scope)</p> </li> </ul> <p>For C, there are no nested procedures so the environment is trivially global. So a closure is represented trivially by a function pointer. In C++, the environment of a class method consists of global declarations and the data members of the class. The environment can be identified from the class name of the receiver object of the method call.</p> <h3 id="representation-of-a-class">Representation of a Class</h3> <p>There is no distinction between the public and private data in memory. Public/Private is a compile time distinction. Every function with \(n\) parameters is converted to a function of \(n + 1\) parameter with the first parameter being the address the address of the object. Internally, the data space is separate for each object, but the code memory is same for all which contains the functions.</p> <h3 id="virtual-functions">Virtual functions</h3> <p>Non-virtual functions are inherited much like data members. There is a single class-wide copy of the code and the address of the object is the first parameter as seen earlier. In the case of <strong>virtual functions</strong>, a pointer to a base class object may point to an object of any derived class in the class hierarchy.</p> <h3 id="virtual-function-resolution">Virtual function resolution</h3> <p>Partially static and partially dynamic activity. At compile time, a compiler creates a virtual function table for each class. At runtime, a pointer may point to an object of any derived class, and the compiler-generated code is used to pick up the appropriate function by indexing into the virtual table for each class.</p> <p>We define a <strong>non-virtual function</strong> as a function which is not virtual in <em>any</em> class in a hierarchy. Resolution of virtual functions depends on the class of the pointee object - needs dynamic information. Resolution of non-virtual functions depends on the class of the pointer - compile time information is sufficient. In either case, a pointee cannot belong to a “higher” class in the hierarchy.</p> <h1 id="lecture-21">Lecture 21</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">01-04-22</code></p> </blockquote> <p>Compiler decides the virtual function for a class statically. Functions are not copied across objects but are used with reference. Functions of the inherited class override the corresponding virtual functions of the parent class. There is no search for the correct function implementation during the runtime. Instead, all the functions are correctly configured at compile time.</p> <p>The runtime support looks up the class of the receiver object. Then, it dereferences the class information to access the virtual function table. Following this, there is another dereference to invoke the function itself.</p> <p><strong>Note.</strong> A pointer of the parent class can hold an object of the inherited class but not the other way around. However, when the pointer for the parent class is used, the functions in the inherited class that are not present in the parent class cannot be invoked. That is, the pointer will only be able to invoke functions from the parent class.</p> <p><strong>Note.</strong> Functions of the parent class can be overridden without using virtual functions. Non-virtual functions are copied in each class. However, pointer references are used in the case of virtual functions. In the case of virtual functions, the pointer of the base class holding an inherited class object will call the functions of the inherited class and not the base class.</p> <p>For example, consider the following</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">A</span> <span class="p">{</span>
<span class="nl">public:</span>
        <span class="k">virtual</span> <span class="kt">int</span> <span class="n">f</span><span class="p">()</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">1</span><span class="p">;</span> <span class="p">}</span>
        <span class="kt">int</span> <span class="nf">g</span><span class="p">()</span> <span class="p">{</span><span class="k">return</span> <span class="mi">2</span><span class="p">};</span>
<span class="p">};</span>

<span class="k">class</span> <span class="nc">B</span> <span class="o">:</span> <span class="k">public</span> <span class="n">A</span> <span class="p">{</span>
<span class="nl">public:</span>
        <span class="kt">int</span> <span class="n">f</span><span class="p">()</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">10</span><span class="p">;</span> <span class="p">}</span>
        <span class="kt">int</span> <span class="nf">g</span><span class="p">()</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">20</span><span class="p">;</span> <span class="p">}</span>
<span class="p">};</span>
<span class="p">...</span>
<span class="n">A</span><span class="o">*</span> <span class="n">p</span><span class="p">;</span> <span class="n">B</span> <span class="n">b</span><span class="p">;</span>
<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">p</span> <span class="o">-&gt;</span> <span class="n">f</span><span class="p">();</span> <span class="c1">// Prints 10</span>
<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">p</span> <span class="o">-&gt;</span> <span class="n">g</span><span class="p">();</span> <span class="c1">// Prints 2</span>
</code></pre></div></div> <blockquote> <p>Where are all the unique implementations of virtual functions stored?</p> </blockquote> <h2 id="optimisations">Optimisations</h2> <h2 id="register-allocation">Register Allocation</h2> <p>Accessing values from the registers in much faster than from the cache.</p> <h3 id="glocal-register-allocation-using-graph-coloring">Glocal register allocation using Graph Coloring</h3> <p>We identify the live ranges for each variable, and then construct an interference graph between variables if there is any overlap in the live ranges. Then, we use graph coloring to register allocation. However, graph coloring with \(k\) colors is NP-complete general. We use some heuristics, and we shall study one of them called - Chaitin-Briggs allocator. The problem is decidable for <strong>chordal graphs</strong> - Every cycle of length 4 or more has a chord connecting two nodes with an edge that is not part of the cycle (applies recursively). Most practical interference graphs are chordal.</p> <h3 id="chaitin-briggs-register-allocator">Chaitin-Briggs Register Allocator</h3> <ol> <li> <p><strong>Coalescing</strong></p> <p>We eliminate copy statements <code class="language-plaintext highlighter-rouge">x = y</code> so that we use the sam register for both the variables. Then, the <strong>copy propagation optimisation</strong> replaces the used of <code class="language-plaintext highlighter-rouge">x</code> by <code class="language-plaintext highlighter-rouge">y</code>.</p> </li> <li> <p><strong>Identification of live ranges</strong></p> <p>It is the sequences of statements from a definition of a variable to its last use of that values. We shall discuss <strong>live variables analysis</strong> later.</p> </li> <li> <p><strong>Identification of interference and construction of interference graph</strong></p> <p>Live ranges \(I_1\) and \(I_2\)</p> </li> <li> <p>Simplification of interferences graph to identify the order in which the nodes should be colored.</p> </li> </ol> <h3 id="copy-propagation-optimisation">Copy Propagation Optimisation</h3> <p>We assume intra-procedural lines of code. That is, the parts of code without any function calls.</p> <blockquote> <p>Slides - Why <code class="language-plaintext highlighter-rouge">e3</code> and not <code class="language-plaintext highlighter-rouge">e</code>? The number <code class="language-plaintext highlighter-rouge">3</code> represents the statement number where <code class="language-plaintext highlighter-rouge">e</code>was generated.</p> </blockquote> <h3 id="discovering-live-ranges">Discovering Live Ranges</h3> <p>Live ranges are calculated by traversing the code from the end to the beginning. We depict the live range as a set of statement indices. Then we check set intersection for interferences.</p> <h1 id="lecture-22">Lecture 22</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">06-04-22</code></p> </blockquote> <p>Following the analysis of live ranges, we construct the interference graph. Along with the degree of each node, we note the number of loads and stores of each variable. We define <strong>spill cost</strong> as the sum of loads and stores for each variable.</p> <h3 id="chaitin-briggs-allocator">Chaitin-Briggs Allocator</h3> <p>Let \(k\) be the number of colors. We discuss Chaitin’s method first.</p> <ol> <li> <p>We simplify the graph by removing the nodes in an arbitrary order such that for each node \(n\), \(D(n) &lt; k\) and push them no a stack. Since \(D(n) &lt; k\), we are guaranteed to find a color for \(n\).</p> <blockquote> <p>How to prove?</p> </blockquote> </li> <li> <p>If the graph is not empty, we find the node with the lease spill cost, and then we spill it. That is, we further simplify the graph by removing nodes with the lowest spill cost. Then, we repeat the first step.</p> </li> <li> <p>Now, after all the simplification is done, we repeatedly pop the node from the top of the stack, plug it in the graph and give it a color distinct from its neighbour.</p> </li> </ol> <p>Now, let us see Briggs’ method</p> <ol> <li>Same as that of Chaitin’s method.</li> <li>Briggs’ conjectured that it is not necessary for all nodes with a degree greater than \(n\) not to have a color in the coloring. So, we mark the nodes as potentially spill-able and stack them.</li> <li>As in Chaitin’s method, we repeatedly color the nodes. If a node cannot be colored, we spill it and go back to the first step.</li> </ol> <h3 id="spilling-decisions">Spilling Decisions</h3> <ul> <li> <p>Spill cost is weighted by loop nesting depth</p> \[C(n) = (L(n) + D(n)) \times 10^d\] </li> <li> <p>Sometimes, we normalise \(C(n)\) and consider \(C(n)/D(n)\).</p> </li> </ul> <p>Chaitin’s method cannot color the square/diamond graph with 2 colors whereas Briggs’ method can.</p> <p><strong>Note.</strong> We consider the degree of nodes in the original graph. That is, when we simplify the graph in step 1, we don’t update the degrees of all the nodes at each iteration.</p> <h3 id="allocating-registers">Allocating registers</h3> <p>Once we allocate the colors, we replace each variable by a register corresponding to a color. After replacing the code with registers, there would be redundant statements like <code class="language-plaintext highlighter-rouge">r4 = r4</code>. We can get rid of such statements, and this is known as <strong>Peephole Optimisation</strong>. (This happens due to control transfer)</p> <h3 id="live-range-spilling">Live range spilling</h3> <p>Spilling a live range \(l\) involves keep the variable of \(l\) in the memory. For RISC architecture, load in a register for every read, store back in the memory for every write. For CISC architectures, we access directly from the memory.</p> <p>Spilling is necessary if the number of interfering live ranges at a program point exceeds the number of registers. <em>Flow sensitivity</em> (interval interferes with, say, 1 interval at any given time) vs <em>Flow insensitivity</em> (total number of overlaps is greater than 1).</p> <p>Splitting a live range \(l\) involves creating smaller live ranges \(l_1, \dots, l_k\) such that \(D(l_i) \leq D(l), 1 \leq i \leq k\). Live ranges \(l_i\) can participate in graph coloring if \(D(l_i) &lt; D(l)\). Therefore, we have a choice between splitting and spilling. The difference between the two approaches is shown here.</p> <p><img src="/assets/img/IPL/image-20220406114350123.png" alt="image-20220406114350123"/></p> <h2 id="registers-across-calls">Registers across Calls</h2> <p>So far, we have seen allocation of local registers. Suppose we have the following situation. A function <code class="language-plaintext highlighter-rouge">g()</code> uses a register <code class="language-plaintext highlighter-rouge">r</code> and calls <code class="language-plaintext highlighter-rouge">f()</code> inside its body. Now, how do we manage <code class="language-plaintext highlighter-rouge">r</code> across the call?</p> <ul> <li>Procedure <code class="language-plaintext highlighter-rouge">g</code> saves it before the call and restores it after the call. However, <code class="language-plaintext highlighter-rouge">g()</code> does not know if <code class="language-plaintext highlighter-rouge">f()</code> requires <code class="language-plaintext highlighter-rouge">r</code>. Here, save and restore is redundant if <code class="language-plaintext highlighter-rouge">f</code> does not require <code class="language-plaintext highlighter-rouge">r</code>, and this is unavoidable. Also, it knows if the value in <code class="language-plaintext highlighter-rouge">r</code> is need after the call. Here, save and restore is redundant if <code class="language-plaintext highlighter-rouge">r</code> is not required across the call, and this is avoidable.</li> <li>Procedure <code class="language-plaintext highlighter-rouge">f</code> saves it at the start and restores it at the end. Now, <code class="language-plaintext highlighter-rouge">f</code> does not know if <code class="language-plaintext highlighter-rouge">g</code> uses <code class="language-plaintext highlighter-rouge">r</code> after the call, but it knows if <code class="language-plaintext highlighter-rouge">r</code> is required in <code class="language-plaintext highlighter-rouge">f</code>. Like before, the first problem is unavoidable but the second one is avoidable.</li> </ul> <p>Now, as both methods are functionally similar, the method used is a matter of convention. The <strong>architecture</strong> decides this for the system. So, we have</p> <ul> <li><strong>Caller-saved register</strong> - Callee can use it without the fear of overwriting useful data</li> <li><strong>Callee-saved register</strong> - Caller can use it without the fear of overwriting useful data.</li> </ul> <p>We use a caller-saved register <code class="language-plaintext highlighter-rouge">r</code> for values that are not live across a call. Then, <code class="language-plaintext highlighter-rouge">r</code> is not saved by the callee and need not be saved by the caller. We use a callee-saved register <code class="language-plaintext highlighter-rouge">r</code> for values that are live across a call. <code class="language-plaintext highlighter-rouge">r</code> is not saved by the caller, and it is saved by the callee only if it is needed.</p> <h3 id="integrating-with-graph-coloring">Integrating with graph coloring</h3> <p>To begin with, the live range of a callee saved register is the entire procedure body of <code class="language-plaintext highlighter-rouge">g</code>, and the live range of a caller saved register is the procedure body of <code class="language-plaintext highlighter-rouge">f</code>. We then construct the interference graphs with these additional live ranges.</p> <h1 id="lecture-23">Lecture 23</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">08-04-22</code></p> </blockquote> <h2 id="instruction-selection">Instruction Selection</h2> <p>We need to generate an assembly code from the “register code” we generated after register allocation. Floating point comparison takes 2 arguments whereas integer comparison takes 3 arguments.</p> <h3 id="integrated-instruction-selection-and-register-allocation-algorithms">Integrated Instruction Selection and Register Allocation Algorithms</h3> <ul> <li><strong>Sethi-Ullman Algorithm</strong> - Used in simple machine models, and is optimal in terms of the number of instructions with the minimum number of registers and minimum number of stores. It is also linear in the size of the expression tree.</li> <li><strong>Aho-Johnson Algorithm</strong> - This algorithm is applicable for a very general machine model, and is optimal in terms of the cost of execution. It is also linear in the size of the expression tree (exponential in the arity of instruction which is bounded by a small constant). The main motivation behind this idea is that a sequence of 4 instructions can be more efficient than 2 instructions.</li> </ul> <h3 id="sethi-ullman-algorithm">Sethi-Ullman Algorithm</h3> <p>WE have a finite set of registers \(r_0, \dots, r_k\) and countable memory locations. We will be using simple machine instructions like loads, store, and computation instructions (\(r_1 \; op \; k\), \(k\) can be a register or a memory location). The input to this algorithm is the expression tree (essentially the AST) without</p> <ul> <li>control flow (no ternary expressions)</li> <li>assignments to source variables inside an expression (so no side effects). Assignments are outside the expression.</li> <li>no function calls</li> <li>no sharing of values (trees, not DAGs). For example, we don’t have expressions like \(b \times c + d - b \times c\). Basically, we shouldn’t use <em>common subexpression elimination</em>.</li> </ul> <p>The key idea behind this algorithm is that the order of evaluation matters. Sometimes, the result is independent of the order of evaluation of some subtrees in the tree. For example, in \(b \times c + a /d\), the order of evaluation of \(b \times c\) and \(a/d\) does not matter.</p> <p>Therefore, we choose the order of evaluation that minimises the number of registers so that we don’t store an intermediate result in the memory.</p> <p>In the algorithm, we traverse the expression tree bottom up and label each node with the minimum number of registers needed to evaluate the subexpression rooted at the node. Then, we traverse the tree top down and generate the code. Suppose we have</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>				op
			/    \
		l1      l2
</code></pre></div></div> <p>Assume \(l_1 &lt; l_2\). If we evaluate the left subtree first, we need \(l_1\) registers to evaluate it, 1 register to hold its result and \(l_2\) registers to evaluate the right subtree. Therefore, the total registers used would be \(\max(l_1, l_2 + 1)\) = \(l_2 + 1\). If we follow the other order, we get the number of registers as \(\max(l_2, l_1 + 1) = l_2\). Therefore, we <strong>evaluate the subtree with larger requirements first</strong>. So, we have the following recursion</p> \[label(n) = \begin{cases} 1 &amp; n \text{ is a leaf and must be in a register n or it is a left child} \\ 0 &amp; n \text{ is a leaf and can be in memory or it is a right child} \\ max(label(n_1), label(n_2)) &amp; n \text{ has two child nodes with unequal labels} \\ label(n_1) + 1 &amp; n \text{ has two children with equal labels} \\ \end{cases}\] <p>If we generalise to instructions and trees of higher arity, then for node \(n\) with \(k\) children we get</p> \[label(n) = \max(l_j + j - 1), 1 \leq j \leq k\] <p>Note that the above algorithm generates a store free program. Why aren’t we interleaving the codes of two subtrees? There is a notion of contiguity. Also, we are assuming that memory does not add any additional overhead in terms of CPU cycles.</p> <p>How do we generate the code for the tree? <code class="language-plaintext highlighter-rouge">rstack</code> is a stack of registers, and <code class="language-plaintext highlighter-rouge">gencode(n)</code> generates the code such that the result of the subtree rooted at \(n\) is contained in <code class="language-plaintext highlighter-rouge">top(rstack)</code>. <code class="language-plaintext highlighter-rouge">tstack</code> is a stack of temporaries used when the algorithm runs out of registers. <code class="language-plaintext highlighter-rouge">swap(rstack)</code> swaps the two top registers in <code class="language-plaintext highlighter-rouge">rstack</code>. Finally, the procedure <code class="language-plaintext highlighter-rouge">emit</code> emits a single statement of the generated code.</p> <p>Then, we have</p> <table> <thead> <tr> <th>Cases</th> <th><code class="language-plaintext highlighter-rouge">gencode(n)</code></th> </tr> </thead> <tbody> <tr> <td>\(n\) is a left leaf</td> <td><code class="language-plaintext highlighter-rouge">emit(top(rstack) = name)</code>. This invariant that the top of the <code class="language-plaintext highlighter-rouge">rstack</code> is the result is maintained.</td> </tr> <tr> <td>The right child of \(n\) is a leaf</td> <td><code class="language-plaintext highlighter-rouge">emit(top(rstack) = top(rstack) op r.name)</code></td> </tr> <tr> <td>\(l_1 \geq l_2\) for children \(l_1, l_2\) of \(n\)</td> <td><code class="language-plaintext highlighter-rouge">gencode(n_1)</code><br/><code class="language-plaintext highlighter-rouge">R = pop(rstack)</code><br/><code class="language-plaintext highlighter-rouge">gencode(n_2)</code><br/><code class="language-plaintext highlighter-rouge">emit(R = R op top(rstack))</code><br/><code class="language-plaintext highlighter-rouge">push(R, rstack)</code></td> </tr> <tr> <td>\(l_1 &lt; l_2\)</td> <td><code class="language-plaintext highlighter-rouge">swap(rstack)</code><br/><code class="language-plaintext highlighter-rouge">gencode(n_2)</code><br/><code class="language-plaintext highlighter-rouge">R = pop(rstack)</code><br/><code class="language-plaintext highlighter-rouge">gencode(n_1)</code><br/><code class="language-plaintext highlighter-rouge">emit(top(rstack) = top(rstack) op R</code><br/><code class="language-plaintext highlighter-rouge">push(R, rstack)</code><br/><code class="language-plaintext highlighter-rouge">swap(rstack)</code></td> </tr> <tr> <td>Both children need more registers than available</td> <td><code class="language-plaintext highlighter-rouge">gencode(n_2)</code><br/><code class="language-plaintext highlighter-rouge">T = pop(tstack)</code><br/><code class="language-plaintext highlighter-rouge">emit(T = top(rstack)</code><br/><code class="language-plaintext highlighter-rouge">gencode(n_1)</code><br/><code class="language-plaintext highlighter-rouge">emit(top(rstack) = top(rstack) op T)</code><br/><code class="language-plaintext highlighter-rouge">push(R, rstack)</code><br/></td> </tr> </tbody> </table> <p>In the above, <code class="language-plaintext highlighter-rouge">R</code> can be seen as a local static variable of the procedure. In the last case, we evaluated the right child first because only the right child can be a memory argument.</p> <h1 id="lecture-24">Lecture 24</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">13-04-22</code></p> </blockquote> <h2 id="analysis-of-sethi-ullman-algorithm">Analysis of Sethi-Ullman algorithm</h2> <p>The register usage in the code fragment for a tree rooted at \(n\) can be described by</p> <ul> <li>\(R(n)\) the number of registers used by the code</li> <li>\(L(n)\) the number of registers live after the code (the intermediate results that are required later)</li> <li>The algorithm minimises \(R(n)\) to avoid storing intermediate results.</li> </ul> <p>If the code computes the left child \(n_1\) first then, \(R(n) = \max(R(n_1), L(n_1) + R(n_2))\) and vice versa for the right child \(n_2\). In order to minimise \(R(n)\), we minimise \(L(n_1)\) and \(L(n_2)\). How do we do that?</p> <ul> <li> <p><strong>Contiguous Evaluation</strong> - We evaluate \(n_1\) completely before evaluating \(n_2\) or vice versa. The reason this minimises the registers is that when we evaluate a subtree completely, we need to hold only the final result in a register during the evaluation of the other subtrees. Otherwise, we may have to hold multiple intermediate results in a register during the evaluation of the other subtree.</p> </li> <li> <p><strong>Strongly Contiguous Evaluation</strong> All subtrees of the children are also evaluated contiguously.</p> </li> </ul> <p>In Sethi-Ullman algorithm, each node is processed exactly, and hence the algorithm is linear in the size of the tree. Also, recall that we always evaluate the lowermost subtrees first for optimisation as mentioned somewhere before.</p> <h3 id="arguing-the-optimality">Arguing the Optimality</h3> <p>We define a node \(n\) to be a</p> <ul> <li><strong>dense node</strong> - if \(label(n) \geq k\)</li> <li><strong>major node</strong> - if both of its children are dense. A major node falls in case 5 of the algorithm.</li> </ul> <p>where \(k\) is the number of registers, and \(label\) refers to the number of registers required at this node. Note that every major node is dense but not vice-versa. The parent of every dense node is dense but the parent of every major node need not be major! Also, these categories are dynamic. That is, when we store a dense node, the parent of this node can cease to be a major node. The major nodes decrease by <strong>at most 1</strong> when we store a node.</p> <p>Now, the algorithm generates</p> <ul> <li>exactly one instruction per operator node</li> <li>exactly one load per left leaf</li> <li>no load for any right leaf</li> </ul> <p>The algorithm is optimal with regards to these counts. The optimality now depends on not introducing extra stores. Consider an expression tree with \(m\) major nodes.</p> <ul> <li>A store can reduce the number of major nodes by at most one. This is because, the node that becomes non-major, still remains a dense node so its parents remain a major node.</li> <li>Hence, the tree would need at least \(m\) stores regardless of the algorithm used for generating code</li> <li>The algorithm generates a single store for every major node as part of Case 5, thus it generates exactly \(m\) stores</li> <li>Since this is the smallest number of stores possible, the algorithm is optimal.</li> </ul>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[A course covering the details of the inner workings of a compiler. We start off with scanning ,parsing and semantic analysis to generate ASTs. Then, we discuss IR generator to create TAC. Lastly, we discuss a few register allocation algorithms.]]></summary></entry><entry><title type="html">Operating System Notes</title><link href="https://sudhansh6.github.io/blog/operating-systems/" rel="alternate" type="text/html" title="Operating System Notes"/><published>2021-09-30T00:00:00+00:00</published><updated>2021-09-30T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/operating-systems</id><content type="html" xml:base="https://sudhansh6.github.io/blog/operating-systems/"><![CDATA[<h1 id="lecture-1---introduction">Lecture 1 - Introduction</h1> <h2 id="what-is-an-operating-system">What is an operating system?</h2> <p>An operating system is a <strong>middleware</strong> between user programs and system hardware.</p> <table> <thead> <tr> <th style="text-align: center">User Programs</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><strong>OS</strong></td> </tr> <tr> <td style="text-align: center"><strong>Hardware</strong>: CPU, Memory, Disk, I/O</td> </tr> </tbody> </table> <p>An operating system manages hardware: CPU, Main Memory, IO devices (disk, network, card, mouse, keyboard, etc.)</p> <h2 id="what-happens-when-you-run-a-program-background">What happens when you run a program? (Background)</h2> <p>A compiler translated high level programs into an <strong><em>executable</em></strong> (“.c” to “a.out”)</p> <p>The executable contains instructions that the CPU can understand and the program’s data (all numbered with addresses).</p> <p>Instructions run on CPU: hardware implements an <strong><em>Instruction Set Architecture</em></strong> (ISA).</p> <p>CPU also consists of a few registers, e.g.,</p> <ul> <li>Pointer to current instruction (PC)</li> <li>Operands of the instructions, memory addresses</li> </ul> <p>To run an exe, the CPU does the following:</p> <ol> <li>Fetches instruction ‘pointed at’ by PC from memory</li> <li>Loads data required by the instructions into registers</li> <li>Decodes and executes the instruction</li> <li>Stores the results to memory</li> </ol> <p>Most recently used instructions and data are in CPU <strong>cache</strong> (instruction cache and data cache) for faster access.</p> <h2 id="what-does-the-os-do">What does the OS do?</h2> <h3 id="os-manages-cpu">OS manages CPU</h3> <p>It initializes program counter (PC) and other registers to begin execution. OS provides the <strong>process abstraction</strong>.</p> <p><strong><em>Process:</em></strong> A running program</p> <p>OS creates and manages processes. Each process has the illusion of having the complete CPU, i.e., OS <strong><em>virtualizes</em></strong> CPU. It <em>timeshares</em> the CPU between processes. It also enables coordination between processes.</p> <h3 id="os-manages-memory">OS manages memory</h3> <p>It loads the program executable (code, data) from disk to memory. It has to manage code, data, stack, heap, etc. Each process thinks it has a dedicated memory space for itself, numbers code, and data starting from 0 (<strong>virtual addresses</strong>).</p> <p>The operating system abstracts out the details of the actual placement in memory, translates from virtual addresses to real physical addresses.</p> <p>Hence, the process does not have to worry about where its memory is allocated in the physical space.</p> <h3 id="os-manages-devices">OS manages devices</h3> <p>OS helps in reading/writing files from the disk. OS has code to manage disk, network card, and other external devices: <strong><em>device drivers</em></strong>.</p> <p><strong><em>Device driver:</em></strong> Talks the language of the hardware devices.</p> <p>It issues instructions to devices (fetching data from a file). It also responds to interrupt events from devices (pressing a key on the keyboard).</p> <p>The persistent (ROM) data is organised as a <strong><em>file system</em></strong> on the disk.</p> <h2 id="design-goals-of-an-operating-system">Design goals of an operating system</h2> <ul> <li><strong>Convenience</strong>, <strong>abstraction of hardware</strong> resources for user programs.</li> <li><strong>Efficiency</strong> of usage of CPU, memory, etc.</li> <li><strong>Isolation</strong> between multiple processes.</li> </ul> <h2 id="history-of-operating-systems">History of operating systems</h2> <p>OS started out as a library to provide common functionality across programs. Later, it evolved from procedure calls to <strong><em>system calls</em></strong>.</p> <p>When a system call is made to run OS code, the CPU executes at a <em>higher privilege level</em>.</p> <p>OS evolved from running a single program to executing <em>multiple processes concurrently</em>.</p> <h1 id="lecture-2---the-process-abstraction">Lecture 2 - The Process Abstraction</h1> <p>An operating system provides <strong>process abstraction</strong>. That is, when you execute a program, the OS creates a <em>process</em>. It timeshares CPU across multiple processes (virtualizing CPU). An OS also has a <strong><em>CPU scheduler</em></strong> that picks one of the many active processes to execute on a CPU. This scheduler has 2 components.</p> <ol> <li><strong>Policy</strong> - It decides which process to run on the CPU</li> <li><strong>Mechanism</strong> - How to “context-switch” between processes</li> </ol> <h2 id="what-constitutes-a-process">What constitutes a process?</h2> <p>Every process has a <strong><em>unique identifier (PID)</em></strong> and a <strong><em>memory image</em></strong> - the fragments of the program present in the memory. As mentioned earlier, a memory image has 4 components (code, data, stack, and heap).</p> <p>When a process is running, a process also has a <strong><em>CPU context</em></strong> (registers). This has components such as program counter, current operands, and stack pointer. These basically store the state of the process.</p> <p>A process also has <strong><em>File descriptors</em></strong> - pointers to open files and devices.</p> <h2 id="how-does-an-os-create-a-process">How does an OS create a process?</h2> <p>Initially, the OS allocates and creates a memory image. It loads the code and data from the disk executable. Then, it makes a runtime stack and heap.</p> <p>After this, the OS opens essential files (<code class="language-plaintext highlighter-rouge">STD IN</code>, <code class="language-plaintext highlighter-rouge">STD OUT</code>, <code class="language-plaintext highlighter-rouge">STD ERR</code>). Then the CPU registers are initialized, and the PC points to the first instruction.</p> <h2 id="states-of-a-process">States of a process</h2> <ul> <li> <p>A process that is currently being executed in the CPU is <strong>Running</strong>.</p> </li> <li> <p>Processes that are waiting to be scheduled are <strong>Ready</strong>. These are not yet executed.</p> </li> <li> <p>Some processes may be in the <strong>Blocked</strong> state. They are suspended and not ready to run. These processes may be waiting for some event, e.g., waiting for input from the user. They are unblocked once an <em>interrupt is issued</em>.</p> </li> <li> <p><strong>New</strong> processes are being created and are yet to run.</p> </li> <li> <p><strong>Dead</strong> processes have finished executing and are terminated.</p> <table> <thead> <tr> <th style="text-align: center"><img src="/assets/img/Operating Systems/image-20210726215935642.png" alt="image-20210726215935642"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Process State Transitions</em></td> </tr> </tbody> </table> </li> </ul> <p>When one process is blocked, another process can be executed to utilize the resources effectively. Here is a simple example reflecting this situation.</p> <table> <thead> <tr> <th style="text-align: center"><img src="/assets/img/Operating Systems/image-20210726220829735.png" alt="image-20210726220829735"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Example: Process States</em></td> </tr> </tbody> </table> <h2 id="os-data-structures">OS data structures</h2> <p>An operating system maintains a data structure of all active processes. One such structure is the <strong><em>process control block (PCB)</em></strong>. Information about each process is stored in a PCB.</p> <p>Different Operating Systems might use other names for this structure. PCB is the generic name. All the processes running on the system are stored in a list of PCBs.</p> <p>A PCB has the following components of a process:</p> <ul> <li>Process identifier</li> <li>Process state</li> <li>Pointers to other related processes (parent)</li> <li>CPU context of the process (saved when the process is suspended)</li> <li>Pointers to memory locations</li> <li>Pointers to open files</li> </ul> <h1 id="lecture-21---xv6-introduction-and-x86-background">Lecture 21 - xv6 introduction and x86 background</h1> <p>xv6 is a simple OS used for teaching OS concepts. We shall be using the x86 version of the OS.</p> <p>An OS enables users to run the processed stored in memory on the CPU.</p> <ul> <li>It loads the process code/data in main memory.</li> <li>CPU fetches, decodes, executes instructions in program code.</li> <li>It fetches the process data from memory to CPU registers for faster access during instruction execution (We studied in Arch that memory access is expensive).</li> <li>Recently fetched code/data is stored in the CPU in the form of cache for future access.</li> </ul> <h2 id="memory-image-of-a-process">Memory Image of a process</h2> <p>The memory image of a process consists of</p> <ul> <li>Compiled code (CPU instructions)</li> <li>Global/static variables (memory allocated at compile time)</li> <li>Heap (dynamic memory allocation via, <code class="language-plaintext highlighter-rouge">malloc</code>, <code class="language-plaintext highlighter-rouge">new</code> etc) that grows (up) on demand.</li> <li>Stack (temporary storage during function calls, e.g., local variables) that usually grows “up” towards lower addresses. It shrinks “down” as memory is freed (exiting function call).</li> <li>Other things like shared libraries.</li> </ul> <p>Every instruction/data has an address, used by the CPU to fetch/store (<em>Virtual addresses</em> managed by OS). Consider the following example</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="o">*</span><span class="n">iptr</span> <span class="o">=</span> <span class="n">malloc</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">))</span>
</code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">iptr</code> itself is on stack but it points to 4 bytes (size of <code class="language-plaintext highlighter-rouge">int</code>) in the heap.</p> <h2 id="x86-registers">x86 registers</h2> <p>Registers are a small space for data storage with the CPU. Every CPU architecture has its set of registers used during computation. The names of these registers vary across different architectures. These are the common types of registers:</p> <ul> <li>General purpose registers - stores data during computations (<code class="language-plaintext highlighter-rouge">eax, ebx, ecx, edx, esi, edi</code>).</li> <li>Pointers to stack locations - base of stack (<code class="language-plaintext highlighter-rouge">ebp</code>) and top of stack (<code class="language-plaintext highlighter-rouge">esp</code>).</li> <li>Program counter or instruction pointer (<code class="language-plaintext highlighter-rouge">eip</code>) - Next instruction to execute.</li> <li>Control registers - Hold control information or metadata of a process (e.g., <code class="language-plaintext highlighter-rouge">cr3</code> has pointer to page table of the process). A page table helps the OS to keep track of the memory of the process.</li> <li>Segment registers (<code class="language-plaintext highlighter-rouge">cs, ds, es, fs, gs, ss</code>) - information about segments (related to memory of process).</li> </ul> <h2 id="x86-instructions">x86 Instructions</h2> <p>Every CPU can execute a set of instructions defined in its ISA. The compiled code is written using this ISA so that the CPU can execute these instructions. Here are some common instructions used:</p> <ul> <li>Load/store - <code class="language-plaintext highlighter-rouge">mov src, dst</code> (AT&amp;T syntax - using <code class="language-plaintext highlighter-rouge">src</code> before <code class="language-plaintext highlighter-rouge">dst</code>) <ul> <li><code class="language-plaintext highlighter-rouge">mov %eax, %ebx</code> - copy contents of <code class="language-plaintext highlighter-rouge">eax</code> to <code class="language-plaintext highlighter-rouge">ebx</code></li> <li><code class="language-plaintext highlighter-rouge">mov (%eax), %ebx</code> - copy contents at the address in <code class="language-plaintext highlighter-rouge">eax</code> into <code class="language-plaintext highlighter-rouge">ebx</code></li> <li><code class="language-plaintext highlighter-rouge">mov 4(%eax), %ebx</code> - copy contents stores at offset of 4 bytes from address stored at <code class="language-plaintext highlighter-rouge">eax</code> into <code class="language-plaintext highlighter-rouge">ebx</code></li> </ul> </li> <li>Push/pop on stack - changes <code class="language-plaintext highlighter-rouge">esp</code> <ul> <li><code class="language-plaintext highlighter-rouge">push %eax</code> - push contents of <code class="language-plaintext highlighter-rouge">eax</code> onto stack, update <code class="language-plaintext highlighter-rouge">esp</code></li> <li><code class="language-plaintext highlighter-rouge">pop %eax</code> - pop top of stack onto <code class="language-plaintext highlighter-rouge">eax</code>, update <code class="language-plaintext highlighter-rouge">esp</code></li> </ul> </li> <li><code class="language-plaintext highlighter-rouge">jmp</code> sets <code class="language-plaintext highlighter-rouge">eip</code> to a specified address</li> <li><code class="language-plaintext highlighter-rouge">call</code> to invoke a function, <code class="language-plaintext highlighter-rouge">ret</code> to return from a function</li> <li>Variants of above (<code class="language-plaintext highlighter-rouge">movw, pushl</code>) for different register sizes.</li> </ul> <h2 id="privilege-levels">Privilege Levels</h2> <p>x86 CPUs have multiple privilege levels called <strong><em>rings</em></strong> (0 to 3). Ring 0 has the highest privilege and OS code runs at this level. Ring 3 has the lowest privilege and user code runs at this level.</p> <p>There are two types of instructions - privileged and unprivileged. <strong>Privileged</strong> instructions perform sensitive operations which ideally should not be performed by user programs. These can be executed by the CPU only when running at the highest privilege level (ring 0) .</p> <p>For example, writing into <code class="language-plaintext highlighter-rouge">cr3</code> register (setting page table) is a privileged instruction. We don’t want a user manipulating memory of another process. instructions to access I/O devices is also privileged.</p> <p>Unprivileged instructions can be run at lower privilege levels.</p> <blockquote> <p><span style="color:red">Even ring 0?</span></p> </blockquote> <p>For example, user code running at a lower privilege level can store a value into a general purpose register.</p> <p>When a user required OS services (system calls), the CPU moves to higher privilege level and executes OS code that contains privileged instructions. User code cannot invoke privileged instructions directly.</p> <h2 id="function-calls-and-the-stack">Function calls and the stack</h2> <p>Local variables and arguments are stored on stack for the duration of a function call. When a function is called:</p> <ul> <li>Arguments are pushed onto the stack.</li> <li><code class="language-plaintext highlighter-rouge">call function</code> - pushes the return address on stack and jumps to function. That is, <code class="language-plaintext highlighter-rouge">eip</code> shifts from top of stack to function implementation.</li> <li>Local variables are allocated on stack</li> <li>Function code is executed</li> <li><code class="language-plaintext highlighter-rouge">ret</code> - instruction pops return address, <code class="language-plaintext highlighter-rouge">eip</code> goes back to the old value.</li> </ul> <blockquote> <p><span style="color:red">What exactly is happening above?</span></p> </blockquote> <p>In this way, stack acts as a temporary storage for function calls.</p> <p>Before making a function call, we may have to store values of some registers. This is because, registers can get clobbered during a function call.</p> <ul> <li>Some registers are saved on stack by <strong>caller</strong> before invoking the function (<em>caller save registers</em>). The function code (<em>callee</em>) can freely change them, and the caller restores them later.</li> <li>Some registers are saved by <strong>callee</strong>, and are restored after function ends (<em>callee save registers</em>). Caller expects them to have same value on return.</li> <li>Return value stored in <code class="language-plaintext highlighter-rouge">eax</code> register by callee (one of caller save registers)</li> </ul> <p>All of the above is automatically done by the C compiler (C calling convention). Every language has a calling convention that decides which registers have to be classified as caller and callee.</p> <blockquote> <p>Caller and Callee both store the registers?</p> </blockquote> <p>Timeline of a function call is as follows (<em>Note.</em> Stack grows up from higher to lower addresses):</p> <ul> <li>Caller save registers are pushed (<code class="language-plaintext highlighter-rouge">eax, ecx, edx</code>)</li> <li>Arguments of the function are pushed in reverse order onto the stack</li> <li>The return address or the old <code class="language-plaintext highlighter-rouge">eip</code> is pushed on stack by the <code class="language-plaintext highlighter-rouge">call</code> instruction</li> <li>The old <code class="language-plaintext highlighter-rouge">ebp</code> is also pushed onto the stack</li> <li>Set <code class="language-plaintext highlighter-rouge">ebp</code> to the current top of the stack (base of new “stack frame” of the function)</li> <li>Push local variables and callee save registers (<code class="language-plaintext highlighter-rouge">ebx, esi, edi</code>). <code class="language-plaintext highlighter-rouge">esp</code> automatically goes up as you push things onto the stack.</li> <li>The function is executed.</li> <li>After the function execution, the current stack frame is popped to restore the old <code class="language-plaintext highlighter-rouge">ebp</code>.</li> <li>The return address is popped and <code class="language-plaintext highlighter-rouge">eip</code> is restored by the <code class="language-plaintext highlighter-rouge">ret</code> instruction.</li> </ul> <p><img src="/assets/img/Operating Systems/Peek 2021-07-31 00-30.gif" alt="function stack"/></p> <p>Stack pointers: <code class="language-plaintext highlighter-rouge">ebp</code> stores the address of base of the current stack frame and <code class="language-plaintext highlighter-rouge">esp</code> stores the address of current top of stack. This way, function arguments are accessible from looking under the stack base pointer.</p> <h2 id="c-vs-assembly-for-os-code">C vs. assembly for OS code</h2> <p>Most of xv6 is in C! The assembly code is automatically generated by the compiler (including all the stack manipulations for function calls).</p> <p>However, small parts of the OS are in assembly language. This is because, the OS needs more controls over what needs to be done in some situations. For example, the logic of switching from stack of one process to stack of another cannot be written in a high-level language.</p> <p>Therefore, basic understanding of x86 assembly language is required to follow some nuances of xv6 code.</p> <h2 id="more-on-cpu-hardware">More on CPU hardware</h2> <p>Some aspects of CPU hardware that are not relevant to studying OS:</p> <ul> <li>CPU cache - CPU stores recently fetched instructions and data in multiple levels of cache. The operating system has no visibility or control intro the CPU cache.</li> <li>Hyper-threading - A CPU core can run multiple processed concurrently via hyper-threading. From an OS perspective, 4-core CPU with 2 hyper-threads per core, and 8-core CPU with no hyper-threading will look the same, even though the performance may differ. The OS will schedule processes in parallel on the 8 available processors.</li> </ul> <h1 id="lecture-22-processes-in-xv6">Lecture 22: Processes in xv6</h1> <h2 id="the-process-abstraction">The process abstraction</h2> <p>The OS is responsible for concurrently running multiple processes (on one or more CPU cores/processors)</p> <ul> <li>Create, run, terminate a process</li> <li>Context switch from one process to another</li> <li>Handle any events (e.g., system calls from process)</li> </ul> <p>OS maintains all information about an active process in a process control block (PCB)</p> <ul> <li>Set of PCBs of all active processes is a critical kernel data structure</li> <li>Maintained as part of kernel memory (part of RAM that stores kernel code and data, more on this later)</li> </ul> <p>PCB is known by different names in different OS</p> <ul> <li><code class="language-plaintext highlighter-rouge">structproc</code> in xv6</li> <li><code class="language-plaintext highlighter-rouge">task_struct</code> in Linux</li> </ul> <h2 id="pcb-in-xv6-struct-proc">PCB in xv6: struct proc</h2> <p>The different states of a process in xv6 (<code class="language-plaintext highlighter-rouge">procstate</code>) are given by <code class="language-plaintext highlighter-rouge">UNUSED, EMBRYO (new), SLEEPING (blocked), RUNNABLE (ready), RUNNING, ZOMBIE (dead)</code></p> <p>The <code class="language-plaintext highlighter-rouge">struct proc</code> has</p> <ul> <li>Size of the process</li> <li>Pointer to the apge table</li> <li>Bottom of the kernel stack for this process</li> <li>Process state</li> <li>Process ID</li> <li>Parent process</li> <li>Pointer to folder in which process is running</li> <li>Some more stuff which we will study later</li> </ul> <h3 id="kernel-stack">Kernel Stack</h3> <p>Register state (CPU context) is saved on user stack during the function calls to restore/resume later. Likewise, the CPU context is stored on <strong><em>kernel stack</em></strong> when process jumps into OS to run kernel code.</p> <p>We use a separate stack because the OS does not trust the user stack. It is a separate area of memory per process within the kernel, not accessible by regular user code. It is linked from <code class="language-plaintext highlighter-rouge">struct proc</code> of a process.</p> <h3 id="list-of-open-files">List of open files</h3> <p>Array of pointers to open files (<code class="language-plaintext highlighter-rouge">struct file</code> has info about the open file)</p> <ul> <li>When user opens a file, a new entry is created in this array, and the index of that entry is passed as a file descriptor to user</li> <li>Subsequent read/write calls on a file use this file descriptor to refer to the file</li> <li> <p>First 3 files (array indices 0,1,2) open by default for every process: standard input, output and error</p> </li> <li>Subsequent files opened by a process will occupy later entries in the array</li> </ul> <h3 id="page-table">Page table</h3> <p>Every instruction or data item in the memory image of process has an address. Page table of a process maintains a mapping between the virtual addresses and physical addresses.</p> <h2 id="process-table-ptable-in-xv6">Process table (<code class="language-plaintext highlighter-rouge">ptable</code>) in xv6</h2> <p>It has a lock for protection. It is an array of all processes. Real kernels have dynamic-sized data structures. However, xv6 being a dummy OS, has a static array.</p> <p>A CPU scheduler in the OS loops over all runnable processes, picks one, and sets it running on the CPU.</p> <h2 id="process-state-transtition-examples">Process state transtition examples</h2> <p>A process that needs to sleep will set its state to <code class="language-plaintext highlighter-rouge">SLEEPING</code> and invoke scheduler.</p> <p>A process that has run for its fair share will set itself to <code class="language-plaintext highlighter-rouge">RUNNABLE</code> and invoke Scheduler. The Scheduler will once again find another <code class="language-plaintext highlighter-rouge">RUNNABLE</code> process and set it to <code class="language-plaintext highlighter-rouge">RUNNING</code>.</p> <h1 id="live-session-1">Live Session 1</h1> <ul> <li><em>Real memory</em> is less than <em>virtual memory</em>. It is easier to let the process think it has the whole memory rather than telling it how much memory it exactly has.</li> <li>Memory for Global variables and Function variables is allocated once! We don’t know how many times each function will be called. Therefore, we just assign it once and use the allocated space for repeated calls.</li> <li>We have <em>caller</em> and <em>callee</em> registers for storing existing computations in the registers before a function call. We can’t have only one set (callee or caller) store all these values due to some subtle reasons. A caller save register would have to pass some arguments. A callee save register would have to return some arguments. To avoid all this, we have a separate set of caller and callee registers.</li> <li>“Only one process can run on a core at any time”. Basically that the OS sees the processor as two different cores in hyper-threading. Therefore, it can run a <em>single</em> process on a <em>core</em>.</li> <li><em>xv6</em> is primarily written in C. If we need an OS to compile and run a program, how do we run xv6? Whenever you boot a system with an OS, you use an existing OS to build the binaries for the needed OS. Then, you run the binary to run the OS. To answer the chicken and egg problem, someone might’ve written an OS in assembly code initially.</li> <li>OS keeps track of locations of memory images using <strong>page tables</strong>.</li> <li>Virtual addresses of an array will be contiguous, but the OS may not allocate contiguous memory.</li> <li>When we print the address of a pointer, we get the <strong>virtual address</strong> of the variable. Exposing the real address is a security risk.</li> </ul> <h1 id="lecture-3---process-api">Lecture 3 - Process API</h1> <p>We will discuss the API that the OS provides to create and manage processes.</p> <h2 id="what-is-the-api">What is the API?</h2> <p>So, the API refers to the functions available to write user programs. The API provided by the OS is a set of <strong>system calls</strong>. Recall, a system call is like a function call, but it runs at a higher privilege level. System calls can perform sensitive operations like access to hardware. Some “blocking” system calls cause the process to be blocked and unscheduled (e.g., <code class="language-plaintext highlighter-rouge">read</code> from disk).</p> <h2 id="do-we-have-to-rewrite-programs-for-each-os">Do we have to rewrite programs for each OS?</h2> <p>No, we don’t. This is possible due to the <strong><em>POSIX API</em></strong>. This is a standard set of system calls that OS must implement. Programs written to the POSIX API can run on any POSIX compatible OS. Almost every modern OS has this implemented, which ensures program portability. Program language libraries hide the details of invoking system calls. This way, the user does not have to worry about explicitly invoking system calls.</p> <p>For ex, the <code class="language-plaintext highlighter-rouge">printf</code> function in the C library calls the <code class="language-plaintext highlighter-rouge">write</code> system call to write to the screen.</p> <h2 id="process-related-system-calls-in-unix">Process related system calls (in Unix)</h2> <p>The most important system call to create a process is the <strong><code class="language-plaintext highlighter-rouge">fork()</code></strong> system call. It creates a new <strong>child</strong> process. All processes are created by forking from a parent. The <strong><code class="language-plaintext highlighter-rouge">init</code></strong> process is the ancestor of all processes. When the OS boots up, it creates the <code class="language-plaintext highlighter-rouge">init</code> process.</p> <p><strong><code class="language-plaintext highlighter-rouge">exec()</code></strong> makes a process execute a given executable. <strong><code class="language-plaintext highlighter-rouge">exit()</code></strong> terminates a process, and <strong><code class="language-plaintext highlighter-rouge">wait()</code></strong> causes a parent to block until a child terminates. Many variants of the above system calls exist with different arguments.</p> <h2 id="what-happens-during-a-fork">What happens during a fork?</h2> <p>A new process is created by making a copy of the parent’s memory image. This means the child’s code is exactly the same as the parent’s code. The new process is added to the <a href="#os-data-structures">OS process list</a> and scheduled. Parent and child start execution just after the fork statement (with different return values).</p> <p>Note that parent and child execute and modify the memory data independently (the memory images being a copy of one another does not propagate).</p> <p>The return values for <code class="language-plaintext highlighter-rouge">fork()</code> are set as follows:</p> <ul> <li><code class="language-plaintext highlighter-rouge">0</code> for the child process</li> <li><code class="language-plaintext highlighter-rouge">&lt; 0</code> if <code class="language-plaintext highlighter-rouge">fork</code> failed</li> <li><code class="language-plaintext highlighter-rouge">PID of the child</code> in the parent</li> </ul> <h2 id="terminating-child-processes">Terminating child processes</h2> <p>A process can be terminated in the following situations</p> <ul> <li>The process calls <code class="language-plaintext highlighter-rouge">exit()</code> (<code class="language-plaintext highlighter-rouge">exit()</code> is called automatically when the end of main is reached)</li> <li>OS terminated a misbehaving process</li> </ul> <p>The processes are not immediately deleted from the process list upon termination. They exist as zombies. These are cleared out when a parent calls <code class="language-plaintext highlighter-rouge">wait(NULL)</code>. A zombie child is then cleaned up or “reaped”.</p> <p><code class="language-plaintext highlighter-rouge">wait()</code> <strong>blocks the parent</strong> until the child terminates. There are some non-blocking ways to invoke wait.</p> <p>What if the parent terminates before its child? <code class="language-plaintext highlighter-rouge">init</code> process adopts orphans and reaps them. Dark, right? If the <code class="language-plaintext highlighter-rouge">init</code> process does not do this, zombies will eat up the system memory. Why do we need zombies? (Too many brains in the world). There are subtle reasons for this, which are out of scope for this discussion.</p> <h2 id="what-happens-during-exec">What happens during exec?</h2> <p>After forking, the parent and the child are running the same code. This is not useful! A process can run <code class="language-plaintext highlighter-rouge">exec()</code> to <strong>load</strong> another executable to its memory image. This allows a child to run a different program from the parent. There are variants of <code class="language-plaintext highlighter-rouge">exec()</code>, e.g., <code class="language-plaintext highlighter-rouge">execvp()</code>, to pass command-line arguments to the new executable.</p> <h2 id="case-study-how-does-a-shell-work">Case study: How does a shell work?</h2> <p>In a basic OS, the <code class="language-plaintext highlighter-rouge">init</code> process is created after the initialization of hardware. The <code class="language-plaintext highlighter-rouge">init</code> spawns a lot of new processes like <code class="language-plaintext highlighter-rouge">bash</code>. <strong><code class="language-plaintext highlighter-rouge">bash</code></strong> is a shell. A shell reads user commands, forks a child, execs the command executable, waits for it to finish, and reads the next command.</p> <p>Standard commands like <code class="language-plaintext highlighter-rouge">ls</code> are all executables that are simply <code class="language-plaintext highlighter-rouge">exec</code>‘ed by the shell.</p> <h2 id="more-funky-things-about-the-shell">More funky things about the shell</h2> <p>A shell can manipulate the child in strange ways. For example, you can redirect the output from a command to a file.</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>prompt <span class="o">&gt;</span> <span class="nb">ls</span> <span class="o">&gt;</span> foo.txt
</code></pre></div></div> <p>This is done via spawning a child, rewires its standard output to a file, and then calls the executable.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">close</span><span class="p">(</span><span class="n">STDOUT_FILENO</span><span class="p">);</span>
<span class="n">open</span><span class="p">(</span><span class="s">"./p4.output"</span><span class="p">,</span> <span class="n">O_CREAT</span><span class="o">|</span><span class="n">O_WRONLY</span><span class="o">|</span><span class="n">O_TRUNC</span><span class="p">,</span> <span class="n">S_IRWXU</span><span class="p">);</span>
</code></pre></div></div> <p>We can similarly modify the input for a process.</p> <h1 id="lecture-23---system-calls-for-process-management-in-xv6">Lecture 23 - System calls for process management in xv6</h1> <h2 id="process-system-calls-shell">Process system calls: Shell</h2> <p>When xv6 boots up, it starts the init process (the first user process). Init forks shell, which prompts for input. This shell process is the main screen that we see when we run xv6.</p> <p>Whenever we run a command in the shell, the shell creates a new child process, executes it, waits for the child to terminate, and repeats the whole process again. Some commands have to be executed by the parent process itself and <strong>not by the child</strong>. For example, <code class="language-plaintext highlighter-rouge">cd</code> command should change the parent’s (shell) current directory, not of the child. Such commands are directly executed by the shell itself without forking a new process.</p> <h2 id="what-happens-on-a-system-call">What happens on a system call?</h2> <p>All the system calls available to the users are defined in the user library header <strong>‘user.h’</strong>. This is equivalent to a C library header (xv6 doesn’t use a standard C library). System call implementation invokes a special <strong>trap</strong> instruction called <strong><code class="language-plaintext highlighter-rouge">int</code></strong> in x86. All the system calls are defined in “usys.S”.</p> <p>The trap (int) instruction causes a jump to kernel code that handles the system call. Every system call is associated with a number which is moved into <code class="language-plaintext highlighter-rouge">eax</code> to let the kernel run the applicable code. We’ll learn more about this later, so don’t worry about this now.</p> <h2 id="fork-system-call">Fork system call</h2> <p>Parent allocates new process in <code class="language-plaintext highlighter-rouge">ptable</code>, copies parent state to the child. The child process set is set to runnable, and the scheduler runs it at a later time. Here is the implementation of <code class="language-plaintext highlighter-rouge">fork()</code></p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span>
<span class="nf">fork</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">i</span><span class="p">,</span> <span class="n">pid</span><span class="p">;</span>
    <span class="k">struct</span> <span class="n">proc</span> <span class="o">*</span><span class="n">np</span><span class="p">;</span>
    <span class="k">struct</span> <span class="n">proc</span> <span class="o">*</span><span class="n">curproc</span> <span class="o">=</span> <span class="n">myproc</span><span class="p">();</span>
    <span class="c1">// Allocate process.</span>
    <span class="k">if</span><span class="p">((</span><span class="n">np</span> <span class="o">=</span> <span class="n">allocproc</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">){</span>
        <span class="k">return</span> <span class="err">−</span><span class="mi">1</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="c1">// Copy process state from proc.</span>
    <span class="k">if</span><span class="p">((</span><span class="n">np</span><span class="err">−</span><span class="o">&gt;</span><span class="n">pgdir</span> <span class="o">=</span> <span class="n">copyuvm</span><span class="p">(</span><span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">pgdir</span><span class="p">,</span> <span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">sz</span><span class="p">))</span> <span class="o">==</span> <span class="mi">0</span><span class="p">){</span>
        <span class="n">kfree</span><span class="p">(</span><span class="n">np</span><span class="err">−</span><span class="o">&gt;</span><span class="n">kstack</span><span class="p">);</span>
        <span class="n">np</span><span class="err">−</span><span class="o">&gt;</span><span class="n">kstack</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="n">np</span><span class="err">−</span><span class="o">&gt;</span><span class="n">state</span> <span class="o">=</span> <span class="n">UNUSED</span><span class="p">;</span>
        <span class="k">return</span> <span class="err">−</span><span class="mi">1</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="n">np</span><span class="err">−</span><span class="o">&gt;</span><span class="n">sz</span> <span class="o">=</span> <span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">sz</span><span class="p">;</span>
    <span class="n">np</span><span class="err">−</span><span class="o">&gt;</span><span class="n">parent</span> <span class="o">=</span> <span class="n">curproc</span><span class="p">;</span>
    <span class="o">*</span><span class="n">np</span><span class="err">−</span><span class="o">&gt;</span><span class="n">tf</span> <span class="o">=</span> <span class="o">*</span><span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">tf</span><span class="p">;</span>
    <span class="c1">// Clear %eax so that fork returns 0 in the child.</span>
    <span class="n">np</span><span class="err">−</span><span class="o">&gt;</span><span class="n">tf</span><span class="err">−</span><span class="o">&gt;</span><span class="n">eax</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NOFILE</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
        <span class="k">if</span><span class="p">(</span><span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">ofile</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">np</span><span class="err">−</span><span class="o">&gt;</span><span class="n">ofile</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">filedup</span><span class="p">(</span><span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">ofile</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="n">np</span><span class="err">−</span><span class="o">&gt;</span><span class="n">cwd</span> <span class="o">=</span> <span class="n">idup</span><span class="p">(</span><span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">cwd</span><span class="p">);</span>
    <span class="n">safestrcpy</span><span class="p">(</span><span class="n">np</span><span class="err">−</span><span class="o">&gt;</span><span class="n">name</span><span class="p">,</span> <span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">name</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">name</span><span class="p">));</span>
    <span class="c1">// Set new pid</span>
    <span class="n">pid</span> <span class="o">=</span> <span class="n">np</span><span class="err">−</span><span class="o">&gt;</span><span class="n">pid</span><span class="p">;</span>
    <span class="n">acquire</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">lock</span><span class="p">);</span>
    <span class="c1">// Set Process state to runnable</span>
    <span class="n">np</span><span class="err">−</span><span class="o">&gt;</span><span class="n">state</span> <span class="o">=</span> <span class="n">RUNNABLE</span><span class="p">;</span>
    <span class="n">release</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">lock</span><span class="p">);</span>
    <span class="c1">// Fork system call returns with child pid in parent</span>
    <span class="k">return</span> <span class="n">pid</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="exec-system-call">Exec system call</h2> <p>The source code is a little bit complicated. The key steps include</p> <ul> <li>Copy new executable into memory, replacing the existing memory image</li> <li>Create new stack, heap</li> <li>Switch process page table to use the new memory image</li> <li>Process begins to run new code after system call ends</li> </ul> <h2 id="exit-system-call">Exit system call</h2> <p>Exiting a process cleans up the state and passes abandoned children to <code class="language-plaintext highlighter-rouge">init</code>. It marks the current process as a zombie and invokes the scheduler.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span>
<span class="nf">exit</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">struct</span> <span class="n">proc</span> <span class="o">*</span><span class="n">curproc</span> <span class="o">=</span> <span class="n">myproc</span><span class="p">();</span>
    <span class="k">struct</span> <span class="n">proc</span> <span class="o">*</span><span class="n">p</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">fd</span><span class="p">;</span>
    <span class="k">if</span><span class="p">(</span><span class="n">curproc</span> <span class="o">==</span> <span class="n">initproc</span><span class="p">)</span>
        <span class="n">panic</span><span class="p">(</span><span class="s">"init exiting"</span><span class="p">);</span>
    <span class="c1">// Close all open files.</span>
    <span class="k">for</span><span class="p">(</span><span class="n">fd</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">fd</span> <span class="o">&lt;</span> <span class="n">NOFILE</span><span class="p">;</span> <span class="n">fd</span><span class="o">++</span><span class="p">){</span>
        <span class="k">if</span><span class="p">(</span><span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">ofile</span><span class="p">[</span><span class="n">fd</span><span class="p">]){</span>
            <span class="n">fileclose</span><span class="p">(</span><span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">ofile</span><span class="p">[</span><span class="n">fd</span><span class="p">]);</span>
            <span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">ofile</span><span class="p">[</span><span class="n">fd</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">begin_op</span><span class="p">();</span>
    <span class="n">iput</span><span class="p">(</span><span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">cwd</span><span class="p">);</span>
    <span class="n">end_op</span><span class="p">();</span>
    <span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">cwd</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">acquire</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">lock</span><span class="p">);</span>
    <span class="c1">// Parent might be sleeping in wait().</span>
    <span class="n">wakeup1</span><span class="p">(</span><span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">parent</span><span class="p">);</span>
    <span class="c1">// Pass abandoned children to init.</span>
    <span class="k">for</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="n">ptable</span><span class="p">.</span><span class="n">proc</span><span class="p">;</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">proc</span><span class="p">[</span><span class="n">NPROC</span><span class="p">];</span> <span class="n">p</span><span class="o">++</span><span class="p">){</span>
        <span class="k">if</span><span class="p">(</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">parent</span> <span class="o">==</span> <span class="n">curproc</span><span class="p">){</span>
            <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">parent</span> <span class="o">=</span> <span class="n">initproc</span><span class="p">;</span>
            <span class="k">if</span><span class="p">(</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">state</span> <span class="o">==</span> <span class="n">ZOMBIE</span><span class="p">)</span>
                <span class="n">wakeup1</span><span class="p">(</span><span class="n">initproc</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="c1">// Jump into the scheduler, never to return.</span>
    <span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">state</span> <span class="o">=</span> <span class="n">ZOMBIE</span><span class="p">;</span>
    <span class="c1">// Invoke the scheduler</span>
    <span class="n">sched</span><span class="p">();</span>
    <span class="n">panic</span><span class="p">(</span><span class="s">"zombie exit"</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <p>Remember, the complete cleanup happens only when a parent reaps the child.</p> <h2 id="wait-system-call">Wait system call</h2> <p>It must be called to clean up the child processes. It searches for dead children in the process table. If found, it cleans up the memory and returns the PID of the dead child. Otherwise, it sleeps until one dies.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span>
<span class="nf">wait</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">struct</span> <span class="n">proc</span> <span class="o">*</span><span class="n">p</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">havekids</span><span class="p">,</span> <span class="n">pid</span><span class="p">;</span>
    <span class="k">struct</span> <span class="n">proc</span> <span class="o">*</span><span class="n">curproc</span> <span class="o">=</span> <span class="n">myproc</span><span class="p">();</span>
    <span class="n">acquire</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">lock</span><span class="p">);</span>
    <span class="k">for</span><span class="p">(;;){</span>
        <span class="c1">// Scan through table looking for exited children.</span>
        <span class="n">havekids</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="k">for</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="n">ptable</span><span class="p">.</span><span class="n">proc</span><span class="p">;</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">proc</span><span class="p">[</span><span class="n">NPROC</span><span class="p">];</span> <span class="n">p</span><span class="o">++</span><span class="p">){</span>
            <span class="k">if</span><span class="p">(</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">parent</span> <span class="o">!=</span> <span class="n">curproc</span><span class="p">)</span>
                <span class="k">continue</span><span class="p">;</span>
            <span class="n">havekids</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
            <span class="k">if</span><span class="p">(</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">state</span> <span class="o">==</span> <span class="n">ZOMBIE</span><span class="p">){</span>
                <span class="c1">// Found one.</span>
                <span class="n">pid</span> <span class="o">=</span> <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">pid</span><span class="p">;</span>
                <span class="n">kfree</span><span class="p">(</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">kstack</span><span class="p">);</span>
                <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">kstack</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
                <span class="n">freevm</span><span class="p">(</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">pgdir</span><span class="p">);</span>
                <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">pid</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
                <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">parent</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
                <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">name</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
                <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">killed</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
                <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">state</span> <span class="o">=</span> <span class="n">UNUSED</span><span class="p">;</span>
                <span class="n">release</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">lock</span><span class="p">);</span>
                <span class="k">return</span> <span class="n">pid</span><span class="p">;</span>
            <span class="p">}</span>
        <span class="p">}</span>
        <span class="c1">// No point waiting if we don’t have any children.</span>
        <span class="k">if</span><span class="p">(</span><span class="o">!</span><span class="n">havekids</span> <span class="o">||</span> <span class="n">curproc</span><span class="err">−</span><span class="o">&gt;</span><span class="n">killed</span><span class="p">){</span>
            <span class="n">release</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">lock</span><span class="p">);</span>
        <span class="k">return</span> <span class="err">−</span><span class="mi">1</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="c1">// Wait for children to exit. (See wakeup1 call in proc_exit.)</span>
    <span class="n">sleep</span><span class="p">(</span><span class="n">curproc</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">lock</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="summary-of-process-management-system-calls-in-xv6">Summary of process management system calls in xv6</h2> <ul> <li>Fork - process marks new child’s <code class="language-plaintext highlighter-rouge">struct proc</code> as <code class="language-plaintext highlighter-rouge">RUNNABLE</code>, initializes child memory image and other states that are needed to run when scheduled</li> <li>Exec - process reinitializes memory image of user, data, stack, heap, and returns to run new code.</li> <li>Exit - process marks itself as <code class="language-plaintext highlighter-rouge">ZOMBIE</code>, cleans up some of its state, and invokes the scheduler</li> <li>Wait - parent finds any <code class="language-plaintext highlighter-rouge">ZOMBIE</code> child and cleans up all its state. IF no dead child is found, it sleeps (marks itself as <code class="language-plaintext highlighter-rouge">SLEEPING</code> and invokes scheduler).</li> </ul> <blockquote> <p>When do we call wait in the parent process?</p> </blockquote> <h1 id="lecture-4---mechanism-of-process-execution">Lecture 4 - Mechanism of process execution</h1> <p>In this lecture, we will learn how an OS runs a process. How it handles system calls, and how it context switches from one process to the other. We are going to understand the low-level mechanisms of these.</p> <h2 id="process-execution">Process Execution</h2> <p>The first thing the OS does is, it allocates memory and create a memory image. It also sets up the program counter and other registers. After the setup, the OS is out of the way, and the process executes directly on the CPU <strong>by itself</strong>.</p> <h2 id="a-simple-function-call">A simple function call</h2> <p>A function call translates to a <strong><code class="language-plaintext highlighter-rouge">jump</code></strong> instruction. A new stack frame is pushed onto the stack, and the stack pointer (SP) is updated. The old value of the PC (return value) is pushed to the stack, and the PC is updated. The stack frame contains return value, function arguments, etc.</p> <h2 id="how-is-a-system-call-different">How is a system call different?</h2> <p>The CPU hardware has multiple privilege levels. The <em>user mode</em> is used to run the user code. OS code like the system calls run in the <em>kernel mode</em>. Some instructions execute only in kernel mode.</p> <p>The kernel does not trust the user stack. It uses a separate kernel stack when in kernel mode. The kernel also does not trust the user-provided addresses. It sets up an <strong><em>Interrupt Descriptor Table (IDT)</em></strong> at boot time. The IDT has addresses of kernel functions to run for system calls and other events.</p> <h2 id="mechanism-of-a-system-call-trap-instruction">Mechanism of a system call: trap instruction</h2> <p>A special trap instruction is run when a system call must be made (usually hidden from the user by libc). The trap instruction initially moves the CPU to a high privilege level. The stack pointer is updated to switch to the kernel stack. Here, the context, such as old PC, registers, etc., is saved. Then, the address of the system call is looked up in the IDT, and the PC jumps to the trap handler function in the OS code.</p> <p>The trap instruction is executed on the hardware in the following cases -</p> <ul> <li>System call - Program needs OS service</li> <li>Program faults - Program does something illegal, e.g., access memory that it doesn’t have access to</li> <li>Interrupt events - External device needs the attention of OS, e.g., a network packet has arrived on the network card.</li> </ul> <p>In all of the cases, the mechanism is the same as described above. The IDT has many entries. The system calls/interrupts store a number in a CPU register before calling trap to identify which IDT entry to use.</p> <p>When the OS is done handling the syscall or interrupt, it calls a special instruction called <strong><code class="language-plaintext highlighter-rouge">return-from-trap</code></strong>. It undoes all the actions done by the trap instruction. It restores the context of CPU registers from the kernel stack. It changes the CPU privilege from kernel mode to user mode, and it restores the PC and jumps to user code after the trap call.</p> <p>The user process is unaware that it was suspended, and it resumes execution as usual.</p> <p>Before returning to the user mode, the OS checks if it has to switch back to the same process or another process. Why do we want to do this? Sometimes when the OS is in kernel mode, it cannot return back to the same process it left. For example, when the original process has exited, it must be terminated (e.g., due to a segfault) or when the process has made a blocking system call. Sometimes, the OS does not want to return back to the same process. Maybe the process has run for a long time, Due to the timesharing responsibility, the OS switches to another process. In such cases, OS is said to perform a <strong><em>context switch</em></strong> to switch from one process to another.</p> <h2 id="os-scheduler">OS scheduler</h2> <p>The OS scheduler is responsible for the context switching mechanism. It has two parts - A policy to pick which process to run and a mechanism to switch to that process. There are two different types of schedulers.</p> <p>A non-preemptive (cooperative) scheduler is polite. It switches only when a process is blocked or terminated. On the other hand, a preemptive (non-cooperative) schedulers can switch even when the process is ready to be continued. The CPU generates a <strong>periodic timer interrupt</strong> to check if a process has run for too long. After servicing an interrupt, a preemptive scheduler switches to another process.</p> <h2 id="mechanism-of-context-switch">Mechanism of context switch</h2> <p>Suppose a process A has moved from the user to kernel mode, and the OS decides it must switch from A to B. Now, the OS’s first job is saving the context (PC, registers, kernel stack pointer) of A on the kernel stack. Then, the kernel stack pointer is switched to B, and B’s context is restored from B’s kernel stack. This context was saved by the OS when it switched out of B in the past. Now, the CPU is running B in kernel mode, <code class="language-plaintext highlighter-rouge">return-from-trap</code> to switch to user mode of B.</p> <h2 id="a-subtlety-on-saving-context">A subtlety on saving context</h2> <p>The context (PC and other registers) of a process is saved on the kernel stack in two different scenarios.</p> <p>When the OS goes from user mode to kernel mode, user context (e.g., which instruction of user code you stopped at) is saved on the kernel stack by the trap instruction. This is later restored using the <code class="language-plaintext highlighter-rouge">return-from-trap</code> instruction. The other scenario where you store the context is during a context switch. The kernel context (e.g., where you stopped in the OS code) of process A is saved on the kernel stack of A by the context-switching code, and B’s context is restored.</p> <h1 id="live-session-2">Live Session 2</h1> <ul> <li> <p>As the shell user, you don’t have to call <code class="language-plaintext highlighter-rouge">fork</code>, <code class="language-plaintext highlighter-rouge">exec</code>, <code class="language-plaintext highlighter-rouge">wait</code> and <code class="language-plaintext highlighter-rouge">exit</code>. The shell automatically takes care of this.</p> </li> <li> <p>The stack pointer’s current location is stored in a general-purpose register before jumping from the user stack to the kernel stack.</p> </li> <li> <p>Why don’t we create an empty system image for a child process? Some instructions (in Windows) require the child to run the parent’s code. We can’t initialize an empty image. There are some advantages to copying the memory image of the parent into the child. The modern OSs utilize copy on demand.</p> <p>When a child is created, the PC points to the instruction after <code class="language-plaintext highlighter-rouge">fork()</code>. This prevents the OS from calling <code class="language-plaintext highlighter-rouge">fork()</code> recursively. <code class="language-plaintext highlighter-rouge">fork()</code> can return \(-1\) instead of the child’s PID if the process creation fails.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">wait()</code> reaps only a <strong>single</strong> child process. We need to call it again if we need to reap more children. With some variants of <code class="language-plaintext highlighter-rouge">wait()</code>, we can delete a specific child.</p> <p><code class="language-plaintext highlighter-rouge">wait()</code> is designed this way so that the parent knows which child has been reaped. This information is important for later instructions. Returning an array of variable size is not a feasible option.</p> </li> </ul> <h1 id="lecture-24---trap-handling">Lecture 24 - Trap Handling</h1> <p>What is a trap? In certain scenarios, the user programs have to <em>trap</em> into the OS/kernel code. The following events cause a user process to “trap” into the kernel (xv6 refers to these events as traps)</p> <ul> <li>System calls - requests by user for OS services</li> <li>Interrupts - External device wants attention</li> <li>Program fault - Illegal action by a program</li> </ul> <p>When any of the above events happen, the CPU executes the special <em><code class="language-plaintext highlighter-rouge">int</code></em> instruction. The user program is blocked. A trap instruction has a parameter <code class="language-plaintext highlighter-rouge">int n</code>, indicating the type of interrupt. For example, syscall has a different value for \(n\) from a keyboard interrupt.</p> <p>Before the trap instruction is executed, <code class="language-plaintext highlighter-rouge">eip</code> points to the user program instruction, and <code class="language-plaintext highlighter-rouge">esp</code> to user stack. When an interrupt occurs, the CPU executes the following step s as part of the <code class="language-plaintext highlighter-rouge">int n</code> instruction -</p> <ul> <li>Fetch the \(n\)th entry interrupt descriptor table (CPU knows the memory address of IDT)</li> <li>Save stack pointer (<code class="language-plaintext highlighter-rouge">esp</code>) to an internal register</li> <li>Switch <code class="language-plaintext highlighter-rouge">esp</code> to the kernel stack of the process (CPU knows the location of the kernel stack of the current process - <strong><em>task state segment</em></strong>)</li> <li>On the kernel stack, save the old <code class="language-plaintext highlighter-rouge">esp</code>, <code class="language-plaintext highlighter-rouge">eip</code>, etc.</li> <li>Load the new <code class="language-plaintext highlighter-rouge">eip</code> from the IDT, which points to the kernel trap handler.</li> </ul> <p>Now, the OS is ready to run the kernel trap handler code on the process’s kernel stack. Few details have been omitted above -</p> <ul> <li>Stack, code segments (<code class="language-plaintext highlighter-rouge">cs</code>, <code class="language-plaintext highlighter-rouge">ss</code>), and a few other registers are also saved.</li> <li>Permission check of CPU privilege levels in IDT entries. For example, a user code can invoke the IDT entry of a system call but not of a disk interrupt.</li> <li>Suppose an interrupt occurs when the CPU is already handling a previous interrupt. In that case, we don’t have to save the stack pointer again.</li> </ul> <h2 id="why-a-separate-trap-instruction">Why a separate trap instruction?</h2> <p>Why can’t we simply jump to kernel code, like we jump to the code of a function in a function call? The reasons are as follows -</p> <ul> <li>The CPU is executing the user code at a lower privilege level, but the OS code must run at a higher privilege.</li> <li>The user program cannot be trusted to invoke kernel code on its own correctly.</li> <li>Someone needs to change the CPU privilege level and give control to the kernel code.</li> <li>Someone also needs to switch to the secure kernel stack so that the kernel can start saving the state.</li> </ul> <h2 id="trap-frame-on-the-kernel-stack">Trap frame on the kernel stack</h2> <p><strong>Trap frame</strong> refers to the state pushed on the kernel stack during trap handling. This state includes the CPU context of where execution stopped and some extra information needed by the trap handler. The <code class="language-plaintext highlighter-rouge">int n</code> instruction pushes only the bottom few entries of the trap frame. The kernel code pushes the rest.</p> <h2 id="kernel-trap-handler-alltraps">Kernel trap handler (<code class="language-plaintext highlighter-rouge">alltraps</code>)</h2> <p>The IDT entries for all interrupts will set <code class="language-plaintext highlighter-rouge">eip</code> to point to the kernel trap handler <code class="language-plaintext highlighter-rouge">alltraps</code>. The <code class="language-plaintext highlighter-rouge">alltraps</code> assembly code pushes the remaining registers to complete the trap frame on the kernel stack. <code class="language-plaintext highlighter-rouge">pushal</code> pushes all the general-purpose registers. It also invokes the C trap handling function named <code class="language-plaintext highlighter-rouge">trap</code>. The top of the trap frame (current top of the stack - <code class="language-plaintext highlighter-rouge">esp</code>) is given as an argument to the C function.</p> <p>The convention of calling C functions is to push the arguments on to the stack and then call the function.</p> <h2 id="c-trap-handler-function">C trap handler function</h2> <p>The C trap handler performs different actions based on the kind of trap. For example, say we have to execute a system call. The function invokes <code class="language-plaintext highlighter-rouge">int n</code>. The system call number is taken from the register <code class="language-plaintext highlighter-rouge">eax</code> (whether fork, exec, etc.). The return value of the syscall is stored in <code class="language-plaintext highlighter-rouge">eax</code> after execution.</p> <p>Suppose we have an interrupt from a device; the corresponding device-related code is called. The trap number is different for different devices. A timer interrupt is a special hardware interrupt, and it is generated periodically to trap to the kernel. On a timer interrupt, a process <code class="language-plaintext highlighter-rouge">yield</code>s CPU to the scheduler. This interrupt ensures a process does not run for too long.</p> <h2 id="return-from-trap">Return from trap</h2> <p>The values from the kernel stack have to be popped. The return from trap instruction <code class="language-plaintext highlighter-rouge">iret</code> does the opposite of <code class="language-plaintext highlighter-rouge">int</code>. It pops the values and changes the privilege level back to a lower level. Then, the execution of the pre-trap code can resume.</p> <h2 id="summary-of-xv6-trap-handling">Summary of xv6 trap handling</h2> <ul> <li>System calls, program faults, or hardware interrupts cause the CPU to run <code class="language-plaintext highlighter-rouge">int n</code> instruction and “trap” to the OS.</li> <li>The trap instruction causes the CPU to switch <code class="language-plaintext highlighter-rouge">esp</code> to the kernel stack, <code class="language-plaintext highlighter-rouge">eip</code> to the kernel trap handling code.</li> <li>The pre-trap CPU state is saved on the kernel stack in the trap frame. This is done both by the <code class="language-plaintext highlighter-rouge">int</code> instruction and the <code class="language-plaintext highlighter-rouge">alltraps</code> code.</li> <li>The kernel trap handler handles the trap and returns to the pre-trap process.</li> </ul> <h1 id="lecture-25---context-switching">Lecture 25 - Context Switching</h1> <p>Before we understand context switching, we need to understand the concepts related to processes and schedulers in xv6. In xv6, every CPU has a attribute called a <strong>scheduler thread</strong>. It is a special process that runs the scheduler code. The scheduler goes over the list of processes and switches to one of the runnable processes. after running for sometime, the process switches back to the scheduler thread. This can happen in the following 3 ways -</p> <ul> <li>Process has terminated</li> <li>Process needs to sleep</li> <li>Process <em>yields</em> after running for a long time</li> </ul> <p>A context switch only happens when the process is already <strong>in the kernel mode</strong>.</p> <h2 id="scheduler-and-sched">Scheduler and sched</h2> <p>The scheduler switches to a user process in the <code class="language-plaintext highlighter-rouge">scheduler</code> function. User processes switch to the scheduler thread in the <code class="language-plaintext highlighter-rouge">sched</code> function (invoked from <code class="language-plaintext highlighter-rouge">exit</code>, <code class="language-plaintext highlighter-rouge">sleep</code>, <code class="language-plaintext highlighter-rouge">yield</code>).</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span>
<span class="nf">scheduler</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">struct</span> <span class="n">proc</span> <span class="o">*</span><span class="n">p</span><span class="p">;</span>
    <span class="k">struct</span> <span class="n">cpu</span> <span class="o">*</span><span class="n">c</span> <span class="o">=</span> <span class="n">mycpu</span><span class="p">();</span>
    <span class="n">c</span><span class="err">−</span><span class="o">&gt;</span><span class="n">proc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(;;){</span>
        <span class="c1">// Enable interrupts on this processor.</span>
        <span class="n">sti</span><span class="p">();</span>
        <span class="c1">// Loop over process table looking for process to run.</span>
        <span class="n">acquire</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">lock</span><span class="p">);</span>
        <span class="k">for</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="n">ptable</span><span class="p">.</span><span class="n">proc</span><span class="p">;</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">proc</span><span class="p">[</span><span class="n">NPROC</span><span class="p">];</span> <span class="n">p</span><span class="o">++</span><span class="p">){</span>
            <span class="k">if</span><span class="p">(</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">state</span> <span class="o">!=</span> <span class="n">RUNNABLE</span><span class="p">)</span>
                <span class="k">continue</span><span class="p">;</span>
            <span class="c1">// Switch to chosen process. It is the process’s job</span>
            <span class="c1">// to release ptable.lock and then reacquire it</span>
            <span class="c1">// before jumping back to us.</span>
            <span class="n">c</span><span class="err">−</span><span class="o">&gt;</span><span class="n">proc</span> <span class="o">=</span> <span class="n">p</span><span class="p">;</span>
            <span class="n">switchuvm</span><span class="p">(</span><span class="n">p</span><span class="p">);</span>
            <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">state</span> <span class="o">=</span> <span class="n">RUNNING</span><span class="p">;</span>
            <span class="n">swtch</span><span class="p">(</span><span class="o">&amp;</span><span class="p">(</span><span class="n">c</span><span class="err">−</span><span class="o">&gt;</span><span class="n">scheduler</span><span class="p">),</span> <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">context</span><span class="p">);</span>
            <span class="n">switchkvm</span><span class="p">();</span>
            <span class="c1">// Process is done running for now.</span>
            <span class="c1">// It should have changed its p−&gt;state before coming back.</span>
            <span class="n">c</span><span class="err">−</span><span class="o">&gt;</span><span class="n">proc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">release</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">lock</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">sched</code> function is as follows</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span>
<span class="nf">sched</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">intena</span><span class="p">;</span>
    <span class="k">struct</span> <span class="n">proc</span> <span class="o">*</span><span class="n">p</span> <span class="o">=</span> <span class="n">myproc</span><span class="p">();</span>
    <span class="k">if</span><span class="p">(</span><span class="o">!</span><span class="n">holding</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">lock</span><span class="p">))</span>
    <span class="n">panic</span><span class="p">(</span><span class="s">"sched ptable.lock"</span><span class="p">);</span>
    <span class="k">if</span><span class="p">(</span><span class="n">mycpu</span><span class="p">()</span><span class="err">−</span><span class="o">&gt;</span><span class="n">ncli</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">panic</span><span class="p">(</span><span class="s">"sched locks"</span><span class="p">);</span>
    <span class="k">if</span><span class="p">(</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">state</span> <span class="o">==</span> <span class="n">RUNNING</span><span class="p">)</span>
    <span class="n">panic</span><span class="p">(</span><span class="s">"sched running"</span><span class="p">);</span>
    <span class="k">if</span><span class="p">(</span><span class="n">readeflags</span><span class="p">()</span><span class="o">&amp;</span><span class="n">FL_IF</span><span class="p">)</span>
    <span class="n">panic</span><span class="p">(</span><span class="s">"sched interruptible"</span><span class="p">);</span>
    <span class="n">intena</span> <span class="o">=</span> <span class="n">mycpu</span><span class="p">()</span><span class="err">−</span><span class="o">&gt;</span><span class="n">intena</span><span class="p">;</span>
    <span class="n">swtch</span><span class="p">(</span><span class="o">&amp;</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">context</span><span class="p">,</span> <span class="n">mycpu</span><span class="p">()</span><span class="err">−</span><span class="o">&gt;</span><span class="n">scheduler</span><span class="p">);</span>
    <span class="n">mycpu</span><span class="p">()</span><span class="err">−</span><span class="o">&gt;</span><span class="n">intena</span> <span class="o">=</span> <span class="n">intena</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>The high level view is</p> <p><img src="assets/image-20210813235433748.png" alt="image-20210813235433748"/></p> <h2 id="when-does-a-process-call-sched">When does a process call <code class="language-plaintext highlighter-rouge">sched</code>?</h2> <p><strong>Yield</strong> - A timer interrupt occurs when a process has run for a long enough time.</p> <p><strong>Exit</strong> - The process has exit and set itself as zombie.</p> <p><strong>Sleep</strong> - A process has performed a blocking action and set itself to sleep.</p> <h2 id="struct-context"><code class="language-plaintext highlighter-rouge">struct context</code></h2> <p>This structure is saved and restored during a context switch. It is basically a set of registers to be saved when switching from one process to another. For example, we must save <code class="language-plaintext highlighter-rouge">eip</code> which signifies where the process has stopped. The context is pushed onto the kernel stack and the <code class="language-plaintext highlighter-rouge">struct proc</code> maintains a pointer to the context structure on the stack.</p> <p>Now, the obvious question is “what is the difference between this and the <a href="#trap-frame-on-the-kernel-stack">trap frame</a>?” We shall look into it now.</p> <h2 id="context-structure-vs-trap-frame">Context structure vs. Trap frame</h2> <p>The trapframe (<code class="language-plaintext highlighter-rouge">p -&gt; tf</code>) is saved when the CPU switches to the kernel mode. For example, <code class="language-plaintext highlighter-rouge">eip</code> in the trapframe is the <code class="language-plaintext highlighter-rouge">eip</code> value where the syscall was made in the user code. On the other hand, the context structure is saved when process switches to another process. For example, <code class="language-plaintext highlighter-rouge">eip</code> value when <code class="language-plaintext highlighter-rouge">swtch</code> is called. Both these structures reside on the kernel stack and <code class="language-plaintext highlighter-rouge">struct proc</code> has pointers to both of them. Although, they differ in the content they store. This sort of clears up the confusion in the <a href="#a-subtlety-on-saving-context">subtlety of memory storage</a> in the kernel stack.</p> <h2 id="swtch-function"><code class="language-plaintext highlighter-rouge">swtch</code> function</h2> <p>This function is invoked both by the CPU thread and the process. It takes two arguments, the <strong>address of the pointer</strong> of the old context and the pointer of the new context. We are not sending the address of the new context, but the context pointer itself.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// When invoked from the scheduler: address of scheduler's context pointer, process context pointer</span>
<span class="n">swtch</span><span class="p">(</span><span class="o">&amp;</span><span class="p">(</span><span class="n">c</span> <span class="o">-&gt;</span> <span class="n">scheduler</span><span class="p">),</span> <span class="n">p</span> <span class="o">-&gt;</span> <span class="n">context</span><span class="p">);</span>
<span class="c1">// When invoked from sched: address of process context pointer, scheduler context pointer</span>
<span class="n">swtch</span><span class="p">(</span><span class="o">&amp;</span><span class="n">p</span> <span class="o">-&gt;</span> <span class="n">context</span><span class="p">,</span> <span class="n">mycpu</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">scheduler</span><span class="p">);</span>

</code></pre></div></div> <p>When a process/thread has invoked the <code class="language-plaintext highlighter-rouge">swtch</code>, the stack has caller save registers and the return address (<code class="language-plaintext highlighter-rouge">eip</code>). <code class="language-plaintext highlighter-rouge">swtch</code> does the following -</p> <ul> <li>Push the remaining (callee save) registers on the old kernel stack.</li> <li>Save the pointer to this context into the context structure pointer of the old process.</li> <li>Switch <code class="language-plaintext highlighter-rouge">esp</code> from the old kernel stack to the new kernel stack.</li> <li><code class="language-plaintext highlighter-rouge">esp</code> now points to the saved context of new process. This is the primary step of a context switch.</li> <li>Pop the callee-save registers from the new stack.</li> <li>Return from the function call by popping the return address after the callee save registers.</li> </ul> <p>The assembly code of <code class="language-plaintext highlighter-rouge">swtch</code> is as follows -</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Context switch
void swtch(struct context **old, struct context *new);
Save the current registers on the stack, creating
a struct context, and save its address in *old.
Switch stacks to new and pop previously−saved registers.
.globl swtch
swtch:
movl 4(%esp), %eax
movl 8(%esp), %edx
# Save old callee−saved registers
pushl %ebp
pushl %ebx
pushl %esi
pushl %edi
# Switch stacks
# opposite order compared to MIPS
# movl src dst
movl %esp, (%eax)
movl %edx, %esp
# Load new callee−saved registers
popl %edi
popl %esi
popl %ebx
popl %ebp
ret
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">eax</code> has the address of the pointer to the old context and <code class="language-plaintext highlighter-rouge">edx</code> has the pointer to the new context.</p> <blockquote> <p>Why address of pointer?</p> </blockquote> <h2 id="summary-of-context-switching-in-xv6">Summary of context switching in xv6</h2> <p>The old process, say P1, goes into the kernel mode and gives up the CPU. The new process, say P2, is ready to run. P1 switches to the CPU scheduler thread. The <strong>scheduler thread</strong> finds P2 and switches to it. Then, P2 returns from trap to user mode. The process of switching from one process/thread to another involves the following steps. All the register states (CPU context) on the kernel stack of the old process are saved. The context structure pointer of the old process is updated to this saved context. Then, <code class="language-plaintext highlighter-rouge">esp</code> moves from the old kernel stack to the new kernel stack. Finally, the register states are restored in the new kernel stack to resume the new process.</p> <h1 id="lecture-26---user-process-creation">Lecture 26 - User process creation</h1> <p>We know that the <code class="language-plaintext highlighter-rouge">init</code> process is created when xv6 boots up. The <code class="language-plaintext highlighter-rouge">init</code> process forks a shell process, and the shell is used to spawn any user process. The function <code class="language-plaintext highlighter-rouge">allocproc</code> is called during both <code class="language-plaintext highlighter-rouge">init</code> process creation and in fork system call.</p> <h2 id="allocproc"><code class="language-plaintext highlighter-rouge">allocproc</code></h2> <p>It iterates over the <code class="language-plaintext highlighter-rouge">ptable</code>, finds an unused entry, and marks it as an embryo. This entry is later marked as runnable after the process creation completes. It also allocates a new PID for the process. Then, <code class="language-plaintext highlighter-rouge">allocproc</code> has to allocate space on the kernel stack. To do this, we start from the bottom of the stack and find some free space. We leave room for the trap frame. Then, we push the return address of <code class="language-plaintext highlighter-rouge">trapret</code> and also the context structure with <code class="language-plaintext highlighter-rouge">eip</code> pointing to the function <code class="language-plaintext highlighter-rouge">forkret</code>. When the new process is scheduled, it begins execution at <code class="language-plaintext highlighter-rouge">forkret</code>, returns to <code class="language-plaintext highlighter-rouge">trapret</code>, and finally returns from the trap to the user space.</p> <p>The role of <code class="language-plaintext highlighter-rouge">allocproc</code> is to create a template kernel stack, and make the process look like it had a trap and was context switched out in the past. This is done so that the scheduler can switch to this process like any other.</p> <blockquote> <p>Where is kernel mode? The sp points to the kernel stack.</p> </blockquote> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">allocproc</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">struct</span> <span class="n">proc</span> <span class="o">*</span><span class="n">p</span><span class="p">;</span>
    <span class="kt">char</span> <span class="o">*</span><span class="n">sp</span><span class="p">;</span>
    <span class="n">acquire</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">lock</span><span class="p">);</span>
    <span class="k">for</span><span class="p">(</span><span class="n">p</span> <span class="o">=</span> <span class="n">ptable</span><span class="p">.</span><span class="n">proc</span><span class="p">;</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">proc</span><span class="p">[</span><span class="n">NPROC</span><span class="p">];</span> <span class="n">p</span><span class="o">++</span><span class="p">)</span>
        <span class="k">if</span><span class="p">(</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">state</span> <span class="o">==</span> <span class="n">UNUSED</span><span class="p">)</span>
            <span class="k">goto</span> <span class="n">found</span><span class="p">;</span>
    <span class="n">release</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">lock</span><span class="p">);</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
    <span class="nl">found:</span>
        <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">state</span> <span class="o">=</span> <span class="n">EMBRYO</span><span class="p">;</span>
        <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">pid</span> <span class="o">=</span> <span class="n">nextpid</span><span class="o">++</span><span class="p">;</span>
        <span class="n">release</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ptable</span><span class="p">.</span><span class="n">lock</span><span class="p">);</span>
        <span class="c1">// Allocate kernel stack.</span>
        <span class="k">if</span><span class="p">((</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">kstack</span> <span class="o">=</span> <span class="n">kalloc</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">){</span>
            <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">state</span> <span class="o">=</span> <span class="n">UNUSED</span><span class="p">;</span>
            <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">sp</span> <span class="o">=</span> <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">kstack</span> <span class="o">+</span> <span class="n">KSTACKSIZE</span><span class="p">;</span>
        <span class="c1">// Leave room for trap frame.</span>
        <span class="n">sp</span> <span class="err">−</span><span class="o">=</span> <span class="k">sizeof</span> <span class="o">*</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">tf</span><span class="p">;</span>
        <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">tf</span> <span class="o">=</span> <span class="p">(</span><span class="k">struct</span> <span class="n">trapframe</span><span class="o">*</span><span class="p">)</span><span class="n">sp</span><span class="p">;</span>
        <span class="c1">// Set up new context to start executing at forkret,</span>
        <span class="c1">// which returns to trapret.</span>
        <span class="n">sp</span> <span class="err">−</span><span class="o">=</span> <span class="mi">4</span><span class="p">;</span>
        <span class="o">*</span><span class="p">(</span><span class="n">uint</span><span class="o">*</span><span class="p">)</span><span class="n">sp</span> <span class="o">=</span> <span class="p">(</span><span class="n">uint</span><span class="p">)</span><span class="n">trapret</span><span class="p">;</span>
        <span class="n">sp</span> <span class="err">−</span><span class="o">=</span> <span class="k">sizeof</span> <span class="o">*</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">context</span><span class="p">;</span>
        <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">context</span> <span class="o">=</span> <span class="p">(</span><span class="k">struct</span> <span class="n">context</span><span class="o">*</span><span class="p">)</span><span class="n">sp</span><span class="p">;</span>
        <span class="n">memset</span><span class="p">(</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">context</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span> <span class="o">*</span><span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">context</span><span class="p">);</span>
        <span class="n">p</span><span class="err">−</span><span class="o">&gt;</span><span class="n">context</span><span class="err">−</span><span class="o">&gt;</span><span class="n">eip</span> <span class="o">=</span> <span class="p">(</span><span class="n">uint</span><span class="p">)</span><span class="n">forkret</span><span class="p">;</span>
        <span class="k">return</span> <span class="n">p</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="init-process-creation"><code class="language-plaintext highlighter-rouge">Init</code> process creation</h2> <p>The <code class="language-plaintext highlighter-rouge">userinit</code> function is called to create the <code class="language-plaintext highlighter-rouge">init</code> process. This is also done using the <code class="language-plaintext highlighter-rouge">allocproc</code> function. The trapframe of <code class="language-plaintext highlighter-rouge">init</code> is modified, and the process is set to be runnable. The <code class="language-plaintext highlighter-rouge">init</code> program opens <code class="language-plaintext highlighter-rouge">STDIN</code>, <code class="language-plaintext highlighter-rouge">STDOUT</code> and <code class="language-plaintext highlighter-rouge">STDERR</code> files. These are inherited by all subsequent processes as child inherits parent’s files. It then forks a child, execs shell executable in the child, and waits for the child to die. It also reaps dead children.</p> <h2 id="forking-a-new-process">Forking a new process</h2> <p>Fork allocates a new process via <code class="language-plaintext highlighter-rouge">allocproc</code>. The parent memory image and the file descriptors are copied. Take a look at the <a href="#fork-system-call">fork code</a> while you’re reading this. The trapframe of the child is copied from that of the parent. This allows the child execution to resume from the next instruction after <code class="language-plaintext highlighter-rouge">fork()</code>. Only the return value in <code class="language-plaintext highlighter-rouge">eax</code> is changed so that the child returns its PID. The state of the new child is set to runnable, and the parent returns normally from the trap/system call.</p> <h2 id="summary-of-new-process-creation">Summary of new process creation</h2> <p>New processes are created by marking a new entry in the <code class="language-plaintext highlighter-rouge">ptable</code> as runnable after configuring the kernel stack, memory image etc of the new process. The kernel stack of the new process is made to look like that of a process that had been context switched out in the past.</p> <h1 id="live-session-3">Live Session 3</h1> <ul> <li> <p>What is a scheduler thread? It is a kernel process that is a part of the OS. It always runs in kernel mode, and it is not a user -level process.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">int n</code> is a hardware instruction and is enabled by a <em>hardware descriptor language</em>. It is similar to, say, the <code class="language-plaintext highlighter-rouge">add</code> instruction. Why is it a hardware instruction? We don’t trust the software. As it is a hardware instruction, we can trust the CPU maker to bake the chip correctly.</p> </li> <li> <p>The <code class="language-plaintext highlighter-rouge">trapframe</code> has the user context, and the context structure has the kernel context. Every context switch is preceded by a trap instruction. A context switch only happens in the kernel mode. Now, what if a user process does not go to the kernel mode ever? The timer interrupt will take care of this. It will prompt the process to go into trap mode if the process has run for too long.</p> </li> <li> <p>In the <a href="#a-subtlety-on-saving-context">subtlety on saving context</a>, the first point refers to storing the trapframe. The second point refers to storing the context structure.</p> </li> <li> <p><code class="language-plaintext highlighter-rouge">forkret</code> is a small function that a program has to execute after a process has been created. This mainly involves the locking mechanism that will be discussed later. <code class="language-plaintext highlighter-rouge">fork</code> is a wrapper of <code class="language-plaintext highlighter-rouge">allocproc</code></p> </li> <li> <p><code class="language-plaintext highlighter-rouge">sched</code> is only called by the user processes in case of <code class="language-plaintext highlighter-rouge">sleep</code>, <code class="language-plaintext highlighter-rouge">exit</code>, and <code class="language-plaintext highlighter-rouge">yield</code>. <code class="language-plaintext highlighter-rouge">sched</code> stores the context of the current context onto the kernel stack. The <code class="language-plaintext highlighter-rouge">scheduler</code> function is called by the scheduler thread to switch to a new process. Both of these functions call the <code class="language-plaintext highlighter-rouge">swtch</code> function to switch the current process. Think of the CPU scheduler (scheduler thread) as an intermediate process that has to be executed when switching from process A to process B.</p> <p>Why do we need this intermediate process? Several operating systems make do without a scheduler thread. This is modular, and xv6 chose this methodology. The <code class="language-plaintext highlighter-rouge">scheduler</code> function simply uses the Round Robin algorithm.</p> </li> </ul> <h1 id="lecture-5---scheduling-policy">Lecture 5 - Scheduling Policy</h1> <p>What is a scheduling policy? It is a program that decides which program to run at a given time from the set of ready processes. The OS scheduler schedules the CPU requests (bursts) of processes. <strong>CPU burst</strong> refers to the CPU time used by a process in a continuous stretch. For example, if a process comes back after an I/O wait, it counts as a fresh CPU burst.</p> <p>Our goal is to maximize CPU utilization, and minimize the <strong>turnaround time</strong> of a process. The turnaround time of a process refers to the time between the process’s arrival and its completion. The scheduling policy must also minimize the average <strong>response time</strong>, from process arrival to the first scheduling. We should also treat all processes fairly and minimize the <strong>overhead</strong>. The amortized cost of a context switch is high.</p> <p>We shall discuss a few scheduling policies and their pros/cons below.</p> <h3 id="first-in-first-out-fifo">First-In-First-Out (FIFO)</h3> <p>Schedule the processes as an when they arrive in the CPU. The drawback of this method is the <strong>convoy effect</strong>. In this situation, a process takes high time to execute and effectively increases the turnaround time.</p> <h3 id="shortest-job-first-sjf">Shortest Job First (SJF)</h3> <p>This is provably optimal when all processes arrive together. Although, this is a non-preemptive policy. Short processes can still be stuck behind the long ones when the long process arrives first.</p> <h3 id="shortest-time-to-completion-first">Shortest Time-to-Completion First</h3> <p>This is a preemptive version of SJF. This policy preempts the running task is the remaining time to execute the process is more than that of the new arrival. This method is also called the Shortest Remaining Time First (SRTF).</p> <blockquote> <p>How do we know the running time/ time left of a process? We schedule processes in bursts! No, that’s wrong! See this</p> <p><img src="/assets/img/Operating Systems/image-20210823105421325.png" alt="image-20210823105421325"/></p> </blockquote> <h3 id="round-robin-rr">Round Robin (RR)</h3> <p>Every process executes for a fixed quantum slice. The slice is big enough to amortize the cost of a context switch. This policy is also preemptive! It has a good response time and is fair. Although, it may have high turnaround times.</p> <h2 id="schedulers-in-real-systems">Schedulers in Real systems</h2> <p>Real schedulers are more complex. For example, Linux uses a policy called Multi-Level Feedback Queue (MLFQ). You maintain a set of queues and prioritize them. A process is picked from the highest priority queue. Processes in the same priority level are scheduled using a policy like RR. The priority of a queue decays with its age.</p> <h1 id="lecture-6---inter-process-communication-ipc">Lecture 6 - Inter-Process Communication (IPC)</h1> <p>In general, two processes do not share any memory with each other. Some processes might want to work together for a task, and they need to communicate information. The OS provides IPC mechanisms for this purpose.</p> <h2 id="types-of-ipc-mechanisms">Types of IPC mechanisms</h2> <h3 id="shared-memory">Shared Memory</h3> <p>Two processes can get access to the same region of the memory via <code class="language-plaintext highlighter-rouge">shmget()</code> system call.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">shmget</span><span class="p">(</span><span class="n">key_t</span> <span class="n">key</span><span class="p">,</span> <span class="kt">int</span> <span class="n">size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">shmflg</span><span class="p">)</span>
</code></pre></div></div> <p>By providing the same key, two processes can get the same segment of memory.</p> <blockquote> <p>How do they know keys?</p> </blockquote> <p>Although, when realizing this idea, we need to consider some problematic scenarios. For example, we need to take care that one process is not overwriting another’s data.</p> <h3 id="signals">Signals</h3> <p>These are well-defined messages that can be sent to processes that are supported by the OS. Some signals have a fixed meaning. For example, a signal to terminate a process. Some signals can also be user-defined. A signal can be sent to a process by the OS or another process. For example, when you type <code class="language-plaintext highlighter-rouge">Ctrl + C</code>, the OS sends <code class="language-plaintext highlighter-rouge">SIGINT</code> signal to the running process.</p> <p>These signals are handled by a <strong>signal handler</strong>. Every process has a default code to execute for each signal. Some (Can’t edit the terminate signal) of these signal handlers can be overridden to do other things. Although, you cannot send messages/bytes in this method.</p> <h3 id="sockets">Sockets</h3> <p>Sockets can be used for two processes on the same machine or different machines to communicate. For example, TCP/UDP sockets across machines and Unix sockets in a local machine. Processes open sockets and connect them to each other. Messages written into one socket can be read from another. The OS transfers the data across socket buffers.</p> <h3 id="pipes">Pipes</h3> <p>These are similar to sockets but are <strong>half-duplex</strong> - information can travel only in one direction. A pipe system call returns two file descriptors. These are simply <strong>handles</strong> that can read and write into files (read handle and write handle). The data written in one file descriptors can be read through another.</p> <p>In regular pipes, both file descriptors are in the same process. How is this useful? When a parent forks a child process, the child process has access to the same pipe. Now, the parent can use one end of the pipe, and the child can use the other end.</p> <p><strong>Named pipes</strong> can be used to provide two endpoints a pipe to different processes. The pipe data is buffered in OS (kernel) buffers between write and read.</p> <h3 id="message-queues">Message Queues</h3> <p>This is an abstraction of a mailbox. A process can open a mailbox at a specified location and send/receive messages from the mailbox. The OS buffers messages between send and receive.</p> <h2 id="blocking-vs-non-blocking-communication">Blocking vs. non-blocking communication</h2> <p>Some IPC actions can block</p> <ul> <li>reading from a socket/pip that has no data, or reading from an empty message queue</li> <li>Writing to a full socket/pip/message queue</li> </ul> <p>The system calls to read/write have versions that block or can return with an error code in case of a failure.</p> <h1 id="lecture-7---introduction-to-virtual-memory">Lecture 7 - Introduction to Virtual memory</h1> <p>The OS provides a virtualized view of the memory to a user process. Why do we need to do this? Because the real view of memory is messy! In the olden days, the memory had only the code of one running process and the OS code. However, the contemporary memory structure consists of multiple active processes that timeshare the CPU. The memory of a single process need not be contiguous.</p> <p>The OS hides these messy details of the memory and provides a clean abstraction to the user.</p> <h2 id="the-abstraction-virtual-address-space">The Abstraction: (Virtual) Address Space</h2> <p>Every process assumes it has access to a large space of memory from address - to a MAX value. The memory of a process, as we’ve seen earlier, has code (and static data), heap (dynamic allocations), and stack (function calls). Stack and heap of the program grow during <strong>runtime</strong>. The CPU issues “loads” and “stores” to these virtual addresses. For example, when you print the address of a variable in a program, you get the virtual addresses!</p> <h2 id="translation-of-addresses">Translation of addresses</h2> <p>The OS performs the address translation from virtual addresses (VA) to physical addresses (PA) via a memory hardware called <strong>Memory Management Unit (MMU)</strong>. The OS provides the necessary information to this unit. The <em>CPU loads/stores to VA</em>, but the memory hardware accesses PA.</p> <h2 id="example-paging">Example: Paging</h2> <p>This is a technique used in all modern OS. The OS divides the virtual address space into fixed-size <strong>pages</strong>, and similarly, the physical memory is segmented into <strong>frames</strong>. To allocate memory, a <em>page</em> is mapped to a free physical frame. The <strong>page table</strong> stores the mappings from the virtual page number to the physical frame number for a process. The MMU has access to these page tables and uses them to translate VA to PA.</p> <h2 id="goals-of-memory-virtualization">Goals of memory virtualization</h2> <ul> <li>Transparency - The user programs should not be aware of the actual physical addresses.</li> <li>Efficiency - Minimize the overhead and wastage in terms of memory space and access time.</li> <li>Isolation and Protection - A user process should not be able to access anything outside its address space.</li> </ul> <h2 id="how-can-a-user-allocate-memory">How can a user allocate memory?</h2> <p>The OS allocates a set of pages to the memory image of the process. Within this image</p> <ul> <li>Static/global variables are allocated in the executable.</li> <li>Local variables of a function are allocated during runtime on the stack.</li> <li>Dynamic allocation with <code class="language-plaintext highlighter-rouge">malloc</code> on the heap.</li> </ul> <p>Memory allocation is done via system calls under the hood. For example, <code class="language-plaintext highlighter-rouge">malloc</code> is implemented by a C library that has algorithms for efficient memory allocation and free space management.</p> <p>When the program runs out of the initially allocated space, it <em>grows</em> the heap using the <code class="language-plaintext highlighter-rouge">brk/sbrk</code> system call. Unlike <code class="language-plaintext highlighter-rouge">fork</code>, <code class="language-plaintext highlighter-rouge">exec</code>, and <code class="language-plaintext highlighter-rouge">wait</code>, the programmer is discouraged from using these system calls directly in the user code.</p> <p>A program can also allocate a page-sized memory using the <code class="language-plaintext highlighter-rouge">mmap()</code> system call and get an <em>anonymous</em> (empty, will be discussed later) page from the OS.</p> <h2 id="a-subtlety-in-the-address-space-of-the-os">A subtlety in the address space of the OS</h2> <p>Where is the OS code run? OS is not a separate process with its own address space. Instead, the OS code is a part of the address space of every process. A process sees OS as a part of its code! In the background, the OS provides this abstraction. However, in reality, the page tables map the OS addresses to the OS code.</p> <p>Also, the OS needs memory for its data structures. How does it allocate memory for itself? For large allocation, the OS allocates itself a page. For smaller allocations, the OS uses various memory allocation algorithms (will be discussed later). <strong>Note.</strong> The OS cannot use <code class="language-plaintext highlighter-rouge">libc</code> and <code class="language-plaintext highlighter-rouge">malloc</code> in the kernel.</p> <blockquote> <p>Why?</p> </blockquote> <h1 id="live-session-4">Live Session 4</h1> <ul> <li>The scheduler/PC does not always know the running time of the processes. Therefore, we can’t implement SJF and SRTF in practice.</li> <li>The shared key is shared offline (say, via a command-line argument) in the shared memory IPC.</li> <li>Every process has a set of virtual addresses that it can use. <code class="language-plaintext highlighter-rouge">mmap()</code> is used to fetch the free virtual addresses. It is mainly used for allocating <em>large</em> chunks of memory (allocates pages). It can be used to get <em>general memory</em> and not specifically for heap. On the other hand, <code class="language-plaintext highlighter-rouge">brk</code> and <code class="language-plaintext highlighter-rouge">sbrk</code> grow the heap in <em>small</em> chunks. <code class="language-plaintext highlighter-rouge">malloc</code> uses these two system calls for expanding memory.</li> <li>Conceptually, sockets and message queues are the same. The two structures just have a different programming interface.</li> <li><code class="language-plaintext highlighter-rouge">libc</code> and <code class="language-plaintext highlighter-rouge">malloc</code> can’t be used in the kernel because these are user-space libraries. The kernel has its own versions of these functions. Variants.</li> <li>The C library grows the heap. The OS grows the stack. This is because the heap memory is an abstraction provided by the C libraries. The C library gets a page of memory using <code class="language-plaintext highlighter-rouge">mmap()</code> and provides a small chunk of this page to the user when <code class="language-plaintext highlighter-rouge">malloc()</code> is called. Suppose the stack runs out of the allocated memory. In that case, the OS either allocates new memory and transfers all the content if required or terminates the process for using a lot of memory.</li> </ul> <h1 id="lecture-8---mechanism-of-address-translation">Lecture 8 - Mechanism of Address Translation</h1> <p>Suppose we have written a C program that initializes a variable and adds a constant to it. This code is converted into an assembly code, each instruction having an address. The virtual address space is set up by the OS during process creation.</p> <blockquote> <p>Can we use heap from an assembly code?</p> </blockquote> <p>The OS places the memory images in various chunks (need not be contiguous). However, we shall be considering a simplified version of an OS where the entire memory image is placed in a single chunk. We need the OS to access the physical memory given the virtual address. Also, the OS must detect an error if a program tries to access the memory that is outside the bounds.</p> <h2 id="who-performs-address-translation">Who performs address translation?</h2> <p>In this simple example, the OS can tell the hardware the base and the bound/size values. The MMU calculates PA from VA. The OS is <strong>not involved</strong> in every translation!</p> <p>Basically, the CPU provides a privileged mode of execution. The instruction set has privileged instructions to set translation information (e.g., base, bound). We don’t want the user programs to be able to set this information. Then, the MMU uses this information to perform translation on every memory access. The MMU also generates <em>faults</em> and <em>traps</em> to OS when an access is illegal.</p> <h2 id="role-of-os">Role of OS</h2> <p>What does the OS do? The OS maintains a free list of memory. It allocates spaces to process during creation and cleans up when done. The OS also maintains information of where space is allocated to each process in PCBs. This information is provided to the hardware for translation. Also, the information has to be <strong>updated on context switch</strong> in the MMU. Finally, the OS handles traps due to illegal memory access.</p> <h2 id="segmentation">Segmentation</h2> <p>The base and bound method is a very simple method to store the memory image. Segmentation is a generalized method to store each segment of the memory image separately. For example, the base/bound values of the heap, stack, etc., are stored in the MMU. However, segmentation is not popularly used. Instead, paging is used widely.</p> <p>Segmentation is suitable for sparse address spaces.</p> <blockquote> <p>Stack and heap grow in the physical address space?</p> </blockquote> <p>Although, segmentation uses variable-sized allocation, which leads to <strong>external fragmentation</strong> - small holes in the memory left unused.</p> <h1 id="lecture-9---paging">Lecture 9 - Paging</h1> <p>The memory image of a process is split into fixed-size chunks called <strong>pages</strong>. Each of these pages is mapped to a <strong>physical frame</strong> in the memory. This method avoids external fragmentation. Although, there might be <strong>internal fragmentation</strong>. This is because sometimes the process requires much less memory than the size of a page, but the OS allocates memory in fixed-size pages. However, internal fragmentation is a small problem.</p> <h2 id="page-table-1">Page Table</h2> <p>This is a data structure specific to a process that helps in VA-PA translation. This structure might be as simple as an array storing mappings from virtual page numbers (VPN) to physical frame numbers (PFN). This structure is stored as a part of the OS memory in PCB. The MMU has access to the page tables and uses them for address translation. The OS has to update the page table given to the MMU upon a context switch.</p> <h2 id="page-table-entry-pte">Page Table Entry (PTE)</h2> <p>The most straightforward page table is a linear page table, as discussed above. Each PTE contains PFN and a few other bits</p> <ul> <li>Valid bit - Is this page used by the process?</li> <li>Protection bits - Read/write permissions</li> <li>Present bit - Is this page in memory? (will be discussed later)</li> <li>Dirty bit - Has this page been modified?</li> <li>Accessed bit - Has this page been recently accessed?</li> </ul> <h2 id="address-translation-in-hardware">Address translation in hardware</h2> <p>A virtual address can be separated into VPN and offset. The most significant bits of the VA are the VPN. The page table maps VPN to PFN. Then, PA is obtained from PFN and offset within a page. The MMU stores the (physical) address of the start of the page table, not all the entries. The MMU has to walk to the relevant PTE in the page table.</p> <p>Suppose the CPU requests code/data at a virtual address. Now, the MMU has to access the physical memory to fetch code/data. As you can see, paging adds <em>overhead</em> to memory access. We can reduce this overhead by using a cache for VA-PA mappings. This way, we need not go to the page table for every instruction.</p> <h2 id="translation-lookaside-buffer-tlb">Translation Lookaside Buffer (TLB)</h2> <p>Ignore the name. Basically, it’s a cache of recent VA-PA mappings. To translate VA to PA, the MMU first looks up the TLB. If the TLB misses, the MMU has to walk the page table. TLB misses are expensive (in the case of multiple memory accesses). Therefore, a <strong>locality of reference</strong> helps to have a high hit rate. For example, a program may try to fetch the same data repeatedly in a loop.</p> <p><strong>Note.</strong> TLB entries become invalid on context switch and change of page tables.</p> <blockquote> <p>Page table can change without context switch?</p> </blockquote> <p>Also, this cache is not taken care of by the OS but by the architecture itself.</p> <h2 id="how-are-page-tables-stored-in-the-memory">How are page tables stored in the memory?</h2> <p>A typical page table has 2^20 entries in a 32-bit architecture (32 bit VA) and 4KB pages</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2^32 (4GB RAM)/ 2^12 (4KB pages)
</code></pre></div></div> <p>If each PTE is 4 bytes, then page table is 4MB! How do we reduce the size of page tables? We can use large pages. Still, it’s a tradeoff.</p> <p>How does the OS allocate memory for such large tables? The page table is itself split into smaller chunks! This is a <strong>multilevel page table</strong>.</p> <h2 id="multilevel-page-tables">Multilevel page tables</h2> <p>A page table is spread over many pages. An “outer” page table or <strong>page directory</strong> tracks the PFNs of the page table pages. If a page directory can’t fit in a single page, we may use more than 2 levels. For example, 64-bit architectures use up to 7 levels!</p> <p>How is the address translation done in this case? The first few bits of the VA identify the outer page table entry. The next few bits are used to index into the next level of PTEs.</p> <p>What about TLB misses? We need to perform multiple access to memory required to access all the levels of page tables. This is a lot of overhead!</p> <h1 id="lecture-10---demand-paging">Lecture 10 - Demand Paging</h1> <p>The main memory may not be enough to store all the page tables of active processes. In such cases, the OS uses a part of the disk (<strong>swap space</strong>) to store pages that are not in active use. Therefore, physical memory is allocated on demand, and this is called demand paging.</p> <h2 id="page-fault">Page Fault</h2> <p>The present bit in the page table entry indicates if a page of a process resides in the main memory or not (swap). When translating from VA to PA, the MMU reads the present bit. If the page is present in the memory, the location is directly accessed. Otherwise, the MMU raises a trap to the OS - <strong>page fault</strong>. (<em>No fault happened, actually</em>).</p> <p>The page fault traps OS and moves CPU to kernel mode like any other system call. Then, the OS fetches the disk address of the page and issues a “read” to the disk. How does the OS know the location of pages on the disk? It keeps track of disk addresses (say, in a page table). The OS context switches to another process, and the current process is blocked.</p> <blockquote> <p>Suppose the CPU context switches from the MMU read from swap to another process. When it comes back to the process, the disk would have fetched the address from the swap. How does the MMU revert back to its previous state? The other process to which the CPU context switched will have used the MMU. -&gt; See the below paragraph</p> </blockquote> <p>Eventually, when the disk read completes, the OS updates the process’s page table and marks the process as ready. When the process is scheduled again, the OS <strong>restarts the instruction</strong> that caused the page fault.</p> <h2 id="summary---memory-access">Summary - Memory Access</h2> <p>The CPU issues a load to a VA for code/data. Before sending a request, the CPU checks its cache first. It goes to the main memory if the cache misses. <strong>Note.</strong> This is not the TLB cache.</p> <p>After the control reaches the main memory, the MMU looks up the TLB for VA. If TLB is hit, the PA is obtained, and the code/data is returned to the CPU. Otherwise, the MMU accesses memory, walks the page table (maybe multiple levels), and obtains the entry.</p> <ul> <li>If the present bit is set in PTE, the memory is accessed. Think about this point carefully. The frame may be present in physical memory or the swap.</li> <li>The MMU raises a page fault if the present bit is not set but is valid access. The OS handles page fault and restarts the CPU load instruction</li> <li>In the case of invalid page access, trap to OS for illegal access.</li> </ul> <h2 id="complications-in-page-faults">Complications in page faults</h2> <p>What does the OS do when servicing a page fault if there is no free page to swap in the faulting page? The OS must swap out an existing page (if modified, i.e., dirty) and then swap in the faulting page. However, this is too much work! To avoid this, the OS may proactively swap out pages to keep the list of free pages. Pages are swapped out based on the <strong>page replacement policy</strong>.</p> <h2 id="page-replacement-policies">Page Replacement policies</h2> <p>The optimal strategy would be to replace a page that is not needed for the longest time in the future. This is not practical. We can use the following policies -</p> <p><strong>FIFO Policy</strong> - Replace the page that was brought into the memory the earliest. However, this may be a popular page.</p> <p><strong>LRU/LFU</strong> - This is commonly used in practical OS. In this policy, we replace the page that was least recently (or frequently) used in the past.</p> <h2 id="example---page-replacement-policy">Example - Page Replacement Policy</h2> <p><img src="/assets/img/Operating Systems/image-20210827230153801.png" alt="image-20210827230153801"/></p> <p>Suppose we can store only 3 frames in the physical memory, and there are 4 pages in the process. The set of accesses is also known as the <strong>reference string</strong>. Note that the initial few accesses are definitely missed, as the cache is empty - cold misses. The goal is to reduce the number of page faults, which leads to reading from the swap space and is slow.</p> <p><strong>Belady’s anomaly</strong> - The performance of the FIFO may get worse when the memory size increases.</p> <p>The LRU works better as it makes use of the locality of references.</p> <h3 id="how-is-lru-implemented">How is LRU implemented?</h3> <p>The OS is not involved in every memory access.</p> <blockquote> <p>Why?</p> </blockquote> <p>Therefore, the OS does not know which page is the LRU. There is some hardware help and some approximations which help the OS to find the LRU page. The MMU sets a bit in the PTE (<em>accessed</em> bit) when a page is accessed. The OS periodically looks at this bit to estimate pages that are active and inactive.</p> <blockquote> <p>How often does the OS check this? Going through all pages also takes time. Are interrupts used?</p> </blockquote> <p>The OS tries to find a page that does not have the access bit set to replace a page. It can also look for a page with the dirty bit not set to avoid swapping out to disk.</p> <blockquote> <p>If the dirty bit is set, using that page would involve writing to disk. Why?</p> </blockquote> <h1 id="lecture-11---memory-allocation-algorithms">Lecture 11 - Memory Allocation Algorithms</h1> <p>Let us first discuss the problems with variable/dynamic size allocation. This is done from the C library - allocates one or more pages from the kernel via <code class="language-plaintext highlighter-rouge">brk/sbrk</code> <code class="language-plaintext highlighter-rouge">mmap</code> system calls. The user may ask for variable-sized chunks of memory and can arbitrarily free the used memory. The C library and the kernel (for its internal data structures) must take care of all this.</p> <h2 id="variable-sized-allocation---headers">Variable sized allocation - Headers</h2> <p>Consider a simple implementation of <code class="language-plaintext highlighter-rouge">malloc</code>. An available chunk of memory is allocated on request. Every assigned piece has a header with info like chunk size, checksum/some magic number, etc. Why store size? We should know how much memory to free when <code class="language-plaintext highlighter-rouge">free</code> is called.</p> <h2 id="free-list">Free List</h2> <p>How is the free space managed? It is usually handled as a list. The library keeps track of the head of the list. The pointer to the next free chunk is embedded within the current head. Allocations happen from the head.</p> <h2 id="external-fragmentation">External Fragmentation</h2> <p>Suppose 3 allocations of size 100 bytes each happen. Then, the middle chunk pointed to by <code class="language-plaintext highlighter-rouge">sptr</code> is freed. How is the free list updated? It now has two non-contiguous elements. The free space may be scattered around due to fragmentation. Therefore, we cannot satisfy a request for 3800 bytes even though we have free space. This is the primary problem of variable allocation.</p> <p><strong>Note.</strong> The list is updated to account for the newly freed space. That is, the head is revised to point to <code class="language-plaintext highlighter-rouge">sptr</code>, and the list is updated accordingly. Don’t be under the false impression that we are missing out on free space.</p> <h2 id="splitting-and-coalescing">Splitting and Coalescing</h2> <p>Suppose we have a bunch of adjacent free chunks. These chunks may not be adjacent in the list. If we had started out with a big free piece, we might end up with small tangled chunks. We need an algorithm that merged all the contiguous free fragments into a bigger free chunk. We must also be able to split the existing free pieces to satisfy the variable requests.</p> <h3 id="buddy-allocation-for-easy-coalescing">Buddy allocation for easy coalescing</h3> <p>Allocate memory only in sizes of power of 2. This way, 2 adjacent power-of-2 chunks can be merged to form a bigger power-of-2 chunk. That is, buddies can be combined to form bigger pieces.</p> <h2 id="variable-size-allocation-strategies">Variable Size Allocation Strategies</h2> <p><strong>First Fit</strong> - Allocate the first chunk that is sufficient</p> <p><strong>Best Fit</strong> - Allocate the free chunk that is closest in size to the request.</p> <p><strong>Worst Fit</strong> - Allocate the free chunk that is farthest in size. Sounds odd? It’s better sometimes as the remaining free space in the chunk is large and is more usable. For example, the best fit might allocate a 20-byte chunk for a malloc(15), and the worst might give a 100-byte chunk for the same call. Now, the 85-byte free space is more usable than the 5-byte free space.</p> <blockquote> <p>Do we use this in the case of buddy allocation?</p> </blockquote> <h2 id="fixed-size-allocations">Fixed Size allocations</h2> <p>Fixed-size allocations are much simpler as they avoid fragmentation and various other problems. The kernel issues fix <strong>page-sized</strong> allocations. It maintains a free list of pages, and the pointer to the following free page is stored in the current free page itself. What about smaller allocations (e.g., PCB)? The kernel uses a <strong>slab allocator</strong>. It maintains <strong>object caches</strong> for each type of object. Within each cache, only fixed size allocation is done. Each cache is made up of one or more <em>slabs</em>. Within a page, we have fixed size allocations again.</p> <p>Fixed size memory allocators can be used in user programs too. <code class="language-plaintext highlighter-rouge">malloc</code> is a generic memory allocator, but we can use other methods too.</p> <h1 id="live-session-5">Live Session 5</h1> <ul> <li> <p><strong>Dirty bit</strong> -The MMU sets this bit (unlike the present bit and valid bit, which are set by the OS) when the page is recently swapped into the memory. The OS needs this while evicting pages from the memory.</p> <p>Basically, when you swap in a page frame from the disk to the main memory, you <strong>copy-paste</strong> the frame. When the page in the main memory is modified, it is not in sync with the page in the disk. At this point, the MMU <strong>sets</strong> the <strong>dirty bit</strong>.</p> </li> <li> <p>A page can be in</p> <ul> <li>Swap</li> <li>Main Memory</li> <li>Unallocated</li> </ul> <p>Now, note that all of the 4GB memory space need not be allocated.</p> <p>When the address is not allocated at all, the <strong>valid bit</strong> is <strong>unset</strong>. Whenever a virtual address is accessed, and consequently, the memory is allocated (allocated in the main memory), the <strong>valid bit</strong> is set. This memory allocation is triggered by a <strong>page fault</strong>. Initially, the newly allocated memory has the <strong>present bit set</strong> (since we just allocated the memory, we will use it). A page is swapped into the disk when not in active use, and the <strong>present bit</strong> is <strong>unset</strong>.</p> </li> <li> <p>Access bit is <strong>set</strong> whenever a page is accessed. The bit is unset periodically by the OS.</p> </li> <li> <p>The device driver maintains a queue to process multiple reads/writes to the disk.</p> </li> <li> <p>The OS code is mapped into the virtual address space of the processes. It has all the OS information, PCBs, etc. This way, there wouldn’t be much hassle during context switching. The page table of a process also has entries for the OS code.</p> <p>The OS memory inside the process memory can increase. If it takes too much space, the modern OS has some techniques to prevent the OS space from encroaching the process’s memory space.</p> <p>A PTE also has <strong>permission bits</strong> that prevent the user code from accessing the OS code. When the program has to access the OS code, the <strong>trap instruction</strong> switches us into a higher privilege level and moves us into the OS code.</p> <p>After we enter the kernel mode, we still cannot modify the OS code in any way we want. This is because we enter the kernel mode using thoroughly defined system calls.</p> <p>Remember, the <code class="language-plaintext highlighter-rouge">int n</code> instruction is run by the <strong>hardware</strong>.</p> </li> <li> <p>Every time the page table is updated, the TLB has to be updated too. This is done via special instructions.</p> </li> <li> <p>The user code runs natively on the CPU. The OS asks the CPU to execute the instructions sequentially. Then, the OS is out of the picture. All the memory fetches are done via the CPU.</p> <p>Now, you may think, when does the OS actually be involved in the memory access. The OS has to periodically check over the processes (maybe during access bit updates). Even when system calls are made, the OS has a play.</p> </li> <li> <p>There is a hardware register where the address of the page table is stored. The MMU accesses the page table using this address.</p> </li> </ul> <h1 id="lecture-27---virtual-memory--and-paging-in-xv6">Lecture 27 - Virtual Memory and paging in xv6</h1> <p>We have a 32-bits version on xv6; thereby, we have \(2^{32} =\) 4GB virtual address space for every process. The process address space is divided into pages (4KB by default). Every valid <strong>logical</strong> page used by the process is mapped to a physical frame by the OS (no demand paging). There is a single page table entry per page containing the physical frame number (<strong>PFN</strong>) and various flags/permissions for the page.</p> <h2 id="page-table-in-xv6">Page table in xv6</h2> <p>There can be up to \(2^{20}\) page table entries for a process with the properties we have described above. Each PTE has a 20-bit physical frame number and some flags</p> <ul> <li><code class="language-plaintext highlighter-rouge">PTE_P</code> indicates if a page is present. If not set, access will cause a page fault.</li> <li><code class="language-plaintext highlighter-rouge">PTE_W</code> indicates if writable. If not set, only reading is permitted</li> <li><code class="language-plaintext highlighter-rouge">PTE_U</code> indicates if user page. If not set, only the kernel can access the page.</li> </ul> <p>Address translation is done via the page number (top 20 bits of a virtual address) to index into the page table, find the PFN, add a 12-bit offset (for navigating inside the frame).</p> <h2 id="two-level-page-table">Two-level page table</h2> <p>All the \(2^{20}\) entries can’t be stored contiguously. Therefore, the page table in xv6 has two levels. We have \(2^{10}\) <em>inner</em> page tables pages, each with \(2^{10}\) PTEs. The <em>outer</em> page director stores PTE-like references to the \(2^{10}\) inner page tables. The physical address of the outer page directory is stored in the CPU’s <code class="language-plaintext highlighter-rouge">cr3</code> register, which is used by the MMU during address translation.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A virtual address ’la’ has a three−part structure as follows:
+−−−−−−−−10−−−−−−+−−−−−−−10−−−−−−−+−−−−−−−−−12−−−−−−−−−−+
| Page Directory | Page Table | Offset within Page |
| 	  Index	     |   Index    |					   |
+−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−+
\−−− PDX(va) −−/ \−−− PTX(va) −−/
</code></pre></div></div> <p>Therefore, we now have 32-bit virtual addresses.</p> <h2 id="process-virtual-address-space-in-xv6">Process virtual address space in xv6</h2> <p>The memory image of a process starting at address 0 has the following: code/data from the executable, <em>fixed</em>-size stack with <strong>guard page</strong> (to prevent overflowing), expandable heap. This is how the user part of the process is organized.</p> <p>The process space also has the kernel code/data beginning at the address <code class="language-plaintext highlighter-rouge">KERNBASE (2GB)</code>. This part contains kernel code/data, free pages maintained by the kernel, and some space reserved for the I/O devices.</p> <p>The page table of a process contains two sets of PTEs.</p> <ul> <li>The user entries map low virtual addresses to the physical memory sued by the process for its code/data/stack/heap.</li> <li>The kernel entries map high virtual addresses to physical memory containing OS code and data structures. These entries are identical across all processes.</li> </ul> <p>A process can only access memory mapped by its own page table.</p> <h2 id="os-page-table-mappings">OS page table mappings</h2> <p>The OS code/data structures are a part of the virtual address space of every process. The page table entries map high virtual addresses (2GB to 2GB + <code class="language-plaintext highlighter-rouge">PHYSTOP</code>) to OS code/data in physical memory (~0 to <code class="language-plaintext highlighter-rouge">PHYSTOP</code>). This part contains the kernel code/data, I/O devices, and primarily free pages. Note that there is only a single copy of OS code in memory, mapped into all process page tables.</p> <p>Can’t we directly access the OS code using its physical address? No. With paging and MMU, the physical memory can only be accessed by assigning a virtual address. During a trap, the same page table can be used to access the kernel. If the OS is not a part of the virtual address space, we would have had to use a new page table during trap which is cumbersome (?).</p> <p>Some of the aforementioned free pages in the OS memory are assigned to processes. Suppose a physical frame P is initially mapped into the kernel part of the address space at virtual address V ( we will have V = P + <code class="language-plaintext highlighter-rouge">KERNBASE</code>). When assigned to a user process, this piece of memory is assigned another virtual address U (&lt; <code class="language-plaintext highlighter-rouge">KERNBASE</code>). This is because a user cannot utilize this free page unless the PTE is in the userspace. Hence, the same frame P is mapped twice into the page table! The kernel and user access the same memory using different virtual addresses.</p> <blockquote> <p>What is going on above?</p> </blockquote> <p>Every byte of RAM can consume 2 bytes of virtual address space, so xv6 cannot use more than 2GB of RAM. Actual kernels deal with this better. For example, the kernel page is deleted when a user page is created.</p> <h2 id="maintaining-free-memory">Maintaining free memory</h2> <p>After bootup, RAM contains the OS code/data and free pages. The OS collects all the free pages into a free list called <code class="language-plaintext highlighter-rouge">run</code> to be assigned to the user processes. This free list is a linked list, and the pointer to the next free page is embedded within the previous free page.</p> <p>Any process that needs a free page uses <code class="language-plaintext highlighter-rouge">kalloc()</code> to get a free page. Memory is freed up using <code class="language-plaintext highlighter-rouge">kfree()</code>. We need to add the free page to the head of the free list and update the free list pointer. Take a look at the codes for this part.</p> <h2 id="summary-of-virtual-memory-in-xv6">Summary of virtual memory in xv6</h2> <p>xv6 only has virtual addressing, no demand paging. There is a 2 tier page table, outer <code class="language-plaintext highlighter-rouge">pgdir</code>, and inner pages tables. The process address space has</p> <ul> <li>User memory image at low virtual addresses</li> <li>Kernel code/data mapped at high virtual addresses.</li> </ul> <h1 id="lecture-28---memory-management-of-user-processes-in-xv6">Lecture 28 - Memory Management of user processes in xv6</h1> <p>The user process needs memory pages to build its address space. Every process requires memory for memory image and page table. The free list of the kernel allocates this memory using <code class="language-plaintext highlighter-rouge">kalloc</code>. The new virtual address space for a process is created during <code class="language-plaintext highlighter-rouge">init</code> process creation, <code class="language-plaintext highlighter-rouge">fork</code> system call, and <code class="language-plaintext highlighter-rouge">exec</code> system call. The existing virtual address space is modified using the <code class="language-plaintext highlighter-rouge">sbrk</code> system call (to expand heap). The page table is constructed in the following manner:</p> <ul> <li>We start with a single page for the outer page directory</li> <li>We allocate inner page tables as and when needed.</li> </ul> <h2 id="functions-to-build-page-table">Functions to build page table</h2> <p>Every page table begins setting up the kernel mappings in <code class="language-plaintext highlighter-rouge">setupkvm</code>. The outer <code class="language-plaintext highlighter-rouge">pgdir</code> is allocated. The kernel mappings defined in <code class="language-plaintext highlighter-rouge">kmap</code> are added to the page table by calling <code class="language-plaintext highlighter-rouge">mappages</code>. After <code class="language-plaintext highlighter-rouge">setupkvm</code>, user page table mappings are added.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This table defines the kernel’s mappings, which are present in</span>
<span class="c1">// every process’s page table.</span>
<span class="k">static</span> <span class="k">struct</span> <span class="n">kmap</span> <span class="p">{</span>
    <span class="kt">void</span> <span class="o">*</span><span class="n">virt</span><span class="p">;</span>
    <span class="n">uint</span> <span class="n">phys_start</span><span class="p">;</span>
    <span class="n">uint</span> <span class="n">phys_end</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">perm</span><span class="p">;</span>
<span class="p">}</span> <span class="n">kmap</span><span class="p">[]</span> <span class="o">=</span> <span class="p">{</span>
    <span class="p">{</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">KERNBASE</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>	<span class="n">EXTMEM</span><span class="p">,</span> 	<span class="n">PTE_W</span><span class="p">},</span> <span class="c1">// I/O space</span>
    <span class="p">{</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">KERNLINK</span><span class="p">,</span> <span class="n">V2P</span><span class="p">(</span><span class="n">KERNLINK</span><span class="p">),</span>	<span class="n">V2P</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> 	<span class="mi">0</span><span class="p">},</span> <span class="c1">// kern text + rodata</span>
    <span class="p">{</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">data</span><span class="p">,</span>	<span class="n">V2P</span><span class="p">(</span><span class="n">data</span><span class="p">),</span>	<span class="n">PHYSTOP</span><span class="p">,</span> 	<span class="n">PTE_W</span><span class="p">},</span> <span class="c1">// kern data + memory</span>
    <span class="p">{</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">DEVSPACE</span><span class="p">,</span> <span class="n">DEVSPACE</span><span class="p">,</span>	<span class="mi">0</span><span class="p">,</span> 	<span class="n">PTE_W</span><span class="p">},</span> <span class="c1">// more devices</span>
<span class="p">};</span>

<span class="c1">// Set up kernel part of a page table.</span>
<span class="n">pde_t</span><span class="o">*</span>
<span class="nf">setupkvm</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">pde_t</span> <span class="o">*</span><span class="n">pgdir</span><span class="p">;</span>
    <span class="k">struct</span> <span class="n">kmap</span> <span class="o">*</span><span class="n">k</span><span class="p">;</span>
    <span class="k">if</span><span class="p">((</span><span class="n">pgdir</span> <span class="o">=</span> <span class="p">(</span><span class="n">pde_t</span><span class="o">*</span><span class="p">)</span><span class="n">kalloc</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
    <span class="n">memset</span><span class="p">(</span><span class="n">pgdir</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">PGSIZE</span><span class="p">);</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">P2V</span><span class="p">(</span><span class="n">PHYSTOP</span><span class="p">)</span> <span class="o">&gt;</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">DEVSPACE</span><span class="p">)</span>
        <span class="n">panic</span><span class="p">(</span><span class="s">"PHYSTOP too high"</span><span class="p">);</span>
    <span class="k">for</span><span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="n">kmap</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="o">&amp;</span><span class="n">kmap</span><span class="p">[</span><span class="n">NELEM</span><span class="p">(</span><span class="n">kmap</span><span class="p">)];</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span>
        <span class="k">if</span><span class="p">(</span><span class="n">mappages</span><span class="p">(</span><span class="n">pgdir</span><span class="p">,</span> <span class="n">k</span><span class="err">−</span><span class="o">&gt;</span><span class="n">virt</span><span class="p">,</span> <span class="n">k</span><span class="err">−</span><span class="o">&gt;</span><span class="n">phys_end</span> <span class="err">−</span> <span class="n">k</span><span class="err">−</span><span class="o">&gt;</span><span class="n">phys_start</span><span class="p">,</span>
        <span class="p">(</span><span class="n">uint</span><span class="p">)</span><span class="n">k</span><span class="err">−</span><span class="o">&gt;</span><span class="n">phys_start</span><span class="p">,</span> <span class="n">k</span><span class="err">−</span><span class="o">&gt;</span><span class="n">perm</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">freevm</span><span class="p">(</span><span class="n">pgdir</span><span class="p">);</span>
            <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="k">return</span> <span class="n">pgdir</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>The page table entries are added by <code class="language-plaintext highlighter-rouge">mappages</code>. The arguments are page directory, range of virtual addresses, physical addresses to map to, and permissions of the pages. This function walks the page table for each page, gets the pointer to PTE via the function <code class="language-plaintext highlighter-rouge">walkpgdir</code>, and fills it with physical address and permissions. The function <code class="language-plaintext highlighter-rouge">walkpgdir</code> walks the page table and returns the PTE of a virtual address.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Return the address of the PTE in page table pgdir</span>
<span class="c1">// that corresponds to virtual address va. If alloc!=0,</span>
<span class="c1">// create any required page table pages.</span>
<span class="k">static</span> <span class="n">pte_t</span> <span class="o">*</span>
<span class="nf">walkpgdir</span><span class="p">(</span><span class="n">pde_t</span> <span class="o">*</span><span class="n">pgdir</span><span class="p">,</span> <span class="k">const</span> <span class="kt">void</span> <span class="o">*</span><span class="n">va</span><span class="p">,</span> <span class="kt">int</span> <span class="n">alloc</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">pde_t</span> <span class="o">*</span><span class="n">pde</span><span class="p">;</span>
    <span class="n">pte_t</span> <span class="o">*</span><span class="n">pgtab</span><span class="p">;</span>
    <span class="n">pde</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">pgdir</span><span class="p">[</span><span class="n">PDX</span><span class="p">(</span><span class="n">va</span><span class="p">)];</span>
    <span class="k">if</span><span class="p">(</span><span class="o">*</span><span class="n">pde</span> <span class="o">&amp;</span> <span class="n">PTE_P</span><span class="p">){</span>
        <span class="n">pgtab</span> <span class="o">=</span> <span class="p">(</span><span class="n">pte_t</span><span class="o">*</span><span class="p">)</span><span class="n">P2V</span><span class="p">(</span><span class="n">PTE_ADDR</span><span class="p">(</span><span class="o">*</span><span class="n">pde</span><span class="p">));</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="k">if</span><span class="p">(</span><span class="o">!</span><span class="n">alloc</span> <span class="o">||</span> <span class="p">(</span><span class="n">pgtab</span> <span class="o">=</span> <span class="p">(</span><span class="n">pte_t</span><span class="o">*</span><span class="p">)</span><span class="n">kalloc</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
        <span class="c1">// Make sure all those PTE_P bits are zero.</span>
        <span class="n">memset</span><span class="p">(</span><span class="n">pgtab</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">PGSIZE</span><span class="p">);</span>
        <span class="c1">// The permissions here are overly generous, but they can</span>
        <span class="c1">// be further restricted by the permissions in the page table</span>
        <span class="c1">// entries, if necessary.</span>
        <span class="o">*</span><span class="n">pde</span> <span class="o">=</span> <span class="n">V2P</span><span class="p">(</span><span class="n">pgtab</span><span class="p">)</span> <span class="o">|</span> <span class="n">PTE_P</span> <span class="o">|</span> <span class="n">PTE_W</span> <span class="o">|</span> <span class="n">PTE_U</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="o">&amp;</span><span class="n">pgtab</span><span class="p">[</span><span class="n">PTX</span><span class="p">(</span><span class="n">va</span><span class="p">)];</span>
<span class="p">}</span>
<span class="c1">// Create PTEs for virtual addresses starting at va that refer to</span>
<span class="c1">// physical addresses starting at pa. va and size might not</span>
<span class="c1">// be page−aligned.</span>
<span class="k">static</span> <span class="kt">int</span>
<span class="nf">mappages</span><span class="p">(</span><span class="n">pde_t</span> <span class="o">*</span><span class="n">pgdir</span><span class="p">,</span> <span class="kt">void</span> <span class="o">*</span><span class="n">va</span><span class="p">,</span> <span class="n">uint</span> <span class="n">size</span><span class="p">,</span> <span class="n">uint</span> <span class="n">pa</span><span class="p">,</span> <span class="kt">int</span> <span class="n">perm</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">char</span> <span class="o">*</span><span class="n">a</span><span class="p">,</span> <span class="o">*</span><span class="n">last</span><span class="p">;</span>
    <span class="n">pte_t</span> <span class="o">*</span><span class="n">pte</span><span class="p">;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">PGROUNDDOWN</span><span class="p">((</span><span class="n">uint</span><span class="p">)</span><span class="n">va</span><span class="p">);</span>
    <span class="n">last</span> <span class="o">=</span> <span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">PGROUNDDOWN</span><span class="p">(((</span><span class="n">uint</span><span class="p">)</span><span class="n">va</span><span class="p">)</span> <span class="o">+</span> <span class="n">size</span> <span class="err">−</span> <span class="mi">1</span><span class="p">);</span>
    <span class="k">for</span><span class="p">(;;){</span>
        <span class="k">if</span><span class="p">((</span><span class="n">pte</span> <span class="o">=</span> <span class="n">walkpgdir</span><span class="p">(</span><span class="n">pgdir</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">return</span> <span class="err">−</span><span class="mi">1</span><span class="p">;</span>
        <span class="k">if</span><span class="p">(</span><span class="o">*</span><span class="n">pte</span> <span class="o">&amp;</span> <span class="n">PTE_P</span><span class="p">)</span>
            <span class="n">panic</span><span class="p">(</span><span class="s">"remap"</span><span class="p">);</span>
        <span class="o">*</span><span class="n">pte</span> <span class="o">=</span> <span class="n">pa</span> <span class="o">|</span> <span class="n">perm</span> <span class="o">|</span> <span class="n">PTE_P</span><span class="p">;</span>
        <span class="k">if</span><span class="p">(</span><span class="n">a</span> <span class="o">==</span> <span class="n">last</span><span class="p">)</span>
            <span class="k">break</span><span class="p">;</span>
        <span class="n">a</span> <span class="o">+=</span> <span class="n">PGSIZE</span><span class="p">;</span>
        <span class="n">pa</span> <span class="o">+=</span> <span class="n">PGSIZE</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="fork-copying-the-memory-image">Fork: copying the memory image</h2> <p>The <code class="language-plaintext highlighter-rouge">copyuvm</code> function is called by the parent to copy the parent’s memory image to the child. Check out <code class="language-plaintext highlighter-rouge">fork</code>’s code <a href="#fork-system-call">here</a>. This function starts out by creating a new page table for the child. Then, it has to walk through the parent’s memory image page by page and copy it to the child while adding the child page table mappings.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">copyuvm</span><span class="p">(</span><span class="n">pde_t</span> <span class="o">*</span><span class="n">pgdir</span><span class="p">,</span> <span class="n">uint</span> <span class="n">sz</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">pde_t</span> <span class="o">*</span><span class="n">d</span><span class="p">;</span>
    <span class="n">pte_t</span> <span class="o">*</span><span class="n">pte</span><span class="p">;</span>
    <span class="n">uint</span> <span class="n">pa</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">flags</span><span class="p">;</span>
    <span class="kt">char</span> <span class="o">*</span><span class="n">mem</span><span class="p">;</span>
    <span class="k">if</span><span class="p">((</span><span class="n">d</span> <span class="o">=</span> <span class="n">setupkvm</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">sz</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">PGSIZE</span><span class="p">){</span>
        <span class="k">if</span><span class="p">((</span><span class="n">pte</span> <span class="o">=</span> <span class="n">walkpgdir</span><span class="p">(</span><span class="n">pgdir</span><span class="p">,</span> <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="p">)</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">panic</span><span class="p">(</span><span class="s">"copyuvm: pte should exist"</span><span class="p">);</span>
        <span class="k">if</span><span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="o">*</span><span class="n">pte</span> <span class="o">&amp;</span> <span class="n">PTE_P</span><span class="p">))</span>
            <span class="n">panic</span><span class="p">(</span><span class="s">"copyuvm: page not present"</span><span class="p">);</span>
        <span class="n">pa</span> <span class="o">=</span> <span class="n">PTE_ADDR</span><span class="p">(</span><span class="o">*</span><span class="n">pte</span><span class="p">);</span>
        <span class="n">flags</span> <span class="o">=</span> <span class="n">PTE_FLAGS</span><span class="p">(</span><span class="o">*</span><span class="n">pte</span><span class="p">);</span>
        <span class="k">if</span><span class="p">((</span><span class="n">mem</span> <span class="o">=</span> <span class="n">kalloc</span><span class="p">())</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">goto</span> <span class="n">bad</span><span class="p">;</span>
        <span class="n">memmove</span><span class="p">(</span><span class="n">mem</span><span class="p">,</span> <span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">P2V</span><span class="p">(</span><span class="n">pa</span><span class="p">),</span> <span class="n">PGSIZE</span><span class="p">);</span>
        <span class="k">if</span><span class="p">(</span><span class="n">mappages</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">i</span><span class="p">,</span> <span class="n">PGSIZE</span><span class="p">,</span> <span class="n">V2P</span><span class="p">(</span><span class="n">mem</span><span class="p">),</span> <span class="n">flags</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
            <span class="n">kfree</span><span class="p">(</span><span class="n">mem</span><span class="p">);</span>
            <span class="k">goto</span> <span class="n">bad</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">d</span><span class="p">;</span>
    <span class="nl">bad:</span>
        <span class="n">freevm</span><span class="p">(</span><span class="n">d</span><span class="p">);</span>
        <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>For each page in the parent,</p> <ul> <li>We fetch the PTE and gets its physical address and permissions</li> <li>We allocate a new page for the child and copy the parent’s page’s contents to the child’s new page.</li> <li>Then, we add a PTE from the virtual address to the physical address of the new page in the child page table.</li> </ul> <p>Real operating systems do copy-on-write fork - The child page table also points to the parent pages until either of them modifies the pages. Here, xv6 creates separate memory images for the parent and the child right away.</p> <h2 id="growing-memory-image---sbrk">Growing memory image - <code class="language-plaintext highlighter-rouge">sbrk</code></h2> <p>Initially, the heap of a process is empty, and the program <em>break</em> is at the end of the stack. The <code class="language-plaintext highlighter-rouge">sbrk</code> system call is invoked by <code class="language-plaintext highlighter-rouge">malloc</code> to expand the heap. The <code class="language-plaintext highlighter-rouge">allocuvm</code> allocates new pages and adds mappings into the page table for the new pages to grow memory. Whenever the page table is updated, we must update the <code class="language-plaintext highlighter-rouge">cr3</code> register and TLB (using <code class="language-plaintext highlighter-rouge">switchuvm</code>).</p> <p>The <code class="language-plaintext highlighter-rouge">allocuvm</code> function walks through the new virtual addresses to be added in the page-sized chunks.</p> <h2 id="exec-system-call-1">Exec System call</h2> <p>Refer to the code in xv6 documentation. It reads the ELF (the new executable) binary file from the disk into memory. It starts with a new page table and adds mappings to the new executable pages to grow the virtual address space. So far, it hasn’t overwritten the old page table. Once the executable is copied to the memory image, we allocate 2 pages for stack (1 for guard page whose permissions are cleared, and upon accessing will trap). All the <code class="language-plaintext highlighter-rouge">exec</code> arguments are pushed on the user stack for the main function of the new program.</p> <p>If no errors have occurred so far, we switch to the new page table. We set <code class="language-plaintext highlighter-rouge">eip</code> in trapframe to start at the entry point of the new program.</p> <h1 id="live-session-6">Live Session 6</h1> <ul> <li> <p>Why do we have <code class="language-plaintext highlighter-rouge">walkpgdir</code> in <code class="language-plaintext highlighter-rouge">copyuvm</code>? <del>Aren’t page tables contiguous? Self-answered: The <code class="language-plaintext highlighter-rouge">copyuvm</code> function copies entire pages. The <code class="language-plaintext highlighter-rouge">walkpgdir</code> is only for the outer page directory.</del></p> <p><code class="language-plaintext highlighter-rouge">walkpgdir</code> simply returns the PFN of the input VPN, and allocates a new inner page if necessary. Therefore, there is no redundancy!</p> </li> <li> <p>Why is the kernel allocation memory from its free pages to the user processes? The purpose of double mapping in virtual space.</p> <p>Think of it this way. The true RAM of your computer is only from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">PHYSTOP</code>. Every piece of allocated memory comes from <code class="language-plaintext highlighter-rouge">0</code> to <code class="language-plaintext highlighter-rouge">PHYSTOP</code>, and this maps to both kernel and user part of the memory image of a process. Yes, it’s naive! Modern Operating systems have other optimizations.</p> </li> <li> <p>In line 2600 of <code class="language-plaintext highlighter-rouge">fork</code> system call, why do we copy trapframe again? <code class="language-plaintext highlighter-rouge">uvm</code> in <code class="language-plaintext highlighter-rouge">copyuvm</code> means user virtual memory. This function only copies the user part of the memory. The kernel stack and other structures are built using <code class="language-plaintext highlighter-rouge">allocproc</code>. You have to manually copy the <code class="language-plaintext highlighter-rouge">struct proc</code> data or any other kernel data for that matter.</p> </li> <li> <p><strong>Note</strong>. A process can think it has \(1\)TB of memory too! The present bit takes care of this.</p> </li> <li> <p>The OS does not take part in every memory access. Why?</p> <p>The OS is only in play when the process is being initialised, page faults, or when it is being context switched. Otherwise, the MMU + TLB take care of the address translations. Therefore, the MMU also needs to perform checks on the virtual address and ensure the query is not invalid.</p> </li> </ul> <h1 id="lecture-12---threads-and-concurrency">Lecture 12 - Threads and Concurrency</h1> <p>So far, we have studied <strong>single-threaded</strong> programs. That is, there is only a single thread of execution. When a given memory image is being executed in the memory, there is only a single <em>execution flow</em>. A program can also have multiple threads of execution.</p> <p>Firstly, what is a <strong>thread</strong>? A thread is like another copy of a process that executes independently. In a multi-threaded program, we have multiple threads of execution. For example, a 2-thread process will have 2 PCs and 2 SPs. Threads share the same address space. However, each thread has a separate PC to run over different parts of the program simultaneously. Each thread also has a separate stack for independent function calls.</p> <h2 id="process-vs-thread">Process vs. Thread</h2> <p>When a parent process P forks a child C, P and C do not share any memory. They need complicated <a href="#lecture-6---inter-process-communication-ipc">IPC mechanisms</a>. If we want to transfer data while running different parts of the process simultaneously, we need extra copies of code and data in memory. It’s very complicated to handle simultaneous execution using processes.</p> <p>However, if a parent P executes two threads T1 and T2, they share parts of the address space. The process can use global variables for communication. This method also has a smaller memory footprint.</p> <p>In conclusion, threads are like separate processes, except they use the same address space.</p> <h2 id="why-threads">Why threads?</h2> <p>Why do we want to run different parts of the same program simultaneously? <strong>Parallelism</strong> - A single process can effectively utilize multiple CPU cores. There is a difference between <strong>concurrency</strong> and <strong>parallelism</strong>.</p> <ul> <li>Concurrency - Running multiple threads/ processes in tandem, even on a single CPU core, by interleaving their executions. Basically, all the stuff we’ve learned in the single thread execution.</li> <li>Parallelism - Running multiple threads/processes in parallel over different CPU cores.</li> </ul> <p>We can exploit parallelism using multiple threads. Even if there is no scope for parallelism, concurrently running threads ensures effective use of CPU when one of the thread blocks.</p> <h2 id="scheduling-threads">Scheduling threads</h2> <p>The OS schedules threads that are ready to run independently, much like processes. The context of a thread (PC, registers) is saved into/ restored from a <strong><em>thread control block (TCB)</em></strong>. Every PCB has one or more linked TCBs. Threads that are scheduled independently by the kernel are called <strong>kernel threads</strong>. For example, Linux <code class="language-plaintext highlighter-rouge">pthreads</code> are kernel threads.</p> <blockquote> <p>What are kernel threads?</p> </blockquote> <p>In contrast, some libraries provide user-level threads. These are not very common. This fact implies that not all threads are kernel threads. The library multiplexes a larger number of user threads over a smaller number of kernel threads. The kernel of the process sees these threads as separate processes. Also, note switching between user threads has a low overhead (nothing like context switching). However, multiple user threads cannot be run in parallel. Therefore, user threads are not very useful.</p> <blockquote> <p>What is going on in the above paragraph?</p> </blockquote> <h2 id="creating-threads-using-pthreads-api">Creating threads using <code class="language-plaintext highlighter-rouge">pthreads</code> API</h2> <p>Here is a simple thread creating code.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp"># include &lt;pthread.h&gt;
</span><span class="kt">int</span> <span class="nf">main</span><span class="p">(...)</span>
<span class="p">{</span>
	<span class="n">pthread_t</span> <span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">;</span>
	<span class="kt">int</span> <span class="n">rc</span> <span class="o">=</span> <span class="n">pthread_create</span><span class="p">(</span><span class="o">&amp;</span><span class="n">p1</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">,</span> <span class="n">fun1</span><span class="p">,</span> <span class="s">"function_argument"</span><span class="p">);</span> 
	<span class="n">assert</span><span class="p">(</span><span class="n">rc</span> <span class="o">==</span> <span class="mi">0</span><span class="p">);</span>
	<span class="n">rc</span> <span class="o">=</span> <span class="n">pthread_create</span><span class="p">(</span><span class="o">&amp;</span><span class="n">p2</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">,</span> <span class="n">fun2</span><span class="p">,</span> <span class="s">"arg2"</span><span class="p">);</span>
	<span class="n">assert</span><span class="p">(</span><span class="n">rc</span> <span class="o">==</span> <span class="mi">0</span><span class="p">);</span>
	<span class="c1">// Join wait for the threads to finish</span>
	<span class="n">rc</span> <span class="o">=</span> <span class="n">pthread_join</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">);</span> <span class="n">asssert</span><span class="p">(</span><span class="n">rc</span> <span class="o">==</span> <span class="mi">0</span><span class="p">);</span>
	<span class="n">rc</span> <span class="o">=</span> <span class="n">pthread_join</span><span class="p">(</span><span class="n">p2</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">);</span> <span class="n">asssert</span><span class="p">(</span><span class="n">rc</span> <span class="o">==</span> <span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <p>We usually want to run different threads for them to perform a job together. We do this using <em>global variables</em>. For example, suppose a function increments a global counter to 10. Suppose we run two threads for this function. The final counter value after the execution of both threads should be 20. Sometimes, it may be a lower value too! Why? An issue with threads.</p> <p>When multiple threads access the same space of data, all weird things happen. For example, a wrong value is propagated across threads when the OS/CPU switches from one thread to another before the thread could save the new value in the global variable. The problem is depicted in the following example.</p> <p>## Race conditions and synchronization</p> <p>What just happened is called a race condition where concurrent execution led to different results. The portion of code that can lead to race conditions is known as the <strong><em>critical section</em></strong>. We need <strong>mutual exclusion</strong>, where only one thread should be executing the critical section at any given time. For this to happen, the critical section should execute like one uninterruptable instruction - <strong><em>atomicity</em></strong> of the critical section. How is this achieved? <strong><em>Locks</em></strong>.</p> <h1 id="lecture-13---locks">Lecture 13 - Locks</h1> <p>We’ve concluded in the previous lecture that locks are the solution to the race conditions in threads. A lock variable helps in the exclusivity of the threads for protection.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lock_t</span> <span class="n">mutex</span><span class="p">;</span>
<span class="n">lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
<span class="c1">// Critical Section</span>
<span class="n">unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
</code></pre></div></div> <p>All threads accessing a critical section share a lock. One of the threads locks the section - <strong>owner of the lock</strong>. Any other thread that tries to lock cannot proceed further until the owner releases the lock. Locks are provided by the <code class="language-plaintext highlighter-rouge">pthreads</code> library.</p> <h2 id="building-a-lock">Building a lock</h2> <p>Goals of a lock implementation</p> <ul> <li>Mutual exclusion</li> <li>Fairness - All threads should eventually get the lock, and no thread should starve</li> <li>Low overhead - Acquiring, releasing, and waiting for a lock should not consume too many resources.</li> </ul> <p>Implementation of locks is needed for both user-space programs (e.g., <code class="language-plaintext highlighter-rouge">pthreads</code> library) and kernel code. Also, implementing locks needs support from hardware and the OS.</p> <h2 id="is-disabling-interrupts-enough">Is disabling interrupts enough?</h2> <p>Previously, the race condition issue arose due to an interrupt at the wrong moment. So can we simply disable interrupts when a thread acquired a lock and is executing a critical section? No! The following issues will occur in that case -</p> <ul> <li>Disabling interrupts is a privileged instruction, and the program can misuse it (e.g., run forever).</li> <li>It will not work on multiprocessor systems,since another thread on another core can enter the critical section.</li> </ul> <p>Basically, we don’t want to give a user program much power after acquiring a lock. However, this technique is used to implement locks on a single process system <strong>inside the OS</strong> (trusted code, e.g., xv6).</p> <h2 id="locks-implementation-failure">Locks implementation (Failure)</h2> <p>Suppose we use a flag variable for a lock. Set the flag for acquiring the lock, and unset it for unlocking.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">typedef</span> <span class="k">struct</span> <span class="n">__lock_t</span> <span class="p">{</span><span class="kt">int</span> <span class="n">flag</span><span class="p">;</span> <span class="p">}</span> <span class="n">lock_t</span><span class="p">;</span>
<span class="kt">void</span> <span class="nf">init</span><span class="p">(</span><span class="n">lock_t</span> <span class="o">*</span><span class="n">mutex</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">mutex</span> <span class="o">-&gt;</span> <span class="n">flag</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
<span class="kt">void</span> <span class="nf">lock</span> <span class="p">(</span><span class="n">lock_t</span> <span class="o">*</span><span class="n">mutex</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">mutex</span> <span class="o">-&gt;</span> <span class="n">flag</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
    	<span class="p">;</span> <span class="c1">// spin-wait (do nothing) </span>
	<span class="n">mutex</span> <span class="o">-&gt;</span> <span class="n">flag</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="p">}</span>
<span class="kt">void</span> <span class="nf">unlock</span> <span class="p">(</span><span class="n">lock_t</span> <span class="o">*</span><span class="n">mutex</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">mutex</span> <span class="o">-&gt;</span> <span class="n">flag</span>  <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>What are the problems here? The race condition has moved to the lock acquisition code! How? Thread 1 spins, the lock is released, spin ends. Suppose thread 1 is interrupted just before setting the flag. Now, thread 2 sets the flag to 1. However, thread 1 still thinks lock is not acquired and sets the flag to 1. Therefore, both the threads have locks, and there is no mutual execution.</p> <p>Seeing the above implementation, it’s clear that we cannot implement locks only using software. Hence, we need hardware atomic instructions.</p> <h2 id="hardware-atomic-instructions">Hardware atomic instructions</h2> <p>Modern architectures provide hardware atomic instructions. For example, <code class="language-plaintext highlighter-rouge">test-and-set</code> - update a variable and return old value, all in one hardware instruction. We can design a simple lock using <code class="language-plaintext highlighter-rouge">test-and-set</code>.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">lock</span> <span class="p">(</span><span class="n">lock_t</span> <span class="o">*</span><span class="n">mutex</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">TestAndSet</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span> <span class="o">-&gt;</span> <span class="n">flag</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
    	<span class="p">;</span> <span class="c1">// spin-wait (do nothing) </span>
<span class="p">}</span>
</code></pre></div></div> <p>If <code class="language-plaintext highlighter-rouge">TestAndSet(flag, 1)</code> returns 1, it means the lock is held by someone else, so wait busily. This lock is called a <strong><em>spinlock</em></strong> - spins until the lock is acquired.</p> <p>There is also a <code class="language-plaintext highlighter-rouge">compare-and-swap</code> instruction which checks the expected value with the current value. If both the values are equal, the value is set to the new value. We can also implement a spinlock using this instruction.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">lock</span><span class="p">(</span><span class="n">lock_t</span> <span class="o">*</span><span class="n">lock</span><span class="p">)</span> <span class="p">{</span>
<span class="k">while</span><span class="p">(</span><span class="n">CompareAndSwap</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lock</span> <span class="o">-&gt;</span> <span class="n">flag</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">;</span> <span class="c1">//spin</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="alternative-to-spinning">Alternative to spinning</h2> <p>We can make the thread sleep instead of spinning and utilizing the CPU. A contending thread could simply give up the CPU and check back later. In literature, a mutex by itself means a sleeping mutex.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">lock</span><span class="p">(){</span>
    <span class="k">while</span><span class="p">(</span><span class="n">TestAndSet</span><span class="p">(</span><span class="o">&amp;</span><span class="n">flag</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">yield</span><span class="p">();</span> <span class="c1">// Name changes across OS</span>
<span class="p">}</span>
</code></pre></div></div> <p>## Spinlock vs. mutex</p> <p>Most user-space lock implementations are of sleeping mutex kind. However, the locks inside the OS are always spinlocks! Why? If we sleep in the OS code, we cannot context switch to another process. When OS acquires a spinlock -</p> <ul> <li> <p>It must disable interrupts (on that processor code) while the lock is held. Why? An interrupt handler could request the same lock and spin it forever - A deadlock situation.</p> <blockquote> <p>What?</p> </blockquote> </li> <li> <p>It must not perform any blocking operation - never go to sleep with a locked spinlock!</p> </li> </ul> <p>In general, we must use spinlocks with care and release them as soon as possible.</p> <h2 id="usage-of-locks">Usage of Locks</h2> <p>The user must acquire a lock before accessing any variable, or data structure shared between multiple threads of a process. This implementation is called a <strong>thread-safe</strong> data structure. All the shared kernel data structures must also be accessed only after locking.</p> <p>Coarse-grained vs. fine-grained locking - One big lock for all the shared data vs. separate locks for each variable. Fine-grained locking allows more parallelism but is harder to manage. The OS only provides locks, but the proper locking discipline is left to the user.</p> <h1 id="lecture-14---condition-variables">Lecture 14 - Condition variables</h1> <p>Locks allow one type of synchronization between threads. There is another typical requirement in multi-threaded applications. It is known as <strong>waiting and signaling</strong>. Here, one thread might want another thread to finish the job and signal to the original thread when the job is executed. We can accomplish such synchronization using <em>busy-waiting</em>, but it is inefficient. Therefore, most modern OS have a new synchronization primitive - <strong>condition variables</strong> (as called in <code class="language-plaintext highlighter-rouge">pthreads</code>).</p> <h2 id="condition-variables">Condition Variables</h2> <p>A condition variable (CV) is a queue that a thread can be placed into when waiting on some condition. Another thread can wake up the waiting thread by signaling CV after making the condition true. <code class="language-plaintext highlighter-rouge">pthreads</code> provides CV for user programs and also for kernel threads. There are two kinds of signaling -</p> <ul> <li>A signal wakes up a single thread</li> <li>A signal broadcast that wakes up all the waiting threads.</li> </ul> <p>Here is an example utilizing CV -</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">done</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">pthread_mutex_t</span> <span class="n">m</span> <span class="o">=</span> <span class="n">PTHREAD_MUTEX_INITIALIZER</span><span class="p">;</span>
<span class="n">pthread_cond_t</span> <span class="n">c</span> <span class="o">=</span> <span class="n">PTHREAD_COND_INITIALIZER</span><span class="p">;</span>
<span class="kt">void</span> <span class="nf">thr_join</span><span class="p">(){</span>
<span class="n">Pthread_mutex_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">m</span><span class="p">);</span> <span class="c1">// Line *</span>
<span class="k">while</span><span class="p">(</span><span class="n">done</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
	<span class="n">Pthread_cond_wait</span><span class="p">(</span><span class="o">&amp;</span><span class="n">c</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">m</span><span class="p">);</span> <span class="c1">//Parent thread goes to sleep</span>
	<span class="n">Pthread_mutex_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">m</span><span class="p">);</span>
<span class="p">}</span>
<span class="kt">void</span> <span class="nf">thr_exit</span><span class="p">()</span> <span class="p">{</span>
	<span class="n">Pthread_mutex_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">m</span><span class="p">);</span>
	<span class="n">done</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
	<span class="n">Pthread_cond_signal</span><span class="p">(</span><span class="o">&amp;</span><span class="n">c</span><span class="p">);</span> <span class="c1">// Send a signal</span>
	<span class="n">Pthread_mutex_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">m</span><span class="p">);</span>
<span class="p">}</span>
<span class="kt">void</span> <span class="o">*</span><span class="nf">child</span> <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">arg</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">printf</span><span class="p">(</span><span class="s">"child</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
	<span class="n">thr_exit</span><span class="p">();</span>
	<span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span>
<span class="p">}</span>
<span class="kt">int</span> <span class="nf">main</span> <span class="p">()</span>
<span class="p">{</span>
	<span class="n">printf</span><span class="p">(</span><span class="s">"Parent - begin</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
	<span class="n">pthread_t</span> <span class="n">p</span><span class="p">;</span>
	<span class="n">Pthread_create</span><span class="p">(</span><span class="o">&amp;</span><span class="n">p</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">,</span> <span class="n">child</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">);</span>
	<span class="n">thr_join</span><span class="p">;</span>
	<span class="n">printf</span><span class="p">(</span><span class="s">"Parent - end</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <blockquote> <p>Doesn’t the parent already wait in line * if the child is executed before?</p> <p>What if the parent acquires lock first? The wait function releases the lock. See below.</p> </blockquote> <p>In the above example, it doesn’t matter which thread (child/parent) gets executed first. The flow is still the same as expected. Note that it is always better to check the condition (in the <code class="language-plaintext highlighter-rouge">while</code> loop) before waiting. Otherwise, the parent thread will not be woken up.</p> <p>Why do we use a <code class="language-plaintext highlighter-rouge">while</code> instead of <code class="language-plaintext highlighter-rouge">if</code>? To avoid corner cases of thread being woken up even when the condition is not true (maybe an issue with some implementation). This is a good programming practice.</p> <p>Why do we hold locks before checking the condition? What if we didn’t use a lock? There would be a race condition for a missed wakeup</p> <ul> <li>Parent checks done to be 0, decides to sleep and is then interrupted.</li> <li>The child runs, sets done to 1, signals, but no one is sleeping.</li> <li>The parent now resumes and goes to sleep forever.</li> </ul> <p>Therefore, a lock must be held when calling wait and signal with CV. The wait function releases the lock before putting the thread to sleep, so the lock is available for the signaling thread.</p> <h2 id="producerconsumer-problem">Producer/Consumer Problem</h2> <p>This setting is a common pattern in multi-threaded programs. There are two pr more threads - producer(s) and consumer(s) which share a shared buffer (having a finite size). We need mutual exclusion in the buffer, and also a signaling mechanism to share information.</p> <p>For example, in a multi-threaded web server, one thread accepts requests from the network and puts them in a queue. The worker threads get requests from this queue and process them. Here’s how we implement this -</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">cond_t</span> <span class="n">empty</span><span class="p">,</span> <span class="n">fill</span><span class="p">;</span>
<span class="n">mutex_t</span> <span class="n">mutex</span><span class="p">;</span>
<span class="kt">void</span> <span class="o">*</span><span class="nf">producer</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">arg</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">i</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">lopps</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="n">Pthread_mutex_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
        <span class="k">while</span><span class="p">(</span><span class="n">count</span> <span class="o">==</span> <span class="n">MAX</span><span class="p">)</span>
        	<span class="n">Pthread_cont_wait</span><span class="p">(</span><span class="o">&amp;</span><span class="n">empty</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
        	<span class="n">put</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
        	<span class="n">Pthread_cond_signal</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fill</span><span class="p">);</span>
        	<span class="n">Pthread_mutex_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="o">*</span><span class="nf">consumer</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">arg</span><span class="p">)</span>
<span class="p">{</span>
	<span class="kt">int</span> <span class="n">i</span><span class="p">;</span>
	<span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">loops</span><span class="p">,</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="n">Pthread_mutex_lock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
		<span class="k">while</span><span class="p">(</span><span class="n">count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
			<span class="n">Pthread_cond_wait</span><span class="p">(</span><span class="o">&amp;</span><span class="n">fill</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
		<span class="kt">int</span> <span class="n">tmp</span> <span class="o">=</span> <span class="n">get</span><span class="p">();</span>
		<span class="n">Pthread_cond_signal</span><span class="p">(</span><span class="o">&amp;</span><span class="n">empty</span><span class="p">);</span>
		<span class="n">Pthread_mutex_unlock</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
		<span class="n">printf</span><span class="p">(</span><span class="s">"%d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span> <span class="n">tmp</span><span class="p">);</span>
	<span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <h1 id="live-session-7">Live Session 7</h1> <ul> <li> <p>The OS does not raise a trap when you access a shared variable protected using a lock in another thread. Why? The usage of locks is wholly left to the user. The OS cannot see these variables too! A lock is nothing but another variable.</p> </li> <li> <p>Condition variable in a queue? That was a wrong statement in the slides. The implementation of the condition variables determines which thread is woken up first among the threads that are waiting. Even all of the threads can wake up at once. For example, in xv6, all the threads are woken up at once.</p> </li> <li> <p>Why <code class="language-plaintext highlighter-rouge">if</code> instead of a <code class="language-plaintext highlighter-rouge">while</code>? If all the waiting threads are woken up, then using <code class="language-plaintext highlighter-rouge">if</code> might cause a problem! <code class="language-plaintext highlighter-rouge">while</code> in case of <em>spurious</em> wake-ups.</p> </li> <li> <p>Why spinlocks in the OS code? The OS itself cannot yield itself to anybody else. It cannot rely on other processes to wake it up in case the lock is not acquired. The alternative to the spinlock implementation is using hardware help. The OS can wake up when the hardware gives an interrupt (after the lock is released). However, we don’t have this support.</p> </li> <li> <p>Why does disabling interrupts on a single core work? This method is only for OS that works on a single thread. The OS can interrupt the user programs. However, only hardware can interrupt the OS. So, when you disable interrupts, there is no way the OS can be interrupted.</p> <p>However, if you have a multicore architecture, another thread can access the shared variables from another core (not using locks here, just talking about disabling interrupts).</p> </li> <li> <p>The <code class="language-plaintext highlighter-rouge">sleep</code> function releases the lock. However, when the process wakes up, it reacquires the lock. So, there is no double-freeing. The doubt was asked based on <a href="#condition-variables">this</a> context.</p> </li> <li> <p>Do we need to disable interrupts on all cores for smooth execution of OS code? However, you need not disable interrupts on other cores. The OS code that needs to access this space has to acquire the lock (which is not possible as the original thread is the owner) and wait before accessing the shared space.</p> <p>We said disabling interrupts won’t work in multicore architecture (two points before). But that situation was not considering locks, and we have locks in this case.</p> <blockquote> <p>What happens if you disable interrupts on all cores anyway?</p> </blockquote> </li> <li> <blockquote> <p>What do we do if a serious interrupt occurs when interrupts are disabled?</p> </blockquote> </li> <li> <p>Once a process commits to a core, it cannot run on another core in between. It has to finish on the current core itself.</p> </li> <li> <p>The concept of user threads and kernel threads. User-level threading libraries create an illusion of multiple threads. However, the kernel does not see these as independent processes but as a single process. For example, the <code class="language-plaintext highlighter-rouge">pthread</code> library is only for kernel threads. It is not an illusion. Nevertheless, even with a user-level thread, you can get interrupted at the wrong time. Therefore, we need locks even in the user-level illusionary threads.</p> </li> <li><code class="language-plaintext highlighter-rouge">int n</code> is an atomic instruction.</li> </ul> <h2 id="lecture-15---semaphores">Lecture 15 - Semaphores</h2> <p>We are going to study another synchronization primitive called semaphores.</p> <h2 id="what-is-a-semaphore">What is a semaphore?</h2> <p>They are very similar to condition variables. It is a variable with an underlying counter. You can call two functions on a semaphore variable -</p> <ul> <li><code class="language-plaintext highlighter-rouge">up/post</code> increment the counter</li> <li><code class="language-plaintext highlighter-rouge">down/wait</code> decrement the counter and block the calling thread if the resulting value is negative.</li> </ul> <p>The above two functions represent the interface to a semaphore. A semaphore with an initial value of 1 acts as a simple lock. That is, a binary semaphore is equivalent to a mutex. Here is a simple usage of a binary semaphore.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sem_t</span> <span class="n">m</span><span class="p">;</span>
<span class="n">sem_init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">m</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">X</span><span class="p">);</span> <span class="c1">// X can be 1 here</span>
<span class="n">sem_wait</span><span class="p">(</span><span class="o">&amp;</span><span class="n">m</span><span class="p">);</span>
<span class="c1">// Critical section</span>
<span class="n">sem_post</span><span class="p">(</span><span class="o">&amp;</span><span class="n">m</span><span class="p">);</span>
</code></pre></div></div> <h2 id="semaphores-for-ordering">Semaphores for Ordering</h2> <p>They can be used to set the order of execution between threads like a CV. This example is similar to what we have seen before with the <a href="#condition-variables">“parent waiting for child”</a> example. An important point to note here is that the semaphore is initialized to 0 here. Why? Because the parent has to wait for the child to finish.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sem_t</span> <span class="n">s</span><span class="p">;</span>
<span class="kt">void</span><span class="o">*</span> <span class="nf">child</span> <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">arg</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">printf</span><span class="p">(</span><span class="s">"child</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
	<span class="n">sem_post</span><span class="p">(</span><span class="o">&amp;</span><span class="n">s</span><span class="p">);</span>
	<span class="k">return</span> <span class="nb">NULL</span><span class="p">;</span> <span class="c1">// Line *</span>
<span class="p">}</span>

<span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="n">chara</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
	<span class="n">sem_init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">s</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
	<span class="n">printf</span><span class="p">(</span><span class="s">"parent: begin</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
	<span class="n">pthread_t</span> <span class="n">c</span><span class="p">;</span>
	<span class="n">Pthread_create</span><span class="p">(</span><span class="o">&amp;</span><span class="n">c</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">,</span> <span class="n">child</span><span class="p">,</span> <span class="nb">NULL</span><span class="p">);</span>
	<span class="n">sem_wait</span><span class="p">(</span><span class="o">&amp;</span><span class="n">s</span><span class="p">);</span>
	<span class="n">printf</span><span class="p">(</span><span class="s">"parent: end</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
	<span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <blockquote> <p>What if line * had a print statement? Where would that get printed?</p> </blockquote> <p>Therefore, it is essential to correctly determine the initial value of the semaphore for correct behavior.</p> <h2 id="producerconsumer-problem-1">Producer/Consumer Problem (1)</h2> <p>Let us revisit <a href="#producerconsumer-problem">this</a> problem in the context of semaphores. We need one semaphore to keep track of empty slots and another to keep track of full slots. The producer waits if there are no more empty slots, and the consumer waits if there are no more full slots. Also, we need another semaphore to act as a mutex for the buffer. Here is how these variables are initialized.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
	<span class="c1">// ...</span>
	<span class="n">sem_init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">empty</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">MAX</span><span class="p">);</span>
	<span class="n">sem_init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">full</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
	<span class="n">sem_init</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// Thumb rule - Locks are initialized with value 1</span>
	<span class="c1">// ...</span>
<span class="p">}</span>
</code></pre></div></div> <p>There is a subtle point in this example. Consider the <code class="language-plaintext highlighter-rouge">producer</code> function.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span><span class="o">*</span> <span class="nf">producer</span> <span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">arg</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">loops</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
	<span class="p">{</span>
		<span class="n">sem_wait</span><span class="p">(</span><span class="o">&amp;</span><span class="n">empty</span><span class="p">);</span>
		<span class="n">sem_wait</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
		<span class="n">put</span><span class="p">(</span><span class="n">i</span><span class="p">);</span>
		<span class="n">sem_post</span><span class="p">(</span><span class="o">&amp;</span><span class="n">mutex</span><span class="p">);</span>
		<span class="n">sem_post</span><span class="p">(</span><span class="o">&amp;</span><span class="n">full</span><span class="p">);</span>
	<span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p><em>*</em> What if we acquire mutex before checking the other semaphores? It would result in a deadlock situation. The waiting thread can sleep with the mutex, and the signaling thread can never wake it up!</p> <h2 id="lecture-16---concurrency-bugs">Lecture 16 - Concurrency Bugs</h2> <p>In general, writing multi-threaded programs is tricky. These bugs are non-deterministic and occur based on the execution order of threads - challenging to debug. We shall discuss two types of bugs in this lecture and methods to prevent them.</p> <ul> <li>Deadlocks - Threads cannot execute any further and wait for each other. These are very dangerous.</li> <li>Non-deadlock bugs - Non-deadlock, but the results are incorrect when the threads execute.</li> </ul> <h2 id="non-deadlock-bugs">Non-deadlock bugs</h2> <p>These are of two types -</p> <ul> <li> <p><strong>Atomicity bugs</strong> - Occur due to false atomicity assumptions that are violated during the execution of concurrent threads. We fix these bugs using locks!</p> <p>Example -</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Thread</span> <span class="mi">1</span><span class="o">::</span>
<span class="k">if</span> <span class="p">(</span><span class="n">thd</span> <span class="o">-&gt;</span> <span class="n">proc_info</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">fputs</span><span class="p">(</span><span class="n">thd</span> <span class="o">-&gt;</span> <span class="n">proc_infor</span><span class="p">,</span> <span class="p">...);</span>
<span class="p">}</span>
<span class="n">Thread</span> <span class="mi">2</span><span class="o">::</span>
<span class="n">thd</span> <span class="o">-&gt;</span> <span class="n">proc_info</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
</code></pre></div> </div> <p>This situation is basically a race condition! Although, the crucial point to note here is atomicity bugs can occur, not just when writing to shared data, but even when reading it!</p> <p>Always use a <strong>lock</strong> when <strong>accessing shared data</strong>.</p> </li> <li> <p><strong>Order-violation bugs</strong> - Occur when desired order of memory accesses is flipped during concurrent execution. We fix these using condition variables.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Thread</span> <span class="mi">1</span><span class="o">::</span>
<span class="kt">void</span> <span class="nf">init</span><span class="p">()</span>
<span class="p">{</span>
	<span class="n">mThread</span> <span class="o">=</span> <span class="n">PR_CreateThread</span><span class="p">(</span><span class="n">mMain</span><span class="p">,</span> <span class="p">...);</span>
<span class="p">}</span>
<span class="n">Thread</span> <span class="mi">2</span><span class="o">::</span>
<span class="kt">void</span> <span class="nf">mMain</span><span class="p">(...)</span>
<span class="p">{</span>
<span class="n">mState</span> <span class="o">=</span> <span class="n">mThread</span> <span class="o">-&gt;</span> <span class="n">State</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div> </div> <p>Thread 1 assumes that Thread 2 had been executed before, and this causes an error.</p> </li> </ul> <h2 id="deadlock-bugs">Deadlock bugs</h2> <p>Here is a classic example of a deadlock situation.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Thread 1:</span>
<span class="n">pthread_mutex_lock</span><span class="p">(</span><span class="n">L1</span><span class="p">);</span>
<span class="n">pthread_mutex_lock</span><span class="p">(</span><span class="n">L2</span><span class="p">);</span>
<span class="c1">// Thread 2:</span>
<span class="n">pthread_mutex_lock</span><span class="p">(</span><span class="n">L2</span><span class="p">);</span>
<span class="n">pthread_mutex_lock</span><span class="p">(</span><span class="n">L1</span><span class="p">);</span>
</code></pre></div></div> <p>Deadlock may not always occur in this situation. It only occurs when the executions overlap, and a context switch occurs from a thread after acquiring only one lock. It is easy to visualize these situations using a dependency graph. A cycle in a dependency graph causes a deadlock situation.</p> <p>When does a deadlock occur?</p> <ol> <li>Mutual exclusion - A thread claims exclusive control of a resource (e.g., lock)</li> <li>Hold-and-wait - The thread holds a resource and is waiting for another</li> <li>No preemption - A thread cannot be made to give up its resource (e.g., cannot take back a lock)</li> <li>Circular wait - There exists a cycle in the resource dependency graph.</li> </ol> <p>All of the above conditions must hold for a deadlock to occur.</p> <blockquote> <p>Isn’t (4) by itself enough for a deadlock? <em>the cycle must be reachable</em></p> </blockquote> <p>To prevent a circular wait, we need to acquire locks in a particular fixed order! e.g., both threads acquire L1 before L2 in the previous example.</p> <p>To do this, we need a total (partial) ordering of locks. For example, this ordering can be done via the address of lock variables.</p> <h3 id="preventing-hold-and-wait">Preventing hold-and-wait</h3> <p>Acquire all locks at once using a <em>master</em> lock. This way, you will hold all locks or none of them. However, this method may reduce concurrent execution and performance gains. <em>Think</em>.</p> <h3 id="other-solutions-to-deadlocks">Other solutions to deadlocks</h3> <p><strong>Deadlock avoidance</strong> - If the OS knows which process needs which locks, it can schedule the processes in that deadlock will not occur. One such algorithm is <strong><em>Banker’s algorithm</em></strong>. But it is impractical in real life to assume this knowledge.</p> <p><strong>Detect and recover</strong> - Reboot system or kill deadlocked processes. There is no standard implementation to tackle deadlocks. :(</p> <h1 id="live-session-8">Live Session 8</h1> <ul> <li>If you disable interrupts on all cores anyway, nothing bad happens. There are no emergency interrupts which might destroy your system if not handled immediately.</li> <li>semaphores no need to acquire lock. cv need to acquire lock. cv reacquires lock too.</li> <li>no context switch on post!</li> </ul> <h2 id="synchronization-problems">Synchronization problems</h2> <h3 id="question-26">Question 26</h3> <p>Allowing N guests all at once into the house.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// host</span>
<span class="n">lock</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="k">while</span><span class="p">(</span><span class="n">guest_count</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)</span>
	<span class="n">wait</span><span class="p">(</span><span class="n">cv_host</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="n">openDoor</span><span class="p">()</span>
<span class="n">signal</span><span class="p">(</span><span class="n">cv_guest</span><span class="p">)</span>
<span class="c1">// signalbroadcast too</span>
<span class="n">unlock</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

<span class="c1">// Guests code?</span>
<span class="n">lock</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">guest_count</span><span class="o">++</span>
<span class="k">if</span><span class="p">(</span><span class="n">guest_count</span> <span class="o">==</span> <span class="n">N</span><span class="p">)</span>
	<span class="n">signal</span><span class="p">(</span><span class="n">cv_host</span><span class="p">)</span>
<span class="n">wait</span><span class="p">(</span><span class="n">cv_guest</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="n">signal</span><span class="p">(</span><span class="n">cv_guest</span><span class="p">)</span> <span class="c1">// To signal other threads</span>
    <span class="c1">// above line not needed if singal braodcast is there</span>
<span class="n">unlock</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">enterHouse</span><span class="p">()</span>
</code></pre></div></div> <p>Whenever you write code, start with the wait signals. Then, put the logic in place and ensure there are no deadlocks. Also, pair up every wait with a signal.</p> <h3 id="question-25">Question 25</h3> <p>Passenger thread given -</p> <pre><code class="language-pseudocode">down(mutex)
waiting_count++
up(mutex)
down(bus_arrived)
board()
up(passenger_boarded)
</code></pre> <p>Bus code?</p> <pre><code class="language-pseudocode">down(mutex)
N = min(waiting_count, K)
for i = 1 to N
    up(bus_arrived)
    down(passenger_boarded)
waiting_count -= N;
up(mutex)
</code></pre> <h1 id="lecture-29---locking-in-xv6">Lecture 29 - Locking in xv6</h1> <p>Why do we need locking in xv6? There are no threads in xv6! Therefore, no two user programs can access the same userspace memory image. However, there is a scope for concurrency in xv6 kernel. For example, two processes in the kernel mode on different CPU cores can access the same data structures. Another example where this sort of thing happens is in the case of interrupts. When an interrupt occurs while processing the trap of another process, the new interrupt handler can access the previous kernel code. Therefore, we need <code class="language-plaintext highlighter-rouge">spinlocks</code> to protect critical sections in xv6. xv6 also has a sleeping lock which is built on spinlock.</p> <h2 id="spinlocks-in-xv6">Spinlocks in xv6</h2> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1">// Mutual exclusion lock.</span>
<span class="k">struct</span> <span class="n">spinlock</span> <span class="p">{</span>
<span class="n">uint</span> <span class="n">locked</span><span class="p">;</span> <span class="c1">// Is the lock held?</span>

<span class="c1">// For debugging:</span>
<span class="kt">char</span> <span class="o">*</span><span class="n">name</span><span class="p">;</span> <span class="c1">// Name of lock.</span>
<span class="k">struct</span> <span class="n">cpu</span> <span class="o">*</span><span class="n">cpu</span><span class="p">;</span> <span class="c1">// The cpu holding the lock.</span>
<span class="n">uint</span> <span class="n">pcs</span><span class="p">[</span><span class="mi">10</span><span class="p">];</span> <span class="c1">// The call stack (an array of program counters)</span>
<span class="c1">// that locked the lock.</span>
<span class="p">};</span>
<span class="c1">/////////////////////////////////</span>
<span class="kt">void</span>
<span class="nf">acquire</span><span class="p">(</span><span class="k">struct</span> <span class="n">spinlock</span> <span class="o">*</span><span class="n">lk</span><span class="p">)</span>
<span class="p">{</span>
<span class="n">pushcli</span><span class="p">();</span> <span class="c1">// disable interrupts to avoid deadlock.</span>
<span class="k">if</span><span class="p">(</span><span class="n">holding</span><span class="p">(</span><span class="n">lk</span><span class="p">))</span>
<span class="n">panic</span><span class="p">(</span><span class="s">"acquire"</span><span class="p">);</span>

<span class="c1">// The xchg is atomic.</span>
<span class="k">while</span><span class="p">(</span><span class="n">xchg</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lk</span><span class="err">−</span><span class="o">&gt;</span><span class="n">locked</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
<span class="p">;</span>

<span class="c1">// Tell the C compiler and the processor to not move loads or stores</span>
<span class="c1">// past this point, to ensure that the critical section’s memory</span>
<span class="c1">// references happen after the lock is acquired.</span>
<span class="n">__sync_synchronize</span><span class="p">();</span>

<span class="c1">// Record info about lock acquisition for debugging.</span>
<span class="n">lk</span><span class="err">−</span><span class="o">&gt;</span><span class="n">cpu</span> <span class="o">=</span> <span class="n">mycpu</span><span class="p">();</span>
<span class="n">getcallerpcs</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lk</span><span class="p">,</span> <span class="n">lk</span><span class="err">−</span><span class="o">&gt;</span><span class="n">pcs</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <p>Locks are acquired using <code class="language-plaintext highlighter-rouge">xchg</code> x86 atomic instruction. It is similar to <code class="language-plaintext highlighter-rouge">test-and-set</code> we’ve seen before. The <code class="language-plaintext highlighter-rouge">xchg</code> instruction sets the lock variable to 1 and returns the previous value. Remember, value of 1 means locked! Also, note that we need to disable interrupts on the CPU core before spinning for lock. It is okay for another process to spin for this lock on another core. This is because, the lock will be eventually released by this core (no deadlock can occur).</p> <h2 id="disabling-interrupts">Disabling interrupts</h2> <p>Interrupts are disabled using <code class="language-plaintext highlighter-rouge">pushcli</code>. The interrupts must stay disabled until <strong>all</strong> locks are released. <code class="language-plaintext highlighter-rouge">pushcli</code> disables interrupts on the first lock acquire, and increments count for future locks. Then, <code class="language-plaintext highlighter-rouge">popcli</code> decrements count and reenables interrupts only when count is zero.</p> <h2 id="ptablelock">ptable.lock()</h2> <p>The process table is protected by this lock. Normally, a process in kernel mode acquires <code class="language-plaintext highlighter-rouge">ptable.lock</code>, changes <code class="language-plaintext highlighter-rouge">ptable</code>, and then release the lock. However, during a context switch, from process P1 and P2, the <code class="language-plaintext highlighter-rouge">ptable</code> structure is changed continuously and repeatedly. So when do we release the lock? P1 acquires the lock, switches to scheduler, switches to P2, and then finally P2 releases the lock. This is a special case where one process acquires the lock and another releases it.</p> <p>Every function that calls <code class="language-plaintext highlighter-rouge">sched</code> will do so with <code class="language-plaintext highlighter-rouge">ptable.lock</code> held. <code class="language-plaintext highlighter-rouge">sched</code> is called from <code class="language-plaintext highlighter-rouge">yield</code>, <code class="language-plaintext highlighter-rouge">sleep</code>, and <code class="language-plaintext highlighter-rouge">exit</code>. Every function that <code class="language-plaintext highlighter-rouge">swtch</code> switches to will release <code class="language-plaintext highlighter-rouge">ptable.lock</code>. <code class="language-plaintext highlighter-rouge">swtch</code> also returns to</p> <ul> <li>Yield, when switching from process that is resuming after yielding is done</li> <li>Sleep, when switching in a process that is waking up after sleep</li> <li>Forkret for newly created processes</li> </ul> <p>Thus, the process of <code class="language-plaintext highlighter-rouge">forkret</code> is to release <code class="language-plaintext highlighter-rouge">ptable.lock</code> after context switch, before returning to <code class="language-plaintext highlighter-rouge">trapret</code>.</p> <p>What about the scheduler? We enter the scheduler with the lock held! Sometimes, the scheduler releases the lock (after it loops over all processes), and then reacquires it. Why does it do this? Whenever the lock is held, all interrupts are disabled. All processes might be in a blocked state waiting for interrupts. Therefore, the lock must be released periodically enabling interrupts and allowing process to become runnable.</p> <h1 id="lecture-30---sleep-and-wakeup-in-xv6"><code class="language-plaintext highlighter-rouge">Lecture</code> 30 - Sleep and Wakeup in xv6</h1> <p>Consider the following example. A process P1 in kernel mode gives up the CPU to block on an event. For e.g., reading data from disk. Then, P1 invokes <code class="language-plaintext highlighter-rouge">sleep</code> which calls <code class="language-plaintext highlighter-rouge">sched</code>. Another process P2 in kernel mode calls <code class="language-plaintext highlighter-rouge">wakeup</code> when an event occurs. This function marks P1 as runnable so that the scheduler loop switches to P1 in the future. For e.g., disk interrupt occurs while P2 is running.</p> <p>How does P2 know which process to wake up? When P1 sleeps, it sets a <strong><em>channel</em></strong> <code class="language-plaintext highlighter-rouge">(void* chan)</code> in its struct proc, and P2 calls <code class="language-plaintext highlighter-rouge">wakeup</code> on the same channel. This channel is any value known to both P1 and P2. Example: The channel value for a disk read can be address of a disk block.</p> <p>A spinlock protects the atomicity of sleep: P1 calls sleep with some spinlock L held, P2 calls wakeup with same spinlock L held. Why do we need this setup?</p> <ul> <li>Eliminating missed <code class="language-plaintext highlighter-rouge">wakeup</code> problem that arises due to P2 issuing wakeup between P1 deciding to sleep and actually sleeping.</li> <li>Lock L is released after sleeping, available for wakeup</li> <li>Similar concept to condition variables studied before.</li> </ul> <p>The Sleep calls <code class="language-plaintext highlighter-rouge">sched</code> to give up the CPU with <code class="language-plaintext highlighter-rouge">ptable</code> lock held. The arguments to sleep are channel and a spinlock (not <code class="language-plaintext highlighter-rouge">ptable.lock</code>). <code class="language-plaintext highlighter-rouge">sleep</code> has to acquire <code class="language-plaintext highlighter-rouge">ptable.lock</code>, release the lock given to sleep (to make it available for wakeup). Unless lock given is <code class="language-plaintext highlighter-rouge">ptable.lock</code> itself, in which case no need to acquire the lock again. One of the two locks is always held!</p> <p>When the control returns back to <code class="language-plaintext highlighter-rouge">sleep</code>, it reacquires the spinlock.</p> <blockquote> <p>Why reacquire?</p> </blockquote> <h2 id="wakeup-function">Wakeup function</h2> <p>This is called by another process with lock held (same lock as when <code class="language-plaintext highlighter-rouge">sleep</code> was called). Since this function changes <code class="language-plaintext highlighter-rouge">ptable</code>, <code class="language-plaintext highlighter-rouge">ptable.lock</code> is also held. Now, this function wakes up all process sleeping on a channel in the <code class="language-plaintext highlighter-rouge">ptable</code>. It is also a good idea to check if the condition is still true upon waking up (use while loop while calling <code class="language-plaintext highlighter-rouge">sleep</code>).</p> <h2 id="example-pipes">Example: pipes</h2> <p>Two process are connected by a pipe (producer consumer). Addresses of pipe structure variables are channels (same channel known to both). There is a <em>pipe lock</em> given as input to <code class="language-plaintext highlighter-rouge">sleep</code>.</p> <h2 id="example-wait-and-exit">Example: wait and exit</h2> <p>If <code class="language-plaintext highlighter-rouge">wait</code> is called in parent while children are still running, parent calls <code class="language-plaintext highlighter-rouge">sleep</code> and gives up the CPU. Here, channel is parent struct proc pointer, and lock is <code class="language-plaintext highlighter-rouge">ptable.lock</code>. In exit, the child acquires <code class="language-plaintext highlighter-rouge">ptable.lock</code> and wakes up the sleeping parent. Notice that here the lock give to <code class="language-plaintext highlighter-rouge">sleep</code> is ptable lock, because parent and child both access <code class="language-plaintext highlighter-rouge">ptable</code> (sleep avoids double locking, doesn’t acquire <code class="language-plaintext highlighter-rouge">ptable.lock</code> if it already held before calling sleep).</p> <p>Why is terminated process memory cleaned up by the parent? When a process calls exit, CPU is using its memory (kernel stack is in use, <code class="language-plaintext highlighter-rouge">cr3</code> is pointing to page table) so all this memory cannot be cleared until the terminated process has been taken off the CPU. Therefore, we need <code class="language-plaintext highlighter-rouge">wait</code>.</p> <h1 id="live-session-9">Live Session 9</h1> <ul> <li>What if 2 threads on <em>different</em> CPU cores perform <code class="language-plaintext highlighter-rouge">test-and-set</code>? This is ensured on the micro-architectural level. These are called <strong>cache coherence protocols</strong>. Memory can only be modified by a single core at a given time. Two or more cores cannot edit a variable simultaneously. Therefore, atomicity at the CPU level is guaranteed.</li> <li>Why don’t we use sleep locks everywhere? The context switch overhead might be more than the wait time in sleeping locks. Suppose we have a single core, and a process on this core is spinning for a lock. Since there are no other cores, the lock owner cannot release lock at this instant. What happens here? The process will keep spinning until it eventually has to yield the CPU.</li> <li>In semaphores, every <code class="language-plaintext highlighter-rouge">up</code> should wake up one thread! <code class="language-plaintext highlighter-rouge">down</code> checks if the counter is negative and goes to sleep. When <code class="language-plaintext highlighter-rouge">up</code> signals these sleeping threads, they wake up even though the counter is negative.</li> <li>Interrupts are queued up when interrupts arrive on an interrupt-disabled core.</li> <li><code class="language-plaintext highlighter-rouge">myproc()</code>? In xv6, we have an assumption that no processes on any other core edit the <code class="language-plaintext highlighter-rouge">struct proc</code> of the running process. Therefore, we don’t need any locks to access the <code class="language-plaintext highlighter-rouge">struct proc</code> of the running process.</li> <li>The reacquiring of the second lock in <code class="language-plaintext highlighter-rouge">sleep</code> is not necessary but is a convention. Why do we send this lock to <code class="language-plaintext highlighter-rouge">sleep</code>? To preserve the atomicity of <code class="language-plaintext highlighter-rouge">sleep</code>!</li> <li>How does an interrupt know the channel to wakeup a process? Every interrupt handler has a logic for deciphering the channel. For ex, a disk read interrupt handler knows that the sector of the address read is the channel.</li> <li>Missed wakeups cause deadlocks!</li> </ul> <h1 id="lecture-17---communication-with-io-devices">Lecture 17 - Communication with I/O devices</h1> <p>I/O devices connect to the CPU and memory via a bus. For example, a high speed bus, e.g., PCI and others like SCSI, USB, SATA, etc. The point of connection to the system is called the <strong><em>port</em></strong>.</p> <h2 id="simple-device-model">Simple Device Model</h2> <p>Devices are of two types -</p> <ul> <li>Block devices - The stored a set of numbered blocks. For ex, disks.</li> <li>Character devices - Produce/consume stream of bytes. For ex, keyboard.</li> </ul> <p>All these devices expose an interface of memory registers. These registers tell the current status of the device, command to execute, and the data to transfer. The remaining internals of the device are usually hidden. There might be a CPU, memory, and other chips inside the device.</p> <h2 id="interaction-with-the-os">Interaction with the OS</h2> <p>There are <strong>explicit I/O</strong> instructions provided by the hardware to read/write to registers in the interface. For example, on x86, <code class="language-plaintext highlighter-rouge">in</code> and <code class="language-plaintext highlighter-rouge">out</code> instructions can be used to read and write to specific registers on a device. These are privileged instructions accessed by the OS. A user program has to execute a system call in order to interact with the devices.</p> <p>The other way to do this is via the <strong>memory mapped I/O</strong>. The devices makes registers appear like memory locations. The OS simply reads and writes from memory. The underlying memory hardware routes the accesses to these special memory addresses to devices.</p> <blockquote> <p>What memory hardware?</p> </blockquote> <h2 id="simple-execution-of-io-requests">Simple execution of I/O requests</h2> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">While</span> <span class="n">STATUS</span> <span class="o">==</span> <span class="n">BUSY</span>
<span class="p">;</span> <span class="c1">//wait</span>
<span class="n">Write</span> <span class="n">data</span> <span class="n">to</span> <span class="n">DATA</span> <span class="k">register</span>
<span class="n">Write</span> <span class="n">command</span> <span class="n">to</span> <span class="n">COMMAND</span> <span class="k">register</span>
<span class="n">White</span> <span class="n">STATUS</span> <span class="o">==</span> <span class="n">BUSY</span>
<span class="p">;</span> <span class="c1">// wait till device is done with request</span>
</code></pre></div></div> <p>The above pseudocode is a simple protocol to communicate with an I/O device. The <strong>polling</strong> status to see of device is ready wastes a lot of CPU cycles. The <strong>Programmed I/O</strong> explicitly copies data to/from device. The CPU need to be involved in this task.</p> <h2 id="interrupts">Interrupts</h2> <p>As polling wastes CPU cycles, the OS can put the process to sleep and switch to another process. When the I/O request completes, the device raises an interrupt.</p> <p>The interrupt switches process to kernel mode. The Interrupt Descriptor Table (IDT) stores pointers to interrupt handlers (interrupt service routines). The interrupt (IRQ) number identifies the interrupt handler to run for a device.</p> <p>The interrupt handler acts upon device notification, unblocks the process waiting for I/O, and starts the next I/O request. Also, handling interrupts imposes kernel mode transition overheads. As a result, polling may be faster than interrupts if devices is fast.</p> <h2 id="direct-memory-access">Direct Memory Access</h2> <p>The CPU cycles are wasted in copying data to/from device. Instead, we can use a special piece of hardware (DMA engine, seen in CS305) copies from main memory to device. The CPU gives DMA engine the memory location of data. In case of a read, the interrupt is raised after DMA completes. On the other hand, in case of a write, the disk starts writing after DMA completes.</p> <h2 id="device-driver">Device Driver</h2> <p>The part of the OS code that talks to the specific device, gives commands, handles interrupts etc. Most of the OS code abstracts the device details. For example, the file system code is written on top of a generic block interface. the underneath implementation is done via the device drivers.</p> <h1 id="lecture-18---files-and-directories">Lecture 18 - Files and Directories</h1> <h2 id="the-file-abstraction">The file abstraction</h2> <p>A <strong><em>file</em></strong> is simply a linear array of bytes, stored persistently. It is identified with a file name and also has a OS-level identifier - <strong><em><code class="language-plaintext highlighter-rouge">inode</code> number</em></strong>.</p> <p>A <strong><em>directory</em></strong> contains other subdirectories and files, along with their inode numbers. A directory is stored like a file whose contents are filename-to-inode mappings. You can think of a directory as a special type of file.</p> <h2 id="directory-tree">Directory tree</h2> <p>Files and directories are arranged in a tree, start with root (<code class="language-plaintext highlighter-rouge">/</code>).</p> <h2 id="operations-on-files">Operations on files</h2> <p>We can create a file using the <code class="language-plaintext highlighter-rouge">open</code> system call with a flag to create. It returns a number called <strong>file descriptor</strong> which is used as a file handle in the program.</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">fd</span> <span class="o">=</span> <span class="n">open</span><span class="p">(</span><span class="s">"foo"</span><span class="p">,</span> <span class="n">O_CREAT</span><span class="o">|</span><span class="n">O_WRONLY</span><span class="o">|</span><span class="n">O_TRUNC</span><span class="p">,</span> <span class="n">S_IRUSR</span><span class="o">|</span><span class="n">S_IWUSR</span><span class="p">);</span>
</code></pre></div></div> <p>We also open an existing file with the same system call. This must be done before reading or writing to a file. All other operations on files use the file descriptor. Finally, the <code class="language-plaintext highlighter-rouge">close</code> system call closes the file.</p> <p>Other operations on a file are -</p> <ul> <li><code class="language-plaintext highlighter-rouge">read</code>/<code class="language-plaintext highlighter-rouge">write</code> system calls. These occur sequentially by default. Successive read/write calls fetch from the current offset. The arguments to these functions are file descriptor, buffer with data and size.</li> <li>To read/write at a random location in the file, we use <code class="language-plaintext highlighter-rouge">lseek</code> system call that lets us seek to a random offset.</li> <li>Writes are buffered in memory temporarily and are flushed using <code class="language-plaintext highlighter-rouge">fsync</code>.</li> <li>There are other operations to rename files, delete (unlink) files, or get statistics of a file.</li> </ul> <h2 id="operations-on-directories">Operations on directories</h2> <p>Directories can also be accessed like files. For example, the <code class="language-plaintext highlighter-rouge">ls</code> program opens and reads all directory entries. A directory entry contains file name, inode number, type of file, etc.</p> <p><strong>Note</strong>. All the shell commands are simply C programs compiled into executables.</p> <h2 id="hard-links">Hard Links</h2> <p>Hard linking creates another file that points to the same inode number (and hence, same underlying data). If one file is deleted, file data can be accessed through the other links! inode maintains a link count, and the file data is deleted only when no further links exist to the data. You can only unlink, and the OS decides when to delete the data.</p> <h2 id="soft-links-or-symbolic-links">Soft links or symbolic links</h2> <p>Soft link is a file that simply stores a pointer to another filename. However, if the main file is deleted, then the link points to an invalid entry - <strong>dangling reference</strong>.</p> <h2 id="mounting-a-filesystem">Mounting a filesystem</h2> <p>Mounting a filesystem connects the files to a specific point in the directory tree. Several devices and file systems are mounted on a typical machine and are accessed using <code class="language-plaintext highlighter-rouge">mount</code> command.</p> <h2 id="memory-mapping-a-file">Memory mapping a file</h2> <p>There exists an alternate way of accessing a file, instead of using file descriptors and read/write system calls. <code class="language-plaintext highlighter-rouge">mmap</code> allocates a page in the virtual address space of a process. We can ask for an <strong>anonymous</strong> page to store program data or a <strong>file-backed</strong> page that contains data of a file. When a file is mmaped, file data is copied into one or more pages in memory and can be accessed like any other memory location in the program. This way, we can conveniently read/write into the file from the program itself.</p> <h1 id="lecture-19---file-system-implementation">Lecture 19 - File System Implementation</h1> <p>A <strong><em>file system</em></strong> is a way of organization of files and directories on a disk. An OS has one or more file systems. There are two main aspects of a file system -</p> <ul> <li>Data structures to organize data and metadata on the disk.</li> <li>Implementation of system calls like open, read, write, etc. using the data structures.</li> </ul> <p>Usually, disks expose a set of blocks - of size 512 bytes in general. The file system organizes files onto blocks, and system calls are translated into reads and write on blocks.</p> <h2 id="a-simple-file-system">A simple file system</h2> <p>The blocks are organized as follows -</p> <ul> <li>Data blocks - File data stored in one or more blocks.</li> <li>Metadata such as location of data blocks of a file, permissions, etc. about every file stored in the inode blocks. Each block has one or more inodes.</li> <li><strong>Bitmaps</strong> - Indicate which inodes/data blocks are free.</li> <li><strong>Superblock</strong> - Holds a master plan of all other blocks (which are inodes, which are data blocks, etc.)</li> </ul> <h3 id="inode-table">inode table</h3> <p>Usually, inodes (index nodes) are stored in an array. Inode number of a file is index into this array. What does an inode store?</p> <ul> <li>File metadata - Permissions, access time, etc.</li> <li>Pointers (disk block numbers) of the file data.</li> </ul> <h3 id="inode-structure">inode structure</h3> <p>The file data need not be stored contiguously on the disk. It needs to be able to track multiple block number of a file. How does an inode track disk block numbers?</p> <ul> <li>Direct pointers - Numbers of first few blocks are sored in the inode itself (suffices for small files)</li> <li>Indirect blocks - For larger file, inode stores number of indirect block, which has block numbers of file data.</li> <li>Similarly, double and triple indirect blocks can be stores - <strong>multi-level</strong> index</li> </ul> <h3 id="file-allocation-table-fat">File Allocation Table (FAT)</h3> <p>Alternate way to track file blocks. FAT stores next block pointer for each block. FAT has one entry per disk block, and the blocks are stored as a linked list. The pointer to the first block is stored in the inode.</p> <h3 id="directory-structure">Directory Structure</h3> <p>A directory stores records mapping filename to inode number. Linked list of records, or more complex structures (hash tables, binary search trees, etc.). A directory is a special type of file and has inode and data blocks (which store the file records).</p> <h3 id="free-space-management">Free space management</h3> <p>How to track free blocks? Bitmaps, for inodes and data blocks, store on bit per block to indicate if free or not. Also, we can have a <strong>free list</strong> in which the super block stores a pointer to the first free block and the free blocks are stored as a linked list. We can also use a more complex structure.</p> <h3 id="opening-a-file">Opening a file</h3> <p>The file has to be opened to have the inode readily available (in memory) for future operations on the file. What happens during open?</p> <ul> <li>The pathname of the file is traversed, starting at root.</li> <li>inode of root is known, to bootstrap the traversal.</li> <li> <p>Then, we recursively fetch the inode of the parent directory, read its data blocks, get inode number of the relevant child, and fetch inode of a child.</p> </li> <li>If a new file, new inode and data blocks will have to be allocated using bitmap, and the directory entry is updated.</li> </ul> <h3 id="open-file-table">Open file table</h3> <p>There is a global open file table which stores on entry for every file opened (even sockets, pipes). The entry point to in-memory copy of the inode (other data structures for sockets and pipes).</p> <p>There also exists a per-process open file table which is an array of files opened by a process. The file descriptor number is an index into this array. The per-process table entry points to the global open file table entry. Every process has three file - standard in/out/err open by default (fd 0, 1, 2).</p> <p>Open system call creates entries in both tables and return the fd number.</p> <h3 id="reading-and-writing-a-file">Reading and writing a file</h3> <p>For reading/writing a file</p> <ul> <li>Access in-memory inode via the file descriptor.</li> <li>Find the location of data block at current read/write offset. We get the locations from the inode itself.</li> <li>Fetch block from disk and perform operation.</li> <li>Write may need to allocate new blocks from disk using bitmap of free blocks.</li> <li>Update time of access and other metadata in inode.</li> </ul> <p>Therefore, any access to a file accesses multiple data blocks - hence, we need multiple accesses to the disk.</p> <h3 id="virtual-file-system">Virtual File System</h3> <p>File systems differ in implementations of data structures - E.g., organization of file records in the directory. So, do the implementations of the system calls need to change across file systems? No! Linux supports virtual file system (<strong>VFS</strong>) abstraction. The VFS looks at a file system as objects (files, directories, inodes, superblock) and operations on these objects. The system call logic is written on these VFS objects.</p> <p>Therefore, in order to develop a new file system, simply implement functions on VFS objects and provide pointers to these functions to the kernel. Syscall implementations need not change with the file system implementation details.</p> <h2 id="disk-buffer-cache">Disk Buffer cache</h2> <p>Results of recently fetched disk blocks are cached. The file system issues block read/write requests to block numbers via a buffer cache. If the block is in the cache, served from the cache and disk I/O is not required. Otherwise, block fetched to cache and returned to the file system.</p> <blockquote> <p>What about inode updates?</p> </blockquote> <p>Write are applied to cache block first. Synchronous/write-through cache write to disk immediately. Asynchronous/write-back caches stores dirty blocks in memory and writes back after a delay.</p> <p>Usually, the page cache is unified in the OS. Free pages are allocated to both processes and disk buffer cache from a common pool. What are the benefits of caches?</p> <ul> <li>Improved performance due to reduced disk I/O.</li> <li>Single copy of block in memory - No inconsistency across processes</li> </ul> <blockquote> <p>Second point? It’s fine.</p> </blockquote> <p>Some applications like databases may avoid caching altogether, to avoid inconsistencies due to crashes - <strong>direct I/O</strong>.</p> <h1 id="live-session-10">Live Session 10</h1> <ul> <li> <p>What is a file system? File system is just part of the OS code. All of the infrastructure related to files constitutes file system. You can implement the system calls related to the files in multiple ways. All of these different implementations are called “file systems.” However, the structure inside the disk can also refer to the file system (All follow VFS). What does it mean by OS can have more than one filesystems?</p> </li> <li> <p>It take a finite amount of time to fetch the data from the interface of the disk to the internal memory. When the DMA is not implemented, the CPU has to oversee the data transfer from the disk to the Memory. Yes, context switching and all happens when a data query is called. However, the interrupt handler is invoked only when the data is ready (in the disk registers) in the disk itself. Once an interrupt is issued, the CPU copies data from the disk registers to multiple layers of caches and the main memory. DMA takes care of all this (The disk directly writes to the main memory).</p> <p>DMA only accesses kernel buffers. User buffers are the <code class="language-plaintext highlighter-rouge">char* buf</code> variables in the user code.</p> </li> <li> <p>Advantage of memory mapped files? When you have to read a large chunk, you don’t have to wait for the disk to send multiple blocks. The file is already available in the memory. The disk buffer cache stores all the changes, and it has its own logic to flush, etc. The main advantage of memory-mapped files is avoiding the extra copy of the data in the memory! Any disk access is first copied into the disk buffer cache. These data blocks are then added to the process’ virtual address space. However, the data blocks are only present in the virtual address space when you use memory-mapped files. As in, the disk buffer cache’s pages are directly used in the page table of the process. On the other hand, if we use a read() system call, the data is copied into some buffer in the process. In any case, disk buffer cache is must!</p> </li> <li> <p>Normal read/write - Get a block from the disk into the kernel data cache, copy the bytes into the user space buffer. In memory mapping, the user space buffer does not have a copy but a pointer to the kernel buffer directly. There exists a concept of block/page cache which is an advanced concept.</p> </li> <li> <p>Mounting - Joining two trees by creating a new node that is the root of another directory tree.</p> </li> <li> <p>We have different entries in file table when two processes open the same file to ensure concurrency and <strong>correctness</strong>. Two files can read/write simultaneously if there are two entries. However, a parent and the child will point to the same file entry! This is why STD ERR/IN/OUT are the same for all processes.</p> </li> <li> <p>What is the use of the global file table? For the OS to keep track of all the open files in a central repository.</p> </li> <li> <p>Why not store the offset in the process’ file descriptor array itself? We need to have some processes sharing the offsets and some to not share them. The implementation is difficult, but it can be done.</p> </li> <li> <p>The indirect blocks are not counted as the data blocks of the process!</p> </li> </ul> <h1 id="lecture-31---device-driver-and-block-io-in-xv6">Lecture 31 - Device driver and block I/O in xv6</h1> <p>Any filesystem is built as multiple layers of abstraction in file systems -</p> <ul> <li>System call implementations - open, read, write.</li> <li>Operations on file system data structures - inodes, files, directories.</li> <li>Block I/O layer - in-memory cache of disk blocks.</li> <li>Device driver - communicates with hard disk to read/write blocks.</li> </ul> <h2 id="disk-blocks-and-buffers">Disk blocks and buffers</h2> <p>Disk maintains data as 512-byte blocks. Any disk block handled by the OS is also backed up in the disk buffer (<code class="language-plaintext highlighter-rouge">struct buf</code> in kernel memory). This is basically a copy of disk block in the memory. All the <code class="language-plaintext highlighter-rouge">struct buf</code>s are stored in a fixed size Buffer cache called as <code class="language-plaintext highlighter-rouge">bcache</code>. This is maintained as a LRU linked list.</p> <p>When we read from the disk, we assign buffer for the block number in the buffer cache, and the device driver sends read request to the disk controller. The disk controller raises an interrupt when the data is ready, and then the data is copied from the disk controller to the buffer cache - <code class="language-plaintext highlighter-rouge">VALID</code> flag is set after data is read.</p> <p>When we write to the disk, we first write into the buffer cache, and then issue a request to the disk. The device driver copies data from the buffer to the disk controller, and the disk controller raises an interrupt when write is complete - <code class="language-plaintext highlighter-rouge">DIRTY</code> flag is set until disk is updated.</p> <h2 id="device-driver-1">Device Driver</h2> <p>Processes that wish to read/write call the <code class="language-plaintext highlighter-rouge">iderw</code> function with buffer as the argument. If the buffer is dirty, it initiates a write request. If a buffer is invalid, then it places a read request. Requests are added to the queue and the function <code class="language-plaintext highlighter-rouge">idestart</code> issues requests after one another. The process sleeps until the request completes. The communication with the disk controller registers is done in <code class="language-plaintext highlighter-rouge">idestart</code> via <code class="language-plaintext highlighter-rouge">in</code>/<code class="language-plaintext highlighter-rouge">out</code> instructions we’ve seen before. This function knows all the register addresses of the devices - code is customized for every device.</p> <p>When the disk controller completes read/write operation, it raises an interrupt</p> <ul> <li>Data is read from the disk controller intos the buffer using <code class="language-plaintext highlighter-rouge">in</code> instruction</li> <li>The sleeping processes are woken up</li> <li>The next request from the queue is issued</li> </ul> <p>All of this is done in <code class="language-plaintext highlighter-rouge">ideintr</code> function which is called via the <code class="language-plaintext highlighter-rouge">trap</code> function. Also, there is no support for DMA in x86. With a DMA, the data is copied by the disk controller into the memory buffers directly before raising an interrupt. However, the CPU has to oversee this in xv6 without the presence of the DMA.</p> <blockquote> <p>so the <code class="language-plaintext highlighter-rouge">insl</code> instruction is run by the DMA in DMA-supported cores?</p> </blockquote> <h2 id="disk-buffer-cache-1">Disk buffer cache</h2> <p>All processes access the disk via the buffer cache. There exists only one copy of the disk block in cache, and only one process can access it at a time.</p> <p>The process calls <code class="language-plaintext highlighter-rouge">bread</code> to read a disk block. This function in turn calls <code class="language-plaintext highlighter-rouge">bget</code> which returns buffer if it already exists in the cache and no other process using it (using locks). Otherwise, if valid buffer is not returned by <code class="language-plaintext highlighter-rouge">bget</code>, <code class="language-plaintext highlighter-rouge">bread</code> reads from the disk using <code class="language-plaintext highlighter-rouge">iderw</code>.</p> <p>A process calls <code class="language-plaintext highlighter-rouge">bwrite</code> to write a block to disk, set dirty bit and request device driver to write. When done with the block, the process calls <code class="language-plaintext highlighter-rouge">brelse</code> to release the block and moves it to the head of the list.</p> <p>Let’s delve into <code class="language-plaintext highlighter-rouge">bget</code>. It returns the pointer to the disk block if it exists in the cache. If the block is in cache and another process is using it, it sleeps until the block is released by the other process. However, if block is not in cache, it fins a least recently used non-dirty buffer and recycles it to use for this block.</p> <blockquote> <p>What if all blocks are dirty? <code class="language-plaintext highlighter-rouge">panic!</code></p> </blockquote> <p>The two goals achieved by the buffer cache are</p> <ul> <li>Recently used disk blocks are stored in the memory for future use.</li> <li>Disk block are modified by only one process at a time.</li> </ul> <h2 id="logging-layer">Logging layer</h2> <p>A system call can change multiple blocks at a time on the disk, and we want atomicity in case the system crashes during a system call. So, wither all changes are made or none is made. <strong>Logging</strong> ensures atomicity by grouping disk block changes into transactions</p> <ul> <li>Every system call starts a transaction in the log, write all changed disk blocks in the log, and commits the transaction.</li> <li>Later, the log installs the changes in the original disk blocks one by one.</li> <li>If a crash happens before the log is written fully, no changes are made. However, if a crash happens after the log entry is committed, the log entries are replayed when the system restarts after crash.</li> </ul> <blockquote> <p>restart from the start or last left entry?</p> </blockquote> <p>In xv6, changes of multiple system calls are collected in memory and committed to the log together. Actual changes happen to disk blocks only after the group transaction commits. The process must call <code class="language-plaintext highlighter-rouge">log_write</code> instead of <code class="language-plaintext highlighter-rouge">bwrite</code> during the system call. (?)</p> <h1 id="lecture-32---file-system-implementation-in-xv6">Lecture 32 - File system implementation in xv6</h1> <h2 id="disk-layout">Disk layout</h2> <p>The disk in xv6 is formatted to contain a superblock (followed by the boot block), log (for crash recovery), inode blocks (multiple inodes packed per block), bitmap (indicating which data blocks are free), and, finally, the actual data blocks.</p> <p>The disk inode contains block number of direct blocks and <strong>one</strong> indirect block. Also, directory is treated as a special file. The data blocks contain directory entries and the corresponding name-inode number mappings. The inode also stores the link count - number of directory entries pointing to a file inode.</p> <h2 id="in-memory-data-structures">In-memory data structures</h2> <p>Every open file has a struct file associated with it which contains some variables such as pointer to inode and pipe structure. All struct files are stored in a fixed size array called the file table (<a href="https://www.cse.iitd.ac.in/~sbansal/os/previous_years/2011/xv6_html/file_8c.html#5e3713b2e8d8fca04c15e52b9a315620"><code class="language-plaintext highlighter-rouge">ftable</code></a>). The <em>file descriptor array of a process</em> contains pointers to struct files in the file table.</p> <p>What happens if two different processes open the same file? They have two different entries in file table because they need to read and write independently at different offsets. However, the file table entries point to the same inode.</p> <p>On the other hand, when P forks C, both file descriptors will point to the same struct file (<code class="language-plaintext highlighter-rouge">ref</code> is increased) and the offsets are shared. The reference count of struct file is number of file descriptors that point to it. Similarly, an inode also has such a reference number.</p> <p>The in-memory <a href="https://www.cse.iitd.ac.in/~sbansal/os/previous_years/2011/xv6_html/fs_8c.html#6baaf26dd83b71b8d684c5d54a709e31">inode</a> is almost a copy of the disk inode, stored in the memory for open files. All of these in-memory inodes are stored in a fixed size array called <strong>inode cache</strong> <a href="https://www.cse.iitd.ac.in/~sbansal/os/previous_years/2011/xv6_html/fs_8c.html#1fbfdebf96af7ed1f992e387cba059b3"><code class="language-plaintext highlighter-rouge">icache</code></a>. The in-memory has a <code class="language-plaintext highlighter-rouge">ref</code> that the disk inode doesn’t have. It stores the number of pointers from the file table entries. This is different from the <code class="language-plaintext highlighter-rouge">nlink</code> which counts the number of files that point to this inode in the disk. A file is cleaned up on the disk only when both <code class="language-plaintext highlighter-rouge">ref</code> and <code class="language-plaintext highlighter-rouge">nlink</code> are zero.</p> <blockquote> <p>Cleaned up from memory or disk?</p> </blockquote> <h2 id="inode-functions">inode functions</h2> <ul> <li>Function <code class="language-plaintext highlighter-rouge">ialloc</code> allocates a free inode from the disk by looking over disk inodes and finding a free on for a file.</li> <li>Function <code class="language-plaintext highlighter-rouge">iget</code> returns a reference counted pointer to in-memory inode in <code class="language-plaintext highlighter-rouge">icache</code> to use in struct file etc. This is a non-exclusive pointer which means the information inside inode structure may not be up to date.</li> <li>Function <code class="language-plaintext highlighter-rouge">iput</code> does the opposite of the above function.</li> <li>Function <code class="language-plaintext highlighter-rouge">ilock</code> locks the inode for use by a process, and updates it information from the disk if needed. <code class="language-plaintext highlighter-rouge">iunlock</code> is the opposite.</li> <li>Function <code class="language-plaintext highlighter-rouge">iupdate</code> propagates changes from in-memory inode to on-disk inode.</li> </ul> <blockquote> <p><code class="language-plaintext highlighter-rouge">ilock</code> updates from disk? Also, why not up to date info? All share pointers right?</p> </blockquote> <p>Inode also has pointers to file data blocks. The function <a href="https://www.cse.iitd.ac.in/~sbansal/os/previous_years/2011/xv6_html/fs_8c.html#965e83e1fa9b15abf268784ce74181bb"><code class="language-plaintext highlighter-rouge">bmap</code></a> returns the address of the nth block of the file. If it’s a direct block, we read from the inode. Otherwise, we read indirect block first and then return block number from it. This function can allocate data blocks too - if n-th data block of the file is not present, it allocates a new block on the disk, writes it to the inode and returns the address. Function <code class="language-plaintext highlighter-rouge">readi</code>/<code class="language-plaintext highlighter-rouge">writei</code> are used to read/write file data at a given offset. They call <code class="language-plaintext highlighter-rouge">bmap</code> to find the corresponding data block. Additionally, <code class="language-plaintext highlighter-rouge">bmap</code> uses <code class="language-plaintext highlighter-rouge">balloc</code> to allocate a data block in the disk, and <code class="language-plaintext highlighter-rouge">bread</code> to read the data blocks.</p> <h2 id="directory-functions">Directory functions</h2> <p>We have two additional functions for directories:</p> <ul> <li>Directory lookup - Read directory entries from the data blocks of the directory. If file name matches, return pointer to the inode from <code class="language-plaintext highlighter-rouge">icache</code>. - <a href="https://www.cse.iitd.ac.in/~sbansal/os/previous_years/2011/xv6_html/fs_8c.html#a182c62fade7a0bae9408830d5e06d4f"><code class="language-plaintext highlighter-rouge">dirlookup</code></a></li> <li>Linking a file to a directory - Check file with the same name does not exist, and add the mapping from file name to inode number to the directory. - <a href="https://www.cse.iitd.ac.in/~sbansal/os/previous_years/2011/xv6_html/fs_8c.html#69a135a0e8a06d9f306d77ebc0c1f7a0"><code class="language-plaintext highlighter-rouge">dirlink</code></a></li> </ul> <h2 id="creating-a-file">Creating a file</h2> <p>We locate the inode of the parent directory by walking the filepath from the root. Then, we look up the filename in the parent directory. We return the inode if the file already exists. Otherwise, we allocate a new inode for it, lock it, and initialize it. If the new file is a directory, we add entries for <code class="language-plaintext highlighter-rouge">.</code> and <code class="language-plaintext highlighter-rouge">..</code>. Otherwise, we link it to its parent directory. All of this is done in the <a href="https://www.cse.iitd.ac.in/~sbansal/os/previous_years/2011/xv6_html/sysfile_8c.html#8700568adc174a9e10c167daf6296c8d">create</a> function.</p> <blockquote> <p>Is create a system call?</p> </blockquote> <h2 id="system-call---open">System call - <code class="language-plaintext highlighter-rouge">open</code></h2> <p>We get the arguments - filename and the mode. We create a file (if specified) and get a pointer to its inode. Then, we allocate a new struct file in the <code class="language-plaintext highlighter-rouge">ftable</code>, and also a new file descriptor entry in struct proc of the process pointing to the struct file in <code class="language-plaintext highlighter-rouge">ftable</code>. Finally, we return the index of new entry in the file descriptor array of the process. <em>Note</em> - <code class="language-plaintext highlighter-rouge">begin_op</code> and <code class="language-plaintext highlighter-rouge">end_op</code> capture the transactions for the log file.</p> <h2 id="system-call---link">System call - <code class="language-plaintext highlighter-rouge">link</code></h2> <p>This function links an existing file from another directory with a new name (hard linking).</p> <blockquote> <p>Is different directory necessary?</p> </blockquote> <p>We get the pointer to the file inode by walking the old file name. Then, we update the link count in the inode. Once we get the pointer to the inode in the new directory, we link the old inode from the parent directory using a new name.</p> <blockquote> <p>Do we need a new filename? Do we need a different directory?</p> </blockquote> <h2 id="system-call---fileread">System call - <code class="language-plaintext highlighter-rouge">fileread</code></h2> <p>The <code class="language-plaintext highlighter-rouge">sys_read</code> system call calls <code class="language-plaintext highlighter-rouge">fileread</code>. This is a general template for the other system calls. For example, file read does the following -</p> <ul> <li>Get arguments (file descriptor number, buffer to read into, number of bytes to read)</li> <li>Fetch inode pointer from the struct file and perform read on inode (or pipe if the file descriptor pointed to pipe).</li> </ul> <blockquote> <p>What pipe?</p> </blockquote> <ul> <li>Function <code class="language-plaintext highlighter-rouge">readi</code> uses the function <code class="language-plaintext highlighter-rouge">bmap</code> to get the block corresponding to the nth byte and reads from it.</li> <li>Offset in struct file is updated.</li> </ul> <h2 id="summary">Summary</h2> <ul> <li>Disk is organized as inodes, data blocks and bitmaps</li> <li>The in-memory organization consists of file descriptor array which points to struct file in file table array which in turn points to in-memory inode in the inode cache.</li> <li>A directory is a special file where data blocks contain directory entries (filenames and corresponding inode numbers).</li> <li>Updates to disk happen via the buffer cache***. Changes to all blocks in a system call are wrapped in a transaction and are logged for atomicity.</li> </ul> <h3 id="references">References</h3> <ul> <li><a href="https://www.cse.iitd.ac.in/~sbansal/os/previous_years/2011/xv6_html/index.html">doxygen documentation of xv6 user code</a></li> </ul>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Basic concepts of Operating Systems like process abstraction, process execution mechanism, inter-process mechanism, memory management, paging, memory allocation and free space management algorithms, threads and concurrency, locks, condition variables, semaphores, I/O and filesystems, etc.]]></summary></entry><entry><title type="html">Philosophy Notes</title><link href="https://sudhansh6.github.io/blog/philosophy/" rel="alternate" type="text/html" title="Philosophy Notes"/><published>2021-08-08T00:00:00+00:00</published><updated>2021-08-08T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/philosophy</id><content type="html" xml:base="https://sudhansh6.github.io/blog/philosophy/"><![CDATA[<h1 id="lecture-1---introduction">Lecture 1 - Introduction</h1> <p>We will start by studying Western (Classical Greek) philosophy. In particular, we shall look into Thales, Pythagoras, and Heraclitus’s philosophy.</p> <p>Moving further, we enter the Socratic period. Here, we shall study Socrates (obviously), Plato, and Aristotle.</p> <p>Then, we shall learn about Modern Philosophy - Thomas Hobbes and Rene Descartes (Why are we studying mathematicians?).</p> <blockquote> <p><em>kairos</em> - Universal truth, the right, critical, or opportune moment.</p> </blockquote> <p>We will also look into Indian Philosophy. It consists of 6 major realms, but we will cover Charvaka, Samkhya (Numbers?), and Buddhism.</p> <p>From what I understand, we will read a book called <a href="https://www.goodreads.com/book/show/55517106-the-courage-to-exist">“The Courage to Exist: A Philosophy of Life and Death in the Age of Coronavirus” by Ramin Jahanbegloo</a>. Reading the book will reduce the syllabus in Modern philosophy.</p> <h3 id="evaluation-policy">Evaluation policy</h3> <ul> <li>Assignment - <strong>10%</strong></li> <li>Quiz - <strong>10%</strong></li> <li>Midsem - <strong>30%</strong> - Short questions (1 line answers) or Objective (Yay!) and 1 Essay question</li> <li>Endsem - <strong>50%</strong> - 2 Essays, Short questions and Objective.</li> </ul> <h2 id="what-is-philosophy">What is Philosophy?</h2> <p><strong>Philosophy</strong> is a humanistic discipline. Philosophy is related to human life. This relation is <strong><em>epistemological</em></strong>. By epistemology, we mean study of knowledge - theory of knowledge. Its epistemological relation unfolds human aspiration for higher knowledge concerning the reality, or the <strong><em>underlying principle of the reality</em></strong>.</p> <p>The underlying principle of human existence or the existence of the entire cosmos or the reality brings out the discourse on <strong><em>metaphysics</em></strong>. Philosophy in fact began with metaphysics. Philosophical inquiry was metaphysical inquiry.</p> <p>Does something exist or not? Is something meaningful or not? Philosophy is an endless quest for gaining an understanding of reality. Humans have cultivated the value of seeking knowledge. <strong>Metaphysics</strong> talks about what really exists in the universe. <a href="http://abyss.uoregon.edu/~js/glossary/thales.html">Thales</a> theorised <em>water</em> is the fundamental element of the universe.</p> <h1 id="lecture-2">Lecture 2</h1> <p>We had a brief discussion on the meaning of philosophy and its origins. It originated from the metaphysical questions that try to understand reality.</p> <p><strong>Metaphysics</strong> studies <strong><em>ontology</em></strong> or the <em>existence of being</em>. At the beginning of the philosophical discourse, the metaphysical question was about <em>how reality has come into being</em>.</p> <p><strong>Epistemology</strong> is the heart of Metaphysics. It refers to the theory of knowledge. It talks about beliefs and ideas and tries to justify/evaluate them. Basically, knowledge is a <em>justified true belief</em>. The thirst for learning about reality or philosophical thinking is due to epistemology.</p> <blockquote> <p>Doxa - common belief or popular opinion</p> </blockquote> <h2 id="who-is-a-philosopher">Who is a Philosopher?</h2> <p>A person who formulates a perspective, refutes ideas, and critiques philosophical theories.</p> <p>A philosopher can express their discontent with an existing idea and point out errors in these ideas. They should be able to prove whatever is acceptable using their knowledge and experience. This way, an idea would sustain through generations. Experience is the foundation of knowledge.</p> <p>Ptolemy believed in the <em>geocentric</em> model of the solar system. On the other hand, Copernicus believed in the <em>heliocentric</em> model. People gradually shifted to Copernicus’ theory. This was known as the <em>Copernican revolution</em>. This developed a new philosophical study called <em>critical thinking</em>.</p> <h2 id="metaphysical-question">Metaphysical question</h2> <p>It refers to questions such as “Who am I?” “What constitutes my being?” and “Am I a body?”.</p> <p>Philosophical inquiring initiates specific fundamental questions concerning humanity. What is life? What is suffering? What is truth? What is happiness? What is justice?</p> <p>In science, we have 3 propositions or <em>Laws of Nature</em></p> <ul> <li>Missed this</li> <li>Law of Causality</li> <li>Law of transformation of energy</li> </ul> <p>The Law of causality is a metaphysical concept. To answer any of the metaphysical questions, we try to develop a causal relationship.</p> <h2 id="philosophizing">Philosophizing</h2> <p>To understand these questions or seek an answer to these questions, we need to study some philosophers. We need to have a deep reflection on experience.</p> <p>Philosophizing is a part of our critical inquiry into these questions. A philosopher should be able to build a new perspective.</p> <h1 id="lecture-3-29-07-21">Lecture 3 <code class="language-plaintext highlighter-rouge">29-07-21</code></h1> <p>Previously, we had discussed about the meaning and nature of philosophy. Philosophy begins with metaphysical inquiry. Science studies about the concrete objects in the universe. On the other hand, philosophy also deals with abstract concepts for knowledge building.</p> <h2 id="order-in-first-philosophy">Order in first Philosophy</h2> <p>Aristotle’s philosophy is known as the first philosophy. He discusses about 3 branches of philosophy -</p> <ul> <li>Science of being <em>qua being</em> - <strong><em>Ontology</em></strong> - General nature of everything. Beings which tentatively exist, and whose understanding is not yet known.</li> <li>Highest kind of being - <strong><em>Theology</em></strong> - Appropriate to divinity.</li> <li>First principle - <strong><em>Universal science</em></strong> - True of every existing thing and lie at the basis of all proof or demonstration. This branch ties together all the concepts.</li> </ul> <blockquote> <p>Qua - It is a technical expression Aristotle uses to indicate an aspect under which something is to be considered. The study of being ‘qua being’ concerns the most general class of things, viz., everything that exists.</p> </blockquote> <p>The <strong>four causes</strong> or four explanations are, in <a href="https://en.wikipedia.org/wiki/Aristotelianism">Aristotelian thought</a>, four fundamental types of answer to the question “why?” in the <a href="https://en.wikipedia.org/wiki/Posterior_Analytics">analysis</a> of change or movement in nature:</p> <ul> <li><strong>Material cause</strong>: “that out of which” it is made. God is an <em>unmoved mover</em>. According to Aristotle, the primordial “stuff” (fundamental particles) exist at the level of God. How is everything designed? Matter by itself does not have any identity. It gets its purpose from a being who gives it a meaning.</li> <li><strong>Efficient Cause</strong>: the source of the object’s principle of change or stability.</li> <li><strong>Formal Cause</strong>: the essence of the object.</li> <li><strong>Final Cause</strong>: the end/goal of the object, or what the object is good for.</li> </ul> <p>Every thing in existence must account for these four causes.</p> <h2 id="the-controversial-turning-points">The controversial turning points</h2> <p>From the 1950s (BC), there was a dramatic change in the notion of knowledge. Some people (<em>logical positivists</em>) gained a scornful attitude towards metaphysics. They considered Metaphysics is meaningless for philosophy.</p> <p>A philosopher called Gracia talks about how people tried to integrate metaphysics into philosophy.</p> <p>“A holistic approach is desirable, but this cannot be reduced to a disorderly aggregate of diverse approaches.” (Gracia 2014: 310)</p> <p>A computer scientist needs to communicate with people who studies metallurgy to better understand the underlying details of the hardware. Similarly, philosophers cannot completely abandon metaphysics for a greater understanding of the reality.</p> <h2 id="epistemology">Epistemology</h2> <p>The term epistemology comes from the Greek words “episteme” and “logos”. “Episteme” can be translated as knowledge or understanding. In contrast, “logos” can be translated as argument or reason.</p> <p>Knowledge is a justified true belief. Plato’s Theatetus - “Knowledge importantly depends on the nature of knower and her relationship to her environment.” We can’t provide proofs in philosophy. Some scientists try to show philosophy as a field in science.</p> <h1 id="recordings---set-1">Recordings - Set 1</h1> <h2 id="introduction-to-philosophy">Introduction to Philosophy</h2> <p>What is Philosophy? It is a humanistic discipline that investigates the nature of values and relationships that integrate humanity with or reality as a whole. It is the love of knowledge about the heart of reality, mind, values, etc. A philosopher needs to understand the very development of such a knowledge system.</p> <h3 id="the-source-of-origin-of-philosophy">The source of origin of philosophy</h3> <p>All knowledge systems are developed from human/thinking thoughts. Speculating about the very nature of existence is the basis of <strong>philosophical thinking</strong>.</p> <h2 id="philosophical-thinking">Philosophical Thinking</h2> <p>A philosophical investigation is conceptual. Philosophers think through concepts, and their effort is to make sense of experiences and ideas concerning the subject matter. Philosophical thinking is speculative, Imaginative/creative, Reflective/Critical, Argumentative, and liberal/inclusive.</p> <p><strong>Speculative thinking</strong> - Knowledge is merely based on experience and <strong>intuitive insights</strong> beyond the expertise of observable facts. It highlights the power of imagination in forming or visualizing something - the existence of the underlying principle. For example, Pythagoras used numbers to speculate about reality. Thales theorized that five fundamental elements constitute reality. Imagination is the spirit of discovery/invention.</p> <p>Imagination is the spirit of discovery/invention.</p> <p><strong>Creative Thinking</strong> - Rabindranath Tagore was a poet, philosopher and his poems are among the finest expressions of creativity that aspired to grasp the truth.</p> <p><strong>Reflective/critical thinking</strong> - Many philosophers and philosophical traditions accept critical thinking as a method of doing philosophy. It involves evaluating information, objective analysis of thoughts, skillful application of concepts, and synthesis of ideas.</p> <p><strong>Argumentative thinking</strong> - Philosophical thinking involves arguments. Logic takes us from one set of thoughts to another collection of ideas.</p> <p><strong>Liberal and inclusive</strong> - Philosophy would have been a disaster if people were not open-minded/ receptive to new ideas and thoughts. Although, sometimes dominant philosophical traditions prevail - Philosophical Blindness.</p> <p>Philosophical enterprise demands self-scrutiny to eliminate prejudices and dogmas and also competitions - <em>R. Solomon 2001:100</em>.</p> <h2 id="metaphysics">Metaphysics</h2> <p>Metaphysics was the central idea of philosophical discourse. Philosophy began with metaphysical inquiries. It raises certain fundamental questions regarding the universe and man’s existence in it.</p> <p>Metaphysics talk about everything that exists or may exist. It is also known as <em>the first philosophy</em>.</p> <p>There were some controversial turning points in the evolution of metaphysics. During the renaissance in the 17th century, the tough-minded philosophers disrupted the glow of metaphysical thinking.</p> <h3 id="aristotle-metaphysics-as-first-philosophy">Aristotle: Metaphysics as First Philosophy</h3> <p>He wrote a Treatise on Metaphysics - <em>ta meta ta physica</em>. The Order in First Philosophy is discussed <a href="#order-in-first-philosophy">here</a> in lecture 3.</p> <p>Metaphysics has the power of integrating knowledge across all fields of science. Therefore, it still has relevance in the contemporary world.</p> <p>Now, we begin studying various philosopher and their philosophies.</p> <h2 id="thales-624-546-bc---thales-of-miletus">Thales (624-546 BC) - Thales of Miletus</h2> <p>He is known as the First Philosopher. He is the founder of the Ionian School of Science. Thales motivated people to think about reality keeping aside religion and mythical wisdom.</p> <h3 id="what-did-thales-pursue">What did Thales pursue?</h3> <p>He embarked upon the study of things. He awakened people to their awareness. He questioned the existing system of knowledge and the frameworks of misunderstanding. He evoked a desire to find out the right knowledge.</p> <p>He wanted to study reality as <em>it is</em> and not how it <em>appears</em>.</p> <h3 id="everything-is-water">Everything is Water</h3> <p>Water has no shape, no form of its own. Thales remarked that water is something vital for the generation and growth of all things.</p> <h1 id="lecture-4-02-08-21">Lecture 4 <code class="language-plaintext highlighter-rouge">02-08-21</code></h1> <p>Our explanations are always <em>causal</em>. Justification is also a part of causal explanation. A decision is not explanatory because it depends on interpretation.</p> <h2 id="skepticism">Skepticism</h2> <p><em>Pyrrho</em> (360BC to 270BC) started this idea that truth is never known, and having complete knowledge is impossible. We can doubt everything we know. Everything that is seen may not always be true.</p> <blockquote> <p>Skepticism - Uncertainty of Human Knowledge</p> </blockquote> <p>Doubting is a form of learning. For example, Rene Descartes, used a mathematical form of skepticism (Probability?).</p> <h2 id="sources-of-knowledge-in-india-philosophy">Sources of Knowledge in India Philosophy</h2> <ul> <li><em>Prathyakhsha</em> - Perception</li> <li><em>Anumana</em> - Inference</li> <li><em>Upamana</em> - Comparison</li> <li><em>Sabdda</em> - Testimony</li> </ul> <p>These are the means through which knowledge is gained via experience.</p> <h2 id="foundation-of-knowledge">Foundation of Knowledge</h2> <p>The rationalists believed <strong>reason</strong> is the primary source of knowledge, whereas empiricists believed <strong>experience</strong> is the primary source of knowledge. We shall learn about both these schools of knowledge in due time.</p> <h2 id="logic">Logic</h2> <p>This is another branch of Philosophy like epistemology. Logic is known as <em>Tarka shastra</em> in Indian philosophy. Laws of thought like the law of identity, the law of contradiction, etc., were developed using a human’s thinking structure.</p> <blockquote> <p>Tarka - Arguments</p> </blockquote> <p>It involves deductive and inductive arguments. Aristotle introduced <strong>deductive</strong> arguments. These arguments are in the premise-predicate form. The premises are based on universal truths. Francis Bacon presented <strong>inductive</strong> arguments. He spoke about \(5\) different types of induction.</p> <ul> <li>Method of Agreement</li> <li>Method of Difference</li> <li>Joint Method of agreement in presence of difference</li> <li>Method of Concomitant variation</li> <li>Method of Residues</li> </ul> <p>In this realm, assumptions are based on experience.</p> <p>Therefore, along with epistemological inquiry, one must check for the arguments associated with the theory. Logic is an integral part of Philosophy.</p> <h2 id="ethics--aesthetics">Ethics / Aesthetics</h2> <p>Philosophy also deals with the study of values and beauty. This is also a branch of Philosophy along with Metaphysics, Epistemology, and Logic.</p> <h1 id="lecture-5-030821">Lecture 5 <code class="language-plaintext highlighter-rouge">03/08/21</code></h1> <p>We shall study about Thales now. Perspectives are different from Opinions. To talk about philosophy, we need to study the history. We are going to start with the pre-Socratic period.</p> <h2 id="thales-of-miletus">Thales of Miletus</h2> <p>In those days, there was no distinction between religions and mystical stuff. Most of the stuff discussed in the lecture has already been summarised <a href="#thales-624-546-bc---thales-of-miletus">here</a>.</p> <h1 id="lecture-6-050821">Lecture 6 <code class="language-plaintext highlighter-rouge">05/08/21</code></h1> <p>We will study Pythagoras in this lecture. We are still in the pre-Socratic period.</p> <h2 id="pythagoras">Pythagoras</h2> <p>He believed numbers could be used to explain the reality. This was quite different from Thales’s perception who believed water, a tangible element, is the fundamental part of reality. However, numbers are intangible! His thoughts were unique and created a blend - Mysticism and Mathematics. He also loved music. Mathematics maintains demonstrative deductive arguments.</p> <h3 id="philosophical-contribution">Philosophical Contribution</h3> <p>He introduced the concept of deciphering the inner working of the mind/ purification of intellectual and perceptual abilities.</p> <p>Self-evidence is a deeply influential notion and Pythagoras applied to the perspectives of moral claims. He introduced the Axiomatic method. Axioms were self-evident/ fundamental propositions.</p> <h1 id="lecture-7-090821">Lecture 7 <code class="language-plaintext highlighter-rouge">09/08/21</code></h1> <p>Now, we shall study Heraclitus. He was not discussed in the previous years.</p> <h2 id="heraclitus">Heraclitus</h2> <p>His ideologies are pretty different from the previous philosophers. Thales talked about a tangible substance - water, and Pythagoras used an intangible substance - Water. Although, they were all natural philosophers - learned about reality from nature.</p> <p>Heraclitus talks about “Becoming”. Being to Becoming. He tried to understand the process of change, transformation, origin, and decay. He used Fire as the first principle. One of his ideas was “Fire lives the death of air, and air lives the death of fire; water lives the death of earth, earth that of water”. He brought upon the concept of <strong>Universal Flux</strong> and cyclical existence.</p> <p>His ideas can be understood as the primitive version of “Matter can neither be created nor destroyed”. There is unity in the world and unity resulting from diversity. The notion of <strong>Logos</strong> - Reason in Things. Logos is also translated as the essence of things. Cosmic process - everything is changing in order; there is nothing arbitrary. He also believed in the concept of soul - cosmic fire. Human thoughts are a subordinate to the Universal Reason or divine reasoning.</p> <h1 id="lecture-8-100821">Lecture 8 <code class="language-plaintext highlighter-rouge">10/08/21</code></h1> <p>We shall study about the Socratic period. This phase of philosophy consists of 3 main philosophers - Socrates, Plato and Aristotle. How do Epistemology and Metaphysics differ? How does Ontology fit in? We shall discuss about it now</p> <h2 id="before-the-arrival-of-socrates">Before the Arrival of Socrates</h2> <p>Men-centric philosophy prevailed during this period. The Greek society developed in the fields of Greek history, Poetry, and Medicine. People generally believed in divinity and occult mystical powers. Also, the zeal of investigation was intense. However, Socrates sought for morals and ethics. He didn’t want to blindly follow the existing beliefs, and wanted to discover his own version of the truth.</p> <p>This age also saw the rise of spirit of independent reflection and criticism. People tried to avoid the false paths, and departed from the school of speculative thought.</p> <h2 id="the-beginning-of-the-socratic-period">The beginning of the Socratic period</h2> <p>There was a shift in the way of philosophical thinking. There was an establishment of Democratic institution fostered in the new age in Athens. Traditional religion, morality, science, etc. were subject to criticism. There was an other city which gave less importance to knowledge and gave more importance to power - Spartan.</p> <p><em>Subjectivism</em> is the theory that talks about each individual’s perception. If one tries to assert his truth as the only true truth, then there would be chaos.</p> <p><strong>Sophists</strong> - A representative of New Movement in Greece. Socrates was a part of this group. However, Socrates was executed because he defied the truth of the group. More on that later. Sophists were wise and skillful, professional teachers, travellers, etc. They used to train people in <strong>dialectics, grammar, rhetorism,</strong> and <strong>oratory</strong>. Sophists had great communication skills.</p> <p>“Every individual has the ability to measure things” - <em>Protagoras</em>.</p> <p>Some technical jargon</p> <blockquote> <p><strong>Oracle</strong> - God, Prediction of Future/ Apollo</p> <p>Oracle of Delphi - Expensice Oracle. Delphi is a person.</p> </blockquote> <blockquote> <p><strong>Rhetoric</strong> - Flow of Speech before the state or Public</p> </blockquote> <blockquote> <p><strong>Dialectic</strong> - A form of argumentative reasoning involving frequent question and answers. It began with the questions “what is justice?”</p> <p>In Dialectical interrogation the questioner attempts to lead the respondent into contradiction - <strong>Aporia</strong>. This is a typical scene in a courtroom.</p> </blockquote> <p>Socrates believed an unexamined life is not worth leading.</p> <h1 id="lecture-9-120821">Lecture 9 <code class="language-plaintext highlighter-rouge">12/08/21</code></h1> <p>Plato conceptualized soul as a thinking medium.</p> <blockquote> <p>Philo - Love, Sophia - Knowledge</p> <p>Philosophy is love of knowledge</p> </blockquote> <p>Socrates had fundamental questions through which he tried to explain the reality. “Life is suffering” - <em>Buddha</em>. These questions sustain through time, and are relevant to every living being.</p> <p>Some philosophers accept belief as the source of knowledge.</p> <p>Socrates is an important philosopher but he didn’t publish anything. His student, Plato, documented all of Socrates’ dialogues and beliefs. Socrates believed in truth, and he went to great lengths to show his belief in his truth. In the end, Socrates accepted death, but did not accept to let go of his truth.</p> <p>In those days, people visited “Oracles” to know about their future. Oracles were believed to be wise humans who had immense knowledge.</p> <h1 id="lecture-10-160821">Lecture 10 <code class="language-plaintext highlighter-rouge">16/08/21</code></h1> <p>Socrates believed virtue has to be practiced.</p> <blockquote> <p>Virtue - behavior showing high moral standards.</p> </blockquote> <p>In those days, Athenians depended upon the Sophists’ guidance. However, Socrates believed every human had the power to acquire knowledge for himself.</p> <p>Socrates urged for clear and rational thinking. He believed knowledge and virtue were necessary for a just society. Actions may be rational, but they need not be ethical. One needs to have moral courage.</p> <h2 id="what-is-knowledge">What is knowledge?</h2> <p>Socrates believed every individual must understand the meaning of mortality - <em>Moral knowledge</em>. It was necessary for resolving social conflicts and establishing a <strong>just society</strong>.</p> <p>Knowledge is necessary and sufficient for virtue. Can virtue be taught? Socrates thought it could not be taught. One can become virtuous by practicing virtue. Every individual must reflect on oneself and examine whatever they are engaged in.</p> <h2 id="agony-of-athenians">Agony of Athenians</h2> <p>Athenians were not ready to accept and evaluate themselves. “Arrogance”. Socrates was raising some fundamental questions about the so-called wise Sophists. He was questioning their practice, and people were not ready to accept this.</p> <p><code class="language-plaintext highlighter-rouge">Need to watch 3 recorded lectures on Socrates!</code></p> <h1 id="lecture-11-170821">Lecture 11 <code class="language-plaintext highlighter-rouge">17/08/21</code></h1> <p>Now, we shall study Plato. As previously discussed, Plato was a disciple of Socrates. His ideology is summarised as follows.</p> <h2 id="plato---theory-of-ideas">Plato - Theory of Ideas</h2> <p>The central theme of the theory of ideas is the <em>nature of the essence</em> of things and how we perceive them. Socrates was interested in moral knowledge and ethics. Plato was inclined to know about the definitions of the very concepts discussed by Socrates. He also gave importance to Ontology and Metaphysics. He felt this was missing in Socrates’ ideas. Therefore, Metaphysics and Ontology once again came into the discourse of philosophy.</p> <h2 id="plato-and-his-philosophical-problem">Plato and his Philosophical problem</h2> <p>Plato tried to compose learnings from other philosophers like Parmenides, Heraclitus, and Protagoras. This comprehensive outlook shows the link between epistemology, ontology, and ethics - What are the meanings of life and its existence?</p> <ul> <li>Plato believed that genuine knowledge is changeless, eternal, and absolute (from Parmenides).</li> <li>Sense experiences vary, and therefore sensuous appearances keep changing (from Heraclitus).</li> <li>Plato emphasized that the real can be comprehended through reason.</li> </ul> <h2 id="dialectical-method">Dialectical Method</h2> <blockquote> <p>“The dialectical method consists, first, in the comprehension of scattered particulars into one idea, and second, the division of the idea into species, that is in the process of generalization and classification. In this way alone can there be clear and consistent thinking; we pass from concepts to concept, upward and downward, generalizing and particularizing, combining and dividing, synthesizing and analysing, carving out concepts as sculptor craves a beautiful figure out of a block of marble”</p> </blockquote> <p>Socrates practiced this, but Plato conceptualized this idea. This method consists of</p> <ul> <li>Comprehension of scattered particulars into one idea</li> <li>The division of the idea into species for generalization.</li> </ul> <p>In this way, one can follow clear and consistent thinking. This is the foundation of deductive reasoning.</p> <h2 id="concepts-ideas-and-forms">Concepts, Ideas, and Forms</h2> <p>Plato’s originality lies in raising the issue of universals. Coming to conclusions using representative of general classes, say, trees. <em>Similar</em> because they partake of similarity. Plato makes a distinction between a universal and a particular.</p> <p>Forms are a manifestation of similarity. Humans are all similar. A quality of “human-ness”. Just and beautiful because they partake of justice and beauty.</p> <h2 id="what-is-an-idea">What is an idea?</h2> <p>Plato disagreed with Parmenides on this topic. Parmenides stated that an idea is not a thought, whereas Plato theorized that an idea may be an <em>object</em> of thought. Parmenides believed that there was only one idea. On the other hand, Plato devised that there are multiple ideas, and they are indivisible.</p> <p>Ideas are not known to us because human knowledge is not absolute - Parmenides. This statement again goes back to Skepticism. Plato said, “Ideas are universal and eternal and not subject to change”. One can see a horse, but one cannot see “horseness”. This inconceivable ontological concept is intelligible. The knowledge of particulars is given to us by the sense experiences, unlike the universals. Therefore, these ideas correspond to abstract concepts. They exist in an <em>independent realm</em>, are <em>non-temporal</em>, and participate in particulars.</p> <p>We say ideas are non-temporal because particulars are temporal/ have a finite existence.</p> <h2 id="epistemology-and-metaphysics">Epistemology and Metaphysics</h2> <p>Plato says <strong>the soul</strong> is the knowledge of <em>Forms</em>. It is identical to the concept of the mind today. A soul can contemplate on pure eternal ideas, and <em>knowledge is latent in the soul</em>.</p> <p>The idea of Unity and Diversity. The idea of Good is <strong>Logos</strong>.</p> <blockquote> <p>Logos - The principle of divine reason or cosmic order</p> </blockquote> <p>The meaning of Good cannot be grasped by the senses. Unity (Metaphysical form) of universals and Diversity (Ontological) of particulars. Ideas are only intelligible. They can be learned by dialectic methods.</p> <p>For a realist, ideas are an illusion. However, Plato believed ideas are real. Our perception through our senses is not the actual reality. A realist believes in sensory experiences. In contrast, reality may be different from what we perceive.</p> <p>Difference between a soul and an idea? The soul is a part of human life. The soul is eternal, according to Plato. We can relate to forms using our souls. A soul can relate itself to ideas.</p> <p>Plato did not believe in the knowledge obtained by senses. This is because he believed Heraclitus’ and Protagorus’ theory that “All things are in the process of becoming”. Hence, Plato believed that knowledge of sense experience is subject to change, and therefore unreliable.</p> <h1 id="lecture-12-230821">Lecture 12 <code class="language-plaintext highlighter-rouge">23/08/21</code></h1> <p><code class="language-plaintext highlighter-rouge">Watch the 3 videos uploaded for Plato</code></p> <p>Platonic Rationalism - a culmination of all the ideas from various philosophers. Plato questioned if truth can be subjectively justified. What is true knowledge? What knowledge has eternal significance? Forms carry the essence of things (“horseness”).</p> <p>Is there any progress in Philosophy? Our understanding of the notion of justice has considerably improved since Plato’s period. Humility is essential for gaining knowledge. Reality may not be what it appears!</p> <p>Plato believed that all knowledge is reminiscence, this reminiscence is called <strong>anamnesis</strong></p> <h1 id="lecture-13-240821">Lecture 13 <code class="language-plaintext highlighter-rouge">24/08/21</code></h1> <p>Forms are in an independent realm. Ideas, a metaphysical concept (?), are being related to epistemological concepts like true knowledge. Check out Plato’s cave of reality to better understand his “theory of ideas”.</p> <p>Plato’s ideology in a nutshell:</p> <p><img src="/assets/img/Philosophy/image-20210908143925564.png" alt="image-20210908143925564"/></p> <p>I give up writing notes for this lec.</p> <h1 id="recordings-3">Recordings 3</h1> <p>I didn’t write the notes for Socrates part. This is for Plato!</p> <h2 id="plato---theory-of-ideas-1">Plato - Theory of Ideas</h2> <p>Plato integrates epistemology, ontology and ethics. Plato believed that genuine knowledge is changeless, eternal and absolute (Parmenides). The concept of genuine knowledge and sense perception/ reality and appearance. Sophists believed Opinions and Knowledge are one and the same. Plato didn’t believe so.</p> <p>Plato tried to obtain genuine knowledge using dialectical methods. The Sophists’ concept of good varied from time to time. Plato wanted to point this out, and show that their thinking was inconsistent.</p> <p>Plato’s originality lies in the raising the issue of universals (“horseness”). Plato’s epistemology raises the issue of universals, and how these universals participate/ relate themselves to particulars. This is the intuition behind “Theory of ideas”. Ideas are not known to us, because human knowledge is not absolute according to Parmenides. However, Plato argued that ideas can be grasped using reason - Soul is akin to ideas.</p> <h2 id="platos-psychology-and-ethics">Plato’s Psychology and Ethics</h2> <p>Humans are considered as agents. Plato studies about what motivates these agents to perform actions. Which actions are good or bad? Kind of like the setting for Reinforcement learning. What is the nature of meaning of the good, and how can we justify such a life to reason?</p> <h2 id="composition-of-agency">Composition of Agency</h2> <p>An agent is composed of a living body and a soul. The living body decays with time. However, Plato believed soul is immortal. He thought of the body-soul; relation as the myth of a charioteer.</p> <p><strong>Reason, Spirit, and Appetite</strong> - Tripartite Relation</p> <ul> <li>The parts of the soul - Pure Reason - Rational part - Intellectual activities</li> <li>The soul enters into the body having moral and irrational part. The irrational part/ spirited part has nobler impulses - Anger, Ambition, Love - Faculty of decision</li> <li>Desire on the other hand represents lower appetites/ passion - Appetitive Faculty. Pleasure of senses and pleasure of reason.</li> </ul> <p>The neutral state between pleasure and pain with relation to preceding pain and earlier pleasure - <strong>psychological relativity</strong>.</p> <p>The body is an impediment of knowledge, from which the soul must free itself to order to behold truth in its purity. According to Plato, every agent must have <em>self-knowledge</em> for making good decisions. “All knowledge is reminiscence and all learning is a reawakening”.</p> <p>The ethical ideal according to Plato is about Justice and Happiness. The soul should not be driven by senses! “The more reasonable the desire, the more pleasurable its gratification”.</p> <p>Moral perfection lies in the power of contemplation and more importantly in the enduring power of truth. One must give importance to the rational element of the soul.</p> <h1 id="lecture-14-260821">Lecture 14 <code class="language-plaintext highlighter-rouge">26/08/21</code></h1> <p>Dialectic is a form of reasoning. It’s the method of connecting different ideas and generalizing stuff. We shall start discussing Aristotle now. Aristotle is the disciple of Plato and the mentor of Alexander the Great. He made significant contributions to science as a discipline.</p> <p>Additionally, he formalized the realm of deductive reasoning, a direct extension to Plato’s conceptual understanding. Also, <em>Causal explanation</em> is another outstanding contribution of Aristotle. Finally, Aristotle believed form and matter are inseparable.</p> <h2 id="aristotle-384---322-bc">Aristotle (384 - 322 BC)</h2> <p>He is known for Metaphysics, Syllogism, and Virtue Ethics. Aristotle made a distinction between metaphysics and science.</p> <h3 id="aristotles-problem">Aristotle’s Problem</h3> <p>Aristotle believed that the love of wisdom must be the sole aim of philosophers, but they must be consistent and scientific. There was a lack of scientific explanation in Plato’s Philosophy which Aristotle sought to address. He questioned Plato’s conception of forms/ideas and denied the division between ideas and material.</p> <p>Ethics is anti-hedonistic and intuitive. Aristotle considered Happiness and Pleasure are different, which is the opposite of what Plato thought.</p> <blockquote> <p>Hedonistic - Engaging in the pursuit of pleasure; sensually self-indulgent</p> </blockquote> <p>According to Aristotle, form and particulars are not different. Plato doesn’t talk about the progressive change of forms as he believed they were eternal. However, Aristotle tried to include change through the “Theory of Causation”.</p> <ul> <li>Forms are not apart from things but inherent in them; they are not transcendent but immanent.</li> <li>Form and matter are not separate but eternally together: matter combines with the form to constitute individual things</li> <li>The phenomenal world is not a mere imitation or copy/ shadow of the real world. Instead, it is the real world!</li> <li>Aristotle advocated realism and encouraged natural science.</li> </ul> <p>The above points summarise how different Aristotle was in comparison to Plato.</p> <h2 id="philosophy-and-the-sciences">Philosophy and the Sciences</h2> <p>He had an Organic and teleological viewpoint of the universe. Everything in the universe has a purpose, and that is how the world came into being.</p> <blockquote> <p>Teleological - relating to or involving the explanation of phenomena in terms of the purpose they serve rather than of the cause by which they arise.</p> </blockquote> <p>He believed experience is the basis of knowledge. Saying that, he thought reason and experience were not significantly different. Genuine knowledge is not mere acquaintance with facts but knowing their reasons and causes. Philosophy and science, in a broad sense, have a rational basis. Science studies the first cause of things.</p> <h2 id="metaphysics-1">Metaphysics</h2> <p>Study of all things as their beings. Analysis of first philosophy. Study of <em>qua being</em>.</p> <p><code class="language-plaintext highlighter-rouge">How is this different from Ontology?</code></p> <p>A thing is a thing by virtue of something that it is. That is, everything with a form has a function.</p> <h2 id="classification-of-sciences">Classification of Sciences</h2> <ul> <li>Logic - Method of inquiry employed in all other sciences</li> <li>Theoretical Sciences - Pure abstract knowledge - Maths, Physics, Biology</li> <li>Practical Sciences - Knowledge employing conduct - Ethics, Politics</li> <li>Productive Sciences - Knowledge is subordinated by the criterion of beauty; Aristotle’s <em>poetics</em> is an investigation in this sphere of aesthetics.</li> </ul> <h1 id="lecture-15-300821">Lecture 15 <code class="language-plaintext highlighter-rouge">30/08/21</code></h1> <blockquote> <p>qua being - A state of higher existence</p> </blockquote> <p>Aristotle continued…</p> <p>Plato thought ‘form’ and ‘matter’ existed at different levels. However, Aristotle says form and matter are inseparable.</p> <h2 id="potentiality-and-actuality-explanation-of-change">Potentiality and Actuality: Explanation of Change</h2> <p>Clay is the primordial stuff. A pot <em>potentially</em> exists in clay. How can a pot be <em>actualized</em>? - Efficient Cause</p> <p>The Form of Pot - The structure given to the object pot - Formal Cause.</p> <p>The final use of Pot - Final Cause.</p> <p>Every object that is created has a purpose of serving - a teleological view of reality. Present science represents things in more objectively and does not use the teleological view.</p> <h2 id="logic-1">Logic</h2> <p>Aristotle was dedicated to segregate Logic from the remaining branches. Logic is the method of reasoning for the acquisition of genuine knowledge.</p> <blockquote> <p>Truth - Something which cannot be other than it is - Necessity</p> <p>Contingent Statements - Not necessarily true</p> </blockquote> <p>Demonstration in the form of Deduction - <strong>Syllogism</strong>. It’s about a form of an argument consisting of Premised and Conclusions.</p> <h3 id="logic-as-a-form-of-thinking">Logic as a form of thinking</h3> <p>Form constitutes the essence of things. Human reasoning has the power of discerning the form. Thought and being/ existence coincide according to Aristotle. “Rational knowledge is implicit in the mind but experience is necessary to make reason aware of them.” - Thilly.</p> <h2 id="syllogism">Syllogism</h2> <p>It’s a form of reasoning to which all deduction is reducible.</p> <ul> <li>Terms</li> <li>Propositions. There are 4 types of propositions as classified by Aristotle <ul> <li>A - Universal Affirmative \(\forall\)</li> <li>E - Universal Negative \(\neg \forall\)</li> <li>I - Particular Affirmative \(\exists\)</li> <li>O - Particular Negative \(\neg \exists\)</li> </ul> </li> <li>Moods - The types of propositions used in the arguments/proof.</li> <li>Figures</li> <li>Rules <ul> <li>The fallacy of undistributed middle - The middle term must be distributed at least once in any of the premises.</li> <li>Illicit major/minor - If a term is distributed in the conclusion then it must be distributed in the premises.</li> <li>The fallacy of two exclusive premises - From two negative premises no conclusion follows.</li> <li>At least one of the premises must be universal.</li> <li>If there is any particular premise, then the conclusion must be particular.</li> <li>If one the premises is negative, then the conclusion will be negative.</li> </ul> </li> </ul> <h1 id="lecture-16-020921">Lecture 16 <code class="language-plaintext highlighter-rouge">02/09/21</code></h1> <p><code class="language-plaintext highlighter-rouge">Listen to Aristotle Recorded lectures</code></p> <h2 id="aristotles-psychology--ethics">Aristotle’s Psychology &amp; Ethics</h2> <p>We again talk about the tripartite relation of soul. Layers of soul - Rational, Sensible, and Nutritional. Body and soul are related as matter and form. Soul is the form of the body, it makes the body as organic whole having purposes as a unit. Soul moves the body and perceives sensible objects. It is characterized by <strong>sensation</strong>, <strong>feeling</strong>, and <strong>motive</strong>. It controls the lower and vital function.</p> <p>The nature Reason - Potentially powerful to carry out functions of thinking. Conceptual thought is actualised reason. Active reason can be identified with universal reason or the mind of the God. It does not arise in the course of soul’s development as do the other psychic functions. Active reason is the divine mind coming to the soul from without.</p> <p><em>Creative</em> Reason is the pure actuality; its concepts are actualised by it, the essences are directly cognized. Creative reason is absolutely immortal, imperishable, and not bound to the body.</p> <p>Aristotle thought desire gave birth to ideas. The layers of soul is not top-down but bottom-up! Reason is placed at the highest level. Aristotle did not identify mind with soul!!!!</p> <blockquote> <table> <tbody> <tr> <td>mind - soul</td> <td>Rationalism - Actual Reason</td> </tr> </tbody> </table> </blockquote> <h2 id="ethics---eudaimonia-highest-good">Ethics - Eudaimonia (Highest Good)</h2> <p>Aristotle’s metaphysics and psychology form the basis of his theory of ethics which is the first comprehensive scientific theory of morality. Aristotle possible aimed answering the Socratic question of Highest Good. Ethics, a study of morality, talks about means and ends. Gather good karma to achieve the highest good.</p> <p>Is pleasure good? Aristotle says pleasure and good are different.</p> <h2 id="virtue">Virtue</h2> <p>Intellectual and Moral Virtue. Every individual is not born as a rational person. As a person transpires in the society, he acquires rational thoughts. Having reason is not enough to talk about morality. You can be a rational person, but need not be moral! Therefore, Aristotle talks about 2 different types of virtue.</p> <ul> <li>Intellectual Virtue - Virtue of Wisdom, quest for knowledge, perfection in thinking conceptually and coherently - and insight</li> <li>Phronesis - Practical virtue</li> <li>Moral virtue - Perfect action of emotional - Impulsive function consists of assuring rational attitude. Temperance, courage, liberality.</li> </ul> <p>Virtue is a kind of moderation in that it aims at the mean - the mean between excess and deficiencies. Courage is a mean between foolhardiness and cowardice. Liberality is a mean between extravagance and avarice. Modesty is a mean between bashfulness and shamelessness. Wow that was tough.</p> <p>Moral judgement is not a subjective opinion o arbitrary choice. Moral conduct must be decided by the right kind of man. Virtuous men are the standard and measure of things. They exhibit moral character and stable conduct.</p> <blockquote> <p>Virtue is a disposition. A person is not born with virtuousness, but acquires it.</p> </blockquote> <h2 id="justice">Justice</h2> <p>Justice is a virtue implying a relation to others, for it promotes the interest of another.</p> <h1 id="recordings-4">Recordings 4</h1> <h2 id="aristotle---first">Aristotle - First</h2> <p>Soul is form of body</p> <p>Material is same as form</p> <p>Mind is different from body</p> <p>Mind is immortal</p> <p>Mind has the power to think</p> <p>Nature of reason</p> <p>Thinking deals with concepts</p> <p>Thought is actualised reason</p> <p>Active reason and creative reason</p> <p>Rational actions are backed by thoughts</p> <p>Souls drives the reason, mind does the thinking, body applies the thought</p> <p>Active reason should correspond to universal reason</p> <p>Active reason is divine mind coming to the soul</p> <p>Creative reason is pure actuality</p> <h2 id="aristotle---second">Aristotle - Second</h2> <p>Highest good derived from Socrates</p> <p>Scientific theory of morality</p> <p>Hedomania happiness</p> <p>Notion of ethics in metaphysics</p> <p>An action is causally explainable</p> <p>Good action is virtuous - ethics life</p> <p>Action is the means to achieve a higher end - Supreme purpose</p> <p>What is highest good? Lies in the very nature of things. Human beings are necessarily to perform good actions just as knife is necessarily to cut things</p> <p>Function of human being is rational thinking and reason</p> <p>Pleasure? It’s a secondary consequence. Not identical to happiness</p> <p>Intellectual and Moral virtues. Intellectual for finding true knowledge. Performing action knowingly.</p> <p>Self knowledge for deciding virtuous actions</p> <p>Temperance, and 3 other things for moral virtue</p> <p>Mean of extremes</p> <p>Mean varies with individuals</p> <p>Virtuous man exhibits moral character</p> <p>Virtue is a disposition too - inherent in things</p> <h2 id="aristotle---third">Aristotle - Third</h2> <p>Self realisation</p> <p>True friend is alter ego</p> <p>Justice? Relation to others. Laws are inclusive.</p> <p>Life of contemplation is worth living</p> <h1 id="lecture-19">Lecture 19</h1> <p>We will study Rene Descartes now.</p> <h2 id="rene-descartes-1596---1650">Rene Descartes (1596 - 1650)</h2> <p>He was a mathematician, and also made great contributions to philosophy using Mathematics as a method. During this time, the philosophers came back to the original metaphysical question, “What constitutes reality?”. They also gave importance to the epistemological question, “What is the ultimate knowledge?”. The modern philosophers, having made great advances in science, sought to find a scientific explanation for the above questions.</p> <p>Rene Descartes primarily focused on Epistemology. Descartes was a rationalist philosopher, and therefore he could not ignore metaphysics. He believed that the knowledge about the world is connected to knowledge about the self. Descartes also tried to establish the foundation for theological arguments.</p> <h2 id="descartes-metaphysics--classification-of-sciences">Descartes’ Metaphysics / Classification of Sciences</h2> <p>Metaphysics contains the principle of knowledge concerning the principal attributes of God, the immateriality of soul, and all the clear and simple ideas that we possess. Physics, on the other hand, is the principle of material things, we examine in general, how the universe is framed/ particular nature of earth, and all the bodies commonly found.</p> <p>Philosophy as a whole is like a tree whose roots are metaphysics, trunk is physics, and branches are all the other sciences. These main sciences can be reduced to medicine, mechanics, and morals.</p> <h2 id="method-and-criterion-of-knowledge">Method and Criterion of Knowledge</h2> <p>Knowledge cannot be built on history but also requires the intervention of method. His method is famously known as <strong>Descartes’ Methodological Skepticism</strong>. Philosophy discovers the truth in terms of mathematics.</p> <h2 id="methodological-skepticism">Methodological Skepticism</h2> <p>Descartes used both deduction and induction. He believed both the methods are equally important to reach conclusions. Senses are sometimes deceiving. Sometimes, the mathematical demonstrations can also be doubted. Memory can be deceptive and so its presentations.</p> <h2 id="doubt-implies-a-doubter">Doubt implies a Doubter</h2> <p>What kind of activity is doubting?</p> <blockquote> <p>Thinking - res cogitans - The mind is a thinking thing</p> <p>I think = Cogito - Most self-evident proposition</p> </blockquote> <p>I doubt, therefore, I think, therefore, I am,</p> <blockquote> <p>Dubito ergo cogito ergo sum</p> </blockquote>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[An introductory philosophical course discussing the transitions in philosophical thought from Greek philosophy to modern philosophy. The notes cover topics such as classical philosophy, Socratic period, Indian philosophy, and modern philosophy.]]></summary></entry><entry><title type="html">Machine Learning Cheatsheet</title><link href="https://sudhansh6.github.io/blog/Machine-Learning-Cheatsheet/" rel="alternate" type="text/html" title="Machine Learning Cheatsheet"/><published>2021-08-05T00:00:00+00:00</published><updated>2021-08-05T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Machine-Learning-Cheatsheet</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Machine-Learning-Cheatsheet/"><![CDATA[<h1 id="miscellaneous">Miscellaneous</h1> <p>Supervised Machine Learning deals with the above 2 problems. Classification usually has <strong>discrete</strong> outputs while Regressions has <strong>continuous</strong> outputs.</p> <h2 id="data-preprocessing">Data preprocessing</h2> <p>Reading a csv using <code class="language-plaintext highlighter-rouge">pandas</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_scv</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
<span class="c1"># Creating dummy variables for Categorical columns
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">get_dummies</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">cat_cols</span><span class="p">)</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="c1">#Encoding bool_cols
</span><span class="n">le</span> <span class="o">=</span> <span class="nc">LabelEncoder</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">bool_cols</span> <span class="p">:</span>
    <span class="n">df</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">le</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</code></pre></div></div> <p>The preprocessing for numerical columns involves <strong>scaling</strong> them so that a change in one quantity is equal to another. The machines need to understand that just because some columns like ‘totalcharges’ have large values, it doesn’t mean that it plays a big part in predicting the outcome. To achieve this, we put all of our numerical columns into a same <strong>scale</strong> so that none of them are dominated by the other.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="c1">#Scaling Numerical columns
</span><span class="n">std</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">scaled</span> <span class="o">=</span> <span class="n">std</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">num_cols</span><span class="p">])</span>
<span class="n">scaled</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">scaled</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">num_cols</span><span class="p">)</span><span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="n">num_cols</span><span class="p">,</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">merge</span><span class="p">(</span><span class="n">scaled</span><span class="p">,</span><span class="n">left_index</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">right_index</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">how</span> <span class="o">=</span> <span class="sh">"</span><span class="s">left</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="evaluation">Evaluation</h2> <p>For classification, the few common metrics we use to evaluate models are</p> <ul> <li>accuracy</li> <li>precision</li> <li>recall</li> <li>f1-score</li> <li>ROC curve, AUC</li> </ul> <p><img src="/assets/img/ML/image-20210805235203493.png" alt="image-20210805235203493"/></p> <p>Precision and recall are defined as \(\text{Precision} = \frac{tp}{tp + fp} \\ \text Recall = \frac{tp}{tp + fn}\) Precision is defined as the percentage of your results which are relevant, Recall is defined as the percentage of relevant results correctly classified.</p> <h3 id="f1-score">F1 Score</h3> <p>In general cases, there is a metric which harmonizes the precision and recall metric, which is the F1 Score. \(F = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}\)</p> <h3 id="roc-curve-and-auc">ROC curve and AUC</h3> <p>We can tweak the threshold of our models to improve the preferable metric. To visualise this change, we plot the effect of each threshold on the false and true positive rates. The curve looks like:</p> <p><img src="/assets/img/ML/image-20210805235743911.png" alt="image-20210805235743911" style="zoom:200%;"/></p> <h3 id="ssim">SSIM</h3> <p>Measures the similarity of images based on Luminance, contrast and Structural relations. Between 1 and -1. Often adjusted to [0, 1].</p> <ul> <li> <p>Luminance is averaged across all pixels \(\mu_x = 1/N\sum_{i = 1}^N x_i \\ \text{Comparison is done using}\\ l(x, y) = \frac{2\mu_x\mu_y + C_1}{\mu_x^2 \mu_y^2 + C_1}\)</p> </li> <li> <p>Contrast is measured by taking the standard deviation of all pixel values. \(\sigma_x = \left(\frac{1}{N - 1}\sum_{i = 1}^N(x_1 - \mu_x)^2\right)^0.5 \\ \text{Comparison is done using}\\ l(x, y) = \frac{2\mu_x\mu_y + C_2}{\mu_x^2 \mu_y^2 + C_2}\)</p> </li> <li> <p>Structural comparison is done by using a consolidated formula which divides the <em>normalised</em> input signal with its <em>standard deviation</em> so that the result has unit standard deviation which allows for a more robust comparison. \(s(x, y) = \frac{\sigma_{xy} + C_3}{\sigma_x\sigma_y + C_3}\)</p> </li> </ul> <p>All of the above scores are multiplied.</p> <h1 id="regression">Regression</h1> <h2 id="linear-regression">Linear regression</h2> <h1 id="latent-semantic-analysis">Latent Semantic Analysis</h1> <p>LSA is a one of the most popular Natural Language Processing (NLP) techniques for trying to determine themes within text mathematically. It is an <strong>unsupervised</strong> learning technique that has two fundamental ideas</p> <ol> <li>the distributional hypothesis, which states that words with similar meanings appear frequently together.</li> <li>Singular value Decomposition (SVD)</li> </ol> <p>In simple terms: LSA takes meaningful text documents and recreates them in <em>n</em> different parts where each part expresses a different way of looking at meaning in the text. If you imagine the text data as a an idea, there would be <em>n</em> different ways of <em>looking</em> at that idea, or <em>n</em> different ways of <em>conceptualising</em> the whole text. LSA reduces our table of data to a table of latent (hidden<em>)</em> concepts.</p> <p>Suppose that we have some table of data, in this case text data, where each row is one document, and each column represents a term (which can be a word or a group of words, like “baker’s dozen” or “Downing Street”). This is the standard way to represent text data.</p> <p>The product we’ll be getting out is the document-topic table (<em>U</em>) <em>times</em> the singular values (𝚺). This can be interpreted as the documents (all our news articles) along with how much they belong to each topic then <strong>weighted</strong> by the relative importance of each topic.</p> <p>SVD is also used in model-based recommendation systems. It is very similar to Principal Component Analysis (PCA), but it operates better on sparse data than PCA does (and text data is almost always sparse). Whereas PCA performs decomposition on the <em>correlation</em> matrix of a dataset, SVD/LSA performs decomposition directly on the dataset as it is.</p> <h2 id="tokenising-and-vectorising-text-data">Tokenising and vectorising text data</h2> <p>Our models work on numbers, not string! So we tokenise the text (turning all documents into smaller observational entities — in this case words) and then turn them into numbers using Sklearn’s TF-IDF vectoriser.</p> <hr/> <p>The above part was not organised (clearly). I will try to summarise points in an organised way from here.</p> <h1 id="data-preprocessing-1">Data preprocessing</h1> <p>Turns out, this is a really important topic for Machine Learning in general. A real-world data generally contains noises, missing values, and maybe in an unusable format which cannot be directly used for machine learning models. Data preprocessing is required tasks for cleaning the data and making it suitable for a machine learning model which also increases the accuracy and efficiency of a machine learning model.</p> <ol> <li>Get the dataset</li> </ol>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[A small overview of major concepts in Machine Learning and Data Science]]></summary></entry><entry><title type="html">Programming Cheatsheet</title><link href="https://sudhansh6.github.io/blog/programming/" rel="alternate" type="text/html" title="Programming Cheatsheet"/><published>2021-07-22T00:00:00+00:00</published><updated>2021-07-22T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/programming</id><content type="html" xml:base="https://sudhansh6.github.io/blog/programming/"><![CDATA[<h1 id="data-structures">Data Structures</h1> <h2 id="arrays">Arrays</h2> <h3 id="vector-syntax">Vector syntax</h3> <ul> <li><code class="language-plaintext highlighter-rouge">vector&lt;type&gt;</code> stores elements of the type <code class="language-plaintext highlighter-rouge">type</code>. They are indexed by <code class="language-plaintext highlighter-rouge">ints</code>.</li> <li>Initialise a vector with <code class="language-plaintext highlighter-rouge">n</code> elements with all equal to <code class="language-plaintext highlighter-rouge">m</code> using <code class="language-plaintext highlighter-rouge">vector&lt;type&gt; vec(n, m)</code>.</li> <li>Copy a part of another vector in a new vector using <code class="language-plaintext highlighter-rouge">vector&lt;type&gt; cpy(m.begin(), m.end())</code>.</li> <li><code class="language-plaintext highlighter-rouge">vector&lt;type&gt;::iterator</code> for an iterator through a vector. The special thing about iterators is that they provide the glue between <a href="https://stackoverflow.com/a/11948413/819272">algorithms and containers</a>. You can use <code class="language-plaintext highlighter-rouge">int</code> for indexing contiguous data structures such as <code class="language-plaintext highlighter-rouge">vector</code></li> <li><strong>Sort</strong> a vector using <code class="language-plaintext highlighter-rouge">sort(v.begin(), v.end(), [](auto a, auto b){return a &lt; b;})</code>.</li> <li><strong>Rotate</strong> a vector by <code class="language-plaintext highlighter-rouge">k</code> indices to the <em>right</em> using <code class="language-plaintext highlighter-rouge">::rotate(nums.begin(), nums.end() - k%size, nums.end());</code>.</li> <li><strong>Reverse</strong> a vector using <code class="language-plaintext highlighter-rouge">reverse(v.begin(), v.end())</code>.</li> <li><strong>Length</strong> of a vector is given by <code class="language-plaintext highlighter-rouge">v.size()</code> and not <code class="language-plaintext highlighter-rouge">v.length()</code>.</li> <li>Finding max sub array and max sub sequence</li> </ul> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">maxSubarray</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">arr</span><span class="p">)</span> <span class="p">{</span>
<span class="kt">int</span> <span class="n">prev</span> <span class="o">=</span> <span class="n">INT_MIN</span><span class="p">,</span> <span class="n">sarr</span> <span class="o">=</span> <span class="n">INT_MIN</span><span class="p">,</span> <span class="n">sseq</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">INT_MIN</span><span class="p">;</span>
<span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">arr</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="o">--</span><span class="n">i</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">prev</span> <span class="o">=</span> <span class="n">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">prev</span><span class="p">)</span> <span class="o">+</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="n">sarr</span> <span class="o">=</span> <span class="n">max</span><span class="p">(</span><span class="n">sarr</span><span class="p">,</span> <span class="n">prev</span><span class="p">);</span>
    <span class="k">if</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">)</span> <span class="n">sseq</span> <span class="o">+=</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">max</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="p">}</span>
<span class="k">if</span> <span class="p">(</span><span class="n">sseq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="n">sseq</span> <span class="o">=</span> <span class="n">M</span><span class="p">;</span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">res</span> <span class="o">=</span> <span class="p">{</span><span class="n">sarr</span><span class="p">,</span> <span class="n">sseq</span><span class="p">};</span>
<span class="k">return</span> <span class="n">res</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <ul> <li>Visit <a href="https://www.geeksforgeeks.org/accumulate-and-partial_sum-in-c-stl-numeric-header/">here</a> for finding sum and partial sums of vector.</li> </ul> <h2 id="strings">Strings</h2> <p>First, all the syntax from <code class="language-plaintext highlighter-rouge">&lt;string&gt;</code> library.</p> <ul> <li><strong>Length</strong> of a string is given by <code class="language-plaintext highlighter-rouge">str.length()</code>.</li> <li><strong>Reverse</strong> a string using <code class="language-plaintext highlighter-rouge">reverse(str)</code> . If you want to store the reversed string elsewhere, use <em>reverse iterators</em> and do the following <code class="language-plaintext highlighter-rouge">string res = string(str.rbegin(), str.rend())</code>. Yes! strings have iterators.</li> <li><strong>Transform</strong> a string using <code class="language-plaintext highlighter-rouge">transform(in.begin(), in.end(), out.begin(), ::tolower)</code>. <code class="language-plaintext highlighter-rouge">in</code> can be <code class="language-plaintext highlighter-rouge">out</code>.</li> <li><strong>Filter</strong> alphabets from a string/<strong>iterable</strong> using <code class="language-plaintext highlighter-rouge">remove_if(vec2.begin(), vec2.end(), [](auto c){return !::isalpha(c);})</code>. <strong>The above does not work perfectly</strong>. Use <code class="language-plaintext highlighter-rouge">s.erase(::remove_if(s.begin(), s.end(), [](auto c){return !::isalpha(c);}), s.end());</code>.</li> <li><code class="language-plaintext highlighter-rouge">::tolower</code> affects only alphabets in a string.</li> <li><strong>Compare</strong> characters with <strong>single quotes</strong>.</li> <li>Refer to this <a href="https://www.javatpoint.com/cpp-strings">link</a> for all string functions in C++. <a href="https://fresh2refresh.com/c-programming/c-strings/c-strstr-function/">Here</a> is another list of useful string functions.</li> <li>String to number using <code class="language-plaintext highlighter-rouge">atoi()</code> and <code class="language-plaintext highlighter-rouge">stoi()</code>. Number to string conversion using <code class="language-plaintext highlighter-rouge">to_string()</code>.</li> <li>Use <code class="language-plaintext highlighter-rouge">push_back</code> <strong>or</strong> <code class="language-plaintext highlighter-rouge">+</code> to add new chars at the end. Use <code class="language-plaintext highlighter-rouge">a + b</code> to <strong>concatenate</strong> strings.</li> <li>Use <code class="language-plaintext highlighter-rouge">size_t</code> for storing lengths of strings.</li> </ul> <h3 id="theory">Theory</h3> <ul> <li>Storing strings in <code class="language-plaintext highlighter-rouge">char</code> arrays. Allocate space equal to <strong>one more</strong> than the length of string of array. The last character is for the <strong>end</strong> character: <code class="language-plaintext highlighter-rouge">\0</code>. Strings in C must end with the null character. Suppose you try to print a string without the end character, the program will print characters until it encounters the null character. Functions like <code class="language-plaintext highlighter-rouge">strlen</code> and <code class="language-plaintext highlighter-rouge">printf("%s", char_array)</code> depend on the end character.</li> <li>You can also initialise a string using ‘double quotes’. For example, <code class="language-plaintext highlighter-rouge">char arr[100] = "Null character is implictly placed"</code>. You can also <em>avoid writing the size</em>. Although, when you initialise the string with comma separated characters, you must mention the size. Also, you must explicitly mention the end character.</li> <li>Arrays and pointers are different types used in a similar manner. Let <code class="language-plaintext highlighter-rouge">p1</code> be an array and <code class="language-plaintext highlighter-rouge">p2</code> be a pointer. <strong><code class="language-plaintext highlighter-rouge">p2 = p1</code></strong> is valid but <strong><code class="language-plaintext highlighter-rouge">p1 = p2; ++p1</code> are invalid</strong>.</li> <li>Arrays are always passed to a function <strong>by reference</strong>.</li> <li>Memory of an application is classified into :</li> </ul> <table> <thead> <tr> <th style="text-align: center">Heap</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><strong>Stack</strong></td> </tr> <tr> <td style="text-align: center"><strong>Static/Global</strong></td> </tr> <tr> <td style="text-align: center"><strong>Code</strong> (Text)</td> </tr> </tbody> </table> <ul> <li> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">char</span> <span class="n">C</span><span class="p">[</span><span class="mi">20</span><span class="p">]</span>  <span class="o">=</span> <span class="s">"Hello"</span><span class="p">;</span> <span class="c1">//String gets stored in the space of the array. Can be edited.</span>
<span class="kt">char</span> <span class="o">*</span><span class="n">C2</span> <span class="o">=</span> <span class="s">"Hello"</span><span class="p">;</span> <span class="c1">//String gets stored as a constant during compile time.</span>
<span class="c1">// C2[0] = 'A'; would be invalid</span>
</code></pre></div> </div> </li> <li>C++ provides <code class="language-plaintext highlighter-rouge">string</code> inbuilt datatype.</li> </ul> <h2 id="linked-lists">Linked Lists</h2> <ul> <li>You can’t delete a node by just using <code class="language-plaintext highlighter-rouge">delete node</code>. You should do <code class="language-plaintext highlighter-rouge">prev -&gt; next = node -&gt; next</code> and then <code class="language-plaintext highlighter-rouge">delete node</code>. You should do this even if <code class="language-plaintext highlighter-rouge">node -&gt; next = NULL</code>.</li> <li><strong>Reverse</strong> a linked list in <code class="language-plaintext highlighter-rouge">O(n)</code> using a sliding window mechanism.</li> </ul> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span><span class="p">(</span><span class="n">head</span> <span class="o">==</span> <span class="nb">NULL</span> <span class="o">||</span> <span class="n">head</span> <span class="o">-&gt;</span> <span class="n">next</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="k">return</span> <span class="n">head</span><span class="p">;</span>
<span class="n">ListNode</span> <span class="o">*</span><span class="n">prev</span><span class="p">,</span> <span class="o">*</span><span class="n">curr</span><span class="p">,</span> <span class="o">*</span><span class="n">next</span><span class="p">;</span>
<span class="n">prev</span> <span class="o">=</span> <span class="n">head</span><span class="p">;</span>
<span class="n">curr</span> <span class="o">=</span> <span class="n">head</span> <span class="o">-&gt;</span> <span class="n">next</span><span class="p">;</span>
<span class="n">head</span> <span class="o">-&gt;</span> <span class="n">next</span> <span class="o">=</span> <span class="nb">NULL</span><span class="p">;</span>
<span class="k">while</span><span class="p">(</span><span class="n">curr</span> <span class="o">-&gt;</span> <span class="n">next</span> <span class="o">!=</span> <span class="nb">NULL</span><span class="p">)</span>
<span class="p">{</span>
    <span class="n">next</span> <span class="o">=</span> <span class="n">curr</span> <span class="o">-&gt;</span> <span class="n">next</span><span class="p">;</span>
    <span class="n">curr</span> <span class="o">-&gt;</span> <span class="n">next</span> <span class="o">=</span> <span class="n">prev</span><span class="p">;</span>
    <span class="n">prev</span> <span class="o">=</span> <span class="n">curr</span><span class="p">;</span>
    <span class="n">curr</span> <span class="o">=</span> <span class="n">next</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">curr</span> <span class="o">-&gt;</span> <span class="n">next</span> <span class="o">=</span> <span class="n">prev</span><span class="p">;</span>
<span class="k">return</span> <span class="n">curr</span><span class="p">;</span>
</code></pre></div></div> <ul> <li>Check if a <strong>cycle</strong> exists in a linked list using the <strong>Hare and Tortoise</strong> algorithm.</li> <li>Check if a list is a <strong>palindrome</strong> in <code class="language-plaintext highlighter-rouge">O(n)</code> time and <code class="language-plaintext highlighter-rouge">O(1)</code> storage using the above two algorithms.</li> <li>In doubly linked lists, make sure you change both <code class="language-plaintext highlighter-rouge">prev</code> and <code class="language-plaintext highlighter-rouge">next</code> of the previous and following nodes.</li> </ul> <h2 id="queues-and-stacks">Queues and Stacks</h2> <table> <thead> <tr> <th>Operations</th> <th>Queues</th> <th>Stacks</th> </tr> </thead> <tbody> <tr> <td>Adding elements</td> <td><code class="language-plaintext highlighter-rouge">queue.push()</code></td> <td><code class="language-plaintext highlighter-rouge">stack.push_back()</code></td> </tr> <tr> <td>First element</td> <td><code class="language-plaintext highlighter-rouge">queue.front()</code></td> <td><code class="language-plaintext highlighter-rouge">stack.top()</code></td> </tr> </tbody> </table> <ul> <li>Stacks and queues can be implemented via arrays and linked lists.</li> </ul> <h2 id="trees">Trees</h2> <ul> <li>When you write recursive algorithms, make sure you write the base cases. Your base case can include <code class="language-plaintext highlighter-rouge">NULL</code> too! Don’t write <code class="language-plaintext highlighter-rouge">left == NULL</code>, <code class="language-plaintext highlighter-rouge">right == NULL</code> etc separately. Let me show what I mean. Consider the problem of validating a BST. Initially, the code I wrote in python was this.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">checker</span><span class="p">(</span><span class="n">root</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">root</span><span class="p">.</span><span class="n">left</span> <span class="o">==</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">root</span><span class="p">.</span><span class="n">right</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="n">root</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">root</span><span class="p">.</span><span class="n">data</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">root</span><span class="p">.</span><span class="n">right</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">left</span> <span class="o">=</span> <span class="n">root</span><span class="p">.</span><span class="n">left</span>
        <span class="n">c_left</span> <span class="o">=</span> <span class="nf">checker</span><span class="p">(</span><span class="n">left</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">c_left</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">root</span><span class="p">.</span><span class="n">data</span> <span class="ow">and</span> <span class="n">c_left</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nf">min</span><span class="p">(</span><span class="n">c_left</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">root</span><span class="p">.</span><span class="n">data</span><span class="p">),</span> <span class="nf">max</span><span class="p">(</span><span class="n">c_left</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">root</span><span class="p">.</span><span class="n">data</span><span class="p">)]</span> 
    <span class="k">if</span> <span class="n">root</span><span class="p">.</span><span class="n">left</span> <span class="o">==</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">right</span> <span class="o">=</span> <span class="n">root</span><span class="p">.</span><span class="n">right</span>
        <span class="n">c_right</span> <span class="o">=</span> <span class="nf">checker</span><span class="p">(</span><span class="n">right</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">c_right</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">root</span><span class="p">.</span><span class="n">data</span> <span class="ow">and</span> <span class="n">c_right</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nf">min</span><span class="p">(</span><span class="n">c_right</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">root</span><span class="p">.</span><span class="n">data</span><span class="p">),</span> <span class="nf">max</span><span class="p">(</span><span class="n">c_right</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">root</span><span class="p">.</span><span class="n">data</span><span class="p">)]</span>
    <span class="n">left</span> <span class="o">=</span> <span class="n">root</span><span class="p">.</span><span class="n">left</span>
    <span class="n">right</span> <span class="o">=</span> <span class="n">root</span><span class="p">.</span><span class="n">right</span>
    <span class="n">c_left</span> <span class="o">=</span> <span class="nf">checker</span><span class="p">(</span><span class="n">left</span><span class="p">)</span>
    <span class="n">c_right</span> <span class="o">=</span> <span class="nf">checker</span><span class="p">(</span><span class="n">right</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">c_left</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c_right</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">root</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
    <span class="n">M</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">c_left</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">c_right</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">root</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">c_left</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">root</span><span class="p">.</span><span class="n">data</span> <span class="ow">and</span> <span class="n">c_left</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">and</span> <span class="n">c_right</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">root</span><span class="p">.</span><span class="n">data</span> <span class="ow">and</span> <span class="n">c_right</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">m</span><span class="p">,</span> <span class="n">M</span><span class="p">]</span> 
            
<span class="k">def</span> <span class="nf">checkBST</span><span class="p">(</span><span class="n">root</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">checker</span><span class="p">(</span><span class="n">root</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div> <p>This is really ugly and redundant. Here is an equivalent solution in C++.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">bool</span> <span class="nf">validate</span><span class="p">(</span><span class="n">TreeNode</span><span class="o">*</span> <span class="n">root</span><span class="p">,</span> <span class="kt">long</span> <span class="n">m</span><span class="p">,</span> <span class="kt">long</span> <span class="n">M</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">if</span><span class="p">(</span><span class="n">root</span> <span class="o">==</span> <span class="nb">NULL</span><span class="p">)</span> <span class="k">return</span> <span class="nb">true</span><span class="p">;</span>
        
        <span class="k">if</span><span class="p">(</span><span class="n">root</span> <span class="o">-&gt;</span> <span class="n">val</span> <span class="o">&gt;=</span> <span class="n">M</span> <span class="o">||</span> <span class="n">root</span> <span class="o">-&gt;</span> <span class="n">val</span> <span class="o">&lt;=</span> <span class="n">m</span><span class="p">)</span> <span class="k">return</span> <span class="nb">false</span><span class="p">;</span>
        <span class="k">return</span> <span class="n">validate</span><span class="p">(</span><span class="n">root</span> <span class="o">-&gt;</span> <span class="n">left</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">min</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="p">(</span><span class="kt">long</span><span class="p">)</span><span class="n">root</span> <span class="o">-&gt;</span> <span class="n">val</span><span class="p">))</span> <span class="o">&amp;&amp;</span> <span class="n">validate</span><span class="p">(</span><span class="n">root</span> <span class="o">-&gt;</span> <span class="n">right</span><span class="p">,</span> <span class="n">max</span><span class="p">((</span><span class="kt">long</span><span class="p">)</span><span class="n">root</span> <span class="o">-&gt;</span> <span class="n">val</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="n">M</span><span class="p">);</span>
    <span class="p">}</span>
    
    <span class="kt">bool</span> <span class="nf">isValidBST</span><span class="p">(</span><span class="n">TreeNode</span><span class="o">*</span> <span class="n">root</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">return</span> <span class="n">validate</span><span class="p">(</span><span class="n">root</span><span class="p">,</span> <span class="p">(</span><span class="kt">long</span><span class="p">)</span><span class="n">INT_MIN</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="kt">long</span><span class="p">)</span><span class="n">INT_MAX</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>
    <span class="p">}</span>
</code></pre></div></div> <p><strong>BSTs</strong> <strong>don’t have</strong> <strong>duplicate values</strong>. Using a Balanced Search Tree (<strong>BST</strong>), we can do the following:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	1. Insert in O(log n)
        2. Delete in O(log n)
        3. Search for an element in O(log n)
        4. Find Min in O(log n)
        5. Find Max in O(log n)
        6. Get all the elements in sorted order in O(n) - Inorder traversal.
        7. Find an element closest in value to x O(log n)
</code></pre></div></div> <p><a href="#hashing">Hashmaps</a> are also a great way to store elements but the following operations cannot be done efficiently in hash tables:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        1. the min / max query in reasonable time
        2. Iterating through the element in sorted order in linear time
        3. Find an element closes to x in logarithmic time.
</code></pre></div></div> <p>Check out Segment Trees <a href="https://www.geeksforgeeks.org/segment-tree-set-1-range-minimum-query/">here</a>.</p> <h2 id="heaps-and-maps">Heaps and Maps</h2> <p><strong>Treemaps</strong> are implemented internally using balanced trees ( They mostly use red black trees). Take a look at <a href="https://stackoverflow.com/questions/6147242/heap-vs-binary-search-tree-bst">this</a> answer for comparison of Heaps and BSTs (Maps).</p> <h3 id="implementation-details">Implementation Details</h3> <p><strong>C++</strong>. <code class="language-plaintext highlighter-rouge">map</code> and <code class="language-plaintext highlighter-rouge">set</code> from the STL library are implemented using balanced red-black trees. Maps are sorted via keys.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Declaration */</span>
<span class="n">map</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">A</span><span class="p">;</span> <span class="c1">// O(1) declaration which declares an empty tree map.</span>
<span class="cm">/* Inserting a key */</span>
<span class="n">A</span><span class="p">[</span><span class="n">K</span><span class="p">]</span> <span class="o">=</span> <span class="n">V</span><span class="p">;</span> <span class="c1">// O(log n). Note that we expect key K to be unique here. If you have keys that will repeat, take a look at multimaps.</span>
<span class="cm">/* Delete a key */</span>
<span class="n">A</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">K</span><span class="p">);</span> <span class="c1">// O(log n)</span>
<span class="cm">/* Find a key */</span>
<span class="n">A</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">K</span><span class="p">)</span> <span class="o">!=</span> <span class="n">A</span><span class="p">.</span><span class="n">end</span><span class="p">()</span>  <span class="c1">// O(log n)</span>
<span class="cm">/* Find minimum key K in the map */</span>
<span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">begin</span><span class="p">())</span><span class="o">-&gt;</span><span class="n">first</span>     <span class="c1">// O(1)</span>
<span class="cm">/* Find maximum key K in the map */</span>
<span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">rbegin</span><span class="p">())</span><span class="o">-&gt;</span><span class="n">first</span>     <span class="c1">// O(1)</span>
<span class="cm">/* Find closest key K &gt; x */</span>
<span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">upper_bound</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">-&gt;</span><span class="n">first</span>     <span class="c1">// O(log n). Do need to handle the case when x is more than or equal to the max key in the map. </span>
<span class="cm">/* Find closest key K &gt;= x */</span> <span class="c1">// Use lower_bound</span>
<span class="cm">/* Iterate over the keys in sorted order */</span>
<span class="k">for</span> <span class="p">(</span><span class="n">map</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="o">&gt;::</span><span class="n">iterator</span> <span class="n">it</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">begin</span><span class="p">();</span> <span class="n">it</span> <span class="o">!=</span> <span class="n">A</span><span class="p">.</span><span class="n">end</span><span class="p">();</span> <span class="o">++</span><span class="n">it</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// it-&gt;first has the key, it-&gt;second has the value. </span>
    <span class="p">}</span>
</code></pre></div></div> <p><strong>Python</strong> - Python does not have treemap. The closest implementation is <code class="language-plaintext highlighter-rouge">heapq</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">A</span> <span class="o">=</span> <span class="p">[];</span> <span class="c1"># declares an empty list / heap. O(1)
</span>            <span class="c1"># Note that heaps are internally implemented using lists for which heap[k] &lt;= heap[2*k+1] and heap[k] &lt;= heap[2*k+2] for all k. 
</span><span class="n">heapq</span><span class="p">.</span><span class="nf">heappush</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">));</span>     <span class="c1"># O(log n)
</span><span class="n">heapq</span><span class="p">.</span><span class="nf">heappop</span><span class="p">(</span><span class="n">A</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Delete the 'smallest' key. Deleting random key is inefficient.
</span><span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># minimum key
</span></code></pre></div></div> <h3 id="heapsort">Heapsort</h3> <p>Heap sort can be understood as the improved version of the binary search tree. It does not create a node as in case of binary search tree instead it builds the heap by adjusting the position of elements within the array itself.</p> <p>In which method a tree structure called heap is used where a heap is a type of binary tree. An ordered balanced binary tree is called a <strong>Min-heap</strong>, where the value at the root of any subtree is less than or equal to the value of either of its children.</p> <p>An ordered balanced binary tree is called a <strong>max heap</strong> where the value at the root of any subtree is more than or equal to the value of either of its children.</p> <p><strong>A heap is a tree data structure that satisfies the following properties:</strong></p> <ol> <li><strong>Shape property</strong>: Heap is always a complete binary tree which means that all the levels of a tree are fully filled. There should not be a node which has only one child. Every node except leaves should have two children then only a heap is called as a complete binary tree.</li> <li><strong>Heap property</strong>: All nodes are either greater than or equal to or less than or equal to each of its children. This means if the parent node is greater than the child node it is called as a max heap. Whereas if the parent node is lesser than the child node it is called as a min heap.</li> </ol> <p><strong>Heapsort</strong> implementation in <strong>C++</strong></p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// To heapify a subtree rooted with node i which is</span>
<span class="c1">// Heapify:- A process which helps regaining heap properties in tree after removal </span>
<span class="kt">void</span> <span class="nf">heapify</span><span class="p">(</span><span class="kt">int</span> <span class="n">A</span><span class="p">[],</span> <span class="kt">int</span> <span class="n">n</span><span class="p">,</span> <span class="kt">int</span> <span class="n">i</span><span class="p">)</span>
<span class="p">{</span>
   <span class="kt">int</span> <span class="n">largest</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span> <span class="c1">// Initialize largest as root</span>
   <span class="kt">int</span> <span class="n">left_child</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span> <span class="c1">// left = 2*i + 1</span>
   <span class="kt">int</span> <span class="n">right_child</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">;</span> <span class="c1">// right = 2*i + 2</span>
   <span class="c1">// If left child is larger than root</span>
   <span class="k">if</span> <span class="p">(</span><span class="n">left_child</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">&amp;&amp;</span> <span class="n">A</span><span class="p">[</span><span class="n">left_child</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">A</span><span class="p">[</span><span class="n">largest</span><span class="p">])</span>
       <span class="n">largest</span> <span class="o">=</span> <span class="n">left_child</span><span class="p">;</span>
   <span class="c1">// If right child is larger than largest so far</span>
   <span class="k">if</span> <span class="p">(</span><span class="n">right_child</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">&amp;&amp;</span> <span class="n">A</span><span class="p">[</span><span class="n">right_child</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">A</span><span class="p">[</span><span class="n">largest</span><span class="p">])</span>
       <span class="n">largest</span> <span class="o">=</span> <span class="n">right_child</span><span class="p">;</span>
   <span class="c1">// If largest is not root</span>
   <span class="k">if</span> <span class="p">(</span><span class="n">largest</span> <span class="o">!=</span> <span class="n">i</span><span class="p">)</span> <span class="p">{</span>
       <span class="n">swap</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">largest</span><span class="p">]);</span>
       <span class="c1">// Recursively heapify the affected sub-tree</span>
       <span class="n">heapify</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">largest</span><span class="p">);</span>
   <span class="p">}</span>
<span class="p">}</span>
<span class="c1">// main function to do heap sort</span>
<span class="kt">void</span> <span class="nf">heap_sort</span><span class="p">(</span><span class="kt">int</span> <span class="n">A</span><span class="p">[],</span> <span class="kt">int</span> <span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
   <span class="c1">// Build heap (rearrange array)</span>
   <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">n</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">--</span><span class="p">)</span>
       <span class="n">heapify</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span><span class="p">);</span>
   <span class="c1">// One by one extract an element from heap</span>
   <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">--</span><span class="p">)</span> <span class="p">{</span>
       <span class="c1">// Move current root to end</span>
       <span class="n">swap</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
       <span class="c1">// call max heapify on the reduced heap</span>
       <span class="n">heapify</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
   <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="trie-advanced">Trie (Advanced)</h2> <p>Properties of the Trie for a set of the strings -</p> <ol> <li>The root node of the trie always represents the null node.</li> <li>Each child of nodes is sorted alphabetically.</li> <li>Each node can have a maximum of <strong>26</strong> children (A to Z).</li> <li>Each node (except the root) can store one letter of the alphabet.</li> </ol> <p>I’m lazy, so check the complete code <a href="https://www.javatpoint.com/trie-data-structure">here</a>.</p> <h1 id="important-topics">Important Topics</h1> <h2 id="bit-manipulation">Bit Manipulation</h2> <ul> <li>Integer data types. <ul> <li>If the <strong>most-significant</strong> byte is given the <strong>highest</strong> address then it is <strong>Little-endian architecture</strong>.</li> <li>If the <strong>most-significant</strong> byte is given the <strong>lowest</strong> address then it is <strong>Big-endian architecture</strong>.</li> </ul> </li> <li>2’s complement is given by adding 1 to inverted bits.</li> <li><code class="language-plaintext highlighter-rouge">long long int</code> is stored in <code class="language-plaintext highlighter-rouge">8 bytes</code>.</li> <li>Get size of a data type using <code class="language-plaintext highlighter-rouge">sizeof(&lt;datatype&gt;)</code>.</li> </ul> <h3 id="tricks-with-bits">Tricks with bits</h3> <ul> <li><code class="language-plaintext highlighter-rouge">x &amp; (x - 1)</code> will clear the lowest set bit of <code class="language-plaintext highlighter-rouge">x</code>.</li> <li><code class="language-plaintext highlighter-rouge">x &amp; ~(x - 1)</code> extracts the lowest set bit of <code class="language-plaintext highlighter-rouge">x</code>.</li> <li>Check others <a href="https://www.interviewbit.com/tutorial/tricks-with-bits/#tricks-with-bits">here</a>. No, I was not lazy to write, they felt unimportant.</li> </ul> <h2 id="design">Design</h2> <ul> <li>You can generate a random number between 0 and 32767 using <code class="language-plaintext highlighter-rouge">rand()</code> in C++.</li> </ul> <h2 id="mathematics">Mathematics</h2> <ul> <li>Counting the number of prime numbers. Follow the logic used in CS251. Initialise all numbers to primes. Make all multiples to false. Or more formally, follow what’s called as <strong>Eratosthenes sieve method</strong>.</li> <li>Simple trick for questions like determine if a number <code class="language-plaintext highlighter-rouge">n</code> has the form \(a^x\): Check <code class="language-plaintext highlighter-rouge">pow(a, max)% n == 0</code>!!!!</li> </ul> <h2 id="pointers">Pointers</h2> <ul> <li>int, float - 4 bytes of memory , char - 1 byte of memory</li> <li>Pointers for 2D arrays and general syntax -&gt; <a href="https://www.geeksforgeeks.org/pointer-array-array-pointer/">here</a></li> </ul> <h2 id="others">Others</h2> <ul> <li> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">hammingWeight</span><span class="p">(</span><span class="kt">uint32_t</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">count</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span>
        <span class="k">while</span><span class="p">(</span><span class="n">n</span><span class="p">){</span>
            <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="o">&amp;</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">);</span>
            <span class="n">count</span><span class="o">++</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">count</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div> </div> </li> </ul> <h1 id="algorithms">Algorithms</h1> <h2 id="kmp---knuth-morris-pratt">KMP - Knuth Morris Pratt</h2> <blockquote> <p>Given a text <code class="language-plaintext highlighter-rouge">txt[0, ..., n-1]</code> and a pattern <code class="language-plaintext highlighter-rouge">pat[0, ..., m-1]</code>, write a function <code class="language-plaintext highlighter-rouge">search(char pat[], char txt[])</code> that prints all occurrences of <code class="language-plaintext highlighter-rouge">pat[]</code> in <code class="language-plaintext highlighter-rouge">txt[]</code>.</p> </blockquote> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// lsp[i] = the longest proper prefix of pat[0..i] which is also a suffix of pat[0..i]</span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">lsp</span><span class="p">(</span><span class="n">needle</span><span class="p">.</span><span class="n">length</span><span class="p">(),</span> <span class="o">-</span><span class="mi">1</span><span class="p">);</span>
<span class="k">for</span><span class="p">(</span><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">needle</span><span class="p">.</span><span class="n">length</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
<span class="p">{</span>
    <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">lsp</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
    <span class="k">while</span><span class="p">(</span><span class="n">j</span> <span class="o">!=</span> <span class="mi">0</span> <span class="o">&amp;&amp;</span> <span class="n">needle</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">!=</span> <span class="n">needle</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">lsp</span><span class="p">[</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
    <span class="n">lsp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>
    <span class="k">if</span><span class="p">(</span><span class="n">needle</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="n">needle</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">++</span><span class="n">lsp</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
<span class="c1">// TIME COMPLEXITY - O(needle.length())</span>
<span class="c1">// STORAGE COMPLEXITY - O(needle.length())</span>
</code></pre></div></div> <p>Counting the occurrences of needle in haystack:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">size_t</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">for</span><span class="p">(;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">haystack</span><span class="p">.</span><span class="n">length</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span><span class="p">(</span> <span class="n">j</span> <span class="o">==</span> <span class="n">needle</span><span class="p">.</span><span class="n">length</span><span class="p">())</span> 
	<span class="p">{</span>
		<span class="o">++</span><span class="n">ANSWER</span><span class="p">;</span>
		<span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
	<span class="p">}</span>
   <span class="k">if</span><span class="p">(</span><span class="n">haystack</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">needle</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
   <span class="p">{</span>
        <span class="k">if</span><span class="p">(</span><span class="n">j</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="k">continue</span><span class="p">;</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">lsp</span><span class="p">[</span><span class="n">j</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>
        <span class="o">--</span><span class="n">i</span><span class="p">;</span> 
   <span class="p">}</span>
   <span class="k">else</span>
       <span class="o">++</span><span class="n">j</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// TIME COMPLEXITY - O(haystack.length() * needle.length()) - Worst case is "AAAAA", "AA"</span>
<span class="c1">// STORAGE COMPLEXITY - O(1)</span>
</code></pre></div></div> <p><strong>Other pattern matching algorithms</strong></p> <ol> <li><a href="https://www.geeksforgeeks.org/rabin-karp-algorithm-for-pattern-searching/?ref=rp">Rabin - Karp algorithm</a> - Something to do with hashes</li> <li><a href="https://www.geeksforgeeks.org/boyer-moore-algorithm-for-pattern-searching/?ref=rp">Boyer Moore algorithm</a> - Something to do finding good and bad heuristics</li> </ol> <p>There are many more algorithms which are covered <a href="https://www.tutorialspoint.com/introduction-to-pattern-searching-algorithms">here</a>.</p> <h2 id="the-two-pointer-technique">The two pointer technique</h2> <p>This technique is a clever optimization on some brute force approaches in certain conditions. Let us take an example to understand this concept.</p> <p>Suppose you have to find two indices in a sorted (non-decreasing) array such that the sum of values at those indices is zero. The naive <code class="language-plaintext highlighter-rouge">O(n^2)</code> approach would be to check every pair of indices satisfying this condition.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">A</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> 
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">A</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">!=</span> <span class="n">j</span> <span class="o">&amp;&amp;</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="k">return</span> <span class="nb">true</span><span class="p">;</span> <span class="c1">// solution found. </span>
                <span class="k">if</span> <span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">break</span><span class="p">;</span> <span class="c1">// Clearly A[i] + A[j] would increase as j increases</span>
            <span class="p">}</span>
</code></pre></div></div> <p>Although, let us make some keen observations in this method. When <code class="language-plaintext highlighter-rouge">i</code> increases, <code class="language-plaintext highlighter-rouge">A[i]</code> increases, and the breaking point of the inner loop decreases. Also, the inner loop need not run till <code class="language-plaintext highlighter-rouge">A.size() - 1</code> but can end before <code class="language-plaintext highlighter-rouge">j = i</code>.</p> <p>We can rewrite the code such that the value of <code class="language-plaintext highlighter-rouge">j</code> starts from the end of the array and goes till <code class="language-plaintext highlighter-rouge">i</code>. In that case, we would break the inner loop when the sum goes <em>below</em> 0. And similarly, as <code class="language-plaintext highlighter-rouge">i</code> increases, the breaking point would decrease.</p> <p>Now, the value of the sum at the breaking point of the <strong>previous</strong> iteration will be positive as <code class="language-plaintext highlighter-rouge">A[i]</code> increases with <code class="language-plaintext highlighter-rouge">i</code>. Therefore, the iterations of the inner loop can begin from the breaking point of the previous iteration. In other words, consider the following code.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">A</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">;</span>    
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">A</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> 
            <span class="k">for</span> <span class="p">(;</span> <span class="n">j</span> <span class="o">&gt;</span> <span class="n">i</span><span class="p">;</span> <span class="n">j</span><span class="o">--</span><span class="p">)</span> <span class="p">{</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">!=</span> <span class="n">j</span> <span class="o">&amp;&amp;</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="k">return</span> <span class="nb">true</span><span class="p">;</span> <span class="c1">// solution found. </span>
                <span class="k">if</span> <span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="k">break</span><span class="p">;</span> <span class="c1">// Clearly A[i] + A[j] would decrease as j decreases.</span>
            <span class="p">}</span>
</code></pre></div></div> <p>Consider the time analysis of this code. <code class="language-plaintext highlighter-rouge">i</code> only moves forward and <code class="language-plaintext highlighter-rouge">j</code> only moves backward. Therefore, this code runs in <code class="language-plaintext highlighter-rouge">O(n)</code>.</p> <p>In general, all two pointer approach work similarly. You look at the naive solution involving multiple loops and then you start analyzing the pattern on each loop. Try to look for monotonicity in one of the loops as other loops move forward. If you find that, you have found your optimization.</p> <h2 id="graph-algorithms">Graph Algorithms</h2> <h3 id="dijkstras-algorithm---graphs">Dijkstra’s Algorithm - Graphs</h3> <p>This algorithm finds the shortest distance from the source vertex to all other vertices in the graph. Refer <a href="#dijkstra's-algorithm">here</a>.</p> <p><strong>Caution</strong>. Dijkstra’s does not work in <em>negative</em> weighted graphs.</p> <h3 id="floyd-warshall-algorithm---ov3">Floyd Warshall Algorithm - <code class="language-plaintext highlighter-rouge">O(V^3)</code></h3> <p>This algorithm finds the shortest distance between <strong>every pair</strong> of nodes in the graph. We initialise the solution matrix equal to the adjacency matrix. Then we perform the following update:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dist[i][k] + dist[k][j] if dist[i][j] &gt; dist[i][k] + dist[k][j]
</code></pre></div></div> <p>The C++ code is given as follows:</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="p">(</span><span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="n">k</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// Pick all vertices as source one by one</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">// Pick all vertices as destination for the</span>
            <span class="c1">// above picked source</span>
            <span class="k">for</span> <span class="p">(</span><span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
                <span class="c1">// If vertex k is on the shortest path from</span>
                <span class="c1">// i to j, then update the value of</span>
                <span class="c1">// dist[i][j]</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">dist</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="p">(</span><span class="n">dist</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">dist</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">])</span>
                    <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">dist</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">!=</span> <span class="n">INF</span>
                        <span class="o">&amp;&amp;</span> <span class="n">dist</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">!=</span> <span class="n">INF</span><span class="p">))</span>
                    <span class="n">dist</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">dist</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">dist</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">j</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
</code></pre></div></div> <p>We can modify the solution to print the shortest paths also by storing the predecessor information in a separate 2D matrix. <a href="https://imada.sdu.dk/~jbj/DM85/lec6a.pdf">Here</a> is the proof of the algorithm.</p> <h3 id="mst---prims-algorithm">MST - Prim’s Algorithm</h3> <p>The basic idea is to maintain a set <code class="language-plaintext highlighter-rouge">S</code> and choose the edge with <strong>minimum</strong> cost connecting <code class="language-plaintext highlighter-rouge">S</code> and <code class="language-plaintext highlighter-rouge">V\S</code> in each iteration. This is implemented via priority queues, similar to Dijkstra’s.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1">// Create a priority queue to store vertices that</span>
    <span class="c1">// are being preinMST. This is weird syntax in C++.</span>
    <span class="c1">// Refer below link for details of this syntax</span>
    <span class="c1">// http://geeksquiz.com/implement-min-heap-using-stl/</span>
    <span class="n">priority_queue</span><span class="o">&lt;</span> <span class="n">iPair</span><span class="p">,</span> <span class="n">vector</span> <span class="o">&lt;</span><span class="n">iPair</span><span class="o">&gt;</span> <span class="p">,</span> <span class="n">greater</span><span class="o">&lt;</span><span class="n">iPair</span><span class="o">&gt;</span> <span class="o">&gt;</span> <span class="n">pq</span><span class="p">;</span> <span class="c1">//greater&lt;iPair&gt; for reverse</span>
    <span class="kt">int</span> <span class="n">src</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="c1">// Taking vertex 0 as source</span>
    <span class="c1">// Create a vector for keys and initialize all</span>
    <span class="c1">// keys as infinite (INF)</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">key</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">INF</span><span class="p">);</span>
    <span class="c1">// To store parent array which in turn store MST</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">parent</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">);</span>
    <span class="c1">// To keep track of vertices included in MST</span>
    <span class="n">vector</span><span class="o">&lt;</span><span class="kt">bool</span><span class="o">&gt;</span> <span class="n">inMST</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="nb">false</span><span class="p">);</span>
    <span class="c1">// Insert source itself in priority queue and initialize</span>
    <span class="c1">// its key as 0.</span>
    <span class="n">pq</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">make_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">src</span><span class="p">));</span>
    <span class="n">key</span><span class="p">[</span><span class="n">src</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="cm">/* Looping till priority queue becomes empty */</span>
    <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">pq</span><span class="p">.</span><span class="n">empty</span><span class="p">())</span>
    <span class="p">{</span>
        <span class="c1">// The first vertex in pair is the minimum key</span>
        <span class="c1">// vertex, extract it from priority queue.</span>
        <span class="c1">// vertex label is stored in second of pair (it</span>
        <span class="c1">// has to be done this way to keep the vertices</span>
        <span class="c1">// sorted key (key must be first item</span>
        <span class="c1">// in pair)</span>
        <span class="kt">int</span> <span class="n">u</span> <span class="o">=</span> <span class="n">pq</span><span class="p">.</span><span class="n">top</span><span class="p">().</span><span class="n">second</span><span class="p">;</span>
        <span class="n">pq</span><span class="p">.</span><span class="n">pop</span><span class="p">();</span>
          <span class="c1">//Different key values for same vertex may exist in the priority queue.</span>
          <span class="c1">//The one with the least key value is always processed first.</span>
          <span class="c1">//Therefore, ignore the rest.</span>
          <span class="k">if</span><span class="p">(</span><span class="n">inMST</span><span class="p">[</span><span class="n">u</span><span class="p">]</span> <span class="o">==</span> <span class="nb">true</span><span class="p">){</span>
            <span class="k">continue</span><span class="p">;</span>
        <span class="p">}</span>
        <span class="n">inMST</span><span class="p">[</span><span class="n">u</span><span class="p">]</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>  <span class="c1">// Include vertex in MST</span>
        <span class="c1">// 'i' is used to get all adjacent vertices of a vertex</span>
        <span class="n">list</span><span class="o">&lt;</span> <span class="n">pair</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="o">&gt;::</span><span class="n">iterator</span> <span class="n">i</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span> <span class="o">=</span> <span class="n">adj</span><span class="p">[</span><span class="n">u</span><span class="p">].</span><span class="n">begin</span><span class="p">();</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">adj</span><span class="p">[</span><span class="n">u</span><span class="p">].</span><span class="n">end</span><span class="p">();</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="c1">// Get vertex label and weight of current adjacent</span>
            <span class="c1">// of u.</span>
            <span class="kt">int</span> <span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">).</span><span class="n">first</span><span class="p">;</span>
            <span class="kt">int</span> <span class="n">weight</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">i</span><span class="p">).</span><span class="n">second</span><span class="p">;</span>
            <span class="c1">//  If v is not in MST and weight of (u,v) is smaller</span>
            <span class="c1">// than current key of v</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">inMST</span><span class="p">[</span><span class="n">v</span><span class="p">]</span> <span class="o">==</span> <span class="nb">false</span> <span class="o">&amp;&amp;</span> <span class="n">key</span><span class="p">[</span><span class="n">v</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">weight</span><span class="p">)</span>
            <span class="p">{</span>
                <span class="c1">// Updating key of v</span>
                <span class="n">key</span><span class="p">[</span><span class="n">v</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="p">;</span>
                <span class="n">pq</span><span class="p">.</span><span class="n">push</span><span class="p">(</span><span class="n">make_pair</span><span class="p">(</span><span class="n">key</span><span class="p">[</span><span class="n">v</span><span class="p">],</span> <span class="n">v</span><span class="p">));</span>
                <span class="n">parent</span><span class="p">[</span><span class="n">v</span><span class="p">]</span> <span class="o">=</span> <span class="n">u</span><span class="p">;</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
</code></pre></div></div> <h3 id="mst---kruskals-algorithm">MST - Kruskal’s Algorithm</h3> <p>The idea is much simpler compared to Prim’s. Although, the implementation is involved. We sort all the edges in the decreasing order of their weights, and add edges sequentially such that <strong>no cycles</strong> are formed. We use something known as the <strong>Union-Find</strong> algorithm to detect cycles.</p> <h4 id="union-find-algorithm">Union-Find algorithm</h4> <p>A <a href="http://en.wikipedia.org/wiki/Disjoint-set_data_structure"><em>disjoint-set data structure</em></a> is a data structure that keeps track of a set of elements partitioned into a number of disjoint (non-overlapping) subsets. A <a href="http://en.wikipedia.org/wiki/Disjoint-set_data_structure"><em>union-find algorithm</em></a> is an algorithm that performs two useful operations on such a data structure. A simple method to do this is maintain a parent array, and check the highest ancestors of the vertices in the new edge. If they are the same, then they form a cycle. Otherwise, the new edge does not form a cycle. This approach is <code class="language-plaintext highlighter-rouge">O(n)</code> in worst case. To improve this to <code class="language-plaintext highlighter-rouge">O(logn)</code> we can use <strong>union by rank</strong>.</p> <p>The basic problem in the above approach is, the parent tree can be highly skewed. We need to keep the tree balanced by always attaching the smaller depth tree under the root of the deeper tree. The second optimization to naive method is <strong><em>Path Compression</em></strong>. The idea is to flatten the tree when <code class="language-plaintext highlighter-rouge">find()</code> is called. When <code class="language-plaintext highlighter-rouge">find()</code> is called for an element x, root of the tree is returned. The <code class="language-plaintext highlighter-rouge">find()</code> operation traverses up from x to find root. The idea of path compression is to make the found root as parent of x so that we don’t have to traverse all intermediate nodes again. This is somewhat like a <em>dynamic programming</em> paradigm.</p> <p>The amortized time complexity of this method becomes constant. The implementation is as follows. For a change, I’m including the code in Python.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># A union by rank and path compression based
# program to detect cycle in a graph
</span><span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="c1"># a structure to represent a graph
</span><span class="k">class</span> <span class="nc">Graph</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_of_v</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_of_v</span> <span class="o">=</span> <span class="n">num_of_v</span>
        <span class="n">self</span><span class="p">.</span><span class="n">edges</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="c1"># graph is represented as an
</span>    <span class="c1"># array of edges
</span>    <span class="k">def</span> <span class="nf">add_edge</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">edges</span><span class="p">[</span><span class="n">u</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">Subset</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">parent</span><span class="p">,</span> <span class="n">rank</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">parent</span> <span class="o">=</span> <span class="n">parent</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">rank</span>
<span class="c1"># A utility function to find set of an element
# node(uses path compression technique)
</span><span class="k">def</span> <span class="nf">find</span><span class="p">(</span><span class="n">subsets</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">subsets</span><span class="p">[</span><span class="n">node</span><span class="p">].</span><span class="n">parent</span> <span class="o">!=</span> <span class="n">node</span><span class="p">:</span>
        <span class="n">subsets</span><span class="p">[</span><span class="n">node</span><span class="p">].</span><span class="n">parent</span> <span class="o">=</span> <span class="nf">find</span><span class="p">(</span><span class="n">subsets</span><span class="p">,</span> <span class="n">subsets</span><span class="p">[</span><span class="n">node</span><span class="p">].</span><span class="n">parent</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">subsets</span><span class="p">[</span><span class="n">node</span><span class="p">].</span><span class="n">parent</span>
<span class="c1"># A function that does union of two sets
# of u and v(uses union by rank)
</span><span class="k">def</span> <span class="nf">union</span><span class="p">(</span><span class="n">subsets</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="c1"># Attach smaller rank tree under root
</span>    <span class="c1"># of high rank tree(Union by Rank)
</span>    <span class="k">if</span> <span class="n">subsets</span><span class="p">[</span><span class="n">u</span><span class="p">].</span><span class="n">rank</span> <span class="o">&gt;</span> <span class="n">subsets</span><span class="p">[</span><span class="n">v</span><span class="p">].</span><span class="n">rank</span><span class="p">:</span>
        <span class="n">subsets</span><span class="p">[</span><span class="n">v</span><span class="p">].</span><span class="n">parent</span> <span class="o">=</span> <span class="n">u</span>
    <span class="k">elif</span> <span class="n">subsets</span><span class="p">[</span><span class="n">v</span><span class="p">].</span><span class="n">rank</span> <span class="o">&gt;</span> <span class="n">subsets</span><span class="p">[</span><span class="n">u</span><span class="p">].</span><span class="n">rank</span><span class="p">:</span>
        <span class="n">subsets</span><span class="p">[</span><span class="n">u</span><span class="p">].</span><span class="n">parent</span> <span class="o">=</span> <span class="n">v</span>
    <span class="c1"># If ranks are same, then make one as
</span>    <span class="c1"># root and increment its rank by one
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="n">subsets</span><span class="p">[</span><span class="n">v</span><span class="p">].</span><span class="n">parent</span> <span class="o">=</span> <span class="n">u</span>
        <span class="n">subsets</span><span class="p">[</span><span class="n">u</span><span class="p">].</span><span class="n">rank</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="c1"># The main function to check whether a given
# graph contains cycle or not
</span><span class="k">def</span> <span class="nf">isCycle</span><span class="p">(</span><span class="n">graph</span><span class="p">):</span>
    <span class="c1"># Allocate memory for creating sets
</span>    <span class="n">subsets</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">graph</span><span class="p">.</span><span class="n">num_of_v</span><span class="p">):</span>
        <span class="n">subsets</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nc">Subset</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="c1"># Iterate through all edges of graph,
</span>    <span class="c1"># find sets of both vertices of every
</span>    <span class="c1"># edge, if sets are same, then there
</span>    <span class="c1"># is cycle in graph.
</span>    <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="n">graph</span><span class="p">.</span><span class="n">edges</span><span class="p">:</span>
        <span class="n">u_rep</span> <span class="o">=</span> <span class="nf">find</span><span class="p">(</span><span class="n">subsets</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">graph</span><span class="p">.</span><span class="n">edges</span><span class="p">[</span><span class="n">u</span><span class="p">]:</span>
            <span class="n">v_rep</span> <span class="o">=</span> <span class="nf">find</span><span class="p">(</span><span class="n">subsets</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">u_rep</span> <span class="o">==</span> <span class="n">v_rep</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">True</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nf">union</span><span class="p">(</span><span class="n">subsets</span><span class="p">,</span> <span class="n">u_rep</span><span class="p">,</span> <span class="n">v_rep</span><span class="p">)</span>
</code></pre></div></div> <h1 id="sorting">Sorting</h1> <ul> <li><strong>C++ -&gt;</strong> <strong><code class="language-plaintext highlighter-rouge">sort(v.begin(), v.end());</code></strong></li> <li><strong>Python -&gt; <code class="language-plaintext highlighter-rouge">v.sort()</code></strong></li> <li>My best implementation of merging algorithm in merge sort:</li> </ul> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">res</span><span class="p">(</span><span class="n">m</span> <span class="o">+</span> <span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
<span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">for</span><span class="p">(;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span> <span class="o">&amp;&amp;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">m</span><span class="p">;)</span>
    <span class="k">if</span><span class="p">(</span><span class="n">nums2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">nums1</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>     <span class="n">res</span><span class="p">[</span><span class="n">c</span><span class="o">++</span><span class="p">]</span> <span class="o">=</span> <span class="n">nums2</span><span class="p">[</span><span class="n">i</span><span class="o">++</span><span class="p">];</span>
    <span class="k">else</span>    <span class="n">res</span><span class="p">[</span><span class="n">c</span><span class="o">++</span><span class="p">]</span> <span class="o">=</span> <span class="n">nums1</span><span class="p">[</span><span class="n">j</span><span class="o">++</span><span class="p">];</span>
<span class="k">while</span><span class="p">(</span><span class="n">i</span> <span class="o">!=</span> <span class="n">n</span><span class="p">)</span>   <span class="n">res</span><span class="p">[</span><span class="n">c</span><span class="o">++</span><span class="p">]</span> <span class="o">=</span> <span class="n">nums2</span><span class="p">[</span><span class="n">i</span><span class="o">++</span><span class="p">];</span>
<span class="k">while</span><span class="p">(</span><span class="n">j</span> <span class="o">!=</span> <span class="n">m</span><span class="p">)</span>   <span class="n">res</span><span class="p">[</span><span class="n">c</span><span class="o">++</span><span class="p">]</span> <span class="o">=</span> <span class="n">nums1</span><span class="p">[</span><span class="n">j</span><span class="o">++</span><span class="p">];</span>
</code></pre></div></div> <h3 id="insertion-sort">Insertion Sort</h3> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">INSERTION</span><span class="o">-</span><span class="n">SORT</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
   <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">n</span>
   	<span class="n">key</span> <span class="err">←</span> <span class="n">A</span> <span class="p">[</span><span class="n">i</span><span class="p">]</span>
    	<span class="n">j</span> <span class="err">←</span> <span class="n">i</span> <span class="err">–</span> <span class="mi">1</span>
  	 <span class="k">while</span> <span class="n">j</span> <span class="o">&gt;</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">and</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">key</span>
   		<span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="err">←</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
   		<span class="n">j</span> <span class="err">←</span> <span class="n">j</span> <span class="err">–</span> <span class="mi">1</span>
   	<span class="n">End</span> <span class="k">while</span> 
   	<span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="err">←</span> <span class="n">key</span>
  <span class="n">End</span> <span class="k">for</span> 
</code></pre></div></div> <h3 id="quick-sort">Quick Sort</h3> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/**
* The main function that implements quick sort.
* @Parameters: array, starting index and ending index
*/</span>
<span class="n">quickSort</span><span class="p">(</span><span class="n">arr</span><span class="p">[],</span> <span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">low</span> <span class="o">&lt;</span> <span class="n">high</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="c1">// pivot_index is partitioning index, arr[pivot_index] is now at correct place in sorted array</span>
        <span class="n">pivot_index</span> <span class="o">=</span> <span class="n">partition</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">);</span>

        <span class="n">quickSort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">low</span><span class="p">,</span> <span class="n">pivot_index</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// Before pivot_index</span>
        <span class="n">quickSort</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">pivot_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="p">);</span> <span class="c1">// After pivot_index</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="cm">/**
* The function selects the last element as pivot element, places that pivot element correctly in the array in such a way
* that all the elements to the left of the pivot are lesser than the pivot and
* all the elements to the right of pivot are greater than it.
* @Parameters: array, starting index and ending index
* @Returns: index of pivot element after placing it correctly in sorted array
*/</span>
<span class="n">partition</span> <span class="p">(</span><span class="n">arr</span><span class="p">[],</span> <span class="n">low</span><span class="p">,</span> <span class="n">high</span><span class="p">)</span>
<span class="p">{</span>
    <span class="c1">// pivot - Element at right most position</span>
    <span class="n">pivot</span> <span class="o">=</span> <span class="n">arr</span><span class="p">[</span><span class="n">high</span><span class="p">];</span>  
    <span class="n">i</span> <span class="o">=</span> <span class="p">(</span><span class="n">low</span> <span class="o">-</span> <span class="mi">1</span><span class="p">);</span>  <span class="c1">// Index of smaller element</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">j</span> <span class="o">=</span> <span class="n">low</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;=</span> <span class="n">high</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="c1">// If current element is smaller than the pivot, swap the element with pivot</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">pivot</span><span class="p">)</span>
        <span class="p">{</span>
            <span class="n">i</span><span class="o">++</span><span class="p">;</span>    <span class="c1">// increment index of smaller element</span>
            <span class="n">swap</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">j</span><span class="p">]);</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">swap</span><span class="p">(</span><span class="n">arr</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">arr</span><span class="p">[</span><span class="n">high</span><span class="p">]);</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="selection-sort">Selection Sort</h3> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">SelectionSort</span><span class="p">(</span><span class="n">Arr</span><span class="p">[],</span> <span class="n">arr_size</span><span class="p">)</span><span class="o">:</span>    
        <span class="n">FOR</span> <span class="n">i</span> <span class="n">from</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">arr_size</span><span class="o">:</span>    
            <span class="n">min_index</span> <span class="o">=</span> <span class="n">FindMinIndex</span><span class="p">(</span><span class="n">Arr</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">arr_size</span><span class="p">)</span>    
        
            <span class="n">IF</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">min_index</span><span class="o">:</span>    
                <span class="n">swap</span><span class="p">(</span><span class="n">Arr</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">Arr</span><span class="p">[</span><span class="n">min_index</span><span class="p">])</span>    
            <span class="n">END</span> <span class="n">of</span> <span class="n">IF</span>    
        <span class="n">END</span> <span class="n">of</span> <span class="n">FOR</span>
</code></pre></div></div> <h3 id="bubble-sort">Bubble Sort</h3> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bubbleSort</span><span class="p">(</span> <span class="n">Arr</span><span class="p">[],</span> <span class="n">totat_elements</span><span class="p">)</span>
   <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">total_elements</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">do</span><span class="o">:</span>
      <span class="n">swapped</span> <span class="o">=</span> <span class="nb">false</span>
      <span class="k">for</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">total_elements</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">2</span> <span class="k">do</span><span class="o">:</span>
         <span class="cm">/* compare the adjacent elements */</span>   
         <span class="k">if</span> <span class="n">Arr</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">Arr</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="n">then</span>
            <span class="cm">/* swap them */</span>
            <span class="n">swap</span><span class="p">(</span><span class="n">Arr</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">Arr</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>		 
            <span class="n">swapped</span> <span class="o">=</span> <span class="nb">true</span>
         <span class="n">end</span> <span class="k">if</span>
      <span class="n">end</span> <span class="k">for</span>
      <span class="cm">/*if no number was swapped that means 
      array is sorted now, break the loop.*/</span>
      <span class="k">if</span><span class="p">(</span><span class="n">not</span> <span class="n">swapped</span><span class="p">)</span> <span class="n">then</span>
         <span class="k">break</span>
      <span class="n">end</span> <span class="k">if</span>
   <span class="n">end</span> <span class="k">for</span>
<span class="n">end</span>
</code></pre></div></div> <h1 id="recursion">Recursion</h1> <p>For Time Analysis of recursive programs, it may be easier to find lower bounds and upper bounds first. For example, consider the time analysis of recursive Fibonacci code.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">T</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">=</span> <span class="n">T</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">T</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">O</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="s">" Lower bound "</span>
<span class="n">T</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="n">T</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">O</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">O</span><span class="p">(</span><span class="mi">2</span><span class="o">^</span><span class="p">(</span><span class="n">n</span><span class="o">/</span><span class="mi">2</span><span class="p">))</span> 
<span class="s">"""</span><span class="err">
</span><span class="s">Writing big-O notation here is not technically right. Instead we can write the following</span><span class="err">
</span><span class="s">If we know that T(n) = T(n - 1) + T(n - 2) + Θ(n), then we can write T(n) &gt; Ω(n)</span><span class="err">
</span><span class="s">"""</span>
<span class="s">" Upper bound "</span>
<span class="n">T</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="n">T</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">O</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">O</span><span class="p">(</span><span class="mi">2</span><span class="o">^</span><span class="n">n</span><span class="p">)</span>
</code></pre></div></div> <p>Recursion builds up an implicit stack in the memory. To calculate the <strong>maximum</strong> space consumed, find the <strong>depth</strong> of the recursion tree.</p> <h1 id="hashing">Hashing</h1> <p>Hashing is the process of converting a given key into another smaller value for O(1) retrieval time. This is done by taking the help of some function or algorithm which is called as <strong>hash function</strong> to map data to some encrypted or simplified representative value which is termed as “hash code” or “hash”. This hash is then used as an index to narrow down search criteria to get data quickly.</p> <p><strong>Hash Table</strong> - A hash table is an array that stores pointers to data mapping to a given hashed key.</p> <p><strong>Bucket</strong> - A list containing all the values having the same hash value</p> <p><strong>Hash Functions</strong> - A hash function is a function or algorithm that is used to generate the encrypted or shortened value to any given key. Types of Hash Functions:</p> <ul> <li>Index Mapping method - The index of the element in the array is its hash.</li> <li>Division method - The hash is given by the remainder of the value with the table size. In this case, we need to take care of certain things. If the table length has the form \(r^p\) then the hash values occupy only the <code class="language-plaintext highlighter-rouge">p</code> lowest-order bits of key.</li> <li>Mid square method - Square the value and take the middle digits. For ex, <code class="language-plaintext highlighter-rouge">h(88) -&gt; 7(74)4 -&gt; 74</code>.</li> <li>Digit folding method - The key is divided into separate parts and by using simple operations these separated parts are combined to produce a hash.</li> </ul> <p><strong>Load Factor</strong> - The load factor is simply a measure of how full (occupied) the hash table is, and is simply defined as: <code class="language-plaintext highlighter-rouge">α = number of occupied slots/total slots</code></p> <h3 id="collisions">Collisions</h3> <p>When multiple elements fall into the same bucket, we say a collision has occured. Handling collisions:</p> <ul> <li> <p><strong>Separate Chaining</strong> - The idea is to maintain linked lists for buckets. Hashing performance can be evaluated under the assumption that each key is equally likely and uniformly hashed to any slot of hash table.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Performance Analysis
Load Factor = α = n/ table_size
Time Complexity for search and delete - O(1 + α)
Time Complexity for insert - O(1)
</code></pre></div> </div> </li> <li> <p><strong>Open Addressing</strong> - In this technique, we ensure that all records are stored in the hash table itself. The size of the table must be greater than or equal to the total number of keys available.</p> <ul> <li><strong>Insert(Key)</strong> - When we try to insert a key to the bucket which is already occupied, we keep probing the hash table until an empty slot is found. Once we find the empty slot, we insert <code class="language-plaintext highlighter-rouge">key</code> into that slot.</li> <li><strong>Search(key):</strong> While searching for <code class="language-plaintext highlighter-rouge">key</code> in the hash table, we keep probing until slot’s value doesn’t become equal to <code class="language-plaintext highlighter-rouge">key</code> or until an empty slot is found.</li> <li><strong>Delete(key):</strong> While performing delete operation, when we try to simply delete <code class="language-plaintext highlighter-rouge">key</code>, then the search operation for that key might fail. Hence, deleted key’s slots are marked as “<strong>deleted</strong>” so that we get the status of the key when searched.</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Performance Analysis
* Load factor, α = n/table_size ( α &lt; 1 )
* Expected time taken to search/insert/delete operation &lt; (1/(1 - α))
* Hence, search/insert/delete operations take at max (1/(1 - α)) time
</code></pre></div> </div> </li> </ul> <h2 id="implementation-details-1">Implementation Details</h2> <p><strong>C++</strong> - Use <code class="language-plaintext highlighter-rouge">unordered_map</code> which are implemented via hashing.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/* Declaration */</span>
<span class="n">unordered_map</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span> <span class="kt">int</span><span class="o">&gt;</span> <span class="n">A</span><span class="p">;</span>
<span class="cm">/* Inserting elements */</span>
<span class="n">A</span><span class="p">.</span><span class="n">insert</span><span class="p">({</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">});</span> <span class="c1">// O(1) on average</span>
<span class="cm">/* Finding elements */</span>
<span class="k">if</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="o">==</span> <span class="n">A</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="k">return</span> <span class="n">null</span><span class="p">;</span>
<span class="k">else</span> <span class="k">return</span> <span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">];</span>	<span class="c1">// Worst case O(n), Average O(1)</span>
<span class="cm">/* Printing size */</span>
<span class="n">A</span><span class="p">.</span><span class="n">size</span><span class="p">()</span> <span class="c1">// O(1)</span>
<span class="cm">/* Erasing keys */</span>
<span class="k">if</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="o">!=</span> <span class="n">A</span><span class="p">.</span><span class="n">end</span><span class="p">())</span> <span class="n">A</span><span class="p">.</span><span class="n">erase</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">k</span><span class="p">));</span> <span class="c1">// or A.erase(k);</span>
</code></pre></div></div> <p><strong>Python</strong> - Use Dictionaries.</p> <h1 id="dynamic-programming">Dynamic Programming</h1> <ul> <li>Make sure you write the base cases in recursion!</li> <li>You can use <strong>static</strong> variables in cpp for storing the memory from previous calls to the function. For example, the Fibonacci numbers code can be written as</li> </ul> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">climbStairs</span><span class="p">(</span><span class="kt">int</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span><span class="p">(</span><span class="n">n</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
        <span class="k">if</span><span class="p">(</span><span class="n">n</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="k">return</span> <span class="mi">1</span><span class="p">;</span>
       <span class="k">static</span> <span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">dict</span><span class="p">(</span><span class="mi">45</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">dict</span><span class="p">[</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> 
            <span class="n">dict</span><span class="p">[</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">climbStairs</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">climbStairs</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="mi">2</span><span class="p">);</span>
        <span class="k">return</span> <span class="n">dict</span><span class="p">[</span><span class="n">n</span> <span class="o">-</span> <span class="mi">1</span><span class="p">];</span>
    <span class="p">}</span>
</code></pre></div></div> <p>Although, make sure the environment you are using supports static variables correctly.</p> <ul> <li>If you pass constant variables in recursion, pass them by reference rather than value to save memory and time!</li> </ul> <h2 id="longest-increasing-subsequence">Longest Increasing Subsequence</h2> <p>This is not a special problem, and the dynamic programming approach you are thinking of right now works. The time complexity is <code class="language-plaintext highlighter-rouge">O(n^2)</code>. Although, there is another interesting approach to this problem, and I felt it was worth mentioning. Let the given array be <code class="language-plaintext highlighter-rouge">A</code>. Make a sorted copy of <code class="language-plaintext highlighter-rouge">A</code> say <code class="language-plaintext highlighter-rouge">B</code>. Now, the <strong>longest common subsequence</strong> of <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> is our answer (Why?). This approach is also <code class="language-plaintext highlighter-rouge">O(n^2)</code>.</p> <h1 id="greedy-algorithms">Greedy Algorithms</h1> <p>A greedy algorithm is a simple and efficient algorithmic approach for solving any given problem by selecting the best available option at that moment of time, without bothering about the future results.</p> <p>In simple words, here, it is believed that the locally best choices made would be leading towards globally best results.In this approach, we never go back to reverse the decision of selection made which is why this algorithm works in a top-bottom manner.</p> <ul> <li>This approach works well for <strong>job scheduling</strong> problems.</li> </ul> <h1 id="graphs">Graphs</h1> <h2 id="breadth-first-search---ov--e">Breadth First Search - <code class="language-plaintext highlighter-rouge">O(V + E)</code></h2> <p><strong>Shortest Path:</strong> In an unweighted graph, the shortest path is the path with least number of edges. With BFS, we <strong>always</strong> reach a node from given source in shortest possible path. Example: Dijkstra’s Algorithm.</p> <h3 id="recursive-bfs">Recursive BFS</h3> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/**
* Pseudo code for recursive BFS
* @Parameters: Graph G represented as adjacency list, 
*  Queue q, boolean[] visited, key
* Initially q has s node in it.
*/</span>

<span class="n">recursiveBFS</span><span class="p">(</span><span class="n">Graph</span> <span class="n">graph</span><span class="p">,</span> <span class="n">Queue</span> <span class="n">q</span><span class="p">,</span> <span class="n">boolean</span><span class="p">[]</span> <span class="n">visited</span><span class="p">,</span> <span class="kt">int</span> <span class="n">key</span><span class="p">){</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">q</span><span class="p">.</span><span class="n">isEmpty</span><span class="p">())</span>
        <span class="k">return</span> <span class="s">"Not Found"</span><span class="p">;</span>

    <span class="c1">// pop front node from queue and print it</span>
    <span class="kt">int</span> <span class="n">v</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">poll</span><span class="p">();</span>
    <span class="k">if</span><span class="p">(</span><span class="n">v</span><span class="o">==</span><span class="n">key</span><span class="p">)</span> <span class="k">return</span> <span class="s">"Found"</span><span class="p">;</span>

    <span class="c1">// do for every neighbors of node v</span>
    <span class="k">for</span> <span class="p">(</span> <span class="n">Node</span> <span class="n">u</span> <span class="n">in</span> <span class="n">graph</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
    <span class="p">{</span>
        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">visited</span><span class="p">[</span><span class="n">u</span><span class="p">])</span>
        <span class="p">{</span>
            <span class="c1">// mark it visited and push it into queue</span>
            <span class="n">visited</span><span class="p">[</span><span class="n">u</span><span class="p">]</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
            <span class="n">q</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">u</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="c1">// recurse for other nodes</span>
    <span class="n">recursiveBFS</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">key</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">Queue</span> <span class="n">q</span> <span class="o">=</span> <span class="k">new</span> <span class="nf">Queue</span><span class="p">();</span>
<span class="n">q</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">s</span><span class="p">);</span>
<span class="n">recursiveBFS</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">key</span><span class="p">);</span>
</code></pre></div></div> <h3 id="iterative-bfs">Iterative BFS</h3> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/**
* Pseudo code for iterative BFS
* @Parameters: Graph G, source node s, boolean[] visited, key
*/</span>

<span class="n">iterativeBFS</span><span class="p">(</span><span class="n">Graph</span> <span class="n">graph</span><span class="p">,</span> <span class="kt">int</span> <span class="n">s</span><span class="p">,</span> <span class="n">boolean</span><span class="p">[]</span> <span class="n">visited</span><span class="p">,</span> <span class="kt">int</span> <span class="n">key</span><span class="p">){</span>
    <span class="c1">// create a queue neeeded for BFS</span>
    <span class="n">Queue</span> <span class="n">q</span> <span class="o">=</span> <span class="n">Queue</span><span class="p">();</span>

    <span class="c1">// mark source node as discovered</span>
    <span class="n">visited</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>

    <span class="c1">// push source node into the queue</span>
    <span class="n">q</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">s</span><span class="p">);</span>

    <span class="c1">// while queue isnt empty</span>
    <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">q</span><span class="p">.</span><span class="n">isEmpty</span><span class="p">())</span>
    <span class="p">{</span>
        <span class="c1">// pop front node from queue and print it</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="n">poll</span><span class="p">();</span>
        <span class="k">if</span><span class="p">(</span><span class="n">v</span><span class="o">==</span><span class="n">key</span><span class="p">)</span> <span class="k">return</span> <span class="s">"Found"</span><span class="p">;</span>

        <span class="c1">// for every neighboring node of v</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">u</span> <span class="o">:</span> <span class="n">graph</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">v</span><span class="p">))</span> <span class="p">{</span>
            <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">visited</span><span class="p">[</span><span class="n">u</span><span class="p">])</span> <span class="p">{</span>
                <span class="c1">// mark it visited and enqueue to queue</span>
                <span class="n">visited</span><span class="p">[</span><span class="n">u</span><span class="p">]</span> <span class="o">=</span> <span class="nb">true</span><span class="p">;</span>
                <span class="n">q</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">u</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="c1">//If key hasnt been found</span>
    <span class="k">return</span> <span class="s">"Not Found"</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Dijkstra’s in an unweighted graph is BFS</strong>.</p> <h2 id="depth-first-search---ov--e">Depth First Search - <code class="language-plaintext highlighter-rouge">O(V + E)</code></h2> <p>The code is much simpler in this case</p> <h3 id="recursive-dfs">Recursive DFS</h3> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/**
* Pseudo code for recursive DFS
* @Parameters: adjacent list G, source node, 
* visited array, key (node to be searched)
*/</span>

<span class="n">DFS</span><span class="p">(</span><span class="n">adjacent</span><span class="p">[][],</span> <span class="n">source</span><span class="p">,</span> <span class="n">visited</span><span class="p">[],</span> <span class="n">key</span><span class="p">)</span> <span class="p">{</span>
   <span class="k">if</span><span class="p">(</span><span class="n">source</span> <span class="o">==</span> <span class="n">key</span><span class="p">)</span> <span class="k">return</span> <span class="nb">true</span> <span class="c1">//We found the key</span>
   <span class="n">visited</span><span class="p">[</span><span class="n">source</span><span class="p">]</span> <span class="o">=</span> <span class="n">True</span>
   
   <span class="n">FOR</span> <span class="n">node</span> <span class="n">in</span> <span class="n">adjacent</span><span class="p">[</span><span class="n">source</span><span class="p">]</span><span class="o">:</span>
       <span class="n">IF</span> <span class="n">visited</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">==</span> <span class="n">False</span><span class="o">:</span>
          <span class="n">DFS</span><span class="p">(</span><span class="n">adjacent</span><span class="p">,</span> <span class="n">node</span><span class="p">,</span> <span class="n">visited</span><span class="p">)</span>
       <span class="n">END</span> <span class="n">IF</span>
   <span class="n">END</span> <span class="n">FOR</span>
   <span class="k">return</span> <span class="nb">false</span>    <span class="c1">// If it reaches here, then all nodes have been explored </span>
                  <span class="c1">//and we still havent found the key.</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="iterative-dfs">Iterative DFS</h3> <p>Just replace <code class="language-plaintext highlighter-rouge">queue</code> with <code class="language-plaintext highlighter-rouge">stack</code> in iterative BFS.</p> <h2 id="dijkstras-algorithm">Dijkstra’s Algorithm</h2> <p>It is used to find the <strong>shortest path</strong> between a node/vertex (source node) to any (or every) other nodes/vertices (destination nodes) in a graph. A graph is basically an interconnection of nodes connected by edges. This algorithm is sometimes referred to as <strong>Single Source Shortest Path Algorithm</strong> due to its nature of implementation.</p> <div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Dijkstra_Algorithm</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">G</span><span class="p">)</span><span class="o">:</span>
    <span class="s">"""</span><span class="err">
</span><span class="s">    parameters: source node--&gt; source, graph--&gt; G</span><span class="err">
</span><span class="s">    return: List of cost from source to all other nodes--&gt; cost</span><span class="err">
</span><span class="s">    """</span>
    <span class="n">unvisited_list</span> <span class="o">=</span> <span class="p">[]</span>			<span class="c1">// List of unvisited vertices</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">cost</span><span class="p">[</span><span class="n">source</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>              <span class="c1">// Distance (cost) from source to source will be 0</span>
    <span class="k">for</span> <span class="n">each</span> <span class="n">vertex</span> <span class="n">v</span> <span class="n">in</span> <span class="n">G</span><span class="o">:</span>       <span class="c1">// Assign cost as INFINITY to all vertices</span>
       <span class="k">if</span> <span class="n">v</span> <span class="err">≠</span> <span class="n">source</span>
             <span class="n">cost</span><span class="p">[</span><span class="n">v</span><span class="p">]</span> <span class="o">=</span> <span class="n">INFINITY</span>
             <span class="n">add</span> <span class="n">v</span> <span class="n">to</span> <span class="n">unvisited_list</span>    <span class="c1">// All nodes pushed to unvisited_list initially</span>

    <span class="k">while</span> <span class="n">unvisited_list</span> <span class="n">is</span> <span class="n">not</span> <span class="n">empty</span><span class="o">:</span>        	     <span class="c1">// Main loop</span>
       <span class="n">v</span> <span class="o">=</span> <span class="n">vertex</span> <span class="n">in</span> <span class="n">unvisited_list</span> <span class="n">with</span> <span class="n">min</span> <span class="n">cost</span><span class="p">[</span><span class="n">v</span><span class="p">]</span>      <span class="c1">// v is the source node for first iteration</span>
       <span class="n">remove</span> <span class="n">v</span> <span class="n">from</span> <span class="n">unvisited_list</span>		            <span class="c1">// Marking node as visited </span>

       <span class="k">for</span> <span class="n">each</span> <span class="n">neighbor</span> <span class="n">u</span> <span class="n">of</span> <span class="n">v</span><span class="o">:</span>			<span class="c1">// Assign shorter path cost to neigbour u</span>
          <span class="n">cost_value</span> <span class="o">=</span> <span class="n">Min</span><span class="p">(</span> <span class="n">cost</span><span class="p">[</span><span class="n">u</span><span class="p">],</span> <span class="n">cost</span><span class="p">[</span><span class="n">v</span><span class="p">]</span> <span class="o">+</span> <span class="n">edge_cost</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">u</span><span class="p">)]</span>
          <span class="n">cost</span><span class="p">[</span><span class="n">u</span><span class="p">]</span> <span class="o">=</span> <span class="n">cost_value</span>            		<span class="c1">// Update cost of vertex u </span>

    <span class="k">return</span> <span class="n">cost</span>
</code></pre></div></div> <h1 id="miscellaneous">Miscellaneous</h1> <ul> <li>Each element in the array appears twice except for one element which appears only once. Use <strong>xor</strong>.</li> <li>Rotate a matrix by sequential reflection operations.</li> <li>32 but integers range from <code class="language-plaintext highlighter-rouge">-2147483648</code> to <code class="language-plaintext highlighter-rouge">2147483647</code>. The small difference may give a wrong answer in your code.</li> <li>Make sure you initialise <strong>flag</strong> variables.</li> <li>When you declare pointers, put a star <code class="language-plaintext highlighter-rouge">*</code> in front of every variable. That is, use <code class="language-plaintext highlighter-rouge">int *p, *q</code> and not <code class="language-plaintext highlighter-rouge">int* p, q</code>.</li> <li>Sometimes, arithmetic operations may cause the result to cross the datatype boundary. Take care of these. For example, instead of <code class="language-plaintext highlighter-rouge">(l + r)/2</code>, use <code class="language-plaintext highlighter-rouge">(l/2 + r/2 + (l%2+ r%2)/2</code>.</li> <li>Missing number from range - Use <code class="language-plaintext highlighter-rouge">xor</code> instead of <code class="language-plaintext highlighter-rouge">sum</code></li> <li>Define a macro using <code class="language-plaintext highlighter-rouge">typedef pair&lt;int, int&gt; ipair</code> in <strong>C++</strong>.</li> <li>You can find the \(n\)th Fibonacci number in <code class="language-plaintext highlighter-rouge">O(log n)</code> time. Think about it.</li> <li>Don’t assume <em>positive</em> numbers when an integer input is mentioned.</li> <li>Initialise <code class="language-plaintext highlighter-rouge">vector&lt;vector&lt;int&gt;&gt;</code> properly. Specifically, ensure you <strong>allocate</strong> memory before accessing.</li> <li>When you write code for a DP problem, make sure you write the <strong>base cases</strong> first.</li> <li>Checking boxes in Sudoku grid using <code class="language-plaintext highlighter-rouge">1-9</code> loop of <code class="language-plaintext highlighter-rouge">i</code> and <code class="language-plaintext highlighter-rouge">j</code> : <code class="language-plaintext highlighter-rouge">int x = (i/3)*3 + j/3, y = (i%3)*3 + j%3;</code></li> <li>In sequence problems, make sure you test your algorithm in the beginning, the middle and the end. <ul> <li>Continuously check the range of values reached by the variables you declare. It may happen that the value of a <code class="language-plaintext highlighter-rouge">int</code> is going beyond <code class="language-plaintext highlighter-rouge">32</code> bits.</li> </ul> </li> <li>Use <code class="language-plaintext highlighter-rouge">bitset</code> instead of 2D vectors for “visited” matrices. <code class="language-plaintext highlighter-rouge">bitset&lt;n&gt; bits</code> declares <code class="language-plaintext highlighter-rouge">n</code> bits which can be accessed using <code class="language-plaintext highlighter-rouge">bits[i]</code>.</li> </ul>]]></content><author><name></name></author><category term="Notes"/><category term="Articles"/><summary type="html"><![CDATA[A quick overview of all the important concepts in DSA.]]></summary></entry><entry><title type="html">Handling broken python packages</title><link href="https://sudhansh6.github.io/blog/Fix-Python-Ubuntu/" rel="alternate" type="text/html" title="Handling broken python packages"/><published>2021-06-21T00:00:00+00:00</published><updated>2021-06-21T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Fix-Python-Ubuntu</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Fix-Python-Ubuntu/"><![CDATA[<p>If you knowingly or unknowingly deleted python files from your Ubuntu, this article is for you. Although a disclaimer: I’m not highly versed with operating systems, this makeshift method seems to fix the problem. I spent a rather frustrating day trying to get out of this situation, and I did not find a single good resource. So, I planned to write out this post to consolidate all the fixes that may resolve the issue. If none of the methods work, your best bet is to reinstall Ubuntu entirely without any hassle.</p> <h1 id="how-i-stumbled-across-the-problem">How I stumbled across the problem</h1> <p>I love updating my OS to the latest build available. On account of this, I updated my Ubuntu to <a href="https://releases.ubuntu.com/21.04/">21.04 Hirsute Hippo</a> yesterday. The update took 3-4 hours to finish, and I was delighted with the new OS. After wasting a whole day updating the OS and testing out <a href="https://sudhansh6.github.io/posts/Windows-11">Windows 11</a>, I decided I should get back to work.</p> <p>I started out with a project which requires python. On executing, the code (which was working previously) gave an error saying some modules were missing. I realized the new OS update had replaced <code class="language-plaintext highlighter-rouge">python3.8</code> with <code class="language-plaintext highlighter-rouge">python3.9</code>. I was annoyed thinking that all my modules were deleted during the update.</p> <p>To check this, I headed out to <code class="language-plaintext highlighter-rouge">/usr/lib/</code> to check for the same. I saw no trace of <code class="language-plaintext highlighter-rouge">python3.8</code>, and there were 5 other versions of python installed. I was foolish enough to think those versions were unnecessary, and I unwittingly deleted <code class="language-plaintext highlighter-rouge">python2.7</code> and <code class="language-plaintext highlighter-rouge">python3</code>. Deleting these files required <code class="language-plaintext highlighter-rouge">sudo</code> and <code class="language-plaintext highlighter-rouge">root</code> access, and for a good reason. This action initiated my day of incessant browsing and fruitless fixing attempts.</p> <p>I later learned that all my modules of <code class="language-plaintext highlighter-rouge">python3.8</code> were safe in the local installation at <code class="language-plaintext highlighter-rouge">/usr/local/lib</code>. I wish I knew this before. Anyway, once I deleted those files, everything started malfunctioning. <code class="language-plaintext highlighter-rouge">apt</code> and <code class="language-plaintext highlighter-rouge">dpkg</code> gave errors, and the terminal was not opening too!</p> <h1 id="how-to-fix-the-problem">How to fix the problem?</h1> <p>Before I go into what finally restored my software, I’ll list out a couple of commands I came across. If you are lucky, these might just solve your problem.</p> <h2 id="method-1-using-apt-and-dpkg"><a name="standard">Method 1: Using <code class="language-plaintext highlighter-rouge">apt</code> and <code class="language-plaintext highlighter-rouge">dpkg</code></a></h2> <p>Try the following commands:</p> <ol> <li> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nt">-f</span> <span class="nb">install</span>
</code></pre></div> </div> <p>According to <code class="language-plaintext highlighter-rouge">man apt-get</code>, the <code class="language-plaintext highlighter-rouge">-f</code> or <code class="language-plaintext highlighter-rouge">--fix-broken</code> flag is an attempt to correct a system with broken dependencies in place. When used with install/remove, this option can omit any packages to permit APT to deduce a possible solution.</p> </li> <li> <p>The above does not work when a system’s dependency structure can be so corrupt as to require manual intervention. Then, you can try</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>dpkg <span class="nt">--configure</span> <span class="nt">-a</span> 
</code></pre></div> </div> <p>This command configures all unpacked packages.</p> </li> </ol> <h2 id="method-2-relinking-links-and-binaries">Method 2: Relinking links and binaries</h2> <p>The above was a cleaner fix using the package installers themselves. Now, we enter the makeshift regime. If you simply deleted <code class="language-plaintext highlighter-rouge">usr/bin/python*</code>, the following may work:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">--reinstall</span> python<span class="k">*</span><span class="nt">-minimal</span>
<span class="c"># Replace the * with the required version. For example, for python2.7 use:</span>
<span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">--reinstall</span> python2.7-minimal
</code></pre></div></div> <blockquote> <p>In case your terminal is not working, you can try this:</p> <ul> <li>If you have Visual Studio Code installed, use the inbuilt terminal to execute the above.</li> <li>If xterm is installed, try using that.</li> <li>Otherwise, just access a <a href="https://askubuntu.com/questions/66195/what-is-a-tty-and-how-do-i-access-a-tty">tty</a> using <code class="language-plaintext highlighter-rouge">Ctrl + Alt + F[1-6]</code> and execute the above.</li> </ul> </blockquote> <p>​ If only the <a name="symlink"><a href="https://www.freecodecamp.org/news/symlink-tutorial-in-linux-how-to-create-and-remove-a-symbolic-link/"><strong>symbolic links</strong></a></a> are broken, you can simply create a new link and restore your system.</p> <p>​ Usually, a <code class="language-plaintext highlighter-rouge">python3</code> link points to the <code class="language-plaintext highlighter-rouge">python3.*</code> binary executable. Suppose you have <code class="language-plaintext highlighter-rouge">python3.8</code> installed, you can create a symlink using:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Creates a symbolic link to python3.8 from python3</span>
<span class="nb">sudo ln</span> <span class="nt">-s</span> /usr/bin/python3.8 /usr/bin/python3
</code></pre></div></div> <h2 id="method-3-my-hack">Method 3: My Hack</h2> <p>Without further ado, let me tell you what worked for me. My idea was to copy the correct files from a faultless installation of Ubuntu. It is unlikely that you have these files (if you do, then skip to <a href="#Restoring-everything">this</a>), and you need to create a bootable to get these.</p> <h3 id="creating-a-bootable">Creating a bootable</h3> <p>The first step is to download the <code class="language-plaintext highlighter-rouge">iso</code> file from the <a href="https://ubuntu.com/download">Ubuntu website</a> and acquire an empty USB stick.</p> <p>If your terminal is not working, it is improbable that the <a href="https://ubuntu.com/tutorials/create-a-usb-stick-on-ubuntu#1-overview"><em>Startup Disk Creator</em></a> tool works. Therefore, we will use a CLI tool called <strong>ddrescue</strong> to create the bootable.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt <span class="nb">install </span>gddrescue
</code></pre></div></div> <p>This is expected to work if <code class="language-plaintext highlighter-rouge">apt</code> is not entirely broken. Next, we check the block device volume of the USB drive (I hope you plugged in your USB).</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lsblk
</code></pre></div></div> <p>The USB drive may be present in <code class="language-plaintext highlighter-rouge">dev/sdb/</code>. Then, we simply use the following to create the bootable</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>ddrescue path/to.iso dev/sd<span class="k">*</span> <span class="nt">--</span> force <span class="nt">-D</span>
<span class="c"># For example, if USB is present in sdb use</span>
<span class="nb">sudo </span>ddrescue ~/ubuntu-21.04-desktop-amd64.iso /dev/sdb <span class="nt">--force</span> <span class="nt">-D</span>
</code></pre></div></div> <p>This may take a few minutes, and your USB will be ready after this.</p> <h3 id="copying-working-files">Copying working files</h3> <p>If your system has only Ubuntu installed, then you might be able to reinstall Ubuntu, keeping your data intact. Otherwise, use the ‘Try Ubuntu’ option.</p> <p>Once the OS loads, open the file explorer. Copy the <code class="language-plaintext highlighter-rouge">usr/bin/</code> and <code class="language-plaintext highlighter-rouge">usr/lib/</code> folder from the Ubuntu instance running on your USB to some location (say <code class="language-plaintext highlighter-rouge">foo/</code> on your hard drive). If you do not have the permissions to do so, use this in the destination directory</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo chmod</span> <span class="nt">-R</span> 777 <span class="nb">.</span>
</code></pre></div></div> <p>Restart the system again, unplug the USB, and boot into Ubuntu.</p> <h3 id="restoring-everything"><a name="Restoring-everything">Restoring everything</a></h3> <p>This part is straightforward, but we must proceed with care.</p> <p>Open up the <code class="language-plaintext highlighter-rouge">foo/usr/bin/</code> folder from the files you copied and the <code class="language-plaintext highlighter-rouge">usr/bin/</code> in your system. List all the files relevant to python using:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">ls</span> <span class="nt">-l</span> | <span class="nb">grep </span>python
</code></pre></div></div> <p>Delete the existing executables, and then copy the python executables first using</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo rm</span> /usr/bin/python3.8
<span class="nb">sudo mv </span>foo/usr/bin/python3.8 /usr/bin/python3.8
</code></pre></div></div> <p>We then create the symbolic links as present in <code class="language-plaintext highlighter-rouge">foo/usr/bin/</code> using the command mentioned <a href="#symlink">above</a>. This fixes the <code class="language-plaintext highlighter-rouge">/bin/</code> folder. Now, do the same in the <code class="language-plaintext highlighter-rouge">lib</code> folder. Copy <code class="language-plaintext highlighter-rouge">python3</code> and <code class="language-plaintext highlighter-rouge">python3.9</code> (these two are present if you are using Ubuntu 21.04) into <code class="language-plaintext highlighter-rouge">usr/lib/</code></p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo rm</span> <span class="nt">-R</span> /usr/lib/python3
<span class="nb">sudo mv</span> <span class="nt">-R</span> foo/usr/lib/python3 /usr/lib
</code></pre></div></div> <p>Change the ownership of the copied files to root using:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo chown </span>root:root /usr/bin/python3.8
<span class="nb">sudo chown</span> <span class="nt">-R</span> root:root /usr/bin/python3
</code></pre></div></div> <p>After transferring the files, repeat <a href="#standard">these</a> to patch everything up. You’re done!</p>]]></content><author><name></name></author><category term="Articles"/><category term="Ubuntu"/><summary type="html"><![CDATA[A post to guide you through mending python libraries on Ubuntu]]></summary></entry></feed>