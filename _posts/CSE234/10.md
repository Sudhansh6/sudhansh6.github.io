
Let us continue our discussion on Quantization. We discussed about quantization granularity (per-tensor, per-channel, group quantizations). 

Per-channel quantization is preferred to tensor quantization in some cases because the channels can have very different ranges, and using a single $$S$$ can result in lopsided representations. Even then, some applications can lose too much on performance for per-channel quantization. In these cases, group quantization is useful. 

Group quantization is more fine-grained. E.g., per vector. It has more accuracy, less quantization error trading off some of the savings. 

> Can we do some sort of statistical grouping here?

The sweet-spot that works in practice is a two-level quantization that quantizes hierarchically. Instead of using $$r = S(q - Z)$$ we use $$r = \gamma S_q (q = Z)$$. $$\gamma$$ is a gloating-point coarse grained scale factor, and $$S_Q$$ is an integer per-vector scale factor. It can be further generalized into multi-level quantization with scale factors for each levels. 

So far, we have done quantization of weights. They are static during inference, and so everything works. What if we want to quantize the activations? The stats change with every input. This problem is solved in two ways:
1. Moving average - Observed ranges are smoothed across thousands of training steps
2. Calibration dataset - Figure out the range from subset of training set

## Mixed Precision
In the previous methods, we have done *uniform quantization* wherein every weight parameter is quantized to the same number of bits. However, what if we use different precision for different weights? Does this improve the performance a lot? The implementation is however very complicated since we have to deal with different data formats. Also, what formats give the best savings and high performance?

These are hard questions. So we throw ML at the problem. We define a cost function to let a model discover the combination of formats that give the best parameters. It was a good research field, and the accuracy of models improved by 10% or so.

After a while, [NVIDIA released a paper](https://arxiv.org/pdf/1710.03740) (it’s a good read) that became the standard for mixed precision training. The intuition for this approach is as follows. 
Some layers are more sensitive to dynamic range and precision. For example, softmax and normalization layers have a large range of values. We identify such operations and assign them higher precision.

![](/assets/img/2025-01-06-data-systems-for-ml/17393289702270.jpg)

Since Adam calculates two moments and normalizes the gradients, we use FP32 for the weight updates. 

*Note.* Deepseek changed the standard with new precisions. 

Let us see how the memory of models changes with this new precision system. Again, for the largest GPT, there are 175B parameters. So it occupies 350G with FP16 for all the weights. The activations occupy 7488G assuming checkpointing at each layer boundary. 

So in this system, the master model with FP32 weights occupies 4*175 = 700G. The gradients occupy 2*175 = 350G. The running copy of the model used for inference is 2*175 = 350G. Finally, we need Adam mean and variance (FP32) that is 2*4*175 = 1400G. The rule of thumb in general is $$(4 + 2 + 2 + 4 + 4)N = 16N$$ memory for LLMs.

### Scaling down ML
Running ML on edge devices is always strongly demanded, and the market is very fragmented. It is easy to build a startup and get acquired in this space. The possible research directions are quantization, pruning, ML energy efficiency, federated ML, etc. 

# Parallelization
Moore’s law came to an end. However, ML models were scaling 3x every 18 months! Why? Bigger model gives better accuracy. People have also started seeing emergent capabilities in larger models.

![](/assets/img/2025-01-06-data-systems-for-ml/17393299103392.gif)


![](/assets/img/2025-01-06-data-systems-for-ml/17393300313845.jpg)


So, the models are going to get bigger. Along with this, the memory demand is increasing too. It was back in 2019 when we last fit an entire model in one GPU. Now we require 100s of GPUs just to store the model parameters. The only way out of this is parallelization. Wait! Aren’t GPUs already doing that?

> GPUs did data parallelization. We are talking about having multiple GPUs and using them together. All the things we considered so far assumed the entire model is on one GPU. Now, we need to distribute the model training. We just seem to be creating problems for ourselves… 

Intuitively there are multiple ways we can go about this
1. Parallelize along the layers (cutting through depth)
2. Parallelize each layer (cutting through breadth) - this one is rather complicated with more data traveling between clusters.

Apart from these considerations, a GPU cluster also has its own constraints. There are different latency communication channels (kind of like memory hierarchy). 

Let us look at the problem from a computational lens. A model involves parameters, weight updates, model spec and the data. 
- Computing - The forward pass and backward pass require compute
- Memory - The data and parameters require memory. 
Between these, we require communication (typically done with interconnects or network, e.g., NVLink) which is the main bottleneck in this whole setup. How do we communicate parameters and activations?

In **data parallelism**, we partition the data across GPUs and give each GPU a copy of the parameters and gradients. Then, we need to synchronize the updates together across the GPUs before the next iteration. 

In **model parallelism**, we partition the model across GPUs and use the data to update parts of the model. This method is more complicated. Let us delve deeper.

How do we partition a computation graph on a device cluster? 

![](/assets/img/2025-01-06-data-systems-for-ml/17393313839159.jpg)

There are more strategies that consider hybrid variants - some parts of the model are inter-op, intra-op and some parameters are replicated across devices. 

![](/assets/img/2025-01-06-data-systems-for-ml/17393315930822.jpg)

These are the standard techniques being used for the models today.

Let us delve deeper into these. 
![](/assets/img/2025-01-06-data-systems-for-ml/17393318019536.jpg)
In the above example, intra-op is similar to what we discussed before for matmul on GPUs - each GPU computes a partial sum. Surprisingly, we can show mathematically that this is the best we can do with inter-op.

What are the pros and cons?
- Inter-op parallelism requires point-to-point communication but results in devices being idle.
- Intra-op parallelism keeps the devices busy but requires collective communication. 

The all-reduce operation is computationally intensive. 
 