<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Structural Visualization for Neural Networks | Sudhansh Peddabomma</title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="Analysing learning patterns in neural networks using graph visualization algorithms."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/Structural-Visualization-for-Neural-Network/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Sudhansh Peddabomma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Articles</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Structural Visualization for Neural Networks</h1> <p class="post-meta">April 24, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> Research</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>The structure of neural networks have been motivated from neurons in brains. Just like how brain has specialized regions for analysing different kinds of data, do neural networks have such mappings? Is there a way we can visually monitor this, analyse patterns and comment on the progress of training of the model?</p> <p>In simpler neural networks, the outputs from each layer can be visualized as linear separating functions. Such interfaces are available on websites like Tensorflow palyground. Other visualization methods like t-SNE, aim to classify data points in high dimensional spaces. Combining such approaches, the paper <a href="https://arxiv.org/abs/1908.02831" rel="external nofollow noopener" target="_blank">Visualizing the PHATE of Neural Networks</a> uses a kernel-based simentionality reduction</p> <blockquote> <p><strong>t-SNE</strong> is a statistical method to visualize high-dimensional data in two or three dimensional maps. The approach involves constructing a probability distribution over pairs of objects such that similar objects have a higher probability. Then, it uses KL-divergence to model a similar distribution in the low-dimensional space for visualization. <a href="https://en.wikipedia.org/wiki/Uniform_manifold_approximation_and_projection" rel="external nofollow noopener" target="_blank">UMAP</a> is another approach for this task. To read in detail refer to the <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="external nofollow noopener" target="_blank">Wiki</a>.</p> </blockquote> <h1 id="multi-layer-perceptrons">Multi-layer Perceptrons</h1> <p>The basic setup boils down to the graph drawing problem where we try to find a layout to better visualize a given graph \(G= (V, E)\). There are two approaches to this problem -</p> <ul> <li> <p><strong>Force-directed layout</strong> - Adds a potential field representing force on edges of the graph to help them spread out</p> </li> <li> <p><strong>Spectral layout</strong> - USes coordinates the eigenveectors of a matric such a the Laplacian derived from the degree matrix and adjacency matrix of the graph.</p> </li> </ul> <h3 id="graph-construction">Graph Construction</h3> <p>Use KNN similarity with spectral layout as the warm start, and then use the force-directed layout to plot the graph. But what do all these mean in the context of latent spaces of a neural network?</p> \[h_i^{(l)} = \phi^{(l)} \left(\sum_{i \in [k]} x_i w_i^{(l)} + b^{(l)}\right)\] <p>Consider the task of classification. What are we trying to visualize? Either the samples or the model weights can be visualized. Let us look at the samples.</p> <h3 id="results">Results</h3> <p>Through out the triaining process, inter-class nodes will gradually move further while the intra-class nodes move closer.</p> <h3 id="experiment---sensitvity-mapping">Experiment - Sensitvity Mapping</h3> <p>Do some embeddings contribute more on the final prediction? We can try visualizing the gradients contributed by the samples at each hidden layer.</p> <h3 id="experiment---neuron-x-sample-mapping">Experiment - Neuron x Sample Mapping</h3> <p>Can we add neurons alongside samples as nodes in the graph? The idea is as follows -</p> <ul> <li> <p>Represent neuron nodes with black nodes and samples nodes as colored ones</p> </li> <li> <p>No connectiones between colored nodes and black nodes</p> </li> <li> <p>A sample node is adjacent to a neuron node if the neuron node</p> </li> </ul> <p>The motivation for this experiment is to figure out if some of the embeddings contribute more on the final prediction.</p> <h1 id="observing-domains-in-generative-models">Observing Domains in Generative Models</h1> <p>Focusing on VAE models, which are characterized with an encoder-decoder architectures to reduce the reconstruction loss (for the decoder) and simialrity loss (for the encoder). The assumption is that any input can be mapped to a lower-dimensional distribution which can then be sampled with a conditional variable to reproduce the high-dimensional input. The similarity loss acts like a regularization term, forcing the distribution learnt by the encoder to match the normal distribution. Depending on the application, the weight of the similarity loss (represented by \(\beta\)) can be varied.</p> <p>Increasing this parameter to a high value causes <em>posterior collapse</em>, reducing the generalizability of the network. Essentially, the network generates only a certain instance/class from the distribution.</p> <h3 id="relevant-work">Relevant Work</h3> <p><a href="https://ar5iv.labs.arxiv.org/html/1812.06775#:~:text=Variational%20Autoencoders%20Pursue%20PCA%20Directions%20%28by%20Accident%29%201,3%20Results%203.1%20The%20problem%20with%20log-likelihood%20" rel="external nofollow noopener" target="_blank">Variational Autoencoders Pursue PCA Directions (by Accident)</a> is one such works that claims that the decoder models will promote orthogonality when transforming the z-embedding to the final output. The question of using this behavior to help us model data better is still an open question.</p> <h2 id="conclusion">Conclusion</h2> <p>These approaches can be used for the following</p> <ul> <li> <p>Learning with trainability - Visualizing the entire course of training instead fthe final model.</p> </li> <li> <p>Learning towards interpretability - Visualizing why models behave in certain ways through the lens of a graph</p> </li> <li> <p>Learning towards robustness - Visualizing scrambled data points</p> </li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Design-and-Analysis-of-Algorithms/">Design and Analysis of Algorithms</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/design-and-analysis-of-algorithms/">Design and Analysis of Algorithms</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Object-Detection/">Object Detection</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/GANs-for-Compressed-Sensing/">Generative Adversarial Networks for Compressed Sensing</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Lattices-in-Cryptography/">Lattices in Cryptography and Quantum Computers</a> </li> </div> <script>document.querySelectorAll("#table-of-contents a").forEach(function(e){e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substring(1);document.querySelectorAll(".content-section").forEach(function(e){e.classList.add("hidden")});var n=document.getElementById(t);n&&n.classList.remove("hidden")})});</script> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Sudhansh Peddabomma. Last updated: October 29, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0K9MLG0V24");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>