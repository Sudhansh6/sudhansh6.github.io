# Lecture 3

## Compilation Models

![image-20220112111235980](assets/image-20220112111235980.png)

### Typical Front end

How does a front end work? Usually, the *parser* calls the *scanner* that reads the source program to extract **tokens**. These tokens are then used by the parser to create the parse tree. We can say that the scanner is a subordinate routine of the parser. On the other side, the parser sends the parse tree to the *semantic analyzer* which generates the AST. Finally, we get the AST/ Linear IR along with a symbol table.

### Typical Back ends in Aho Ullman Model

Constant Propagation, Elimination of Redundant computations (deadcode).

> Rest of the lecture was advanced technical summary of the course which I could not follow. Skipped those notes!

## Scanners

To discover the structure of the program, we first get a sequence of **lexemes** or **tokens** using the smallest meaningful units. Once this is done, we remove spaces and other whitespace characters in the **lexical analysis** or **scanning** step.

In the second step, we group the lexemes to form larger structures (parse tree). This is called as **syntax analysis** or **parsing**. There are tools like `Antlr` that combine scanning with parsing.

## Lexemes, Tokens, and Patterns

***Definition.*** *Lexical Analysis* is the operation of dividing the input program into a sequence of lexemes (tokens). *Lexemes* are the smallest logical units (words) of a program. Whereas, *tokens* are sets of similar lexemes, i.e. lexemes which have a common syntactic description.

Lexemes such as comments and white spaces are not passed to the later stages of a compiler. These have to be detected and ignored. Apart from the token itself, the lexical analyzer also passes other information regarding the token. These items of information are called **token attributes**. 
