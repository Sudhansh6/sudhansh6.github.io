## Temporal Difference TD($$\lambda$$) Algorithms
The idea of this class of algorithms is to improve the policy as we keep exploring the environment more. 

The first variant of these algorithms is TD(0) - We sample state transition from the trajectory (one from each) and update the existing value function based on the action and reward obtained. Formally, given $$V_t(.)$$ and a sample from the trajectory $$(s_t, a_t, r_t, s_{t + 1})$$, how do we obtain $$V_{t + 1}(s_t)$$? 

From the Bellman’s equations, we have

$$
V_\pi(s) = \mathbb E_{a \sim \pi(s)} [R(s, a) + \gamma \mathbb E_{s’ \in P(s, a)} V_\pi(s’)}
$$

So, can we do

$$
    V_{t + 1}(s_t) \gets r_t + \gamma V_t(s_{t + 1})
$$


This equation omits our previous estimate $$V_t(s_t)$$. How do we use it? We can do a sort of an averaging or gradient descent

$$
\begin{align*}
    V_{t + 1}(s_t) &\gets V_t(s_t) + \alpha_{s_t} \delta_t \\
    &\delta_t = r_t + \gamma V_t(s_{t + 1}) - V_t(s_t)
\end{align*}
$$

In essence, we are averaging over the previous visits in the trajectory but with slightly different update rules.

How do we choose the $$\alpha$$’s? For convergence purposes, we require $$\sum_{t = 0}^{\infty} \alpha(t) \to \infty$$ and $$\sum_{t = 0}^\infty \alpha^2(t) < \infty$$. These parameters are important for controlling the bias and variance of the estimators as we will discuss later. Possible learning schedules include $$\alpha(t) = \frac{1}{t^{0.5 + \epsilon}}$$ with $$0 < \epsilon \leq 0.5$$ and $$\alpha(t) = \frac{1}{\sqrt{t}\log t}.

With these learning schedules, you can prove the convergence thinking of the process as a contraction. It converges to the Bellman’s equation for some new value $$\gamma$$. 

To compare various statistical approaches (Monte Carlo methods), we need to compare the bias, variance, convergence and sampling complexity into account. To improve on some of these criteria, we have the general TD($$\lambda$$) algorithms.

Instead of considering a single transition, the general algorithm considers more transitions to update the value function. 

$$
    V^k_t = r_t + \gamma r_{t + 1} + \dots + \gamma^{k} r_{t + k} +\gamma^{k + 1} V_t{s_t + k}
$$
          
What is the advantage of this approach? We slowed the updates, which seemingly increases the bias but may decrease the variance. Another way to understand TD($$\lambda$$) is to think of it as a combination of Temporal Difference and Monte Carlo learning. It is an average of $$k$$-step returns. Kind of *truncated Monte Carlo method*.

What’s more? We can generalize the above mention equation a bit more. For different states, we can consider different $$k$$’s, and take the average of them. 

$$
    V_t^\lambda(s_t) = (1 - \lambda)\sum_{i = 0}^\infty \lambda^i V_t^k(s_t)
$$ 

where the update algorithm is

$$
    V_{t + 1} \gets V_t(s_t) + \alpha_t (V_t^{\lambda} (s_t) - V_t(s_t))  
$$

It reduces the variance because we are considering a a geometric weighted mean. Since longer windows $$k$$ have higher variance, we reduce their weight in the average with $$lambda$$. 

In practice, when we approach a new state, we take our current estimate of $$V_t$$, update all our previous calculations to recompute the new value function. 