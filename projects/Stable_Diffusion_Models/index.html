<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Stable Diffusion Models | Sudhansh Peddabomma</title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="A brief introduction to image synthesis applications, stable diffusion models and its applications."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/projects/Stable_Diffusion_Models/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Sudhansh Peddabomma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Articles</a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects<span class="sr-only">(current)</span></a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Stable Diffusion Models</h1> <p class="post-description">A brief introduction to image synthesis applications, stable diffusion models and its applications.</p> </header> <article> <h2 id="image-synthesis">Image Synthesis</h2> <ul> <li>Artificially generating images with some desired content</li> <li>Typically provided with some prompts for generation</li> <li>Multi-modal generation</li> <li>Generative models as solutions - <ul> <li>Guided Synthesis</li> <li>Image editing</li> </ul> </li> </ul> <h2 id="previous-work">Previous Work</h2> <h3 id="generative-adversarial-networks">Generative Adversarial Networks</h3> <p>A Generative Adversarial Network consists of a generator and a discriminator pair to synthesize images representative of the dataset. These two ‘agents’ compete against each other, and try to optimize opposite cost functions. The generator is responsible for creating synthetic data that should resemble the real data from a specific dataset, such as images, audio, or text. The discriminator, on the other hand, evaluates the data it receives and tries to distinguish between real data from the dataset and fake data generated by the generator.</p> <p>A variant of GANs, called <em>conditional</em> GANs operate in a latent space. That is, we have multi-dimensional space where each point corresponds to a set of parameters that can be mapped to data in the target distribution. The generator is then ‘conditioned’ on a random vector sampled from this latent space as input. These vectors act as a source of randomness that the generator uses to produce diverse data samples. By exploring different points in the latent space, the generator can generate a wide variety of data, allowing it to produce novel and creative outputs.</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation0.png" alt=""></p> <h3 id="gans">GANs</h3> <p>The loss function for a Generative Adversarial Network (GAN) consists of two components: the generator loss and the discriminator loss. Here’s the LaTeX code for the GAN loss function:</p> \[\mathcal{L}_{\text{GAN}}(G, D) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]\] <p>In this equation:</p> <ul> <li>\(\mathcal{L}_{\text{GAN}}(G, D)\) represents the GAN loss.</li> <li>\(G\) is the generator network.</li> <li>\(D\) is the discriminator network.</li> <li>\(x\) represents real data samples drawn from the true data distribution \(p_{\text{data}}(x)\) .</li> <li>\(z\) represents noise samples drawn from a prior distribution \(p_z(z)\) .</li> <li>\(G(z)\) is the generator’s output when given noise \(z\) .</li> <li>\(D(x)\) represents the discriminator’s output when given a real data sample \(x\) .</li> <li>\(1 - D(G(z))\) represents the discriminator’s output when given a generated sample \(G(z)\) .</li> </ul> <p>The goal of training a GAN is to find the generator and discriminator that minimize this loss function. The generator aims to generate data that fools the discriminator (maximizing the second term), while the discriminator aims to distinguish between real and generated data (maximizing the first term). This adversarial training process leads to the generation of realistic data by the generator.</p> <p><strong>Issues</strong></p> <ul> <li>Mode collapse - The process of training prompts generator to find one plausible example</li> <li>Monotonous output/Lack of diversity - unable to capture the complete dataset</li> <li>Difficult to optimize, unstable training, <strong>vanishing gradient</strong> </li> </ul> <blockquote> <p>Diversity and Fidelity tradeoff <br> <em>Cost functions may not converge using gradient descent in a minimax game</em></p> </blockquote> <hr> <h3 id="autoregressive-ar-transformers">AutoRegressive (AR) Transformers</h3> <p>AR models treat an image as a sequence of pixels and represent its probability as the product of the conditional probabilities of all pixels</p> <p>Models inherently forced to capture entire distribution unlike GANs</p> <p>Allows us to use Likelihood Maximization</p> <p>Not very high-resolution images due to memory constraints</p> <p>Stable training process as compared to GANs</p> <p>DALL-E uses transformers</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation2.png" alt=""></p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation3.png" alt=""></p> <hr> <p>pixelCNN</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation4.png" alt=""></p> <ul> <li><strong>Issues -</strong></li> <li>Accumulated errors - Since pixels generated in sequence</li> <li>Computationally Expensive <ul> <li>Pixel based</li> <li>Likelihood maximization unnecessary</li> <li>Captures barely perceptible\, high frequency details</li> </ul> </li> </ul> <h2 id="diffusion-models">Diffusion Models</h2> <p>Destroy training data by adding noise to generate Gaussian noise.</p> <p>Then learn to recover the data by <em>reversing this process</em></p> <p>A Diffusion Model is trained by finding the reverse Markov transitions that maximize the likelihood of the training data.</p> <p>x0 is the image and xT is the noise!</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation5.png" alt=""></p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation6.png" alt=""></p> <hr> <p>T is in the order of 1000s</p> <p>β’s are progressively increased typically to make xT pure Gaussian</p> <p>Smaller variances are preferred for better posterior estimation</p> <p>With a large number of steps - process is reversible - mathematically shown</p> <p>Training motivated from VAEs</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation7.png" alt=""></p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation8.png" alt=""></p> <hr> <p>T is in the order of 1000s Explain with diagram 2nd point Single network needed for diffusion models since forward process is fixed in DMs</p> <p>__Reverse Diffusion __ - The parameters have to be learned via a neural network</p> <p>Encoder-Decoder type architecture</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation9.png" alt=""></p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation10.gif" alt=""></p> <hr> <p>https://arxiv.org/pdf/2006.11239.pdf</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation11.png" alt=""></p> <hr> <p>T is in the order of 1000s Explain with diagram https://arxiv.org/pdf/2006.11239.pdf</p> <p>Forward process allows sampling at an arbitrary _t _</p> <p>This allows efficient training using stochastic gradient descent for optimizing random terms</p> <p>The simplified cost function in diffusion models is - \(DDPM\)</p> <p>The model learns the data distribution p \(x\) by denoising a normal variable</p> <p>These can be interpreted as a sequence of denoising autoencoders</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation12.png" alt=""></p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation13.png" alt=""></p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation14.png" alt=""></p> <hr> <p>https://arxiv.org/pdf/2006.11239.pdf</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation15.png" alt=""></p> <p><strong>Difference from GANs</strong></p> <p><strong>Not adversarial</strong></p> <p><strong>Not in latent space</strong></p> <hr> <p>How is t added? Check out this <a href="https://arxiv.org/pdf/2006.11239.pdf" rel="external nofollow noopener" target="_blank">paper</a></p> <p>Inductive bias of images via U-Net architecture</p> <p>Latent space captures low-level features and up-scales them</p> <p>Skip connections preserve the high-level features</p> <ul> <li> <p><strong>Advantages</strong> -</p> <ul> <li> <p>Stable from mode collapse - since based on likelihood</p> </li> <li> <p>Smooth distribution due to diffusion</p> </li> </ul> </li> <li> <p><strong>Disadvantages</strong> -</p> <ul> <li> <p>Computationally very expensive - repeated evaluations. 5 days on A100 GPU</p> </li> <li> <p>Image generation time is high</p> </li> </ul> </li> </ul> <hr> <p>Variational encoder - makes distribution U Nets are encoder decoder with skip connections</p> <h2 id="latent-diffusion-models">Latent Diffusion Models</h2> <ul> <li>Denoising every pixel is unnecessary and computationally expensive.</li> <li>Work in the __latent space __ to deal with lower dimensions</li> <li>Image generation done in two-steps <ul> <li>The <strong>perceptual compression</strong> stage removes high-frequency details</li> <li>Generative model learns the semantic variation in __semantic compression __ stage</li> </ul> </li> <li>__Motivation - __ Perceptually equivalent but computationally suitable space</li> <li><strong>Advantages</strong></li> <li>Autoencoding done only once and various diffusion models can be explored</li> <li>Other conditional generation tasks</li> </ul> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation17.png" alt=""></p> <p><strong>Features</strong></p> <p>High-resolution synthesis of images</p> <p>Application to multiple tasks such as unconditional image synthesis\, inpainting\, stochastic super-resolution</p> <p>General-purpose conditioning mechanism based on cross-attention\, enabling multi-modal training</p> <p>Variable compression rate for the latent space</p> <p>Latent space once generated\, can be used for multiple DM models</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation18.png" alt=""></p> <h3 id="latent-spaces---autoencoding">Latent Spaces - Autoencoding</h3> <p>Explicit separation of compressing and generation stages</p> <p>Exploit the inductive bias of DMs in the latent space via U-Nets due to perceptual equivalence</p> <p><strong>Process</strong></p> <p>Given an image _x _ in RGB\, the encoder E encodes the image into z = E \(x\)</p> <p>Encoder downsamples the image by a factor of f</p> <p>Decoder D\, reconstructs image from latent space as x’ = D \(z\) = D \(E\)x\(\)</p> <h2 id="stable-diffusion-models">Stable Diffusion Models</h2> <p>Refined version of LDMs</p> <p>Employ a frozen CLIP text encoder\, allowing it to generate images based on text prompts</p> <h2 id="contrastive-language-image-pre-training">Contrastive Language-Image Pre-Training</h2> <p>Generates text and image embeddings</p> <p>Images and relevant text will have similar representations</p> <p>A neural network trained on a variety of \(image\, text\) pairs</p> <p>It can be instructed in natural language to predict the most relevant text snippet\, given an image\, without directly optimizing for the task</p> <p>Zero-shot - model attempts to predict a class it saw zero times in the training data</p> <p>An image encoder and a text-encoder</p> <h3 id="attention">Attention</h3> <p>For conditioning the generation process</p> <p>The prior is often either a text\, an image\, or a semantic map</p> <p>A <em>Transformer</em> network encodes the condition text/image into a latent embedding which is in turn mapped to the intermediate layers of the U-Net via a cross-attention layer</p> <p>Attention mechanism will learn the best way to combine the input and conditioning inputs in this latent space</p> <p>These merged inputs are now the initial noise for the diffusion process.</p> <h2 id="experiments">Experiments</h2> <h3 id="perceptual-compression-tradeoffs">Perceptual Compression Tradeoffs</h3> <p>The downsampling factor in the universal autoencoder has to be chosen</p> <p>Low downsampling factors \(1\, 2\) result in slow training progress - the diffusion model has to do the compression job</p> <p>High factors cause stagnating fidelity after comparably few training steps - sample quality is limited due to information loss</p> <p>Factors 4\, 8 and 16 strike a good balance</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation19.png" alt=""></p> <h3 id="image-generation">Image Generation</h3> <p>This model has half the parameters and requires 4 times lesser computation resources!</p> <p>Precision and recall to assess the mode-coverage</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation20.png" alt=""></p> <hr> <p>Precision and recall estimated by nearest neighbours</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation21.png" alt=""></p> <h3 id="conditional-tasks">Conditional Tasks</h3> <p><strong>Transformer Encoders for LDMs</strong></p> <p>For text-to-image generation\, BERT-tokenizer is used to infer a latent code that is mapped into the U-Net via cross-attention</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation22.png" alt=""></p> <p><strong>Image-to-Image translations</strong></p> <p>Semantic representations in the latent spaces are simply concatenated</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation23.png" alt=""></p> <h3 id="super-resolution">Super-Resolution</h3> <p>The low-resolution image is simply concatenated as the conditioned input after bi-cubic interpolation</p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation24.png" alt=""></p> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation25.png" alt=""></p> <h3 id="image-inpainting">Image Inpainting</h3> <p><img src="/assets/img/Stable_Diffusion_Models.assets/KnowDis%20Presentation26.png" alt=""></p> <h2 id="summary">Summary</h2> <ul> <li>Generative Models <ul> <li>Autoregressive Transformers</li> <li>GANs</li> <li>Diffusion Models</li> </ul> </li> <li>DMs <ul> <li>Principles</li> <li>Issues</li> </ul> </li> <li>Latent Diffusion Models <ul> <li>Latent Spaces and Perceptible Compression</li> <li>Cross-attention for multi-modal generation</li> <li>Open Source</li> </ul> </li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Sudhansh Peddabomma. Last updated: October 22, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0K9MLG0V24");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>