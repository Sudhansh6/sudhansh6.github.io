To prevent these limitations, the idea of *disaggregating prefill and decode* phases has been proposed - Essentially, the prefill and decode steps are performed on different GPUs. There is an overhead of communication to transfer the KV cache from prefill phase to the decode phase, but the requests can be processed in parallel increasing the throughput. The result of this can be seen below -

![](/assets/img/2025-01-06-data-systems-for-ml/17419162187482.jpg)

The curve for decoding in the disaggregate system is flatter since there is no prefill phase competing for resources. 

This system became a norm in the past few months. Deepseek-v2 uses prefill-decode disaggregation combined with different parallelisms for prefill and decoding instances. 

Furthermore, continuous batches and disaggregation are actually orthogonal to each other (doesn’t seem so at the first glance)
1. Continuous batching improves GPU utilization and hence the throughput
2. Disaggregation addresses goodput such that SLOs are satisfied

The key insights from CB still carry to disaggregation.

LLM Inference is a very hot research topic right now. We have seen many techniques from this field
1. Scheduling - continuous batching, chunked prefill, disaggregated prefill and decoding
2. Speculative Decoding
3. Memory bottleneck of KV Cache - new attention mechanisms - paged, sparse, etc and sparse KV cache
4. Kernel optimizations

With that, we move to the final topic of this article. 

# FlashAttention

Coming back to the computations in the transformer, we saw that the attention operation is limited by the sequence size (proportional to $$\mathcal O(n^2)$$).

![](/assets/img/2025-01-06-data-systems-for-ml/17419166000781.jpg)

The internal matrices have sizes proportional to square of the sequence length and we have to materialize these matrices. That is, we explicitly compute and store these matrices. Putting this into perspective with numbers we have

For $$b = 1k, n = 32, h = 4k, s= 4k$$,
- Memory = $$bs^2n = 512Gb$$
- Compute = $$4bs^2h = 256 TFLOPs$$

The GPU memory is much more scarce than compute with our current hardware. Is there a way we can reduce the memory usage to improve the attention block?

The implementations before FlashAttention explicitly calculated the internal matrices and moved them between HBM and kernel SRAM. These repeated reads/write add a lot of latency. 

![](/assets/img/2025-01-06-data-systems-for-ml/17419168508774.jpg)

The issue arises due to the softmax operation. It is not trivial to calculate the final softmax output without the intermediate matrices. Furthermore, the backward pass also needs to be done without the intermediate activations. 

We were able to calculate matrix multiplication results without materializing the complete input and output. Can we do the same here?

Let us understand softmax first. Softmax is usually implemented in a *numerically stable* manner. 

$$
y_i = \frac{e^{x_i - \max_{k = 1}^V x_k}}{\sum_{j = 1}^V e^{x_j - \max_{k = 1}^V x_k}}
$$

This introduces another level of complexity for calculating the attention output. We observe the following relation while iterating through the array elements to find the softmax

$$
    \begin{align*}
    d_i &= \sum_{j = 1}^i e^{x_j - m_i} \\
    d_i = d_{i - 1} e^{m_{i - 1} - m_i} + e^{x_i - m_i}
    \end{align*}
$$

Now, we observe that the final output can be represented as a recurrence depending on some *precomputed values*. We move from two loops (one for maximum and one for exponentiation) to one single loop for softmax based on the recurrence relation. It is known as **online softmax** calculation. 

How do we use this to optimize attention? 




If we can find some recursion structure in $$o$$, we can use it to optimize attention by preventing explicit calculation of $$a$$, and fuse the two loops together. 

$$
\begin{align*}
o’_i &= \sum_{j = 1}^i \frac{e^{x_j- m_i}}{d_i’} V[j, :] \\
&= \cdots \\
&= o’_{i - 1} \frac{d’_{i - 1} e^{m_{i - 1} - m_i}}{d’_i} + \frac{e^{x_i - m_i}{d_i’} V[i, :] 
\end{align*}
$$

Voila! That gives us FlashAttention

![](/assets/img/2025-01-06-data-systems-for-ml/17419183745135.jpg)

We never materialize the intermediate results, and transfer the matrices only once. That gives a huge speed up! FlashAttention also has tiling to utilize the GPUs to their fullest. 

What about the backward pass? FlashAttention resorts to recomputation of required results instead of storing the activations, and this still gives a speed up! That is how memory-bottlenecked we are.


Overall, the method offers 104x speed up and 10-20x memory reduction! 

FlashAttention got very successful because it has a cascading effect. Due to this method, we were able to train with larger batch sizes leading to much higher throughput (better AI). Also, the cost of training decreased significantly because we can now turn off gradient check pointing (no need to save memory anymore). 

The method was further developed to FlashAttention2 and FlashAttention3 with more kernel-level optimizations with more aggressive fusion and memory access patterns. The success of FlashAttention is because of its unique insight into an opportunity that was created due to hardware limitations and rise of popularity of self-attention. 


## Connecting everything!

![](/assets/img/2025-01-06-data-systems-for-ml/17419187842427.jpg)

1. Outer Loop 1: There is inter-op parallelism with pipelining such as 1F1B
2. Outer Loop 2: There is intra-op parallelism based on model architecture (Zero-2/3 and data parallelism or Megatron-LM tensor parallelism)
3. Outer Loop 3: Gradient check pointing and recomputation at backward pass
4. Inner Loop 4: Graph Fusion
5. Inner Loop 5: Operator-level optimization: tiling, flash attention, etc

### Deepseek-v3!

Deepseek-v3 was the first **open-source** model that reached the performance of GPT-4o! This was a big feat. 

Furthermore, they also made a bold claim of training the model with 100x lower cost! Many people argued that this is not a responsible claim since most of the budget is usually spent on the planning phase and not on the actual training phase. Previously it was all experimenting, and by the time Deepseek-v3 came into picture, many of these practices were set into stone. Either way, it was a big feat. 

### Model Architecture 

People were familiar with Llama before this, but Deepseek had fundamentally changed things from minute-details to large architectural decisions. Firstly, they went big on **Mixture-of-Experts (MoE)**. Companies usually chose 8-10 MoEs in models, and Deepseek went ahead with 256 experts! This decision while introducing new challenges, significantly made model training and inference much more scalable. 

![](/assets/img/2025-01-06-data-systems-for-ml/17419191877713.jpg)


MoEs typically have much larger number of parameters requiring more communication between GPUs - we need expert parallelism with expert balancing. 

They introduced a new kind of attention known as **Multi-head Latent Attention (MLA)** that reduced the inference cost by a huge amount. 

![](/assets/img/2025-01-06-data-systems-for-ml/17419193632951.jpg)

They introduced a new vector $$c$$ that essentially stores the KV values in a low-dimensional latent space. This trades memory for FLOPs and significantly reduces the KV Cache per token.

![](/assets/img/2025-01-06-data-systems-for-ml/17419195081438.jpg)

They claim that MLA has much stronger ML capabilities.

Another optimization in the model architecture is **multi-token prediction**. Inspired from the EAGLE series, they introduced speculative decoding in the pre-training phase. EAGLE claims 50% acceptance rate and the introduction of the speculative decoding into pre-training increases it to 80-85% acceptance! We essentially are getting a free speculation head. 

### System optimization

US infamously restricted the imports of the top NVIDIA GPUs (H100) to China, and Deepseek had to rely on a less powerful variant (H800) that does not have NVLink. Without NVLink, the communication overhead increases significantly. As a result, Deepseek cannot use tensor parallelism (all-reduce is $$N\times$$ more complex than all-to-all). So they relied on pipeline parallelism, data parallelism and expert parallelism. 

We have also seen the issues with expert parallelism - need to have a good balancing. Typically, systems add an expert balancing loss. Instead, Deepseek argues that this loss hurts the pre-training and they use an alternative auxiliary-loss-free bias term.

They use a sequence level balancing loss that encourages the expert load on each sequence to be balanced. (They said no loss but use another loss).

So, overall, they have 16 way pipeline parallelism, 64 way expert parallelism, and ZERO-based data parallelism. Assuming they have 2048 GPUs (which they claimed), they have 16 PP, 64 EP and 2DP for a total of 2048 GPUs. Note that in each stage of inter-op parallelism, we need two all-to-all for EP and only one all reduce for 2DP. 

![](/assets/img/2025-01-06-data-systems-for-ml/17419200930191.jpg)
For pipeline parallelism, as we saw before, they introduced a new parallelism strategy on top of Chimera. Chimera required two copies of model weights in each rank. Chimera also has all-to-all communications that introduce some overhead. Let us look at the pipeline again -

![](/assets/img/2025-01-06-data-systems-for-ml/17419202751568.jpg)


To top all this, they made their own All-to-all communication kernel that does some extreme optimizations. 


Phew! 


Final set of innovations 
- They made their own mixed-precision training standard different from the rest of the industry that reduced the usage from $$16N$$ (we’ve seen this before) to $$13N$$ using FP8 tensor core.
- Even more fine-grained quantization for FP8 kernels
- Prefill-decode disaggregation based inference. 

So, that is why Deepseek is big. 

# Conclusion

LLMs are big, we’ve said this too many times now. But LLM systems is getting even bigger, and there are innovations happening much much faster in all fields both because of new paradigms and AI helping humans becoming more productive. All this article content can be replaced in 1-2 years. So, us, as researchers or developers, need to be able to identify the right problems, the ability to understand trends and the ability to predict the future!  Identify the right papers and invest in the right companies. 