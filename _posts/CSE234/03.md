Strides can separate the underlying storage and the view of the tensor. Consider the following operations
1. `slice` - simply changing the offsets and shape will output the slice without any copying involved.
2. `transpose` - modifying strides will transpose the tensor without any copying! For example, consider the following example
    ```python
        M.strides() # (24, 12, 4, 1)
        M.permute((1, 2, 3, 0)
        M.t.strides() # (12, 4, 1, 24)
    ```
3. `broadcast` - Suppose we have to extend a tensorâ€™s data across a dimension for performing operations with another tensor, then by simply adding `0` stride in the appropriate dimensions would be enough! Again, no copying

Many more operations can be done without copying the data and simply modifying the strides. For example, consider the following example -

![](assets/17370816868291.jpg)

However, strides also has an issue - Memory access may become non-contiguous, and many vectorized ops require continuous storage.

### Summary
To make operators efficient, we have seen the following tactics -
1. Vectorization - leverage platform-specific vectorized functions that reduce seek time
2. Data layout - strides format that allow zero-copies enabling fast array-manipulations
3. Parallelization on CPUs

These were techniques for general operations. However, we can optimize certain operators with their special properties.

### Matmul optimization
The brute-force approach takes $$\mathcal O(n^3)$$. The best approach humans know is $$\mathcal O(n^{2.371552})$$! 

How to improve the speed in practice then? Recall that we are trying to increase AI = #ops/#bytes.

> **Memory Hierarchy**
>  If everything ran on registers, things would be super-fast. But, that is expensive.
>  Remember that L1-Cache has 0.5ns latency, L2-Cache has 7ns and DRAM has 200ns (400x slower!)

Let us analyze the AI of `matmul` considering the different layers of memory 
1. We can directly move data to registers in every iteration in inner loop

## GPUs and accelerators
Recall that parallelizing operations across threads is super useful! CPUs have some level of parallelism through SIMD operations (vectorization) but they are limited. Building on the same idea, GPUs were born.

When we started out, the ALU units were limited by the physical space on the chips. As technology improved, we moved from 70nm process all the way 3nm process! That is, we can fit up to 20x more cores in the same area! The majority of the area on CPUs is consumed by Control and Cache, and Jensen thought, ditch those and put cores.

Graphical Processing Unit (GPU) are tailored for matrix or tensor operations. The basic idea is to use tons of ALUs (weak but specialized) with massive parallelism (SIMD on steroids).

There are other hardware accelerators like Tensor Processing Unit (TPU) or Application specific integrated circuit (ASIC), etc. The common theme across all these is the same - there are specialized cores. What are specialized cores? They can only compute certain computations. Specialized cores can be super powerful - 
![](assets/17370849151083.jpg)

Companies also tried reducing precision and maintain the same performance. Additionally, they also tune the distribution of different components for specific workloads.

> Why does quantization work in ML systems? 

