<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://sudhansh6.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sudhansh6.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-22T07:58:24+00:00</updated><id>https://sudhansh6.github.io/feed.xml</id><title type="html">Sudhansh Peddabomma</title><entry><title type="html">Machine Learning Systems</title><link href="https://sudhansh6.github.io/blog/machine-learning-systems/" rel="alternate" type="text/html" title="Machine Learning Systems"/><published>2025-01-16T00:00:00+00:00</published><updated>2025-01-16T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/machine-learning-systems</id><content type="html" xml:base="https://sudhansh6.github.io/blog/machine-learning-systems/"><![CDATA[<blockquote> <p>Acknowledging use of Perplexity, ChatGPT and the reference books and papers to aid with the content.</p> </blockquote> <h1 id="introduction">Introduction</h1> <p>As with every other article, this one starts with the same remark too - We are at an inflection point in human history. Artificial Intelligence (AI) is everywhere, and its growth has been unparalleled. We are in the AI revolution.</p> <p>At times like these, it is important to think about the future - how do we create systems that work alongside humanity to tackle the biggest problems life faces? We must master a new field of engineering to maintain this unprecedented pace.</p> <h2 id="history-of-ai">History of AI</h2> <p><img src="assets/reading01/2025-01-16-17-56-14-image.png" alt=""/></p> <p>Starting with symbolic AI, one of the early approaches, STUDENT system developed by Daniel Bobrow in 1964, demonstrated natural language understanding by converting English text into algebraic equations. However, it was a rule-based system that was very brittle to the grammatical structure - although the solution may appear intelligent, it did not have a genuine understanding of language or the actions it was taking. This issue is the primary problem behind rule-based approaches.</p> <p>Then followed expert systems in 1970s, which focused on domain specific problems over generalized AI. For example, MYCIN developed by Stanford showed a major leap in medical AI with it’s 600 rules-based system. Nevertheless, scaling these rules with newer discoveries is infeasible.</p> <h3 id="statistical-learning">Statistical Learning</h3> <p>In the 1990s, something radical happened - the field moved away from rules to data-driven approaches. With the digital revolution, new capabilities got unlocked. From regression to neural networks, we allowed machines to discover patterns in the data leading to diverse applications. This new paradigm changed the way we approached machine learning. Quality of data, metrics to evaluate performance and trade-offs in design choices - all became relevant research questions.</p> <p>During the 2000s, algorithms like decision trees and KNNs made their way into practical applications. SVMs with their “kernel trick” became very popular. Human engineered features combined with statistical learning was the characteristic in the algorithms, and they had strong mathematical foundations as well. The models performed well with limited data, and produced reliable results. Even modalities like face detection with Viola-Jones algorithm (2001) became feasible.</p> <h3 id="deep-learning">Deep Learning</h3> <p>A double-edged sword, deep learning became the new kid in the block since the 2010s. Deep learning networks automatically discovered features in the data, doing away with feature engineering. Starting with AlexNet in 2012, the successes with deep learning models have never been shy. Researchers realized that bigger models equals better performance. Now, in the 2020s, we have entered the age of large models which have parameters reaching into few hundreds of billions! The datasets are well into the Petabytes stage.</p> <p>The three pillars required for these models to be successful, big data, better compute, and algorithmic breakthroughs, have successfully been put in place over the past few decades.</p> <p>These new depths have raised important questions about the future: how do we store and serve such models and datasets?!</p> <h2 id="ml-systems-engineering">ML Systems Engineering</h2> <p>It is the discipline of designing, implementing, and operating AI systems across computing scales. A machine learning system is an integrated computing system with three main parts - data, models and the compute infrastructure.</p> <p>Software Engineering as a field has been well established over the past decade. Even though the field is not yet mature, it has practices in place to enable reliable development, testing and deployment of software systems. However, these ideas are not entirely applicable to Machine Learning systems. Changes in the data distribution can alter the system behavior - this is a new paradigm we have not addressed before.</p> <p>More than often, the performance requirements guide the design decisions in the architecture. The complexities arise due to the broad spectrum across which ML is deployed today - from edge-devices to massive GPU-cloud clusters, each presents unique challenges and constraints. Operational complexities increase with system distribution. Some applications may have privacy requirements. The budget of the system acts as an important constraint. All these tradeoffs are rarely simple choices.</p> <p>Modern ML systems must seamlessly connect with existing software, process diverse data sources, and operate across organizational boundaries, driving new approaches to system design. FarmBeats by Microsoft, Alphafold by Deepmind and Autonomous vehicles are excellent examples of how proper systems in place can really push the extent of ML applicability.</p> <h2 id="challenges">Challenges</h2> <ol> <li> <p>Data Challenges - How do we store and process different kinds of data, and how to accommodate patterns with time?</p> </li> <li> <p>Model Challenges - How do we create efficient systems for different forms of learning, and test their performance across a wide range of scenarios?</p> </li> <li> <p>Systems Challenges - How do we set up pipelines in place to combine all of this in place? Systems that have monitoring and stats, that allow model updates and handle operational challenges.</p> </li> <li> <p>Ethical and Social Considerations - How do we address bias in such large-scale models? Can we do something about the “black-box” nature? Can we handle data privacy and handle inference attacks?</p> </li> </ol> <p>A major solution to address all these challenges has been to <em>democratize AI technology</em>. Similar to an “all hands-on deck” solution, with the involvement of a large amount of people in this evolution, we are tackling one of the most innovative problem’s humanity has ever faced - how do we achieve AGI?</p> <h1 id="dnn-architectures">DNN Architectures</h1> <p>Assuming the reader’s know enough about different model architectures, we shall now discuss the core computations involved in these models to design the systems around them.</p> <h2 id="architectural-building-blocks">Architectural Building Blocks</h2> <p>So far, we have the following major architectures summarized below -</p> <h3 id="multi-layer-perceptrons-mlps">Multi-Layer Perceptrons (MLPs)</h3> <ul> <li> <p>Purpose: Dense pattern processing</p> </li> <li> <p>Structure: Fully connected layers</p> </li> <li> <p>Key operation: Matrix multiplication</p> </li> <li> <p>System implications:</p> <ul> <li>High memory bandwidth requirements</li> <li>Intensive computation needs</li> <li>Significant data movement</li> </ul> </li> </ul> <h3 id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</h3> <ul> <li> <p>Purpose: Spatial pattern processing</p> </li> <li> <p>Key innovations: Parameter sharing, local connectivity</p> </li> <li> <p>Core operation: Convolution (implemented as matrix multiplication)</p> </li> <li> <p>System implications:</p> <ul> <li>Efficient memory access patterns</li> <li>Specialized hardware support (e.g., tensor cores)</li> <li>Opportunity for data reuse</li> </ul> </li> </ul> <h3 id="recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</h3> <ul> <li> <p>Purpose: Sequential pattern processing</p> </li> <li> <p>Key feature: Maintenance of internal state</p> </li> <li> <p>Core operation: Matrix multiplication with state update</p> </li> <li> <p>System implications:</p> <ul> <li>Sequential processing challenges</li> <li>Memory hierarchy optimization for state management</li> <li>Variable-length input handling</li> </ul> </li> </ul> <h3 id="transformers-and-attention-mechanisms">Transformers and Attention Mechanisms</h3> <ul> <li> <p>Purpose: Dynamic pattern processing</p> </li> <li> <p>Key innovation: Self-attention mechanism</p> </li> <li> <p>Core operations: Matrix multiplication and attention computation</p> </li> <li> <p>System implications:</p> <ul> <li>High memory requirements</li> <li>Intensive computation for attention</li> <li>Complex data movement patterns</li> </ul> </li> </ul> <p>Some innovations like skip connections, normalization techniques, and gating mechanisms are highlighted as important building blocks that require specific architectures. With these in mind, we require the following primitives -</p> <ul> <li> <p><strong>Core Computational Primitives</strong> -Matrix multiplication, Sliding window operations, Dynamic computation</p> </li> <li> <p><strong>Memory Access Primitives</strong> - Sequential access, Strided access, Random access</p> </li> <li> <p><strong>Data Movement Primitives</strong> - Broadcast, Scatter, Gather and Reduction</p> </li> </ul> <p>We address these primitives, by designing efficient systems as well as hardware.</p> <ul> <li> <p><strong>Hardware</strong> - Development of specialized processing units (e.g., tensor cores), Complex memory hierarchies and high-bandwidth memory, Flexible interconnects for diverse data movement patterns</p> </li> <li> <p><strong>Software</strong> - Efficient matrix multiplication libraries, Data layout optimizations, Dynamic graph execution support</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <p>In summary, understanding the relationship between high-level architectural concepts is important for their implementation in computing systems. The future advancements in deep learning will likely stem from both novel architectural designs and innovative approaches to implementing and optimizing fundamental computational patterns.</p> <p>Now that we have understood the basics of Machine Learning Systems, let us delve into the two biggest frameworks that support the ML systems today.</p> <h1 id="tensorflow-a-system-for-large-scale-machine-learning"><a href="https://arxiv.org/pdf/1605.08695">TensorFlow: A system for large-scale machine learning</a></h1> <p>A product of Google brain built to tackle large scale systems in heterogeneous environments. TensorFlow uses data-flow graphs to represent computations, shared states and operations. These can be mapped across many machines giving a flexibility to the developers.</p> <p>TensorFlow is based on a previous product of Google Brain, DistChild, that was used for a large number of research and commercial tasks. The team recognized the recent advances in ML - CNNs that broke records reaching up to millions of parameters and GPUs that accelerated the training processes. They developed a framework that is well-optimized for general scenarios for both training and inference. They also meant for it to be <em>extensible</em> - to allow ability to experiment and scale in production with the same code.</p> <p>Prior to this work, Caffe, Theano and Torch were the major frameworks. Caffe was known for its high-performance, Theano for its data flow model and Torch for its imperative programming model. Along with these works, TensorFlow also draws inspiration from the map-reduce paradigm that improved the performance of data systems significantly.</p> <h2 id="tensorflow-execution-model">TensorFlow execution model</h2> <p>The computation and state of the algorithm, including the mathematical operations are represented in a single dataflow graph. The communications between the subcomputations are made explicitly to execute independent computations in parallel and partition the computation across distributed devices. The key point to note here is that the graph has mutable states for the individual vertices, meaning that data can be shared between different executions of the graph. Although this point may seem trivial now, architectures prior to TensorFlow worked with static computations graphs that did not allow changes to the weights - algorithms such as mini-batch gradient descent were not scalable to large parameter models. Due to this change in the architecture, developers can experiment with different optimization algorithms, consistency schemes and parallelization strategies.</p> <h3 id="dataflow-graph-elements">Dataflow graph elements</h3> <p>Each vertex in the dataflow graph is an atomic unit of computation and each edge is the output or input to a vertex - values flowing through tensors.</p> <p><strong>Tensors</strong> - Notably, <em>tensors</em> as a computational quantity were introduced in this paper. Their work assumes all tensors are dense - this allows simple implementations for memory allocation and serialization.</p> <blockquote> <p>Modern neural networks on the contrary have sparse tensors in many scenarios - can be optimized greatly</p> </blockquote> <p>TensorFlow does allow representing sparse tensors but at the cost of more sophisticated shape inference.</p> <p><strong>Operations</strong> - An operation can simply be thought of as a function that takes \(m \geq 0\) tensors as input and returns \(n \geq 0\) tensors as output. The number of arguments to these operators can be constant or variable (<em>variadic</em> operators). <strong>Stateful operations</strong> (variables and queues) contain mutable states that can be updated every time the operator executes. Variables are mutable buffers storing shared parameters and queues support advanced forms of coordination.</p> <h3 id="partial-and-concurrent-execution">Partial and concurrent execution</h3> <p>The key advantage of storing the dataflow as a graph is the ability to execute independent operations in parallel. Once a graph is defined by the user, the API allows for executing any sub-graph the user queries. Each invocation is a <em>step</em> and TensorFlow supports multiple <em>concurrent steps</em>. This ability shines for the batch-processing workflows in neural networks. Furthermore, TensorFlow has a checkpointing subgraph that runs periodically for fault tolerance.</p> <p>This functionality of running graphs partially and concurrently contribute much to TensorFlow’s flexibility.</p> <h3 id="distributed-execution">Distributed execution</h3> <p>Since the communication between subcomputations is explicit, the distribution of the dataflow becomes simpler. Each operation resides on a particular <em>device</em> (note that this feature has also been adapted in PyTorch), and a device is responsible for executing a <em>kernel</em> for each operation assigned to it. TensorFlow allows multiple kernels to be registered for a single operation.</p> <p>The placement algorithm computes a feasible set of devices for each operation, calculates the sets of operations that must be colocated and selects a satisfying device for each colocation group. In addition, the users can also specify their device preferences for a particular task. <em>Yes, TensorFlow was advanced since the beginning</em>.</p> <p>Once the operations are placed, the partial subgraphs are computed for a step, and are pruned and partitioned across the devices. The communication mechanisms between devices is also put in place with specialized implementations to optimize for latency with large-subgraphs running repeatedly. On the user-end, a <em>client session</em> maintains a mapping from step definitions to cached subgraphs. This computation model performs the best with static, reusable graphs but also supports dynamic computations with control flows.</p> <h3 id="dynamic-control-flow">Dynamic control flow</h3> <p>Although most evaluation in TensorFlow is <em>strict</em>, wherein it requires for all inputs to an operation to be computed before the operation executes, advanced algorithms (such as RNNs), require dynamic control flow, requiring non-strict evaluation. To aid with this, TensorFlow also supports primitive <strong>Switch</strong> (demultiplexer) and <strong>Merge</strong> (multiplexer) operations for dynamic dataflow architectures! These primitives can be used to build a non-strict conditional sub-graph that executes one of two branches based on the runtime values of a tensor. These primitives also support loops with additional structural constraints based on the dataflow!</p> <h2 id="extensibility">Extensibility</h2> <ul> <li> <p><strong>Differentiation</strong> - TensorFlow has a user-level library that <strong>automatically differentiates</strong> expressions. It performs a breadth-first search to identify all of the backwards paths from the target operation (loss function) to a set of parameters in the computation graph, and sums the partial gradients that each path contributes. With optimizations such as batch normalization and gradient clipping, the algorithm also supports backpropagation through conditional and iterative subcomputations! All of this done with memory management on GPU.</p> </li> <li> <p><strong>Optimization</strong> - SGD is a simple algorithm encoded in the framework. However, for more advanced optimization schemes like Momentum, TensorFlow relies on community driven implementations that are easily pluggable without modifying the underlying system. Such a modular framework was not available before.</p> </li> <li> <p><strong>Handling Large models</strong> - Even back then, the language models were too large to store in RAM of a single host. For the language specific case, TensorFlow has <em>sparse embedding layers</em> that is a primitive operation that abstracts storing and reading a tensor across different memory spaces. They are implemented with operators such as <code class="language-plaintext highlighter-rouge">gather</code>, <code class="language-plaintext highlighter-rouge">part</code> and <code class="language-plaintext highlighter-rouge">stitch</code>, and their gradients are also implemented. Along with innovations such as Project Adam, TensorFlow’s training was ultimately made efficient through community driven improvements.</p> </li> <li> <p><strong>Fault Tolerance</strong> - Since many learning algorithms do not require consistency and writing at every step is compute intensive, TensorFlow implements user-level checkpointing for fault tolerance. This design decision leaves it to the user to build their own best fault handling mechanisms. <em>I wish they had an automatic version as well</em>.</p> </li> <li> <p><strong>Synchronous replica coordination</strong> - SGD is robust to asynchrony, and TensorFlow is designed for asynchronous training. In the asynchronous case, each worker reads the current value when the step begins, applies its gradient to the different current value at the end. The synchronous cases use queues to coordinate execution, allowing multiple gradient updates together. Although the throughput is reduced, this way of training is more efficient. TensorFlow implements <em>backup workers</em> to improve the throughput of this synchronous case by 15%.</p> </li> </ul> <h2 id="implementation">Implementation</h2> <p>The core TensorFlow library is implemented in C++ for portability and performance with its implementation being open-source. It consists of</p> <ul> <li> <p>Distributed master - given a graph and a step definition, it prunes and partitions the graphs to each devices, and caches these subgraphs so that they may be reused in subsequent steps.</p> </li> <li> <p>Dataflow executor - Schedules the execution of the kernels that comprise a local subgraph - optimized for running large fine-grained graphs with low overhead.</p> </li> </ul> <p>The API can be accessed both via C++ and Python.</p> <p>The library does not provide huge gains for single-system training but has higher throughput for large models across multiple devices.</p> <h2 id="conclusion-1">Conclusion</h2> <p>When this paper was published, TensorFlow was still a work in progress. Later, the library transformed significantly with the introduction of v2, and other optimizations.</p> <h1 id="pytorch-an-imperative-style-high-performance-deep-learning-library"><a href="https://arxiv.org/abs/1912.01703">PyTorch: An Imperative Style, High-Performance Deep Learning Library</a></h1> <p>A product of Facebook research, PyTorch is the most popular deep-learning library. They addressed the biggest limitation in previous frameworks - usability. They targeted both performance and usability by designing an imperative-style ML framework.</p> <p>In contrast to PyTorch, the previous approaches created a static dataflow graph that represents the computation. Since the whole computation is visible ahead of time, it can be leveraged to improve performance and scalability. However, due to this, the usability is reduced and cannot be used to iteratively build experiments. PyTorch, a python library, performs immediate execution of dynamic tensor computations with automatic differentiation and GPU acceleration while maintaining comparable performance to the static libraries.</p> <h2 id="background">Background</h2> <p>There were four major trends in scientific computing that have become important for deep learning:</p> <ol> <li>Development of domain-specific languages and libraries for tensor manipulation (e.g., APL, MATLAB, NumPy made array-based productive productive)</li> <li>Automatic differentiation, making it easier to experiment with different machine learning approaches</li> <li>Shift towards open-source Python ecosystem from proprietary software. The network effects of Python contributed to its exponential growth.</li> <li>Availability of general-purpose parallel hardware like GPUs</li> </ol> <p>PyTorch builds on these trends by providing an array-based programming model accelerated by GPUs and differentiable via automatic differentiation integrated in the Python ecosystem.</p> <h2 id="design-principles">Design Principles</h2> <p>PyTorch’s design is based on four main principles:</p> <ol> <li>Be Pythonic: Integrate naturally with Python ecosystem, keep interfaces simple and consistent</li> <li>Put researchers first: Make writing models, data loaders, and optimizers easy and productive</li> <li>Provide pragmatic performance: Deliver compelling performance without sacrificing simplicity</li> <li>“Worse is better”: Prefer simple, slightly incomplete solutions over complex, comprehensive designs</li> </ol> <h2 id="usability-centric-design">Usability-centric Design</h2> <p>The approach to PyTorch starts by considering deep-learning models as just another Python program. By considering so, PyTorch maintains the imperative programming model inspired from Chainer and Dynet. Defining layers, composing models, loading data, running optimizers and parallelizing the training process can all be expressed using familiar Python syntax. It allows any new potential neural network architecture to be easily implemented with composability.</p> <p>Mainly, since PyTorch programs execute eagerly, all features of Python like print, debugging and visualization work as expected. In addition, PyTorch has -</p> <ul> <li> <p><strong>Interoperability</strong> - Integrated well with other libraries to allow bidirectional exchange of data (NumPy, Matplotlib, etc).</p> </li> <li> <p><strong>Extensibility</strong> - Supports custom differentiable functions and datasets. The abstractions take care of shuffling, batching, parallelization, and management of pinned CUDA memory to improve the throughput and performance. In general, PyTorch components are completely interchangeable!</p> </li> </ul> <h3 id="automatic-differentiation">Automatic differentiation</h3> <p>Python is a dynamic programming language that has most of the behavior defined at run-time, making it difficult to pre-calculate the differentiation. Instead, PyTorch uses operator overloading approach to build up a representation of the computed function every time it is executed. It notes the difference between forward mode and backward mode automatic differentiation and adopts the latter which is better suited for ML applications.</p> <p>Their system can differentiate through code with mutation on tensors as well. They also have a versioning system for tensors as a failsafe to track the modifications.</p> <h2 id="performance-focused-implementation">Performance-focused Implementation</h2> <p>With all these considerations to usability, the developers implemented many tricks to maintain the performance of the library. Since the models have to run on Python interpreter, which has its own limitations such as the global interpreter lock (ensures only one of any concurrent threads is running at a given time), PyTorch optimized every aspect of execution and also enabled users to add their own optimization strategies. Prior frameworks avoided these constraints by deferring the evaluation to their custom interpreters.</p> <p>Most of PyTorch is implemented C++ for high performance. The core <code class="language-plaintext highlighter-rouge">libtorch</code> library implements the tensor data structure, GPU and CPU operators, automatic differentiation system with gradient formulas for most built-in functions and basic parallel primitives. The pre-computed gradient functions allow computation of gradients of core PyTorch operators in a multithreaded evaluation evading the Python global interpreter lock. The bindings are generated using YAML meta-data files, that allowed the community to create bindings to other languages.</p> <p>PyTorch separates the control (program branches, loops) and the data flow (tensors and the operations). The Control flow handled by Python and optimized C++ code on CPU and Data flow can be executed on CPU or GPU. PyTorch is designed to run asynchronously leveraging the CUDA stream mechanism. This allows overlap of Python code execution on CPU with tensor operations on the GPU, effectively saturating GPU with large tensor operations. The main performance cover-up comes from this design.</p> <p>Since every operator needs to allocate an output tensor to hold the results, the speed of <em>dynamic memory allocators</em> needs to be optimized. CPU has efficient libraries to handle this. However, to avoid the bottleneck of <code class="language-plaintext highlighter-rouge">cudaFree</code> routine that blocks code until all previously queued work on GPU completes, PyTorch implements its custom allocator that incrementally builds up a cache of CUDA memory. It reassigns it to later allocations without CUD APIs for better interoperability allowing users to use other GPU enabled Python packages. This allocator is further optimized with memory usage patterns and its implementation is simplified with the <em>one-pool-per-stream</em> assumption.</p> <p>The <code class="language-plaintext highlighter-rouge">multiprocessing</code> library was developed to evade the global interpreter lock on Python. However, this is inefficient for large arrays, so it is extended as <code class="language-plaintext highlighter-rouge">torch.multiprocessing</code> to allow automatic movement of data to shared memory improving the performance significantly. It also transparently handles sharing of CUDA tensors to build analytical systems on top of this.</p> <p>Since users write models to consume all the memory resources, PyTorch treats memory as a scarce resource and handles it carefully. The overheads of garbage collection are too large. To solve this, PyTorch relies on a reference counting scheme to track the number of uses of each tensors, and frees the underlying memory <em>immediately</em> when this count reaches zero. This ensures immediate freeing of memory when tensors become unneeded.</p> <p>With all these optimizations, PyTorch achieves</p> <ul> <li> <p>ability to asynchronously execute dataflow on GPU with almost perfect device utilization</p> </li> <li> <p>custom memory allocator showing improved performance</p> </li> <li> <p>performance within 17% of the fastest framework across all benchmarks</p> </li> </ul> <h2 id="conclusion-2">Conclusion</h2> <p>The paper concludes by highlighting PyTorch’s success in combining usability with performance. Future work includes:</p> <ul> <li>Developing PyTorch JIT for optimized execution outside the Python interpreter</li> <li>Improving support for distributed computation</li> <li>Providing efficient primitives for data parallelism</li> <li>Developing a Pythonic library for model parallelism based on remote procedure calls</li> </ul> <p>##</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Few keypoints from the amazing book at mlsysbook.ai and other important papers in the field.]]></summary></entry><entry><title type="html">Data Systems for Machine Learning</title><link href="https://sudhansh6.github.io/blog/data-systems-for-ml/" rel="alternate" type="text/html" title="Data Systems for Machine Learning"/><published>2025-01-06T00:00:00+00:00</published><updated>2025-01-06T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/data-systems-for-ml</id><content type="html" xml:base="https://sudhansh6.github.io/blog/data-systems-for-ml/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Machine Learning systems have played a pivotal role in the rapid adaptation of Ai in the world today. This domain is essential for solving future problems and also making the current architectures more efficient. This is crucial considering that companies are reactivating nuclear power plants to power AI in the real-world.</p> <p>Along with the progress in AI from small neural networks to large language models, there has been a development in the size of datasets as well. Big data arrived, and AI today relies on these internet-scale datasets. After all, doesn’t ChatGPT just do pattern-matching in the internet?</p> <p>Moreover, the compute capabilities have been scaling exponentially. Just last year (2024), NVIDIA released a new super-chip architecture <a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing">Blackwell</a> that has 97 billion transistors that can reach up to 1.4 exa-flops! The largest super-computer was barely able to reach 1 exa-flop. All this power in the palm of your hand…</p> <p>Richard Sutton once said, search and learning can scale unparalleled with growing computation power.</p> <p>Consider the year 2012 — AlexNet made waves showing SOTA capabilities with images. They use Stochastic Gradient Descent, dropout, convolution networks and initialization techniques. Without ML systems (CUDA, etc), the code would have been 44,000 lines with days of training! With these systems (Jax, PyTorch, TensorFlow) in place, you can achieve the same result in 100 lines within hours of training.</p> <h3 id="in-practice">In Practice</h3> <p>In industry, problems are typically of the form - improve the self-driving car’s pedestrian detection to be X-percent accurate at Y-ms latency budget. For an ML engineer is, the general approach is to design a better model with better learning efficiency followed by hyper-parameter running, pruning, distillation. An ML systems engineer would take the best model by ML researchers, specialize the implementation to target the H/W platform to reduce latency. <em>Streamlining the entire process from development to deployment</em>.</p> <h2 id="overview">Overview</h2> <p>From ad-hoc methods having diverse models and optimization algorithms with various data pre-processing techniques - we have arrived at an optimal algorithm that is <em>iterative and convergent</em>. As our models have become more and more specialized, the computation resources scaled exponentially.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17363053041704.jpg" alt=""/></p> <p>Through the course of this article, we will cover deep learning basics, computational graphs, Autodiff, ML frameworks, GPUs, CUDA and collective communication.</p> <p>There are more related topics to the ones discussed here</p> <ul> <li>ML for systems</li> <li>ML Hardware design</li> </ul> <p>Unfortunately, the textbook for this content is just miscellaneous research papers.</p> <h1 id="background">Background</h1> <h2 id="dl-computation">DL Computation</h2> <p>The idea is to concatenate composable layers</p> \[\theta^{(t + 1)} = f(\theta^{(t)}, \nabla_L(\theta^{(t)}, D^{(t)})\] <p>A <strong>model</strong> is a parameterized function that describes how we map inputs to predictions. The parameters are optimized using optimization methods like <strong>SGD</strong>, Newton methods, etc. A <strong>loss function</strong> guides the model to give feedback on how well the model is performing.</p> <p>Having these basic definitions, we will build abstractions to map all the models being used today. It is not possible to build systems to support all models. A quick refresher of important models</p> <ul> <li><strong>CNNs</strong> - Learnable filters to convolute across images to learn spatial features. The top 3 breakthrough architectures were - AlexNet, ResNet, U-Net. What are the important components in CNNs? <ul> <li>Convolution (1D, 2D, 3D)</li> <li>Matmul</li> <li>Softmax</li> <li>Element-wise operations - ReLU, add, sub, pooling, normalization, etc.</li> </ul> </li> <li><strong>Recurrent Neural Networks</strong> - Many problems in nature are many-to-many. RNNs maintain an internal state that is updated as a sequence is processed. Arbitrary inputs and outputs can be generated, and any neural network can be used in the RNN architecture. The top 3 breakthrough architectures were - Bidirectional RNNs, LSTMs, GRU. What are the important components in RNNs? <ul> <li>Matmul</li> <li>Element-wise non-linear - ReLU, Sigmoid, Tanh</li> <li>Underlying MLP RNNs have a problem of forgetting (\(0.9*0.9*… \approx 0\)). Additionally, they lack <strong>parallelizability</strong> - both forward and backward passes have \(O(sequence length)\).</li> </ul> </li> <li><strong>Transformers</strong> (Attention + MLP) - Treat representations of each element in the sequences as queries to access and incorporate information from a set of values. Transformers have an encoder part (BERT most famous) and a decoder part (GPT most famous). Along with these, DiT is one of the top 3 models. What are the important components in Transformers? <ul> <li>Attention - Matmul, softmax, Normalization</li> <li>MLP</li> <li>Layernorm, GeLU, etc.</li> </ul> </li> <li><strong>Mixture of Experts</strong> - Voting from many experts is better than one expert. Latest LLMs are mostly MoEs - Grok, Mixtral, Deepseek-v3. A router (Matmul, softmax) is the novel component in MoE - it makes system design difficult.</li> </ul> <h2 id="machine-learning-systems">Machine Learning Systems</h2> <p>As mentioned before, the three pillars for the systems are data, model and compute. The foal is to express as manny as models as possible using one set of programming interface by connecting math primitives.</p> <h3 id="computational-dataflow-graph">Computational Dataflow Graph</h3> <p>A representation to show data flow in programs. A <strong>node</strong> represents the computation (operator) and an <strong>edge</strong> represents the data dependency (data flowing direction). A node can also represent the input/output tensor of the operator.</p> <h3 id="example-deep-learning-with-tensorflow-v1">Example: Deep learning with TensorFlow v1</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tinyflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span> <span class="c1"># Forward declaration 
</span><span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span><span class="n">tf</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">reduction_indices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># Loss function declaration 
</span><span class="n">W_grad</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">,</span> <span class="p">[</span><span class="n">W</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Automatic differentiation
</span><span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">assign</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">W_grad</span><span class="p">)</span> <span class="c1"># SGD update rule 
</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">})</span> <span class="c1"># Real-execution happens here
</span></code></pre></div></div> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17364786998993.jpg" alt=""/></p> <p>This DAG representation opens up all possibilities of optimizations. However, creating such a graph doesn’t allow flexibility - once a graph is defined, it cannot be changed based on the input.</p> <h3 id="example-pytorch">Example: PyTorch</h3> <p>PyTorch also uses computational graphs, but it creates it on the fly. Previously, we had defined the graph and then executed it. Symbolic declaration vs imperative programming. Define-then-run vs Define-and-run. C++ vs Python.</p> <p>What are the pros and cons?</p> <table> <thead> <tr> <th> </th> <th>Good</th> <th>Bad</th> </tr> </thead> <tbody> <tr> <td>Symbolic</td> <td>Easy to optimize, much more efficient (can be 10x faster)</td> <td>The way of programming can be counter-intuitive, hard to debug and less flexible</td> </tr> <tr> <td>Imperative</td> <td>More flexible, easy to program and debug</td> <td>Less efficient and more difficult to optimize</td> </tr> </tbody> </table> <p>How does TensorFlow work in Python then? Tensorflow has Python as the interface language.</p> <p>Apart from these two famous frameworks, there were more like Caffe, DyNet, mxnet (has ability to switch between both), etc. Recently, Jax (derived from Tensorflow) has been getting more popular.</p> <h3 id="just-in-time-jit-compilation">Just-in-time (JIT) compilation</h3> <p>Ideally, we want define-and-run during development and define-then-run during deployment. However do we combine both? PyTorch introduced a deploy mode through a decorator <code class="language-plaintext highlighter-rouge">torch.compile()</code>. So is there an issue with JIT? It creates only static graphs, and cannot work with conditionals or loops in the code.</p> <h3 id="static-vs-dynamic-models">Static vs Dynamic models</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17364797895738.jpg" alt=""/> Static graphs are defined and optimized only once. The execution follows a defined computation. On the other hand, dynamic graphs depend on the input. It is difficult to express complex flow-control logic and debug. The implementation is also difficult.</p> <p>As seen above, LSTMs are trying to replace the dynamics in the natural language problem.</p> <p><strong>How to handle dynamics?</strong></p> <ul> <li>Just do Define-and-run and forget about JIT - most popular unforunately :(</li> <li>Introduce Control Flow Ops - <ul> <li>Example: Switch and Merge. This can be added a computational primitive in the graph and introduce dynamics in the graph.</li> <li>These ideas are natural across all programming languages - conditionals and loops. However, the problem with this approach is that graphs becomes complex, and more importantly, how does we do back propagation? What is the gradient of “switch”? TensorFlow team has been working on this.</li> </ul> </li> <li>Piecewise compilation and guards - This approach is better adopted than control flow. <ul> <li>Case 1: A graph accepting input shapes of \([x, c1, c2]\) where \(x\) is variable. The solution is to compile for different values of \(x\) (powers of 2).</li> </ul> </li> </ul> <p>So far, we have seen representations that express the forward computations using primitives. But, how do we represent backward computations?</p> <h1 id="autodiff-ad">Autodiff (AD)</h1> <p>Derivative can be taken using the first order principles. However, this approach can be slow since we have to evaluate the function twice \(f(\theta + \epsilon) , f(\theta)\) and it is also error prone \(\theta(\epsilon^2)\).</p> <p>To optimize the derivative calculation, we pre store the gradients of primitives and map the derivative chain rules in the computational graph. There are two ways of doing this as well</p> <ol> <li>Calculating the derivative from left (inside) to right (outside) in a network - from inputs to outputs</li> <li>Calculating it from right to left - from outputs to inputs</li> </ol> <p>Both are valid approaches and we will discuss them in detail.</p> <h2 id="forward-mode-autodiff">Forward Mode Autodiff</h2> <p>We start from the input nodes, and derive the gradients all the way to the output nodes. <strong>Cons</strong> - - For \(f: R^n \to R^k\), we need \(n\) forward passes to get the gradients with respect to each input. - However, it is usually the case that \(k = 1\) (loss) and \(n\) is very large.</p> <blockquote> <p>If this is confusing, think of it this way - we want the gradient of output with respect to all parameters to update weights. However, forward mode calculates the gradient of inputs with respect to all parameters.</p> </blockquote> <h2 id="reverse-mode-autodiff">Reverse Mode Autodiff</h2> <p>We define the quantity <em>adjoint</em> \(\bar v_i = \frac{\partial y}{\partial v_i}\). We then compute each \(\bar v_i\) in the reverse topological order of the graph. This way, we can simply do one backward pass to get the necessary gradients.</p> <p>In some scientific scenarios, we can have \(k &gt;&gt; n\) where the forward mode can be more efficient.</p> <blockquote> <p>What are the size bounds of the backward graph as compared to the neural network?</p> </blockquote> <p>We construct backward graphs in a symbolic way to reuse it multiple times.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369099776396.jpg" alt=""/></p> <h2 id="backpropagation-vs-reverse-mode-ad">Backpropagation vs. Reverse-mode AD</h2> <p>In old frameworks like Caffe/cuda-convnet, the backward computations were done through the forward graph itself. Newer frameworks like Tensorflow and PyTorch construct the backward graph explicitly. The reasons to do so are -</p> <ol> <li>Explicit graphs allow backward computation with any input values. They have flexibility to even calculate gradient of gradients.</li> <li>Having an explicit backward graph can help optimization!</li> <li>Gradient update rules can be efficiently implemented.</li> </ol> <h2 id="gradient-update-rules">Gradient update rules</h2> <p>Typically done via gradient descent, the weights are updated with the gradients with the following simplified rule</p> \[f(\theta, \nabla_l) = \theta - \eta \nabla_L\] <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369103443367.jpg" alt=""/></p> <h1 id="architecture-overview-of-ml-systems">Architecture Overview of ML systems</h1> <p>The aim is to make the systems fast, scalable, memory-efficient, run on diverse hardware, energy efficient and easy to program/debug/deploy. Phew.</p> <p>We have discussed dataflow and Autodiff graphs. However, there are numerous things that can be added to these - graph optimization, parallelization, runtime memory, operator optimizations and compilation.</p> <h2 id="graph-optimization">Graph Optimization</h2> <p>The goal is to rewrite the original graph \(G\) as \(G’\) that is faster.</p> <p>Consider the following motivating example - Typically, convolution is followed by batch normalization. Instead of performing batch normalization, just update the weights in convolution to do everything in one step!</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369110087495.jpg" alt=""/></p> <p>Note that some steps can become slower based on the hardware, but you get the general idea.</p> <p>Similarly, in attention calculations, the code is typically written with a concatenated vector of queries, keys and values. This version is optimal - it can be understood with <em>Arithmetic Intensity</em> which the ratio of #operations and #bytes. For example, an addition operation has intensity of \(1/3\) (2 loads and one store). However, fusing multiple arithmetic operations reduces the loads and stores by bringing all variables into memory once, improving the arithmetic intensity.</p> <h3 id="so-how-do-we-optimize-graphs">So how do we optimize graphs?</h3> <p>We write rules or templates for opportunities to simplify graphs. There is also implementation of <em>auto-discovering</em> optimizations in the latest libraries, we shall study these.</p> <h2 id="parallelization">Parallelization</h2> <p>The goal is to parallelize the graph computation over multiple devices. Note that devices can be connected with fast (memory communication NVLink) and slow connections (across GPUs), with up to 10x performance difference. Ideally, we do not want to describe partitioning rules for every new model that comes up. Based on these communication patterns, distributing the tasks is not an easy problem. So, we shall discuss how we partition the computational graph on a device cluster.</p> <h2 id="runtime-and-scheduling">Runtime and Scheduling</h2> <p>How do we schedule the compute, communication and memory in a way that execution is as fast as possible, communication is overlapped with compute and is subject to memory constraints?</p> <h2 id="operator-implementations">Operator Implementations</h2> <p>The goal is this layer is to get the fastest possible implementation of <code class="language-plaintext highlighter-rouge">matmul</code>s, for different hardware, different precision and different shapes.</p> <p>NVIDIA releases a GPU every 2 years, and they have rewrite all operations every time! Notably, previously, models were trained using 32-bit floating points, but now researchers are emphasizing on lower and lower precisions.</p> <p>Now, we shall delve into each of these architectures.</p> <h1 id="operator-optimization-and-compilation">Operator Optimization and Compilation</h1> <p>The goal is maximize arithmetic intensity. In general there are three ways to speed up operators</p> <h3 id="vectorization">Vectorization</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369120762344.jpg" alt=""/></p> <p>The right version is faster because of the hardware - cache sizes, etc. Tensorflow and PyTorch have this built-in.</p> <h3 id="refactoring-data-layout">Refactoring data layout</h3> <p>This is again related to how data is stored in memory. For example, C++ stores matrices in row-major order. Accessing columns of a matrix can be 10x slower! Remember this while writing code to lower cache misses and reduce pointer movements.</p> <p>ML systems don’t store tensors in row or column major but in a new format called <strong>strides format</strong> - <code class="language-plaintext highlighter-rouge">A[i, j, …] = A.data[offset + i*A.strides[0] + j*A.strides[1] + …</code>. It is a generalization of row and column major storage, and it offers more flexibility - so based on the batch-sizes or other parameters in a neural network.</p> <p>Strides can separate the underlying storage and the view of the tensor. Consider the following operations</p> <ol> <li><code class="language-plaintext highlighter-rouge">slice</code> - simply changing the offsets and shape will output the slice without any copying involved.</li> <li><code class="language-plaintext highlighter-rouge">transpose</code> - modifying strides will transpose the tensor without any copying! For example, consider the following example <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     <span class="n">M</span><span class="p">.</span><span class="nf">strides</span><span class="p">()</span> <span class="c1"># (24, 12, 4, 1)
</span>     <span class="n">M</span><span class="p">.</span><span class="nf">permute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
     <span class="n">M</span><span class="p">.</span><span class="n">t</span><span class="p">.</span><span class="nf">strides</span><span class="p">()</span> <span class="c1"># (12, 4, 1, 24)
</span></code></pre></div> </div> </li> <li><code class="language-plaintext highlighter-rouge">broadcast</code> - Suppose we have to extend a tensor’s data across a dimension for performing operations with another tensor, then by simply adding <code class="language-plaintext highlighter-rouge">0</code> stride in the appropriate dimensions would be enough! Again, no copying</li> </ol> <p>Many more operations can be done without copying the data and simply modifying the strides. For example, consider the following example -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17370816868291.jpg" alt=""/></p> <p>However, strides also has an issue - Memory access may become non-contiguous, and many vectorized ops require continuous storage.</p> <h3 id="summary">Summary</h3> <p>To make operators efficient, we have seen the following tactics -</p> <ol> <li>Vectorization - leverage platform-specific vectorized functions that reduce seek time</li> <li>Data layout - strides format that allow zero-copies enabling fast array-manipulations</li> <li>Parallelization on CPUs</li> </ol> <p>These were techniques for general operations. However, we can optimize certain operators with their special properties.</p> <h3 id="matmul-optimization">Matmul optimization</h3> <p>The brute-force approach takes \(\mathcal O(n^3)\). The best approach humans know is \(\mathcal O(n^{2.371552})\)!</p> <p>How to improve the speed in practice then? Recall that we are trying to increase AI = #ops/#bytes.</p> <blockquote> <p><strong>Memory Hierarchy</strong> If everything ran on registers, things would be super-fast. But, that is expensive. Remember that L1-Cache has 0.5ns latency, L2-Cache has 7ns and DRAM has 200ns (400x slower!)</p> </blockquote> <p>Let us analyze the AI of <code class="language-plaintext highlighter-rouge">matmul</code> considering the different layers of memory</p> <ol> <li>We can directly move data to registers in every iteration in inner loop</li> </ol> <h2 id="gpus-and-accelerators">GPUs and accelerators</h2> <p>Recall that parallelizing operations across threads is super useful! CPUs have some level of parallelism through SIMD operations (vectorization) but they are limited. Building on the same idea, GPUs were born.</p> <p>When we started out, the ALU units were limited by the physical space on the chips. As technology improved, we moved from 70nm process all the way 3nm process! That is, we can fit up to 20x more cores in the same area! The majority of the area on CPUs is consumed by Control and Cache, and Jensen thought, ditch those and put cores.</p> <p>Graphical Processing Unit (GPU) are tailored for matrix or tensor operations. The basic idea is to use tons of ALUs (weak but specialized) with massive parallelism (SIMD on steroids).</p> <p>There are other hardware accelerators like Tensor Processing Unit (TPU) or Application specific integrated circuit (ASIC), etc. The common theme across all these is the same - there are specialized cores. What are specialized cores? They can only compute certain computations. Specialized cores can be super powerful - <img src="/assets/img/2025-01-06-data-systems-for-ml/17370849151083.jpg" alt=""/></p> <p>Companies also tried reducing precision and maintain the same performance. Additionally, they also tune the distribution of different components for specific workloads.</p> <blockquote> <p>Why does quantization work in ML systems?</p> </blockquote> <h2 id="recap">Recap</h2> <p>Consider the following question - What is the arithmetic intensity of multiplying two matrices?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Load A
Load B
C = matmul(A, B)
</code></pre></div></div> <p>If the size of the matrix \(A\) is \((m, n)\) and \(B\) is \((n, p)\) then the Flops is \(2mnp\).</p> <p><code class="language-plaintext highlighter-rouge">matmul</code> is an important operation. <Check></Check></p> <p>Now, consider the following operations</p> <ul> <li><code class="language-plaintext highlighter-rouge">broadcast_to</code></li> <li><code class="language-plaintext highlighter-rouge">slice</code></li> <li><code class="language-plaintext highlighter-rouge">reshape</code></li> <li>Permute dimensions</li> <li><code class="language-plaintext highlighter-rouge">transpose</code></li> <li><code class="language-plaintext highlighter-rouge">contiguous</code></li> <li>indexing like <code class="language-plaintext highlighter-rouge">t[:, 1:5]</code></li> </ul> <p>Just to recap, the strides of a tensor of shape <code class="language-plaintext highlighter-rouge">[2, 9, 1]</code> stored in row major order are <code class="language-plaintext highlighter-rouge">[9, 1, 1]</code></p> <p>Consider the cache tiling operation -</p> <ul> <li>It increases the memory allocated on Cache and memory transfers between cache and register</li> <li>It reuses the memory movement between Dram and Cache</li> <li>The arithmetic intensity <em>decreases</em> since there is more load and store</li> </ul> <h1 id="gpu-and-cuda">GPU and CUDA</h1> <p>We have seen that specialized cores offer much better performance over traditional CPUs. Consider the following basic architecture of a GPU</p> <p>Let us see the basic terminology for understanding the architecture -</p> <ul> <li><strong>Threads</strong> - Smallest units to process a chunk of data.</li> <li><strong>Blocks</strong> - A group of threads that share memory. Each block has many threads mapped to a <em>streaming multiprocessor</em> (SM/SMP).</li> <li><strong>Grid</strong> - A collection of blocks that execute the same kernel.</li> <li><strong>Kernel</strong> - CUDA program executed by many CUDA cores in parallel.</li> </ul> <p>A GPU can be made more powerful by</p> <ul> <li>Adding SMs</li> <li>Adding more cores per SM</li> <li>Making the cores more powerful - at a point of <em>diminishing rewards</em>.</li> </ul> <p>NVIDIA, the largest GPU company, has released P100, V100, A100, H100 and B100 (Blackwell) for ML development. K80, P4, T4 and L4 were a lower tier of GPUs. Let us analyze how the compute has changed across these versions</p> <ol> <li>V100 (2019 -) - 80SMs, 2048 threads/SM - $3/hour</li> <li>A100 (2020 -) - 108SMs, 2048 threads/SM - $4/hour</li> <li>H100 (2022 -) - 144SMs, 2048 threads/SM - $12/hour</li> <li>B100 and B200 (2025 -)-</li> </ol> <p>The numbers are not doubling, then how has the performance doubled? They decreased the precisions.. :(</p> <h2 id="cuda">CUDA</h2> <p><strong>What is CUDA?</strong> It is a C-like language to program GPUs, first introduced in 2007 with NVIDIA Tesla architecture. It is designed after the grid/block/thread concepts.</p> <p>CUDA programs contain a hierarchy of threads. Consider the following host code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">const</span> <span class="kt">int</span> <span class="n">Nx</span> <span class="o">=</span> <span class="mi">12</span><span class="p">;</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">Ny</span> <span class="o">=</span> <span class="mi">6</span><span class="p">;</span>

<span class="kt">dim3</span> <span class="nf">threadsPerBlock</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// 12</span>
<span class="kt">dim3</span> <span class="nf">numBlocks</span><span class="p">(</span><span class="n">Ns</span><span class="o">/</span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">x</span> <span class="p">,</span> <span class="n">Ny</span><span class="o">/</span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">y</span> <span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// (3, 2, 1) = 6</span>

<span class="c1">// the following call triggers execution of 72 CUDA threads</span>
<span class="n">matrixAddDoubleB</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">);</span>
</code></pre></div></div> <p>The GPUs are associated with constants such as</p> <ul> <li><code class="language-plaintext highlighter-rouge">GridDim</code> - dimensions of the grid</li> <li><code class="language-plaintext highlighter-rouge">blocking</code> - the block inter within the grid</li> <li><code class="language-plaintext highlighter-rouge">blockDim</code> - the dimensions of a block</li> <li><code class="language-plaintext highlighter-rouge">threadIdx</code> - the thread index within a block With these in mind, the CUDA kernel for the above code is designed as</li> </ul> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__device__</span> <span class="kt">float</span> <span class="nf">doubleValue</span><span class="p">(</span><span class="kt">float</span> <span class="n">x</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// kernel definition </span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">matrixAddDoubleB</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">])</span>
    <span class="p">{</span>
        <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="n">C</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">doubleValue</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
</code></pre></div></div> <p>The host code launched a grid of CUDA blocks, which then call the <code class="language-plaintext highlighter-rouge">matrixAdd</code> kernel. The function definition starts with <code class="language-plaintext highlighter-rouge">__global__</code> which denotes a CUDA kernel function that runs of the GPU. Each thread indexes its data using <code class="language-plaintext highlighter-rouge">blockIdx</code>, <code class="language-plaintext highlighter-rouge">blockDim</code>, <code class="language-plaintext highlighter-rouge">threadIdx</code> and execute the compute. It is the user’s responsibility to ensure that the job is correctly partitioned and the memory is handled correctly.</p> <p>The host code has a serial execution. However, the device code has SIMD parallel execution on the GPUs. When the kernel is launched, the CPU program <em>continues executing</em> without <em>halting</em> while the device code runs on the GPU. Due to this design, it is important that the device code does not have any return values - causes erroneous behavior. To get results from the GPU, <code class="language-plaintext highlighter-rouge">CUDA.synchronize</code> is used (an example will be shown later).</p> <p>It is the developers responsibility to map the data to blocks and threads. The blockDim, shapes etc should be statically declared. This is the reason why compilers like <code class="language-plaintext highlighter-rouge">torch.compile</code> requires static shapes. The CUDA interface provides a CPU/GPU code separation to the users.</p> <p>The SIMD implementation has a constraint for the control flow execution - it requires all ALUs/cores to process in the same pace. In a control flow, not all ALUs may do useful work and it can lead to up to 8 times lower peak performance.</p> <h3 id="coherent-and-divergent-execution">Coherent and Divergent execution</h3> <p>A coherent execution applied the same instructions to all data. Divergent executions do the opposite and they need to be minimized in CUDA programs. This distinction is important to note - even the latest models like the LLMs have this behavior. Concepts such as attention masking and sliding window attention are examples of divergent behavior and they need to be specially implemented to extract the most compute from the GPU.</p> <h2 id="cuda-memory-model">CUDA Memory model</h2> <p>CUDA device (SIMD execution on GPU) has its own memory called the <em>HBM</em>.</p> <p>Unlike host (CPU) memory that is stored as pages in the RAM, GPU memory does not use pages but has memory pools (bulk data) that are accessed all at once.</p> <p>Memory can be allocated <code class="language-plaintext highlighter-rouge">cudaMalloc</code> and populated with <code class="language-plaintext highlighter-rouge">cudaMemcpy</code> like usual. CUDA has a concept called <strong>pinned memory</strong> that is part of the host memory which is optimized for data transfer between CPU/GPU. Ig is not pagable by the OS and is locked, and only certain APIs can access it.</p> <p>Every thread has its own private memory space, and every block has a shared memory that all its threads can access. The HBM is the global device memory in the GPU that can be accessed by all threads. The memory complexity is to balance between speed and shared memory parallelism.</p> <p>For example, consider the program for window averaging -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mf">3.0</span>
</code></pre></div></div> <p>How can this be parallelized? Since every 3-element tuple reduction is independent, each reduction can be mapped to a CUDA core. So, each thread can compute the result for one element in the output array.</p> <p>The host code -</p> <pre><code class="language-C">int N = 1024*1024;
cudaMalloc(&amp;devInput, sizeof(float)*(N+2)); // To account for edge conditions
cudaMalloc(&amp;devOutput, sizeof(float)*N);

convolve&lt;&lt;&lt;N/THREADS_PER_BLK, THREADS_PER_BLK&gt;&gt;&gt;(N, devInput, devOutput); 
</code></pre> <p>The device code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="cp">#define THREADS_PER_BLK = 128
</span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolve</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
        <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">//thread-local variable</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span>
        <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span><span class="mf">3.</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div> <p>This program can be optimized - each element is read thrice!<br/> Notice that the number of blocks assigned is much more than what a typical GPU has. This is a general practice in CUDA programming where the blocks are <em>oversubscribed</em>.</p> <p>How to optimize? The memory hierarchy can be utilized -</p> <p>The new device code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="cp">#define THREADS_PER_BLK = 128
</span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolve</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
        <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">//thread-local variable</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span>
        <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span><span class="mf">3.</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div> <p>We introduced a synchronization primitive here. <code class="language-plaintext highlighter-rouge">_syncthreads()</code> waits for all threads in a block to arrive at this point. Another primitive <code class="language-plaintext highlighter-rouge">cudasynchronize()</code> that syncs between host and the device.</p> <h2 id="compilation">Compilation</h2> <p>A CUDA program also needs to be converted to low-level instructions to be executed. A compiled CUDA device binary includes -</p> <ul> <li>Program text (instructions)</li> <li>Information about required resources - 128 threads per block, 8 types of local data per thread and 130 floats (520 bytes) of shared space per thread block.</li> </ul> <p>The issue is that different GPUs have different SMs. If the user asks for a static (large) number of blocks, how to handle this? The first solution is that GPUs have varying (limited) number of blocks.</p> <p>Furthermore, CUDA schedules the threadblocks to many cores using a dynamic scheduling policy that respects the resource requirements. It assumes that the thread blocks can be executed in any order. The blocks are assigned based on the available resources and the remaining ones are <em>queued</em>.</p> <h2 id="understanding-a-gpu">Understanding a GPU</h2> <p>Consider a NVIDIA GTX 980 (2014) that has the following specs -</p> <ul> <li>96KB of shared memory</li> <li>16 SMs</li> <li>2048 threads/SM</li> <li>128 CUDA cores/SM Note that the number of CUDA cores is not equal to the number of CUDA threads.</li> </ul> <p>As the GPUs became better, NVIDIA tried to increase the shared memory per SMM. This is similar to the SRAM which is very important for LLM inference.</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Prompting ChatGPT is not enough. To build large-scale AI systems, it is imperative to understand how to design the proper systems to optimize all the computations. The following blog is a deep-dive into system/data design for Machine Learning frameworks.]]></summary></entry><entry><title type="html">AI Agents</title><link href="https://sudhansh6.github.io/blog/ai-agents/" rel="alternate" type="text/html" title="AI Agents"/><published>2025-01-06T00:00:00+00:00</published><updated>2025-01-06T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/ai-agents</id><content type="html" xml:base="https://sudhansh6.github.io/blog/ai-agents/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>The content on this article is based on the <a href="https://github.com/pearls-lab/ai-agents-course">course</a> by <a href="https://prithvirajva.com">Prof. Prithviraj</a> at UC San Diego. This <a href="https://www.kaggle.com/whitepaper-agents">whitepaper</a> about AI Agents by Google is also a good read.</p> <h2 id="what-is-an-agent">What is an agent?</h2> <p>Agent is an entity with <em>agency</em>. <a href="https://minecraft.wiki/w/Agent">A Minecraft agent?</a>. Agents see applications within the workspaces in the form of workflow automations, household or commercial robotics, software development and personal assistants. Generally, the theme is that <em>agents</em> take actions.</p> <p>Historically, the use of agents started in the early 1900s in the field of control theory. They were used for dynamic control of flight systems, and in 1940s it expanded to flight guidance, etc. By the 1950s the concepts of MDPs and dynamic programming were being expanded to many use cases. Surprisingly, one of the first natural language chatbots, Eliza, was created as a psychotherapist simulator in the 1960s! Finally, reinforcement learning became a field of study in the 1990s for sequential decision making.</p> <h2 id="sequential-decision-making">Sequential Decision Making</h2> <p>These tasks are different from other ML problems like classification. A model that has an accuracy of 99% at each step, has a cumulative accuracy of ~30% after 120 steps!</p> <p>These problems are formalized as a Markov Decision Process - an <strong>agent</strong> performs <strong>actions</strong> in an <strong>environment</strong>, and in turn receives <strong>rewards</strong> as feedback. These configurations are distinguished as <strong>states</strong>, and the whole process can be seen as sequential decision making.</p> <p>The core components of an agent, often agreed on, are</p> <ul> <li><strong>Grounding</strong> - Language is anchored to <em>concepts</em> in the world. Language can be grounded to different forms of information systems - images, actions and cultural norms. <ul> <li>Agency (ability to act) - At each state, an agent needs to have multiple choices to act. <em>If an agent has to select what tools to use but there’s always only one tool, is that agency?</em> The action space has to be well-defined to look for agency. Although there is a single tool call, different parameters for the tool call can probably be considered as different actions. Actions can be defined as something the agent does and changes the environment. The distinction between an agent and environment is not very clear in many cases. Although, our approximations mostly serve us well.</li> <li>Planning (Long horizon)</li> <li>Memory - <ul> <li>Short-term - What is the relevant information around the agent that it needs to use to act now</li> <li>Long term - What information has the agent already gathered that it can retrieve to take an action</li> </ul> </li> <li>Learning (from feedback) - Doesn’t necessarily always mean <em>backpropagation</em>.</li> </ul> </li> <li><strong>Additional</strong> - <ul> <li>Embodiment (physically acting in the real-world). <em>Embodied hypothesis</em> - embodiment is necessary for AGI.</li> <li>Communication - Can the agent communicate its intentions to other agents. Very necessary pre-requisite for multi-agent scenarios.</li> <li>World Modeling - Given the state of the world and an actions, predict the next state of the world. Is <a href="https://deepmind.google/technologies/veo/veo-2/">Veo</a>/<a href="https://sora.com">Sora</a> a world model? It is an attempt for world model since they have no verifiability. <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie</a> is another such attempt. So is <a href="https://genesis-embodied-ai.github.io">Genesis</a> - this is much better if it works.</li> <li>Multi-modality - The clean text on the internet is only a few terabytes, and our models have consumed it (took use 2 decades though). YouTube has 4.3 Petabytes of new videos a day. CERN generates 1 Petabyte a day (modalities outside vision and language). Some people believe this form of scaling is the way to go. There are more distinctions -</li> </ul> </li> </ul> <table> <thead> <tr> <th>Model</th> <th>AI System</th> <th>Agent</th> </tr> </thead> <tbody> <tr> <td>GPT-4</td> <td>ChatGPT</td> <td>ChatGPT computer use</td> </tr> <tr> <td>Forward passes of a neural net</td> <td>Mixing models together</td> <td>Has agency</td> </tr> </tbody> </table> <p>It is important to remember that not every use case needs an agent and most use cases just need models or AI systems. <em>Occam’s razor</em>.</p> <h1 id="simulated-environments-and-reality">Simulated Environments and Reality</h1> <p>Why do we need simulations? Most tasks have many ways of completing them. There is no notion of <em>global</em> optimal solutions ahead of time but usually known once the task is complete.</p> <p>The agent needs to explore to find many solutions to compare and see what is the most efficient. However, exploration in the read world is expensive - wear and tear of robots, excessive compute, danger to humans, etc.</p> <p>Simulations offer an easy solution to these problems. Assign a set of rules, and let a world emerge. One of the early examples of this is <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Conway’s Game of Life</a> which theorized that complicated behaviors can emerge by just a few rules.</p> <p>From an MDP perspective, a simulation contains \(&lt;S, A, T&gt;\) where</p> <ul> <li>\(S\) is the set of all states. It consists of propositions that are true/false. Example: You are in a house, door is open, knife in drawer</li> <li>\(A\) is the set of all actions. Example: Take knife from drawer, walk through door</li> <li>\(T\) is the transition matrix - (You are in the house, you walk out of the door) -&gt; You are outside the house.</li> </ul> <p>A simulation need not have an explicit reward.</p> <h2 id="sim2real-transfer">Sim2Real Transfer</h2> <p>The ability of an agent trained in simulation transfer to reality is dependent on how good the model extrapolates out of distribution. With the current stage of agents, the simulation is made as close to reality as possible to reduce the Sim2Real gap.</p> <p>How do we measure closeness to reality? The tasks in the real world have different types of complexities -</p> <ol> <li>Cognitive complexity - Problems that requires long chains of <em>reasoning</em> - puzzles, math problems or moral dilemmas</li> <li>Perceptive complexity - Requires high levels of vision and/or precise motor skills - bird watching, threading a needle, Where’s Waldo</li> </ol> <p>Examples of simulations -</p> <ol> <li>Grid world - low cognitive and almost zero perceptive. However, this idea can arbitrarily scale to test algorithms for their generalization potential in controllable settings.</li> <li>Atari - low perceptive, medium cognitive. Atari games became very popular in 2013, when Deepmind released their <a href="https://arxiv.org/pdf/1312.5602">Deep Q-Net</a> paper that achieved human level skills on these games.</li> <li><a href="https://en.wikipedia.org/wiki/Zork">Zork</a>, <a href="https://www.nethack.org">NetHack</a> - low perceptive, high cognitive. These are configurations or worlds that you purely interact with text. The worlds are actually so complex that there is no agent that is able to finish the challenge!</li> <li><a href="https://cs.stanford.edu/people/jcjohns/clevr/">Clevr simulation</a> - medium perceptive, low cognitive - This simulation generates images procedurally with certain set of objects and has reasoning questions for each image.</li> <li><a href="https://ai2thor.allenai.org">AI2 THOR</a> - medium perceptive, medium cognitive. Worlds with ego-centric views for robotics manipulation and navigation simulations</li> <li><a href="https://arxiv.org/pdf/2407.18901">AppWorld</a> - medium perceptive, medium cognitive. A bunch of different apps that you would generally use in daily life. The agents can access apps, and the simulation also has human simulators. This simulation is one that is closest to reality in the discussed so far!</li> <li><a href="https://www.minecraft.net/en-us">Minecraft</a> - medium perceptive, high cognitive. A voxel based open-world game that lets players take actions similar to early-age humans.</li> <li><a href="https://mujoco.org">Mujoco</a> - high perceptive, low cognitive. It is a free and open source physics engine to aid the development of robotics.</li> <li><a href="https://ai.meta.com/research/publications/habitat-a-platform-for-embodied-ai-research/">Habitat</a> - high perceptive, medium cognitive. A platform for research in embodied AI that contains indoor-world ego-centric views similar to AI2 THOR, but with much better graphics. They have recently added sound in the environment too!</li> <li> <p>High perceptive, high cognitive - Real world, and whoever gets this simulation right, wins the race to AGI. It requires people to sit down and enumerate all kinds of rules. Game Engines like Unreal and Unity are incredibly complex, and are the closest we’ve gotten.</p> <p>Some researchers try to “learn” the simulations from real-world demonstrations.</p> </li> </ol> <p>In each of these simulators, think of the complexity and reward sparsity in the environment. It is easy to build a simulator that gives rewards at a goal state than the one that gives a reward for each action. There are some open-lines of research in this domain -</p> <ol> <li>Which dimensions of complexity transfer more easily? Curriculum learning</li> <li>Can you train on lower complexity and switch to a higher complexity?</li> <li>Can we learn the world model holy grail?</li> </ol> <h2 id="how-to-make-simulations">How to make simulations?</h2> <p>As we’ve seen, simulations can range from games to real-world replications with physics involved. Most simulations are not designed keeping AI in mind. However, with the current state of AI, this is an important factor to keep in mind.</p> <p>Classical environments like in Zork/AI2 Thor/Mujoco have something known as <strong>PDDLs</strong>. Some simulations are built through AI, like <em>AI Dungeon</em> that spins up worlds for role-play games.</p> <h3 id="planning-domain-definition-language-pddl">Planning Domain Definition Language (PDDL)</h3> <p>Standard encoding for classic planning tasks. Many specific languages for creating simulations have similarities with PDDL.</p> <p>A PDDL Task consists of the following</p> <ul> <li>Objects - things in the world that interest us</li> <li>Predicates - Properties of objects that we are interested in, can be true or false</li> <li>Initial state - The state of the world that we start in</li> <li>Goal specification - Things that we want to be true</li> <li>Actions/Operators - Ways of changing the state of the world.</li> </ul> <p>These are split across two files - domain and problem <code class="language-plaintext highlighter-rouge">.pddl</code> files.</p> <p>Classic symbolic planners read PDDLs and give possible solutions. Checkout the <a href="https://planning.wiki/ref/planners/atoz">Planning.wiki</a>. In many cases these planners are used over reinforcement learning due to lack of algorithmic guarantees.</p> <p>There were other attempts</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Everyone is talking about agents. But what is an agent? Is it just a buzzword being thrown around? This article talks deeply about this issue along with the technical ideas associated.]]></summary></entry><entry><title type="html">Statistical Natural Language Processing</title><link href="https://sudhansh6.github.io/blog/Statistical-NLP/" rel="alternate" type="text/html" title="Statistical Natural Language Processing"/><published>2024-11-03T00:00:00+00:00</published><updated>2024-11-03T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Statistical-NLP</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Statistical-NLP/"><![CDATA[<h1 id="introduction">Introduction</h1> <p><strong>Natural Language Processing (NLP)</strong> is the study of computer systems that take as input or produce as output natural language - languages created by humans. The goal is to give machines the power to understand not just words, but entire sentences, paragraphs and documents.</p> <p>It it worth keeping in mind that the notion of “understanding” is contrived. There is no clear definition - when we claim Large Language Models (LLMs) understand our language, we really don’t know if it is understanding.</p> <p>NLP develops systems for</p> <ol> <li> <p>Analysis of language (NL to some useful output) - text classification, question answering, etc.</p> </li> <li> <p>Generation of language (NL to NL; Image to NL, etc) - summarization, image captioning, machine translation</p> </li> <li> <p>Representation of language (NL to some representation) - learning word embeddings</p> </li> </ol> <p>In the part, systems were task-specific, now we have more general purpose systems capable of all of the above and more.</p> <h3 id="origins">Origins</h3> <p>Back in 1949, Warren Weaver, a wartime US mathematician and scientist, brought the idea of the first computer based application related to natural language - machine translation (MT). He considered the problem of translation as a problem in cryptography. We still use the notation of encoder and decoder in the present techniques. He developed a rule-based program to convert Russian to English.</p> <p>Over time, it became obvious that human language are <strong>ambiguous</strong> (unlike programming and other formal languages) and they are <strong>rich</strong> - any meaning may be expressed in many ways. Human language interpretation depends on the real world, common sense and <strong>contextual knowledge</strong>. Furthermore, there is <strong>linguistic diversity</strong> across genres, styles, and so more.</p> <p>In 1957, Chomsky proposed a <em>generative grammar</em>, a rule based system of syntactic structures, brought insight into how linguistics can help MT. Since the results were not satisfactory, funding was cut-off and then came the winter of AI in 1966.</p> <p>In 1971, Terry Winograd’s MIT thesis has motivated the notion of <strong>grounded language understanding</strong>. In late 80’s, statistical techniques revolutionized NLP. They used early ML algorithms - decision trees with rule based systems.</p> <p>From 90’s to early 2000s, methods like logistic regression, Support Vector Machines (SVM), Hidden Markov Models (HMMs), Conditional Random Fields (CRFs), etc were introduced. Moreover, papers introduced feature engineering for specific tasks - POS tagging, Named Entity Recognition, Parsing, etc.</p> <p>The main language models during this time were n-grams with smoothing.</p> <p><img src="/assets/2024-11-03-Statistical-NLP/2024-11-09-15-15-16-image.png" alt=""/></p> <h3 id="dawn-of-deep-learning-era">Dawn of Deep Learning Era</h3> <p>Bengio et al. in 2003 proposed first neural language models with 1-hidden layer feed-forward neural network. It introduced the notion of <strong>word embeddings</strong> with a real-valued feature vector in \(\mathbb R^d\). In 2008, a new paper proposed training neural network along with a word embedding matrix jointly. There was no need of feature engineering anymore.</p> <p>In 2013, Mikolov et al. introduced arguably the most popular word embedding model - <strong>Word2Vec</strong> - they got rid of hidden layer in the model as well.</p> <p>From 2013 to 2018, Recurrent Neural Networks (RNNs; Elman 1990), Long-Short Term Memory Models (LSTMs), Convolution Neural Networks (CNNs), recursive neural networks (Socher et al.), etc were used for NLP. There were feats of Architectural engineering as well - combining RNNS with CRFs for sequence labeling, CNNs for text classification, summarization with pointer-generators RNNs (2017). <em>In present date, there are little to no changes in the model architecture.</em></p> <p>In 2014, Google introduced <strong>Sequence-to-sequence</strong> learning, a general end-to-end approach for mapping one sequence to another using a single neural network (encoder-decoder architecture). This proved very important for NLP tasks going forward. It was a fundamental shift in paradigm to perform tasks like translation with a single model instead of complicated designed models.</p> <p>Then, in 2015 came the notion of <strong>Attention</strong> - to reduce the bottleneck of sequence-to-sequence models that was compressing the entire content of source sequence into a fixed-size vector. This notion still required sequential processing with RNNs. Finally in 2017, <strong>Transformers</strong> were proposed which eschewed recurrence and relied entirely on attention mechanisms. The parallel nature of the model enabled fast computations.</p> <p>In 2020, people realized instead of just pre-training the word-embedding layer they could just pre-train the whole network and add a layer-head in the end if required for other specialized tasks. Pre-trained LMs then acted as an initialization for fine-tuning on downstream tasks - ELMo (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), GPT (Radford et al., 2018), and BERT (Devlin et al,. 2019). The impact of pre-training all the layers was significant.</p> <p><img src="/assets/2024-11-03-Statistical-NLP/2024-11-09-15-15-02-image.png" alt=""/></p> <h3 id="present-date">Present Date</h3> <p>NLP systems are increasing used in everyday life - in the form of chatbots and other AI assistants. Consider ChatGPT - fastest growing consumer computing applications in history.</p> <p>The key advantage os language models is that there is no need of annotation - nearly unlimited training data*. People also realized that using larger and larger models gives higher performance as data scales. The final ingredient to achieve all this is compute - GPU gave a huge advantage over CPU to train these networks. These three key ingredient - hardware scalability (GPUs), Model scalability (Transformer with many deep layers) and Data scalability (Large datasets with lots of text) enabled the success of GPT models.</p> <p><img src="/assets/2024-11-03-Statistical-NLP/2024-11-09-15-14-43-image.png" alt=""/></p> <p>Realizing the power of scale, GPT1 was trained with a few million parameters and now GPT4 has a few hundred billion parameters. In 2022, researchers at OpenAI realized some tasks were only possible at larger scales - scaling LMs leads to emergent abilities. Another paper (one of the best papers in NeurIPS) questioned this asking whether this finding is just an artifact of how we designed our metrics. The metrics used in the OpenAI paper did not allow continuous rewards which caused the sudden jump in performance after a certain point in scale. With a more continuous metric, the gains due to scale increase continuously without sudden jumps.</p> <p>Then came the question of prompting - how do we talk to these LMs? <strong>Prompt</strong> is a cue given to the pre-trained LM to allow it to better understand people’s questions (Best paper in NeurIPS 2020).</p> <p>GPT3.5 introduced the notion of <strong>Instruction Tuning</strong> - collect examples of (instruction, output) pairs across many tasks and then evaluate on unseen tasks. Furthermore, the output of LMs can be tuned with <strong>Reinforcement Learning with Human Feedback</strong> (RLHF) - explicitly attempt to satisfy human preferences using RL. This was implemented in an ingenious manner -</p> <p><img src="/assets/2024-11-03-Statistical-NLP/2024-11-09-15-13-21-image.png" alt=""/></p> <p>After adding some safety features, GPT3.5 was transformed into ChatGPT.</p> <p><strong>LLM as a reasoning engine</strong> - The knowledge base of LLMs is large but incomplete. To address this limitation, they need to retrieve information from elsewhere, add it to the prompt and then ask the LLM to process it to get an answer. This is the idea behind <strong>Retrieval Augmented Generation (RAG)</strong> for knowledge intensive NLP tasks. The pipeline is more sophisticated, and will be described later.</p> <h3 id="summary">Summary</h3> <p>We have the following big picture -</p> <ol> <li> <p>Feature Engineering in 1990s to 2000s</p> </li> <li> <p>Architecture Engineering - 2010 - 2018 (LSTMs, CNNs, Transformers)</p> </li> <li> <p>Objective Engineering - 2018 (ELMo, BERT, GPT)</p> </li> <li> <p>Prompt Engineering - 2020s - present (instruction-tuning, chain-of-thought, etc)</p> </li> </ol> <p><strong>NLP vs Humans.</strong> General language understanding is still a bit difficult for LLMs</p> <ul> <li> <p>Sample efficiency - LLMs require a lot of data</p> </li> <li> <p>Robustness - LLMs are brittle, can be easily fooled</p> </li> <li> <p>Originality - LLMs lack ability to create truly original content</p> </li> <li> <p>Causality and other forms of reasoning - LLMs have limited understanding of logic</p> </li> </ul> <p>In the following post, we will start from the basis such as text classification with simple neural networks and make our way to the modern sophisticated techniques being used. Although the initial part of the article may seem very straightforward, it is important to understand the motivations and ideas behind the approaches.</p> <h1 id="text-classification">Text Classification</h1> <p>The task is to assign a text document to one or more categories from a predefined set of labels. For example, Sentiment Analysis, Spam Detection, Topic Classification, Authorship Attribution and Language Identification fall under this domain of tasks.</p> <p>Unlike tasks like sorting numbers which have clear rules, text classification does not have a standard algorithm. The issues with rule-based algorithms are as follows -</p> <ol> <li> <p>Semantic Gap - Computer does not know what words mean - their relationships, sentiment, etc.</p> </li> <li> <p>Intra-class variations - There are many ways to be a particular label.</p> </li> <li> <p>Scalability - Have to write rules for every class label</p> </li> </ol> <p>The task rather requires a data-driven approach (in the form of machine learning) -</p> <ol> <li> <p>Collect a dataset of example text inputs and their labels - <strong>training data</strong></p> </li> <li> <p>Use Machine Learning algorithms to train a classifier on the training examples</p> </li> <li> <p>Evaluate the classifier of new text data - <strong>test data</strong> - a good classifier generalizes well.</p> </li> </ol> <p>This is the standard process (used to be) to work with machine learning models. Another important aspect with this approach is modeling of data - i.e., input representation. Machine Learning models require numerical input.</p> <blockquote> <p>At their core, machine learning models are optimization models relying on mathematical operations - cannot be done with text data.</p> </blockquote> <p>Therefore, we need to create a <em>feature vector</em> for the input text for text classification. By creating feature vectors, we are essentially creating another veil of abstraction to reduce the semantic and other complexities in text data.</p> <p><strong>Bag of words</strong> is one such early idea that represents text as an unordered collection of words, disregarding grammar and word order. The vector essentially contains the frequency of each word occurring in the training text. For unseen words in test data, we just treat them as a separate entity under the unknown tag.</p> <p>These feature vectors although being one of the simplest forms of representation have some limitations (owing to the simplicity)</p> <ol> <li> <p>Sparsity - high-dimensional, sparse vectors (size of vocabulary)</p> </li> <li> <p>Loss of sentence structure - This context is very important in some tasks</p> </li> <li> <p>Semantic Gap - Lacks the word meanings</p> </li> </ol> <h3 id="nearest-neighbor-classifier">Nearest Neighbor Classifier</h3> <p>The idea is to represent all training examples in a feature space. Given a test instance, we find the closest training example based on a <strong>distance metric</strong> and assign its label to the test instance. This algorithm does not have any training of sorts and takes constant time. However, the inference time is \(\mathcal O(n)\) since it has to be compared against every test instance. We do not want such behavior with ML models - even if the training is slow, we want the inference to be very fast.</p> <p>Furthermore, the decision boundaries created by this algorithm are not smooth - <em>from experience this is a bad outcome, and we ideally want a smooth decision boundary</em> without too many changes. Motivating from this idea, then came along \(k\)-Nearest Neighbor classification where the nearest \(k\) neighbors are chosen to decide the label. The parameter \(k\) is called as a <strong>hyperparameter</strong> that is tuned based on the dataset, model, etc. There is no clear answer as to which hyper-parameter gives the best performance; they have to be chosen empirically. The distance metric and word representation are other hyperparameters in this algorithm.</p> <p>Suppose we choose the hyperparameter that works best on the <em>test data</em>, it may not perform that well on other test/unseen data. To overcome this, a new split called <strong>validation</strong> is introduced. The hyperparameters are chosen with the validation data and the algorithm is tested on the test data. The golden rule is to run the model on the test set once after everything (training and hyper-parameter tuning) is completed.</p> <p>The advantages of these models are that they are non-parametric - they make no assumptions about the underlying data. Now, we see another form of classifier that does not have this property.</p> <h3 id="linear-classifier">Linear classifier</h3> <p>A linear classifier assumes a specific form of the decision boundary - predefined model complexity. These boundaries are defined by a set of fixed parameters - intercept \(b\) and slope \(m\). This form of a classifier is still used today in practice.</p> <p>The mathematical model can be represented as</p> \[y \text{ (label) } = \underbrace{f(x)}_{n \times 1} = \underbrace{W}_{n \times m} \underbrace{x}_{m \times 1} + b\] <p>Here, \(n\) is the number of classes and \(m\) is the feature dimension. The bias term \(b\) essentially incorporates prior knowledge into the model.</p> <p>Linear layers are the building block of <strong>neural networks</strong>. Neural networks consist of small functions that are stacked together to form a complex function, and linear layers are a common building block in these architectures.</p> <p>How do we find the best \(W, b\)? We define a <strong>loss function</strong> that quantifies our unhappiness with the scores across the training data. This loss function is minimized via <strong>optimization</strong> to get these best values. We typically average the loss function across all the training examples to get the total loss in a training dataset.</p> <p>Finally, the output from a classifier is unnormalized, and we generally want to interpret these raw scores as probabilities. To normalize the outputs, we use a softmax function -</p> \[P(Y = k, X = x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}}\] <p>where \(e\) is the raw output from the model. This interpretation allows us to now define a loss-function for (binary class) classifiers - <strong>negative log likelihood</strong>.</p> \[L_i = - \log P(y = y_i \vert X = x_i)\] <p>Such method of classification is termed as <strong>logistic regression</strong>.</p> <blockquote> <p>For logistic regression, with a small initialization of weights, the expected initial loss is \(\log (# classes)\). A good sanity check to look out for.</p> </blockquote> <p>The above loss function is essentially doing <strong>maximum likelihood estimation</strong> on the training data. Given training samples \((x_i, y_i)\), the maximum likelihood is given by \(W_{ML} = {\arg \max}_{W \in \mathbb R^{\gamma \times C}} \mathcal L(\bf W)\).</p> <p>The likelihood of the data is given as \(\mathcal L(\bf W) = \sum_{i = 1}^n \log p (Y_i \vert x_i; \bf W)\).</p> <h3 id="feed-forward-neural-networks">Feed-forward Neural Networks</h3> <p>Neural networks introduce a powerful regime of data-driven methods. These models are essentially linear layers stacked (without the softmax) against one another to form a huge network. Each linear layer is associated with an <strong>activation function</strong> that introduces non-linearity in the models. For example, the linear output \(Wx + b\) from a linear classifier layer is sent through a sigmoid activation function which essentially maps the input \(x\) to \((1 + e^{-x})^{-1}\). Each weight row in the network is referred to as a <strong>neuron</strong>.</p> <p>Note that we want our models to generalize well on unseen data. Since neural networks are powerful models, they can <em>overfit</em> on the training data to perform really well on the training set but poorly on the test set. Through <strong>regularization</strong>, we prevent overfitting by discouraging the model from fitting the training data too closely. This occurs frequently when in deep and complicated models. With regularization, we are no longer doing Maximum Likelihood Estimation (MLE) with our models. The loss function then becomes</p> \[L(W) = \frac{1}{N} \sum_{i = 1}^N L_i (F(x_i, W), y_i) + \lambda R(W)\] <p>where \(R(W)\) is the regularization function and \(\lambda\) is a <em>regularization parameter</em>, and it represents the regularization strength. There are more complex methods like <strong>dropout</strong> and <strong>batch normalization</strong> to prevent overfitting. Dropout refers to <em>randomly dropping</em> neurons in the network while training to simplify the model complexity during training time.</p> <p>With this model, we have described the basic recipe for supervised machine learning!</p> <p>How do we find the best \(W\) with these building blocks? Start with a random \(W\) and iteratively improve it to lower the loss function - <strong>gradient descent</strong>. The size of iterative improvement is given by the <strong>learning rate</strong> - another important hyperparameter. A small learning rate leads to slower convergence whereas a high learning rate can overshoot the minimum. Typically, the learning rate is changed across training epochs based on a <strong>learning rate schedules</strong> (ex. cosine learning).</p> <p>The gradient function is pre-computed to prevent recomputing the gradient for every example in practice. <strong>Backpropagation</strong> is a method to compute the gradient of the loss function with respect to the weights in the network - it is an application of the chain rule of calculus.</p> <p>The naïve version of gradient descent can be optimized much further using better convergence algorithms like ADAM and using stochasticity to descent over batches rather than the whole training dataset to increase speed.</p> <h1 id="word-embeddings-and-tokenization">Word Embeddings and Tokenization</h1> <p>How do we convert words to numerical values? A simple idea is to consider a <strong>one-hot vector</strong> - maps words into fixed length vectors and they contain only the identity information of the object without any semantic information. <strong>Bag-of-words</strong> is essentially the summation of one-hot vectors across the input text.</p> <p>An interesting result is that word meanings can also be captured through vectors of real numbers - a vector space where similar words (by meaning) have similar vectors (by some distance metric). How do we come up with such vectors that also have a reasonable size?</p> <h3 id="distributional-semantics">Distributional Semantics</h3> <p>Words that appear in similar contexts have similar meanings. The idea is to understand the context around the word and their relative ordering to understand the meaning of the word itself. To do so, we will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts, measuring similarity as the vector dot (scalar) product.</p> <h3 id="word2vec">Word2Vec</h3> <p>A simple and fast model to capture semantic meanings. They use two algorithms - skip-gram and Continuous Bag of Words (CBOW).</p> <p><strong>Skip-gram</strong> - Given a corpus of text as the input, the output is a set of embeddings which is a real valued vector. To generate these embeddings, we set up a fake prediction task - predict a word’s context from that word. For example, in the sentence “the dog bit the man”, for the <em>word</em> “bit”, the <em>context</em> can be “dog” or “the”.</p> <p>For each position $t = 1, \dots, T$ in the corpus, we predict context words within a window of fixed size \(m\) (<em>context window size</em>), given a center word \(w_j\). The joint probability expression is given by</p> \[\text{ Data Likelihood } = \prod_{t = 1}^{T} \prod_{-m \leq j \leq m; j \neq 0} P(w_{t + j} \vert w_t; \theta)\] <p>where \(\theta\) are all the parameters of the model. The loss function can be chosen as</p> \[L(\theta) = -\frac{1}{T} \sum_{t = 1}^T \sum_{-m \leq j \leq m; j \neq 0}\log P(W_{t + j} \vert w_t; \theta)\] <p>The ground truth probability is calculated by looking at all examples of the word occurring in the training corpus. The original paper predicts two vectors per word \(w\) - \(v_w\) when \(w\) is a center word, \(u_w\) when \(w\) is a context word. The probability is then given by</p> \[P(o \vert c) = \frac{\exp (u_o^T v_c)}{\sum_{w \in V} \exp(u^T_w v_c)}\] <p>The gradient of this loss function comes out to be expected context vector subtracted from the observed context vector. The center word is pulled towards words that are observed in its context and away from those that are not</p> \[v_c^{new } = v_c^{old} + \text{observed} - \text{expected}\] <h1 id="low-rank-adaptation-lora">Low-rank Adaptation (LoRA)</h1> \[h = W\] <h1 id="multi-layer-prompt-tuning">Multi-layer Prompt Tuning</h1> <p>Continuous prompts \(\phi_i\) are concatenated with the keys and values in the self-attention layer</p> <h1 id="adapters-in-transformer-models">Adapters in Transformer Models</h1> <p>An adapter in a Transformer layer</p> \[f_{\phi_i}(x) = W^U(\sigma(W^D x))\] <p>where \(W_^D \in \mathbb R^{d\times r},\)W^U$$</p> <h1 id="towards-a-unified-view-for-parameter-efficient-fine-tuning">Towards a Unified View for Parameter Efficient fine-tuning</h1> <p>This works shoes that LoRA, prefix-tuning, and adapters can be expressed with a similar functional form - all methods can be expressed as modifying a model’s hidden representation \(h\)</p> <h3 id="optimizer-state-comparison">Optimizer State comparison</h3> <h2 id="model-compression">Model compression</h2> <h3 id="knowledge-distillation">Knowledge Distillation</h3> <p>A classic approach from Hinton et. al</p> <h3 id="distilbert">DistilBERT</h3> <p>The idea is to use the classification model with output \(P_{\text{teacher}} (y \vert x)\) to minimize \(KL(P_{\text{teacher}}\vert\vert P_{\text{student}})\) to bring student distribution close to the teacher. Note that this approach does not require any labels since the student uses <em>pseudo-labels</em> that the teacher has.</p> <p>For example, we can choose BERT as the teacher model and create a small student model that has half the layers of BERT. The number of parameters reduce by half and so does the inference time. The performance difference is negligible and this is a huge gain for efficiency.</p> <h1 id="knowledge-representation-in-transformer-lms">Knowledge Representation in Transformer LMs</h1> <p>Factual knowledge is captured by the model during the training is stored in the form of model parameters. Which part of the transformer is this information stored? Token embeddings, feedforward laters or attention layers?</p> <h3 id="parameter-distribution-in-transformers">Parameter Distribution in Transformers</h3> <ul> <li> <p>Self-attention layers</p> <ul> <li> <p>Query, Key, Value matrices - \(W_q, W_k, W_v\) each of dimension \(d \times d\)</p> </li> <li> <p>Output matrix is \(W_o\) of dimension \(d \times d\)</p> </li> <li> <p>\(4d^2\) attention parameters per layer.</p> </li> </ul> </li> <li> <p>Feed-forward Network Layers</p> <ul> <li> <p>First linear transformation via \(W_1: 4d^s\) (input to \(4d\))</p> </li> <li> <p>Second linear transformation via \(W_2: 4d^2\) (\(4d\) to output)</p> </li> <li> <p>\(8d^2\) feedforward parameters per layer</p> </li> </ul> </li> <li> <p>The embedding parameters are not usually considered since the vocabulary across all the models is assumed to be the same</p> </li> </ul> <p>Note that feedforward layers have much higher number of parameters than the attention layers - they account for 2/3 of the total parameters.</p> <h2 id="transformer-feed-forward-layers-are-key-values-memories">Transformer Feed-Forward Layers Are Key-Values Memories</h2> <p>An interesting paper that explores the previous ideas. The feedforward layer is represented as \(y = W_2 \sigma(W_1 x)\).</p> <p>\(W_1\) corresponds to keys, and \(W_2\) to values - when the output from the first weight matrix is passed through ReLU - it is similar to <em>selecting</em> some entries of the vector (positive ones) which then choose the corresponding rows in \(W_2\).</p> <p>The authors tested this idea by considering which neurons are selected in the feedforward layers for different tokens - <strong>key trigger analysis</strong>. They analyzed the patterns in the activations to categorize them.</p> <p>Given a key \(k_i^l\) corresponding to \(i\)th row of the \(l\)th feed-forward layer \(W_1\) computer memory efficient for every prefix \(x_1, \dots, x_j\) of every sentence in the training data.</p> <p><strong>Memory coefficient calculation</strong> - calculate \(ReLU(x_j^l \cdot k)\) - incomplete</p> <p>They found the following results -</p> <ul> <li> <p>Shallow layers detect shallow patterns</p> </li> <li> <p>Middle FF layers store knowledge; Upper attention layers “aggregate” relevant knowledge for prediction</p> </li> </ul> <h2 id="editing-knowledge-in-transformer-lms">Editing Knowledge in Transformer LMs</h2> <p>Can we directly edit LM parameters to fix incorrect, obsolete facts? The edits must be deep -</p> <ul> <li> <p>Eiffel tower is located in the city ____ (change from Paris to Rome)</p> </li> <li> <p>Model should understand full implications - The tallest building in Rome is <em>__Eiffel Tower</em>__</p> </li> </ul> <p>The edit must be robust, should not edit all the facts in the model.</p> <p>Can we simply modify the columns of \(W_2\) to change the model’s behavior? This ends up breaking the model. Another work, Meg et al., suggested applying a rank-1 update \(W_2 \to W_1 + uv^T\) to maximize the probability of the edit output. This change minimizes the change in the behavior of \(W_2\) on other inputs.</p> <p>It works well in some scenarios and does not in some other - it is a new research direction!</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Enter the world of Natural Language Processing.]]></summary></entry><entry><title type="html">Large Language Model Reasoning</title><link href="https://sudhansh6.github.io/blog/Large-Language-Models-Research/" rel="alternate" type="text/html" title="Large Language Model Reasoning"/><published>2024-10-14T00:00:00+00:00</published><updated>2024-10-14T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Large-Language-Models-Research</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Large-Language-Models-Research/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>As a part of this article, we delve into the paradigm of Chain of Though reasoning in Large Language Models. The aim is to highlight the importance of this idea and summarize the main research in this area. The blog should provide enough context for the reader in the field of AI to understand the basic concepts and think about the potential research ideas addressing the limitations of the current models.</p> <h1 id="chain-of-thought-reasoning"><a href="https://arxiv.org/pdf/2201.11903">Chain of thought Reasoning</a></h1> <p>Chain of thought (CoT) refers to manifesting the human thought process in large language models by endowing language models with the ability to generate a chain of thought - a coherent series of intermediate reasoning steps.</p> <p>It is hypothesized that CoT prompting helps LLMs to tackle complex arithmetic, commonsense and symbolic reasoning tasks. The following demonstration highlights this improvement.</p> <p><img src="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&amp;w=1920&amp;q=75" alt="COT"/></p> <p>However, there are some limitations with this paradigm of reasoning with the current models.</p> <ul> <li>Small models are unable to improve with CoT prompting. LLMs with more than 100B parameters show performance gains with CoT.</li> <li>The performance improvements are larger with larger models. In other words, the benefits of CoT scale with the size of the models.</li> <li>Sometimes the models arrive at the correct answers with the wrong reasoning. The errors have been classified as <ul> <li><strong>Calculation error</strong> - LLMs are probabilistic models, predicting what token occurs next. So when an LLM tries to do \(3* 25* 8 =\), it does not really calculate the answer but probabilistically guesses the answer which is the next token. This highlights a fundamental limitation in the current architectures of LLMs.</li> <li><strong>Symbol mapping error</strong> - When there are too many variables involved, LLMs sometimes mix up the variables and arrive at the wrong answer. Again, the problem arises from the fundamental architecture flaw highlighted in the previous point.</li> <li>Other than these major errors, the models also have semnatic understanding problems, missing steps, incoherent chain of thought errors</li> </ul> </li> </ul> <h1 id="large-language-models-are-human-level-prompt-engineers"><a href="https://arxiv.org/abs/2211.01910">Large Language Models are Human-level prompt engineers</a></h1> <p>The motivation of this paper is as follows -</p> <ul> <li> <p><strong>Human effort in prompt engineering</strong> - Crafting effective prompts for LLMs is time-consuming and requires significant human expertise.</p> </li> <li> <p><strong>Optimization challenge</strong> - Primpts greatly influence LLM performance, but users often lack insight into how to optimize them for specific tasks.</p> </li> <li> <p><strong>Scalability</strong> - As LLMs grow in size and capabilities, manuallt designing prompts becomes less deasible for a wide range of applications.</p> </li> <li> <p><strong>Automating promtp design</strong> - There is a growing need to automate the prompt engineering process to enhance LLM usability and performance.</p> </li> <li> <p><strong>Real-world impact</strong> - Applications in diverse domains (e.g., AI chatbots, automated content generation) can benefit from optimized and automated prompts.</p> </li> </ul> <p>This work promposes an <strong>Automatic Prompt Engineer (APE)</strong> - asystem that automates prompt generationg and selection for Large Language Models. This task is treated as a program synthesis task wherein the input-output pairs (natural language questions and answers) are given to the APE, and it has to generate the instruction needed to generate these pairs.</p> <p>In essence, the APE is trying to learn the prompts generated by humans. The framework is as follows -</p> <ol> <li> <p>Instruction Generation. An LLM is used as an ingeerence model where the “instruction candidates” are generated based on a small set of input-output demonstrations</p> <p>Example: The input to APE is of the form -</p> <p><em>Input 1</em> - Forward generation technique</p> <p>”””</p> <p>I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs</p> <p>Input: [ ] Output: [ ]</p> <p>Input: [ ] Output: [ ]</p> <p>…</p> <p>The instruction was &lt;COMPLETE&gt;</p> <p>””””</p> <p><em>Input 2</em> - Reverse generation technique</p> <p>”””</p> <p>I instructed my friend to &lt;INSERT&gt;</p> <p>The friend read the instructions and wrote an output for every one of the inptus. Here are the input-output pairs:</p> <p>Input: [ ] Output: [ ]</p> <p>Input: [ ] Output: [ ]</p> <p>…</p> <p>”””</p> </li> <li> <p>Scoring Instructions. Evaluate each instruction by computing a score that reflects how well the instruction guides the target LLM for the task. This is simply the confidence score associated with the log likelihoods of token generation. The authors consider a <em>moving average</em> score considering the probabilities for a window of tokens.</p> <p>They also consider an <strong>execution accuracy</strong> - the success of an instruction by checking if the model produces the correct output (0-1 loss). However, this cannot. be used for all kinds of instructions.</p> <p>The top \(k\)-percentile prompts are selected and the rest are discarded.</p> </li> <li> <p>LLM as Resampling Model. They apply an Iterative Monte search method to resample more prompts. The LLM generates semnatically similar instructions variants to improve the top-performing candidates.</p> <p>Once the prompts are generated, the moving average scores are generated for each of the prompts and the better scoring prompts are selected again.</p> </li> </ol> <p>Can APE be used to guide LLMs?</p> <p><img src="/assets/img/LLMs/2024-10-23-10-41-54-image.png" alt=""/></p> <p>Although this is a very simple example, the work shows potential in taking such framework forward to work with more complex applications.</p> <p>Another interesting approach is to not generate the prompts from scratch, but to help humans design better prompts. Essentially, augment with context from humans to generate better prompts. On the flipside, RLHF can be used to improve these APE.</p> <h1 id="tree-of-thoughts"><a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts</a></h1> <p>The early Language Models were limited by their token-level, left-to-right decision making. However, some tasks require exploration, stratefic lookahead, planning, and backtracking. The vanilla architecture does not support such mechanisms.</p> <h2 id="framework">Framework</h2> <p><img src="/assets/img/LLMs/2024-10-30-09-36-33-image.png" alt=""/></p> <p>Mathematically, these models are depicted as</p> <ul> <li> <p>Input output programming - \(y \sim p_\theta^{IO}(y \vert x)\)</p> </li> <li> <p>Chain</p> </li> </ul> <p>Theoretically, the model seems promising. However, there are some intricate details that need to be figured out -</p> <ul> <li> <p>How to decompose the porcess into steps</p> </li> <li> <p>How to generate the potential thoughts from each state</p> <ul> <li> <p>Need to be small enough so LMs can generate promising and diverse samples</p> </li> <li> <p>Big enough so LMs can evaluate the difference contributing to the results</p> </li> </ul> </li> <li> <p>How to heuristically evaluate each state</p> </li> <li> <p>How to navigate through the generated tree</p> </li> </ul> <h2 id="thought-decomposition">Thought decomposition</h2> <p><img src="/assets/img/LLMs/2024-10-30-09-41-30-image.png" alt=""/></p> <h3 id="method-1---direct-prompting">Method 1 - Direct Prompting</h3> <p>The prompts themselves can ask the LM to segment the problem into multiple problems. Due to the voting mechanism, LM generates multiple possibilities for an answer and chooses the best model. This works better when thought space is rich and i.i.d samples lead to diversity.</p> <h3 id="method-2---backtracking">Method 2 - Backtracking</h3> <p>Propose thoughts sequentially using a “propose prompt”. When the thought space is constrained, this works better - proposing different thoughts in the same context avoids duplication.</p> <h2 id="state-evaluator">State Evaluator</h2> <p>There are two strategies to evaluate each generated state</p> <ul> <li> <p><strong>Value</strong> each state independently, where a value prompt reasons about the state \(s\) to generate a scalar value \(v\). This value is very context dependent.</p> </li> <li> <p><strong>Vote</strong> across states by deliberately comparing different states in \(S\) in a vote prompt.</p> </li> </ul> <h2 id="search-algorithm">Search Algorithm</h2> <ul> <li> <p><strong>BFS</strong> is helpful when the tree depth is limited and the initial thought steps can be evaluated and pruned to a small set</p> </li> <li> <p><strong>DFS</strong> explores longer trees well - subtrees are pruned to trade exploration for exploitation.</p> </li> <li> <p>More advanced approaches such as \(A^*\) and MCTS are left to future work in the paper.</p> </li> </ul> <p>An interesting summary of all thought paradigms - <a href="https://arxiv.org/pdf/2401.14295">Demytifying Chains, Trees, and Graphs of Thoughts</a>.</p> <h1 id="on-second-thought-lets-not-think-step-by-step-bias-and-toxicity-in-zero-shot-reasoning"><a href="https://urldefense.com/v3/__https://arxiv.org/abs/2212.08061__;!!Mih3wA!FUSiREHKHqULp_GaFY0sSmJRsiVZqYBdk9nJf8WWrKLI4UoxKUzc3ir1rIQaWXw6bk6_UVVe0kXW$">On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</a></h1> <p>We have seen how chain of thought improves problem solving capabilities. However, in some cases, CoT actually causes issues -</p> <p><img src="/assets/img/LLMs/2024-10-30-10-17-51-image.png" alt=""/></p> <p>The authors explore such effects in the paper. They consider</p> <h1 id="least-to-most-prompting-enables-complex-reasoning-in-large-language-models"><a href="https://arxiv.org/pdf/2205.10625">Least-to-most prompting enables complex reasoning in Large Language Models</a></h1> <p>The key motivators for the paper are as follows -</p> <ul> <li>Given a new task</li> </ul> <p>In the prior works, CoT reasoning has been effective for many tasks but struggled with “Easy-to-hard generalization”. Inspired from educational philosophies, the model is implemented by few-show ptompting in 2 stages - decomposition stage and subproblem solving stage.</p> <ul> <li> <p>Decomposition stage - The problem is divided into subtasks <em>once</em> before solving</p> </li> <li> <p>Subsequent solving stage - Solve the subsequent problems one by one.</p> </li> </ul> <p>The key difference from CoT prompting is that CoT starts each sub-problem from scratch and is unable to build from previous reasoning. This behaviour is depicted using the symbolic manipulation task in the paper - The performance of CoT progressively decreases as the length of the list increases.</p> <p>This method of prompting achieves significantly better results borrowing the context from the previous subproblems to arrive at the final answer.</p> <p>However, decomposition thoughts don’t generalize across domains. This limitation mainly shows up for math problems where the subproblems need to be correctly decomposed to solve the original problem.</p> <h1 id="chain-of-thoughtlessness"><a href="https://arxiv.org/pdf/2405.04776">Chain of Thoughtlessness</a></h1> <p>CoT prompting sounds too good to be true. The paper aims to test this paradigm rigorously to verify the claims. The paper also tries to identify the difference between complex reasoning and pattern matching - What seems like “complex reasoning” may just be a case of pattern matching?</p> <p>Consider the chain of thought reasoning -</p> <ul> <li> <p>How specific do the prompt examples have to be to the original problem?</p> </li> <li> <p>How generalizable are these prompts or how specific do they need ot be?</p> </li> <li> <p>How much human effort is needed to craft prompts for each problem subclass?</p> </li> </ul> <p>Furthermore, there are issues with the test domains as well. For example, GSM8K</p> <ul> <li> <p>They are non scalable - problem instances cannot be scaled</p> </li> <li> <p>The problems are static and be easily found in the training data</p> </li> </ul> <p>The main point the paper is trying to address the question - “Is it really possible to teach an LLM how to solve a generalizable problem?”. To test this claim, the authors choose “Blocks world” as the problem domain - given an initial and end configuration, output a series of steps to reach the end configuration from the initial configuration.</p> <p>They perform the following experiments</p> <ul> <li> <p><strong>Zero shot CoT</strong> - Simply append “Let’s think step by step” to the prompts.</p> </li> <li> <p><strong>Progression proof</strong> - Specific to planning problems. Each example’s steps describe the init state, action taken, reason of the action and the final step.</p> </li> </ul> <p>They see that zero-shot CoT achieves insignificant performance gains from zero-shot prompting. The progression proof CoT achieves a lower performance - this may be due to overfitting to the training examples. The LLM fails to learn the <em>universal block algorithm</em> (break the tower and put everything back) even with multiple version of CoT prompting. The authors chose a planning domain on purpose because these problems can be scaled up very well.</p> <p>The authors just wanted to highlight that there is a need for more rigorous testing. One might argue that planning problems are way out of domain of LLMs. So, the authors test the findings with commonly tested problems, and they find similar trends.</p> <h1 id="chain-of-thought-without-prompting"><a href="https://arxiv.org/abs/2402.10200">Chain of Thought without prompting</a></h1> <p>Prompting techniques, while effective, often encode task-specific human priors, thereby making it difficult to assess a language model’s intrinsic reasoning abilities. Ideally, a language model should be able to reason independently and provide the optimal response, without requiring humans to tweak the prompts or refine repeatedly if the initial response is unsatisfactory. Model-tuning can be expensive and requires a substantial amount of supervised data. In this work, we explore a different perspective and ask: Can LLMs reason effectively without prompting? And to what extent can they reason? We find that, perhaps surprisingly, there exists a task-agnostic way to elicit CoT reasoning from pre-trained LLMs by simply altering the decoding procedure. Figure 1 illustrates this phenomenon: given a reasoning question, the LLM generates a wrong answer via the standard greedy decoding path, yet alternative top-𝑘 token inspection unveiled inherent CoT paths (e.g., decoding paths 2 and 4), which accurately resolved the query. This decoding modification bypasses prompting and is entirely unsupervised without the need for model tuning.</p> <p><strong>Why can’t LLMs reason if we only consider greedy decoding path?</strong></p> <h1 id="large-language-models-are-zero-shot-reasoners"><a href="https://arxiv.org/abs/2205.11916">Large Language Models are Zero-shot reasoners</a></h1>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[A survey of papers to better understand the workings of Large Language Models.]]></summary></entry><entry><title type="html">Design and Analysis of Algorithms</title><link href="https://sudhansh6.github.io/blog/Design-and-Analysis-of-Algorithms/" rel="alternate" type="text/html" title="Design and Analysis of Algorithms"/><published>2024-09-27T00:00:00+00:00</published><updated>2024-09-27T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Design-and-Analysis-of-Algorithms</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Design-and-Analysis-of-Algorithms/"><![CDATA[<h1 id="greedy-algorithms">Greedy Algorithms</h1> <h2 id="minimum-spanning-tree">Minimum Spanning Tree</h2> <p>Consider a graph \(G\) describe by \(V\) and \(E\) (positive weights). A <strong>spanning tree</strong> of a graph is defined as an edge set \(T \subset E\) such that \((V, T)\) is a tree. A minimum spanning tree is such that \(\sum_{l \in T} l_e\) is minimized.</p> <p>For a graph with \(n\) vertices, there are \(n^{n - 2}\) spanning trees for a complete graph (<strong>Cayley’s formula</strong>).</p> <p>How do we calculate the number of spanning trees for a general graph? <strong>Kirchoff’s theorem</strong> states the following -</p> <ul> <li>Let \(M\) be the adjacency matrix of \(G\)</li> <li>Let \(L - M\) except \(L_{i, i} = -deg(i)\) - This is generally called as <strong>Graph Laplacian</strong></li> <li>Then, #spanning trees is the determinant of any \(m-1\) square sub-matrix (obtained by removing \(i\)th row and column) of \(L\).</li> </ul> <p>Notice how any sub-matrix yields the same value!</p> <h2 id="greedy-idea-1-kruskals-algorithm">Greedy Idea 1: Kruskal’s algorithm</h2> <ul> <li>Sort all the edges with their weights</li> <li>Pick edges as long as they don’t form a cycle</li> </ul> <p>Does this work? If it does, how do we prove it?</p> <p>Firstly, why is it a greedy idea? At each point of the algorithm, we select the current greedy edge (a local minimum) to obtain the minimum spanning tree (a global optimum).</p> <ul> <li> <p><strong>The cut property -</strong> Let \(S \subseteq V\) such that \(S\) and \(V - S\) are non-empty. If \(e\) is an edge across \(S\) and \(V - S\) with the minimum cost, then there always exists a minimum spanning tree with \(e\).</p> <p><strong>Proof.</strong> Exchange argument. If there an MST with \(e’\) across \(S\) and \(V - S\), then replace \(e\) with \(e’\) to obtain another MST.</p> <blockquote> <p>Shouldn’t this argument be more delicate? Why is there a single edge from S to V - S?</p> </blockquote> <p>Essentially, the exchange argument argues replacing a part of the solution improves the solution but does not worsen it.</p> </li> <li> <p>The time complexity of the algorithm comes out to be \(O(m \log m + m \alpha(n))\). The second term in the expression comes from union-find data structures.</p> </li> <li> <p><strong>Correctness of the algorithm</strong> - We shall prove this via induction.</p> <ul> <li>Induction hypothesis - The edges selected in the \(i\)th round of Kruskal’s algorithm can form an MST along with a subset of edges from the remaining edges.</li> <li>Base statement - True for \(i = 0\)</li> <li>Induction step - Cut property</li> </ul> </li> <li> <p><strong>Union-find data structure</strong> - A data structure that supports</p> <ul> <li>Merging elements of two sets into a single set - <code class="language-plaintext highlighter-rouge">union(x, y)</code></li> <li>Checking whether two elements are in the same set - <code class="language-plaintext highlighter-rouge">find(x)</code></li> </ul> <p>efficiently. The amortized time complexity for these operations is \(\alpha(n)\) where \(\alpha(n) \leq 4\) for \(n\) of the order \(2^{2^{2^{2^{16}}}}\). As a result, \(\alpha(n)\) can be regarded as a constant for practical purposes.</p> <p>In our case, the elements are edges and sets represent connected components.</p> </li> </ul> <h2 id="greedy-idea-2-prims-algorithm">Greedy Idea 2: Prim’s Algorithm</h2> <p>Start with any node and expand with the smallest edge connecting to the remaining set of edges. Note that this is different from Kruskal’s algorithm where we sort all the edges and create individual connected components that eventually merge together.</p> <p>The proof for Prim’s algorithm is very similar to that of Kruskal’s. The time complexity is \(O(n^2 + m)\) similar to Djikstra’s algorithm without a data structure. We can maintain a <strong>priority queue</strong> to maintain all edges that come from \(S\) to reduce the time-complexity to \(O((n + m) \log m)\) (without decrease-key). With decrease key and a binary heap, the complexity becomes \(O((n + m) \log n)\). Furthermore, with decrease key and a Fibonacci heap, the complexity reduces to \(O((n\log n + m)\).</p> <h2 id="other-algorithms">Other algorithms</h2> <ul> <li><strong>Reverse deletion</strong> - For every cycle in the original graph and the edge \(e\) with the maximum cost, there always exists an MST without \(e\). Until there are no cycles in the graph, find a cycle and delete the edge with a maximum cost. Note that this algorithm has a higher time complexity since we try and find a cycle for each iteration of the algorithm. How do we implement this?</li> </ul> <h2 id="union-find-data-structure">Union-Find data structure</h2> <p>The idea is to maintain trees with pointers to merge and find elements. The main complication comes while merging the individual sets.</p> <ul> <li> <p>Merging by size (consuming smaller ones by larger sets) - The complexity of merging sets of size \(n\), \(m\) times takes \(O(m \log n)\)</p> </li> <li> <p>To optimize this further, we merge by rank (generalizing the previous approach where rank was simply the size of the set). We add another trick to reduce the amortized time complexity.</p> <ul> <li>Path compression - When <code class="language-plaintext highlighter-rouge">find(x)</code> is called, attach the found elements along the path directly to the root to reduce the path size.</li> </ul> <p>The time complexity then becomes \(O(m \log^* n)\) where \(\log^* n\) is the minimum \(k\) such that \(\log^{(k)} n \leq 1\).</p> </li> </ul> <h1 id="more-greedy-problems-related-to-mst">More Greedy Problems related to MST</h1> <h3 id="k-clustering">\(k\)-clustering</h3> <p>A <strong>maximum spacing</strong> for \(k\)-clustering of \(G =(V, E)\) is defined as</p> <ul> <li>An edge set \(T \subset E\) such that \((V, T)\) has exactly \(k\) connected components</li> <li>The <strong>spacing</strong> is then \(\min d(u, v)\) for \(u, v\) in different connected components</li> <li>The goal is to maximize the spacing</li> </ul> <p>This problem can be solved again with Kruskal’s algotihm to find \(k\)-connected components - perform the <code class="language-plaintext highlighter-rouge">union</code> operation for \(n - k\) times. Why is this correct? WE can show this using a contradiction.</p> <ul> <li>Consider two nodes that lie in the same connected component in the result obtained by Kruskal’s (with spacing \(d’\)). Let them be in different connected components in the optimal solution (with spacing \(d\)). Then,</li> </ul> <h3 id="second-mst">Second MST</h3> <p>A second MST is essentially the spanning tree with the <em>second</em> lowest edge summation cost. How do we find this tree?</p> <ul> <li>Find an MST with weight \(w\)</li> <li>For every edge \(e\) not in the MST, if adding \(e\) yields a cycle in the graph; then remove the largest edge \(e’\) other than \(e\) in the cycle to obtain the second MST</li> <li>The cost of the tree would be \(w + l_e - l_{e’}\)</li> </ul> <p>The time complexity of this algorithm is \(\mathcal O(T_{MST} + mn)\) and can be improved to \(\mathcal O(T_{MST} + m \log n)\) with better data-structures and divide-and-conquer.</p> <p><strong>Lemma.</strong> The second MST only differs by one edge from the MST. Multiple MSTs? <strong>Proof.</strong> Can be shown using contradiction. The idea is that one can move from one spanning tree to another with local changes in the trees. The argument is that you can replace the edges in the second MST with the edges in the MST to obtain a tree with a lower cost. This process can be repeated until there is only one edge that is different from an MST and replacing that would cause the tree to become the MST.</p> <h2 id="more-greedy-problems">More Greedy Problems</h2> <h2 id="needle-in-haystack">Needle in Haystack</h2> <p>Given two strings \(s, t\), decide whether there is a subsequence (need not be contiguous) in \(s\) that matches with \(t\). A naive greedy algorithm is depicted as follows -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">break</span> 
    <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">t</span><span class="p">[</span><span class="n">j</span><span class="p">]:</span>
        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># s[i] is matched with t[j]
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> <p>On first glance, it looks as if this is a very intuitive algorithm. However, there are more intricate details and the proof makes these clearer. The proof relies on the Exchange argument. If there exists a subsequence \(i_1 i_2 \dots i_m\) in \(s\) matching \(t\) (\(\vert t \vert = m\)). If \(i^*_1 &lt; i_1\) is the first index that \(s[i_1] = t[1]\), then \(i^*_1i_2\dots i_m\) also matches \(t\). This way we can find a set of indices \(i^*_1i^*_2 \dots i^*_m\) through the greedy algorithm that gives the correct answer.</p> <p>In general, greedy algorithms can be proven in two general ways</p> <ul> <li>Consider any iteration of the algorithm and show that the decision made by the greedy algorithm is the best and conclude using induction</li> <li>Exchange argument: Pick an optimal solution and gradually change it to the solution produced by the greedy algorithm showing that the optimality is not affected.</li> </ul> <h2 id="matroids">Matroids</h2> <p>A finite matroid \(M\) consists of \((E, I)\) where</p> <ul> <li>\(E\) is a finite set (called the ground set). For example, \(E\) is the set of all edges in the graph \(G = (V, E)\)</li> <li>\(I\) is a collection of subsets of \(E\) (called the independent set). For example, \(I\) consists all subsets of \(S\) of \(E\) such that all the edges in \(s \in S\) form a forest.</li> </ul> <p>\((E, I)\) should satisfy the following properties -</p> <ul> <li>Null set should be in \(I\) - \(\phi \in I\)</li> <li>if \(A \subset B\) and \(B \in I\) then \(A \in I\)</li> <li>If \(A, B \in I\), \(\vert A\vert &gt; \vert B \vert\) then \(\exists e \in A - B\), such that \(B \cup \{e\} \in I\)</li> </ul> <p>Isn’t 2 inclusive of 1 and 3?</p> <p>How does this data structure help us? Suppose we have a graph \(G = (V, E)\) with weights \(c_e \geq 0\). Then, design an algorithm to find an independent set \(S\) that maximizes \(\sum_{e \in S} c_e\). Consider the following algorithm -</p> <ul> <li>Sort \(e\) in decreasing order of weights</li> <li>Let \(S = \phi\). Add \(e\) to \(S\) if \(e\) does not add cycles in \(S\)</li> </ul> <p>This algorithm is very similar to the reverse deletion algorithm and has a time complexity \(\mathcal O(\vert E \vert \log \vert E \vert + T_{check \text{ IS}})\).</p> <p><strong>Lemma.</strong> Let \(e \in E\) have the maximum cost \(c_e\)m then there always exists an IS \(A\) with maximum weight containing \(e\).</p> <p>The proof is very similar to what we have shown with MSTs. This example can be applied to MSTs, and it demonstrates how Matroids can be useful for greedy algorithms.</p> <h2 id="task-scheduling">Task Scheduling</h2> <p>Given \(n\) jobs each with \(t_i\) time to finish and deadlines \(d_i\). Consider that there is a single resource solving tasks sequentially from time \(0\), and a job has to completely finished before moving onto the next one.</p> <p>Suppose in a scheduling algorithm, job \(i\) finishes at time \(f_i\), then the lateness is defined as \(l_i = \max\{0, f_i - d_i\}\). The goal is find an algorithm that minimizes the maximum lateness \(minimize \max_i l_i\).</p> <p>Let us consider a simple case. Suppose there are two jobs with \(d_i \leq d_2\). Note that any scheduling algorithm should not have any idle time between jobs. Why so? If there is idle time, you can always do a job earlier to reduce the lateness. Furthermore, a scheduling algorithm should not have an <em>inversion.</em> That is, if job 1 has deadline earlier than job 2, then it is always optimal to perform job 1 before job 2. This claim can be easily proved using the exchange argument.</p> <h3 id="algorithm">Algorithm</h3> <p>Sort all jobs according to the increasing order of these deadlines \(d_i\), then complete each job without any idle time.</p> <p><strong>Proof.</strong> Generalize the previous two observations to \(n\) jobs.</p> <h2 id="huffman-codes">Huffman Codes</h2> <p>How do we encode an alphabet in binaries to have no ambiguities.</p> <h3 id="prefix-codes">Prefix codes</h3> <p>A prefix code for an alphabet \(T\) is a function \(f:T \to \{0, 1\}^*\), such that for distinct \(x, y \in T\), \(f(x)\) is not a prefix of \(f(y)\).</p> <p>It can be shown that a prefix code gives unique decoding.</p> <p>How do we design an encoding that is most efficient? Let us define efficiency. For every letter \(x\) in \(T\), let its frequency be \(p_x (\sum_{x \in T} p_x = 1)\). Let \(f\) be a prefix code and for every letter \(x \in T\), let \(\vert f(x)\vert\) is the number of bits. The goal is to find a prefix code \(f\) that minimizes the expected number of bits when encoding \(R\) under the frequency \(\{p_x\}\).</p> \[\text{minimize } \sum_{x \in T} p_x \cdot |f(x)|\] <p>It is beneficial to represent prefix codes as a binary tree. Each node has two children: 0 and 1. The paths from the root to other nodes in the tree represent the binary encodings.</p> <ul> <li>For prefix codes, no node of a symbol is an ancestor of node of another symbol (from the alphabet).</li> <li>Another observation is that any optimal prefix code is a full tree (every inner node has two children) - if anode has a single chide, the parent node can itself be used for the symbol deleting the leaf node making the encoding more efficient.</li> <li>There is an optimal tree (prefix code) such that two lower frequent letters are siblings, and are as deep as possible in the tree. This claim can be proved easily with the exchange argument.</li> </ul> <p>With these observations, consider the following algorithm</p> <ul> <li> <p>Initialize each letter \(x\) as a node and label it with \(p_x\)</p> </li> <li> <p>Put all nodes into a min-heap (according to the frequency)</p> </li> <li> <p>While min-heap has atleast two elements</p> <ul> <li> <p>Pop out the two smallest elements \(u, v\) (corresponds to two trees)</p> </li> <li> <p>Combine them to a single tree</p> </li> <li> <p>Push it into the heap, label with \(p_u + p_v\)</p> </li> </ul> </li> </ul> <p>This is the Huffman’s coding algorithm which has a time complexity of \(n \log n\).</p> <h2 id="shannons-source-coding-theorem">Shannon’s source coding theorem</h2> <p>Let \(T\) be an alphabet with frequency \(\{p_x\}\). The entropy of the alphabet is defined as</p> \[H := \sum_{x \in T} p_x \cdot \log \frac{1}{p_x}\] <p>The Shannon’s source coding theorem then states that you cannot send a letter from \(T\) with frequenct \(\{p_x\}\), with expected bits less than \(H\). Huffmman’s encoding gives a solution with expected bits at most \(H + 1\).</p> <p><strong>Important point</strong>. One can suggest to increase the alphabet size with dummy symbols to virtually reduce the value of \(H\) significantly. However, this introduces complexity for encoding algorithms. Therefore, there is a tradeoff with space occupied by encoding and the time for encoding.</p> <blockquote> <p>Even with augmented alphabet, the size of the encoding does not change for the original symbols in the alphabet?</p> </blockquote> <h1 id="binary-search-x-greedy-algorithms">Binary Search X Greedy Algorithms</h1> <p>The basic binary search takes advantage of the monotone structure in arrays to identify elements with certain properties. Any binary search problem can be converted to the following simpler version: For an array \(B\) that has binary elements with all zeros occurring before all ones, find the index of the first occuring \(1\).</p> <p>The binary search algorithm can be simply proved using induction.</p> <p>Let us consider an example - Ternary search. Given an array \(A[1\dots n]\) that is first strictly increasing and then strictly decreasing, dind the largest element. The array \(B[1\dots n - 1]\) is constructed as</p> <ul> <li> \[B[i] = 1 \iff A[i + 1] &gt; A[i]\] </li> <li> \[B[i] = 0 \iff A[i + 1] &lt; A[i]\] </li> </ul> <h2 id="split-array-largest-sum">Split-array largest sum</h2> <h2 id="minimum-fractional-st">Minimum fractional ST</h2> <p>Given an undirected graph \(G = (V, E)\) and each edge has two costs \(a_e, b_e\) both of which are positive, find a spanning tree \(T\) that minimizes</p> \[\frac{\sum_{e \in T} a_e}{\sum_{e \in T} b_e}\] <p>How is this related to binary search? Firstly, we will convert this problem to a decisional version - Given an undirected graph \(G = (V, E)\) and a real number \(U\), decide whether there exists a spanning tree \(T\) such that \(\frac{\sum_{e \in T} a_e}{\sum_{e \in T} b_e} \leq U\).</p> <p>This is equivalent to find a spanning tree such that \(\sum_{e \in T} a_e - U b_e \leq 0\). Construct a new graph with the weights \(a_e - Ub_e\). The reduction is easy to follow.</p> <p>How do we find the monotone structure for binary search? If the decision problem \((G, U)\) is satisfiable, then \((G, U')\) is also satisfiable for any \(U' &gt; U\). Conceptually, assume a function \(B\) (with continuous index) such that \(B[0, S] \to \{0, 1\}\) where \(S\) is an upper bound. \(B(U) = 1\) iuf and only if \((G, U)\) is satisifiable, and \(B\) is monotone.</p> <h1 id="divide-and-conquer">Divide and Conquer</h1> <h2 id="master-theorem">Master Theorem</h2> <p>Consider an algorithm that has the following relationship for running time complexity -</p> \[T(n) = 2T \left(\frac{n}{2}\right) + c n \log^k n \quad (k \geq 0)\] <p>then \(T(n) = \mathcal O(n \log^{k + 1} n)\).</p> <h2 id="closest-point">Closest Point</h2> <h2 id="fast-multiplication">Fast Multiplication</h2> <p>Suppose we have two integers in binary \(a = \sum_{0 \leq 1 \leq n} a_i \cdot 2^i, b = \sum_{0 \leq 1 \leq n} b_i \cdot 2^i\). The goal is to compute \(c = ab = \sum_{0 \leq j &lt; 2n} c_j \cdot 2^j\) where \(c_j = \sum_{0 \leq k \leq j} a_k b_{j - k}\). The naïve brute force approach takes \(\mathcal O(n^2)\) to compute the answer.</p> <p>This question is related to matrix multiplication as well. The naïve algorithm takes \(\mathcal O(n^3)\).</p> <h3 id="algorithm-1">Algorithm 1</h3> <p>We segment \(a, b\) as follows -</p> <ul> <li> <p>\(a = A_1 \cdot 2^{\frac{n}{2}} + A_0\) where \(A_0 = \sum_{0 \leq 1 &lt; n/2} a_i \cdot 2^i\) and \(A_1 = \sum_{n/2 \leq i &lt; n} a_i \cdot 2^{i - n/2}\)</p> </li> <li> <p>\(b = B_1 \cdot 2^{\frac{n}{2}} + B_0\) similarly.</p> </li> </ul> <p>Then, \(ab = (A_1 \cdot 2^{\frac{n}{2}} + A_0)(B_1 \cdot 2^{\frac{n}{2}} + B_0)\). The strategy then is to do a divide and conquer on these halves to get the final answer.</p> \[ab = A_1 B_1 2^n + (A_0 B_1 + A_1 B_0)2^{n/2} + A_0B_0\] <p>The time complexity is then \(T(n) = 4T(\frac{n}{2}) + \mathcal O(n)\). This is essentially \(\mathcal O(n^2)\) that does not give any improvement.</p> <p>This can be optimized further -</p> \[ab = A_1 B_1 2^n + ((A_0 + A_1)(B_0 + B_1) - A_0B_0 - A_1 B_1)2^{n/2} + A_0B_0\] <p>The number of multiplications reduced to 3 - \(T(n) = 3T(\frac{n}{2}) + \mathcal O(n)\). Deriving the final expression, \(T(n) = cn + cn\frac{3}{2} + \dots + cn\left(\frac{3}{2}\right)^{\log n} = \mathcal(3^{\log n})\).</p> <p>This algorithm can be extended to matrix multiplications as well.</p> \[C = AB = \begin{bmatrix} A_{00}B_{00} + A_{01}B_{10} &amp; A_{00}B_{01} + A_{01} B_{11} \\A_{10}B_{00} + A_{11}B_{10} &amp; A_{10}B_{01} + A_{11}B_{11}\end{bmatrix}\] <p>The naïve algorithm shown above is still \(O(n^3)\). Strassen’s algorithm reduces the number of multiplications to \(7\) providing an improvement over the \(\mathcal O(n^3)\) algorithm giving \(\approx \mathcal O(n^{2.81})\).</p> <p>The current state of the art algorithm for matrix multiplication achieves \(\mathcal O(n^{2.371552})\). We do not know if there is an algorithm that achieves \(\mathcal O (n^{2 + o(1)})\).</p> <h3 id="algorithm-2">Algorithm 2</h3> <p>Multiplication can be seen as a special case of convolution and we can use <strong>Fast Fourier Transform (FFT)</strong> to perform this in \(\mathcal O(n \log n)\). The details will be elaborated in the next section.</p> <h2 id="convolution">Convolution</h2> <p>Consider two vectors of the following form -</p> <ul> <li> \[a = (a_{n - 1}, a_{n - 2}, \dots, a_2, a_1, a_0)\] </li> <li> \[b = (b_{n - 1}, b_{n - 2}, \dots, b_2, b_1, b_0)\] </li> </ul> <p>The convolution operation \(\star\) is defined as</p> \[c = a\star b = (c_{n - 1}, \dots, c_0) \quad \text{ where } c_j = \sum_{0 \leq k &lt; n} a_j b_{(j - k)\mod n}\] <p>Convolution is a generalization of integer multiplication (padding + convolution = multiplication). Also, convolution is a central operation in signal processing - used for blurring images and also to learn features from spatial data.</p> <p>The naïve algorithm can be done in \(\mathcal O(n^2)\) time. We can perform convolution using \(\mathcal O(n\log n )\) using <strong>Fourier Transform</strong>.</p> <h1 id="fourier-transform">Fourier Transform</h1> <p>Consider the \(n\) dimensional vector \(a = (a_{n - 1}, a_{n - 2}, \dots, a_2, a_1, a_0)\) and \(b = (b_{n - 1}, b_{n - 2}, \dots, b_2, b_1, b_0)\). Let \(\{e_i\}_i\) form a unit basis of \(\mathbb R^n\) such that \(a = \sum_{0 \leq i &lt; n} a_i e_i, b = \sum_{0 \leq i &lt; n} b_i e_i\).</p> <p>Consider another basis \(\hat e_i(j) = \omega_n^{ij}\) where \(\omega_n = e^{\frac{1\pi \bf{i}}{n}}\) is the \(n\)-th root of unity. Therefore, \(\hat e_i = \frac{1}{\sqrt{n}} \omega_n^{(n - 1)i}, \dots, \omega_n^{2i}, \omega_n^{i}, 1)\).</p> <p>It is easy to check that this is a valid basis. So, again, \(a, b\) can be uniquely represented as</p> <ul> <li> <p>\(a = \sum_{0 \leq i &lt; n} \hat a_i \hat e_i\), \(\hat a_i = \langle a_i, \hat e_i\rangle = \frac{1}{\sqrt{n}} \sum_j a_j \omega_n^{-ij}\)</p> </li> <li> \[b = \sum_{0 \leq i &lt; n} \hat b_i \hat e_i\] </li> </ul> <p>A <strong>Fourier transform</strong> is then defined as - Given \(\{a_i\}_{i \in [n]}\), compute \(F(a) = \{\hat{a_i}\}_{i \in [n]}\).</p> <p>The <strong>inverse problem</strong> is to find \(F^{-1} (\hat a) = \{a_i \}_{i \in [n]}\). It essentially is a change of basis between \(\{e_i\} \iff \{\hat e_i\}\).</p> <h2 id="convolution-theorem">Convolution Theorem</h2> <p>Let \(a, b\) be two vectors in \(\mathbb R^n\); then,</p> \[a \star b = F^{-1} (F(a) \cdot F(b))\] <p>With this claim, convolution can be f=done in \(\mathcal O(2T_{FT} + T_{IFT} + n)\).</p> <h1 id="dynamic-programming">Dynamic Programming</h1> <h2 id="longest-path-on-a-dag">Longest path on a DAG</h2> <p>Given a DAF with \(n\) vertices, \(m\) edges, every edge \(e\) has a weight \(l_e\), compute the longest path on the DAG. The length of a path is defined as the weight sum over all edges in the path.</p> <p>Consider the following algorithm</p> <pre><code class="language-pseudocode">DFS(u):
    if marked[u] = true:
        return DP[u]
    cost &lt;- 0
    for all v that (v, u) in E:
        cost &lt;- max(cost, DFS(v) + l_{v, u})
    marked[u] &lt;- true
    DP[u] &lt;- cost
    return cost
</code></pre> <p>The time complexity of the algorithm is \(\mathcal O(n + m)\). The key point to notice is that instead of recomputing the cost of each path, we have essentially stored the costs in the array <code class="language-plaintext highlighter-rouge">DP</code> to reduce the redundant calculations. This step is known as <strong>memoization</strong>.</p> <h2 id="knapsack-problem">Knapsack Problem</h2> <p>Consider an integer \(U\) representing total capacity and a list of integers \(\{v_i, c_i\}\) that represents the volume and cost of each item respectively. The goal is to pick items such that their total volume is at most \(U\) and their value is maximized.</p> <p><strong>Idea 1</strong>. Sort everything by \(v_i/c_i\) and pick the items until value if \(U\). It is easy to see that this greedy algorithm will not work.</p> <h3 id="a-backtracking-algorithm">A Backtracking algorithm</h3> <p>Consider an iterative algorithm that at step \(i\) has \(C\) volume left and is considering whether to pick or skip the \(i\)-th item. Considering these two possibilities, we can implement a brute force algorithm with memoization for dynamic programming.</p> <p>We set a 2D matrix of size \((U, n)\) where each row \(i\) represents the set of items that need to be picked to maximize the cost within volume \(U\). The time complexity of this algorithm would be \(\mathcal O(2^n)\).</p> <p>The algorithm is as follows -</p> <pre><code class="language-pseudocodedfs(C,">   dfs(C, i):
       if i = 0: return 0
       Cost &lt;- dfs(C, i - 1)
       if C &gt;= c_i:
           Cost &lt;- max(Cost, dfs(C - c_i, i - 1) + v_i)
       return Cost
</code></pre> <h3 id="alternative-view">Alternative view</h3> <p>We can treat every possible \((C, i)\) as a vertex in a graph. Every vertex has at most two outcoming edges - \((C, i) \to (C - c_i, i - 1)\) with cost \(v_i\) and \((C, i) \to (C, i - 1)\) with cost \(0\). This constructed graph is a DAG and we essentially reduced Knapsack problem to longest path on a DAG.</p> <p>Based on the algorithm we have seen earlier, we modify the algorithm to include memoization</p> <pre><code class="language-psuedocode">   dfs(C, i):
       if marked[C][i] = true: return DP[C][i] // Modification
       if i = 0: return 0
       Cost &lt;- dfs(C, i - 1)
       if C &gt;= c_i:
           Cost &lt;- max(Cost, dfs(C - c_i, i - 1) + v_i)
       marked[C][i] &lt;- true // Modification
       DP[C][i] &lt;- cost // Modification
       return Cost
</code></pre> <p>The modified algorithm now has the time complexity \(\mathcal O(Un)\) since there are \(Un\) vertices in total with atmost 2 edges each.</p> <h2 id="general-observation">General observation</h2> <p>Dynamic Programming problems can be typically thought og as a decision-making processes. These decision problems can be converted to graphs where the states are vertices on a graph and the transitions are edges on a graph. Typically, the problem have a DP solution if the graph is a DAG and the number of states is not too large.</p> <p>The algorithm shown above can then be used as a general procedure to solve the problems. Sometimes, it is beneficial to implement the algorithms with a loop rather than recursion.</p> <h2 id="knapsack-with-unlimited-items">Knapsack with unlimited items</h2> <p>The algorithm remains pretty much the same except that teh recrusion call has <code class="language-plaintext highlighter-rouge">dfs(C - c_i, i)</code> instead of <code class="language-plaintext highlighter-rouge">dfs(C - c_i, i - 1)</code>.</p> <h2 id="knapsack-with-limited-items">Knapsack with limited items</h2> <p>We can consider another variant where item \(i\) can be used at most \(k_i\) times. Then, a similar algorithm would have the time complexity \(\mathcal (U \sum_i k_i)\).</p> <p>A better solution treats the \(i\)th item as \(\lceil \log k_i\rceil\) items. For example, if \(k_i = 8\), then divide the item as \((c_i, v_i), (2c_i, 2v_i), (4c_i, 4v_i), (c_i, v_i)\). Then the time complexity would be reduced to \(\mathcal O(U \sum_i \log k_i)\).</p> <p>However, it can be improved to \(\mathcal O(Un)\) using a <strong>monotonic queue</strong>.</p> <h2 id="2d-knapsack">2D Knapsack</h2> <p>In this variant, each item has value \(v_i\), volume integer \(c_i &gt; 0\) and a weight integer \(w_i &gt; 0\). The goal is to find a subset of items that has the total volume at most \(U\), total weight at most \(W\) and the total value is maximized. The dynamic programming algorithm has a runtime of \(\mathcal O(WUn)\).</p> <p>In the loop variant of the algorithm, it is better to iterate over the items first rather than the weights. Why is that? Furthermore, it is better to iterate decreasing the costs, because</p> <h2 id="summary">Summary</h2> <p>Many intractable problems can be efficiently solved on trees - combining with DFS enforces the computation ordering.</p> <h1 id="bellman-ford-algorithm">Bellman-Ford algorithm</h1> <h3 id="single-source-shortest-path-sssp">Single Source Shortest Path (SSSP)</h3> <p>Given a directed graph of \(n\) vertices and \(m\) edge, find the shortest paths for a vertex pair \((s, t)\).</p> <p>Recently, researchers showed that Dijkstra’s algorithm is the most optimal algorithm possible with a <em>specially</em> designed heap.</p> <p>Would dynamic programming work for SSSP? \(dp[u][i]\) represents a path from \(s\) to \(u\) using at most \(i - 1\) edges. This simply is the Bellman-Ford algorithm.</p> <h1 id="floyd-warshall-algorithm">Floyd-Warshall algorithm</h1> <p>All pair shortest path algorithm in \(O(n^3)\).</p> <h1 id="traveling-salesman-problem">Traveling Salesman Problem</h1> <p>Given.a complete graph of \(n\) vertices, every edge has a cost \(l_e\). The goal is to find a path that visits every node exactly once while minimizing the total cost. It is an NP-hard problem where the brute-force algorithm takes \(\\mathcal O(n!)\) - enumerating all possible permutations of nodes.</p> <p>There is a dynamic programming solutions of the order \(O(2^n n^2)\). This is computable with the modern computers upto \(n = 15\). We discuss the framework here -</p> <ul> <li> <p><strong>States</strong> - At node \(u\), set of all unvisited states - minimum cost to finish the rest of the task. The number of states is \(\mathcal O(n2^n)\).</p> </li> <li> <p><strong>Decision-making</strong></p> </li> <li> <p>What will be the next node to visit? If we choose \(v \in S\), then the next state will be $$(v, S - {v})$</p> </li> </ul> <p>This can be implemented using a bitmask for representing the state - saves space and is faster.</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[A collection of ideas for design algorithms and analyzing them.]]></summary></entry><entry><title type="html">Tabletop Manipulation Algorithms</title><link href="https://sudhansh6.github.io/blog/Tabletop-Rearrangement/" rel="alternate" type="text/html" title="Tabletop Manipulation Algorithms"/><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Tabletop-Rearrangement</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Tabletop-Rearrangement/"><![CDATA[<h2 id="problem">Problem</h2> <p>Given a table top with various objects, their initial positions and final goal positions, we want to find the algorithm to rearrange the objects with the <strong>least number of actions</strong> and <strong>low buffer space</strong>.</p> <h3 id="buffers">Buffers</h3> <p>If one object is obstructing the goal position of the other, we would want to use <em>buffer space</em> to free-up the goal space. In a brute force manner, we can place everything on the side and put objects in their goals states. We want to optimize the number of actions, and doing this doesn’t do well in that scenario.</p> <p>The problem in the general case reduces to traveling salesman problem or the vertex cover problem, which are both NP-hard. We want to find a good approximation algorithm to get the least amount of actions.</p> <p>We also want real-time execution time because we do not want the algorithm to run for a long time.</p> <h3 id="variations">Variations</h3> <ol> <li> <p>Mobile and mobile base</p> </li> <li> <p>Disk objects and different shape objects</p> </li> <li> <p>stacking vs not stacking objects</p> </li> <li> <p>External buffer or not</p> </li> <li> <p>Single vs multiple tables</p> </li> <li> <p>Overlaps vs non-overlapping objects</p> </li> </ol> <h3 id="search-problem">Search Problem</h3> <p>The problem can be converted to a search problem where the search space is a tree representing all the possibilities of movements of objects (to goal positions) possible with the current configuration. The actions would be of the form “move object ‘i’ to buffer”, “move object ‘i’ to goal position”, etc. At each state, we have at most \(2n\) actions (if there are \(n\) objects on the table) - moving each object to buffer or goal.</p> <p>Obviously, this is a very high-dimensional space, and rudimentary search algorithms would not fare well if used directly.</p> <p>Let us now see some algorithms that try to tackle this.</p> <h2 id="trlb">TRLB</h2> <p><a href="https://arxiv.org/abs/2110.12325">Fast High-Quality Tabletop Rearrangement in Bounded Workspace</a> does the following -</p> <ol> <li> <p>Calculate a primitive plan by assuming there is always a feasible buffer space on the table (assume you have a second table to place objects)</p> </li> <li> <p>Try executing this plan, and assign buffer as we go along the path.</p> </li> <li> <p>If we fail to assign a buffer, we add a node to mark this search path.</p> </li> <li> <p>We repeat the procedure by finding a primitive plan from this stage until we find a feasible path.</p> </li> </ol> <p>The goal of this algorithm is to quickly calculate the solutions but it has a suboptimal traveling cost (example, robot mobile base needs to move a lot). Note that this is a non-deterministic algorithm, since we calculate random primitive plans.</p> <h2 id="orla">ORLA*</h2> <p>Aims to calculate the optimal path without considering the time as a heuristic. It essentially is \(A^*\) adapted to the table-top rearrangement problem - \(f(n) = g(n) + h(n)\) where \(g(n)\) is the travel cost from start to current node and \(h(n)\) expected travel cost from current node to goal.</p> <p>It results in an optimal plan but has a very long execution time. Another point to note is that ORLA* considers a mobile base which not many papers have considered previously.</p> <h2 id="our-research">Our Research</h2> <p>We aim to build an algorithm that gives the optimal solution and also executes fast. We assume mobile base manipulation robot with arbitrary shape objects and no external buffer.</p> <h3 id="approach-1">Approach 1</h3> <p>Perform search in TRLB using MCTS to find the most optimal search plan. Basically, execute TRLB for a fixed time to get multiple feasible paths. TRLB stops as soon as it finds a feasible path, but we execute it until we get a fixed number of possible paths.</p> <h3 id="bit">BIT*</h3> <p>We initially find a suboptimal path using TRLB* - this is fast.</p> <h2 id="future-directions">Future Directions</h2> <ol> <li> <p>Extend the algorithms for multi-agent scenarios where the problem becomes much harder.</p> </li> <li> <p>Learn conflict detection in TRLB or the modified algorithm to do better backtracking - one of the key ideas for combinatorial search algorithms. This can be done via SAT solving - learn conflicts as new constraints. This is particularly important when the number of objects on the table is high with a high density.</p> </li> </ol>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[Various methods for optimal mobile tabletop rearrangement - rearrange objects on the table with the least cost.]]></summary></entry><entry><title type="html">Evaluating interactions in music</title><link href="https://sudhansh6.github.io/blog/Evaluting-Music-with-RL/" rel="alternate" type="text/html" title="Evaluating interactions in music"/><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Evaluting-Music-with-RL</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Evaluting-Music-with-RL/"><![CDATA[<p>How do we identify how well two pieces of music complement each other? From notes, scales, rhythms, chords, time signatures, and many more abstract concepts associated with music, it is an interesting problem to capture the interactions between music pieces. This problem requires concepts from signal processing</p> <p>How is music represented? A Mel spectrogram is used to represent frequencies across time with amplitudes for audio files. A MIDI file is a tabular description of beats, positions, pitch, durations and instruments. These tabular instances can easily be converted to vector representations using one-hot encodings and feature normalization.</p> <p>Using such representations, the paper <a href="https://arxiv.org/abs/2207.06983">Multitrack Music Transformer</a> attempts to capture the interactions with attention mechanisms in a transformer. To understand this better, let us look at some theory.</p> <h3 id="total-information-flow">Total Information Flow</h3> <p>The total information flow is the sum of transfer entropies from \(X\) to \(Y\) and \(Y\) to \(X\).</p> \[T_{X \to Y} + T_{Y \to X} = H(X \vert \bar X) + H(Y \vert \bar Y) - H(XY \vert \bar{XY})\] <p>If the combined sequence \(XY\) does not make sense musically, then it will have a higher entropy, leading to lower total information flow. This concept is explored in depth in this context in this paper - <a href="https://arxiv.org/abs/2402.06810">Evaluating Co-Creativity using Total Information Flow</a>.</p> <h3 id="conditional-entropy">Conditional Entropy</h3> <p>Represented as \(H(Y \vert X)\) it measures how unpredicatble \(Y\) is, given that we have \(X\). How is this useful? A pre-trained music transformer is used to model \(p(X \vert \bar X)\) which represents the probability of a particular <em>token</em> (next part of music) after seeing a particular set of tokens (music till that point).</p> <h2 id="research">Research</h2> <p>Using the features from a MIDI file directly would not yield the best results. It composes of monotonically increasing data (beat), categorical features (instruments) and repeated values (position).</p> <p>This problem can also be posed as a Reinforcement Learning Problem - using total information flow as a reward. For example, <a href="https://arxiv.org/abs/2002.03082">RL-Duet: Online Music Accompaniment Generation Using Deep Reinforcement Learning</a> formulated a Deep RL algorithm for online accompaniment generation. The generation agent learns a policy to generate a musical note (action) based on previously generated context (state). RL has potential for real-time human-machine duel improvisation.</p> <h1 id="rl-duet">RL-Duet</h1> <p>What is the goal? Create an agent that can generate music <em>interactively</em> with a human. Again, this is done via Symbolic MIDI pitch + beat. The earlier approaches used GANs which have large data requirements and usntable training. Another approach, using Gibbs sampling, iteratively modifies the music fragments based on the previous context. However, this approach cannot be done in an online fashion.</p> <p>Typical approaches in reinforcement learning for sequence generation use maximum likelihood estimation. However, to improve the perceptual quality, global coherence and harmony, specific hand-crafted music rules-based rewards work much better.</p> <p>The work in RL-Duet captures the horizontal temporal consistency and vertical harmony relations for the reward function of the RL-agent. The current state is the previously generated context (by both human and the agent) and the action as mentioned before is symbolic MIDI (note and pitch). This involves horizontal view (like linear bi-directional language modeling in NLP), vertical part, joint modeling and hand-crafted rewards.</p>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[Capturing the quantifiers required to comment on how well two pieces of music complement each other.]]></summary></entry><entry><title type="html">Reinforcement Learning in Nuclear Fusion</title><link href="https://sudhansh6.github.io/blog/Control-for-Nuclear-Fusion/" rel="alternate" type="text/html" title="Reinforcement Learning in Nuclear Fusion"/><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Control-for-Nuclear-Fusion</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Control-for-Nuclear-Fusion/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>What is Nuclear fusion? A process where lighter nuclei combine to form heavier atoms. It is of interest to researchers because of its potential to provide clean and long-term energy source. Humans are able to (atleast trying) perform these reactions by carefully controlling plasma through magnetic fields. Plasma is a state of matter where the electrons are stripped from their atoms past a certain temperature. These are extremely hot and require precisely controlled magnetical fields to be stored.</p> <h2 id="fusion-methods">Fusion Methods</h2> <p>There are two methods through which nuclear fusion is being performed currently -</p> <ul> <li> <p>Inertial Confinement Fusion (ICF)</p> <ul> <li>These facilities tend to be very large, and are impractical to scale.</li> </ul> </li> <li> <p>Magnetic Confinement Fusion (MCF)</p> <ul> <li>The goal is to confine hot plasma without touching walls. The device used for this is called as a <strong>tokamak</strong> that uses a central solenoid and induced current to control the magnetic fields.</li> </ul> </li> </ul> <h3 id="tokamak">Tokamak</h3> <p>It’s a sonu shaped device with a central solenoid that generates toroidal and poloidal magnetic fields to confine plasma and maintain high temperatures. Through these fields we want to control the radial position, vertical extent and overall shape of the plasma which is important for the fusion reactions.</p> <p>Recently, KSTAR in South Korea was able to confine plasma for about a minute which is an impressive result for the current sota. In a huge international effort, ITER (International Thermonuclear Experimental Reactor), the world’s largest tokamak is currently being built in Southern France.</p> <h3 id="controls-engineering-process">Controls Engineering Process</h3> <ul> <li> <p><strong>Feedforward stage</strong> - Find the ideal setting for fusion experiment objectives - precompute coil curents, plasma shape, etc.</p> </li> <li> <p><strong>Feedback control stage</strong> - Closed loop control with magnetic measurements to determine the state of the plasma. Maintaining the required state boils down to solving a nonlinear PDE.</p> </li> </ul> <h2 id="better-control">Better control</h2> <p>Tradtional algorithms are able to control the vertical position of the plasma really well but they can be improved for radial control and are a result of intricately designed controllers stiched together. A black-box AI solution shows promise as a single control policy for more robust non-linear control. However, there can be a lack of performance guarantees along with non-interpretability with AI algorithms.</p> <p>To emphasise on safety certifications, there has been works using Lyapunov functions.</p> <h3 id="stability-guarantees">Stability Guarantees</h3> <p><strong>Lyapunov</strong> functions are scalar functions used to verify stability of a system by modeling evolution of system moving from one energy level to another. Recently, work in Neural Lyapunov controls has extended these stability guarantees with the use of neural networks.</p> <h3 id="deep-reinforcement-learning-approaches">Deep Reinforcement Learning Approaches</h3> <p>“<a href="https://www.nature.com/articles/s41586-021-04301-9">Magnetic control of tokamak plasmas through deep reinforcement learning</a>” by Degrace aims to provide a solution for feedback control stage by estimating the current state using various measurements. The challenge arises in modelling such a high-dimensional space - 92 system measurements (poloidal coil currents, flux loops, magnetic probes) and the actions are continuous (19 control coil voltages) and also the absence of a good simulator for this problem.</p> <p>The authors use a simulated environment which is implemented using a Forward FRad-Shafranov solve (FGE). Their algorithm was able to maintain different shapes of the plasma which is not possible with conventional algorithms.</p> <p>Another paper by KSTAR - <a href="https://iopscience.iop.org/article/10.1088/1741-4326/ac79be">Development of an operation trajectory design algorithm for control of multiple 0D parameters using deep reinforcement learning in KSTAR</a> - used <em>TD3</em> (RL algorithm) for feedforward controls. That is, to estimate the desired state of plasma required for the experiments. The states capture “how well the fusion reaction is going”, the safety indices, etc to determine the required controls.</p> <p>Interestingly, instead of a physics-based simulator, they used a data-based simulator trained using an LSTM network on around 5 years of data.</p> <h2 id="current-work">Current Work</h2> <p><strong>Neural Lyapunov control</strong> - Roughly, we want to generate a lyapunov risk formulation for the fusion environment. The challenge is the for the fusion environment, we do not know the system dynamics, and we want to build a neural approximation for estimating this value. This can then be used for the lyapunov risk, which can finally be used for the control algorithms <em>with safety guarantees</em>. In summary, we are trying to improve the above papers by embedding safety guarantees with the methods.</p>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[Introduction to reinforcement learning algorithms for control in nuclear fusion reactors]]></summary></entry><entry><title type="html">Scalable Behavior Planning for Autonomous Driving</title><link href="https://sudhansh6.github.io/blog/Behavior-Planning-for-Autonomous-Driving/" rel="alternate" type="text/html" title="Scalable Behavior Planning for Autonomous Driving"/><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Behavior-Planning-for-Autonomous-Driving</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Behavior-Planning-for-Autonomous-Driving/"><![CDATA[<p>A typical urban scene has numerous people and vehicles with various behaviors paths coupled with rules of the road. Building a framework for predicting behaviors for path planning in such scenes is a difficult task. The frameworks for building algorithms in these scenes involve</p> <ul> <li> <p>Sensors for Perception - RGB, LIDAR, Infra-red</p> </li> <li> <p>Control Algorithms - Motor commands</p> </li> </ul> <p>The planning algorithms have two components to them</p> <ul> <li> <p>Global planning - To plan the coarse waypoints from the source to the destination. Dijkstra’s and A\(^*\) search are typically used for these.</p> </li> <li> <p>Motion planning - To plan the control commands based on the environment between the waypoints. It involves behavior planning and path planning algorithms. Behavior planning involves predicting motion of objects on the road and analysing the traffic signs. The path planning part involves generating multiple trajectories which are pruned down from the perception data (sorting them based on a cost function).</p> </li> </ul> <h2 id="behavior-planning">Behavior Planning</h2> <h3 id="state-diagram-algorithms">State Diagram Algorithms</h3> <p>Some classical approaches for this problem involves state diagrams that have a deterministic policy based on symbolic descriptions of the world. Basically, think of these as complicated <code class="language-plaintext highlighter-rouge">if-else</code> structures. <a href="https://ieeexplore.ieee.org/document/4290200/">Behavior Nets</a> is one of the state-of-the-art approaches in this realm.</p> <p>However, these approaches are prone to errors due to sensor uncertainity, state uncertainiity, uncertain temporal evolution and occlusions. These are corrected using Markov Decision Processes</p> <h3 id="markov-decision-processing">Markov Decision Processing</h3> <p>They involve deterministic or stochastic policy within a framework that models uncertainty by evaluating the future. The MDPs are coupled with a risk-factor to make the optimal strategy the safe strategy. One such approach is listed in <a href="https://ieeexplore.ieee.org/document/6082928">Probabilistic MDP-behavior planning for cars</a>. However, these do not work quite well in dynamic world. Also, in dense scenes the dimensionality becomes very high making this approach unscalable. The number of agent vary as well which cannot be modeled completely using an MDP.</p> <p>In <a href="https://arxiv.org/pdf/2011.04697">Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning</a> the authors simply choose the nearest $5$ agents to model the algorithm and use an RL algorithm to plan the path. However, this approach is data hungry, requires simulators, has low data coverage and oversimplifies the information in dense scenes.</p> <h3 id="online-algorithms">Online Algorithms</h3> <p>In <a href="https://www.ijcai.org/Proceedings/2017/664">Online Decision-Making for Scalable Autonomous Systems (MODIA)</a> the algorithm decomposes the dynamic scene into small decision problems for each agent in the environment. Each decision problem has a pre-defined MDP based on the class of the agent. After obtaining optimal actions from each of these problems, an <em>executor</em> finds the best action based on a <em>preference</em> function defined on a notion of safety. This way, the algorithm has a linearly-growing complexity and state abstractions which are explainable.</p> <h2 id="status-quo">Status-Quo</h2> <p>Taking a step back, we need to deal with dynamic and complex environments. We need to ensure the algorithms use limited computation power and work with the variety of sensors equipped in the vehicle.</p> <p>We do not have a good behavior-realisitc simulator and sufficient human labor annotations (in academic labs). Without enough data, how do we build algorithms for planning? The AVL Lab at UCSD does the following -</p> <ul> <li> <p>Collect rollouts - Data collection stack for behavior planning - point cloud maps, teleoperation platform that collects high and low-level control</p> </li> <li> <p>Training phase - Offline RL for each decision problem</p> </li> <li> <p>Real-world deployment - Online decision framework from MODIA.</p> </li> </ul> <p>This approach has many limitations. Since the number of states is very large, the state-transition matrix is very sparse, and this is not good for determining the optimal policy. These can be fixed using generative simulators (which do not work quite well cuurently) or bridging large dataset by outsourcing the data collection.</p>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[Discussion on algorithms for predicting behavior patters in dense urban scenes.]]></summary></entry></feed>