Just to summarize all ideas and the motivations behind our discussions, let us go through this again.

## Data Parallelism

Data parallelism was first suggested by Dean et. al. In DistBelief based on a Parameter server. This method was mainly based on sharding the data through batches. Then, this paradigm evolved, and integrated into PyTorch as `DDP`. 

When using data parallelism, there are two solutions to update the model parameters (assuming the models can fit inside one GPU worker)

### Parameter server
As proposed in the original DistBelief paper, the parameter server is to help with gradient descent (not stochastic). The gradients are accumulated across the workers (have to wait for all workers) and are used to update the parameters. These parameters have to be replicated across all workers for further forward computations (server bottleneck). The compute to communication ratio for this is 1:10 in 2021. 

This approach can be improved to some extent by using a distributed parameter server (sharded KV stores). This ensures a redundancy across different PS shards. The savings essentially comes from using AllReduce - no server bottleneck and faster gradient sharing by workers. When the parameter server nodes are same as the worker nodes in the previous setup, the configuration simplifies a lot more. The operations then become Reduce-scatter followed by Allgather. 

Even then, one slow worker can slow down the whole system.

![](/assets/img/2025-01-06-data-systems-for-ml/17399335217090.jpg)
The problem is that we have been waiting for all workers to update the parameters. However, machine learning systems are error-tolerant to some extent. Deviating from the gradient can still lead to the same performance (similar to why stochastic gradient descent works).

So adopting a form of asynchronous communication wherein the parameters are updated every time a worker finished the computation. We can set a staleness threshold to discard very old messages for improved performance. It turns out that the training stability is heavily dependent on this parameter. There were a lot of theorems for this line of work too!

### All-Reduce 
`DDP` is a form of Allreduce type of data parallelism. Allreduce was initially implemented in Horovod (was designed by Uber!). It was further optimized by NVIDIA in NCCL. It was then adopted in PyTorch as DDP. 

Also, Allreduce is less fault-tolerant than parameter server! However, this paradigm is more popular since ML systems are somewhat robust to noisy updates. Allreduce does not have consistency mechanisms too!

It became popular due to its simplicity as also because of the release of NVLink - more helpful for Allreduce. 

## Model Parallelism
As we’ve discussed model parallelism is usually paired with data parallelism (once models stopped fitting in one worker). There are two kinds of sharding - inter-op and intra-op. 

### Inter-operator Parallelism
Inter-op parallelism usually has smaller communication requirements since only outputs at stage boundaries have to be transferred. However, unless pipelining, there are many hardware bubbles. Without pipelining, we have a bubble percentage of $$(D - 1)/D$$ and with pipelining we have $$(D - 1)/(D - 1 + N)$$ where $$D$$ is the number of devices and $$N$$ is the number of inputs. 

Note that we are ignoring the computation models for starting GPU kernels and other overheads for simpler models. 

However, this is mainly for inference. For doing backward pass, the devices need to be executed in reverse! Pipelining won’t do. So, how do we reduce the bubbles for training?

1. Device Placement - Slice the branches of a neural network into multiple stages so they can be calculated concurrently. 

    ![](/assets/img/2025-01-06-data-systems-for-ml/17399350251810.jpg)
    This only works for models with branches such as the Inception model. Contrastive models also include branches and can be used. Other convolutional networks and transformers cannot be used with this. 
2. Synchronous Pipeline Parallel Schedule - Modify pipeline schedule to improve efficiency, but keep the computation and convergence semantics exactly the same as if training with a single device. The advantage is that the training process is exactly the same. However, pipeline bubbles are always present and reducing pipeline bubbles typically requires splitting inputs into smaller components, and smaller inputs may not use the hardware to the fullest extent.
    1. **GPipe** (Google) - Partition the input batch into multiple “micro-batches”, and accumulate the gradients of the micro-batches.
        ![](/assets/img/2025-01-06-data-systems-for-ml/17399351787754.jpg)
        The size of the microbatch has to be chosen so that each device is utilized to the full extent to maintain high arithmetic intensity. This choice is hard to consider. Due to the structure of GPipe, the memory usage of each device increases linearly to a peak and reduces. Each device has to store the outputs from the micro-batches until the backpropagation is called.
    2. **1F1B** - To prevent this memory peak, the 1 forward, 1 backward schedule was introduced. The idea is to perform backward pass as early as possible. 
        ![](/assets/img/2025-01-06-data-systems-for-ml/17399357286232.jpg)
        It maintains the same latency as GPipe but with a lower memory usage. (Exam)
    3. **Interleaved 1F1B** - The previous idea can be optimized further to slice the neural network into more fine-grained stages and assign multiple stages to reduce pipeline bubbles.
        ![](/assets/img/2025-01-06-data-systems-for-ml/17399360266930.jpg)
        This is super hard to debug. 
    4. **TeraPipe** - This method was mainly suggested for autoregressive models. The key idea is that the computation of an input token only depends on previous tokens but not the future tokens. The bubbles are reduced by pipelining with a sequence. 
        ![](/assets/img/2025-01-06-data-systems-for-ml/17399362100774.jpg)

    1. **Chimera** - Store bi-directional stages and combine bidirectional pipeline to further reduce bubbles. Deepseek used this! Optimized 1F1B schedule
        ![](/assets/img/2025-01-06-data-systems-for-ml/17399363005548.jpg)
        The only issue is that the model parameters have to be stored twice. But if you use a model with half the number of parameters, then you’re good!
 1. **Asynchronous pipeline schedules** - The idea is to start the next round of forward pass before the backward pass finishes. It breaks the synchronous training semantics, and training involves stalled gradients. The algorithms may have to store multiple versions of model weights for consistency. 
     1. **AMPNet** - Each device performs forward pass whenever free and updates the weights after every backward pass. It does not generalize well to larger datasets. 
        ![](/assets/img/2025-01-06-data-systems-for-ml/17405374850797.jpg)

     1. **Pipedream** - Enforce the same version of weight for a single input batch by storing multiple weight versions. Has a similar accuracy on Image Net with a 5x speed up compared to data parallel. 
     2. **Pipedream-2BW** - Reduced memory usage by only storing 2 copied and updating the weights less frequently. The weights are always stalled by 1 update. It achieved similar accuracy on larger models like BERT and GPT.

     
**Imbalanced pipelines** - All these schedules work best when the stages are balanced. However, there can scenarios where the stages do not have equal workloads. There have been works for automatic stage partitioning that try to minimize maximum stage latency and maximize parallelization. 

**Placeto: RL-based partitioning algorithm** - The state is given by the device assignment plan for a computation graph, and the actions are modifying the device assignment of a node. The rewards are latency differences between new and old placements. 
