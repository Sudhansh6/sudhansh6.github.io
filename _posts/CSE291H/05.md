## Temporal Difference

In Monte Carlo methods, we update the value function from a complete episode, and so we use the actual accurate discounted return of the episode. However, with TD learning, we update the value function from a step, and we find an estimated return called **TD target** - a bootstrapping method similar to DP.

$$
\begin{align*}
\text{Monte Carlo }&: V(S_t) \gets V(S_t) + \alpha[G_t - V(S_t)] \\ 
\text{TD Learning }&: V(S_t) \gets V(S_t) + \alpha[R_{t + 1} + \gamma V(S_{t + 1}) - V(S_t)]
\end{align*}
$$


The high-level view of MCTS is
![](/assets/img/AIAgents/17388852350762.jpg)

## AlphaGo: A case study
The game has a large number of states. The rewards we use are $$\plusminus 1$$ based on the player won. We define policies for both the players and *train* the policies with self-play. 

Making use of the symmetry of the game, we can use the episodes of the opponent player seen before to train the policy.

AlphaGo used these exact MC methods with neural networks (CNNs, which is super useful for Go) to learn the probabilities and outcome rewards. It was trained with a lot of human games to train initial value networks. The developers also hand-crafted features to represent knowledge in the game.

AlphaZero, an extension of this, relaxed the constraint of requiring a lot of human data and scaled.

All these algorithms have been model-free. That is, we cannot estimate the consequences of our actions in the environment, and are simply learning based off our experiences. We are not learning anything about the dynamics of the environment.

On the flip side, if we know the model of the world, can we do better? So given a *world model*, how do we use it?

# Model-based RL

We learn the model of the world from experiences, and then plan a value function (and/or policy) from the model. What is a model? A representation of an MDP $$(S,A,T,R, \gamma)$$, and we try to approximate $$T, R$$.

*Assumption.* A key assumption that developers make is that the state transitions and rewards are conditionally independent.

We have the experience $$S_1, A_1, R_2, \dots, S_T$$, and we just train a model in a supervised problem setting $$S_i, A_i \to R_{i + 1}, S_{i + 1}$$. Learning $$R$$ is a regression problem and learning $$P$$ is 

How do we use the learned model? Since the learned model has errors and uncertainty, training a policy would take a long time. It is like we are learning the rules of Go whereas previously we knew the rules, and were just trying to win. 

The advantages of model-based RL is the it can use all the (self, un) supervised learning tricks to learn from large scale data and can reason about uncertainty. The disadvantage is that we need to first build a model, and then estimate a value from the estimated model - introduced two sources of error. 




