## Optimal policies

> Using this formula, how quickly do we reach the local/optimal policy? What is the inductive bias to design the neural network architecture? Can we formulate better loss functions? How do effectively perform the computations?
> *Need to research about thisâ€¦*

Let us try answering the computational feasibility question. In a way, performing gradient ascent on the gradient formula we derived previously is analogous to performing gradient descent on the complete data set in supervised training. We can do better with a technique analogous to stochastic gradient descent (this is my understanding that could be incorrect), using Monte Carlo methods to perform the ascent on an estimate of the gradient $$\nabla J(\theta)$$. How do we get a Monte Carlo method out of the equation?

$$
\begin{align*}
    \nabla J(\theta) &= \sum_{t geq 0} \mathbb E_{S_t, A_t} Q(S_t, A_t) \frac{\partial}{\partial \theta} \ln (S_t, A_t, \theta) \\
    &\approx \mathbb E_{S_t, A_t} \underbrace{\left(\sum_{t geq 0}  Q(S_t, A_t) \frac{\partial}{\partial \theta} \ln (S_t, A_t, \theta) \right)}_{\text{samples}}\\
\end{align*}
$$

So, we take the average of the quantity $$Q \cdot \ln (\pi)$$ over multiple episodes using Monte Carlo sampling. These equations form the basis of the **REINFORCE** algorithm.

We further analyze this equation - how do we reduce the variance of the _unbiased_ estimator? We use something known as a **controlled variate approach** wherein for an estimator of the random variable $$X$$, we build another random variable $$Y$$ such that 

$$
\begin{align*}
    var(Y) &< 2cov(X, Y) \\
    var(X - Y + \mathbb E(Y)) &= var(X - Y) \\
    &= var(X) + var(Y) - 2cov(X, Y) < var(X)
\end{align*}
$$

