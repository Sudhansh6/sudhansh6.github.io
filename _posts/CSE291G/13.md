# Multi-Arm Bandit Problems

The multi-armed bandit problem involves a decision maker iteratively selecting one of multiple fixed choices when the properties of each choice are only partially known at the time of allocation. More formally, given $$K$$ bandits/actions/choices each returning a value from $$\{0, 1\}$$ guided by a distribution $$\mu_k$$. The goal of the decision maker is to choose the bandits so that the returned value is the highest.

Since the player does not known the underlying distributions, they have to learn about the properties through experience. Ideally, they may greedily sample the bandit having the highest $$mu_k$$, and the estimate for this distribution is built over time. The player has a tradeoff between exploitation of the choice that gives the highest expected payoff and exploration to get more information about the expected payoffs of the other machines. 

To quantify these choices, we introduce a quantity called **regret** - defined as the expected different between the reward sum associated with an optimal strategy and the sum of the collected rewards. So we define the following quantities

$$
    \begin{align*}
    N_k(T) &= \sum_{t = 1}^T 1 [I(t) = k] \\
    W_k(T) &= \sum_{t = 1}^T X_k(t) 1 [I(t) = k] \\
    \hat \mu_k(T) &= \frac{W_k(T)}{N_k(T)} \\
    W(T) &= \sum_{k = 1}^K W_k(T) \\
    \mathbb E(R(T)) = T \mu_1 - \sum_k \mu_k N_k(T)
    \end{align*}
$$

We say $$k = 1$$ is the best choice without any loss of generality. Our goal is minimize the expected regret $$\mathbb E(R(T))$$. 

Is it possible to make the regret sub-linear? The Upper Confidence Bound (**UCB**) algorithm achieves logarithmic regret in terms of $$T$$, and that is the best we have so far. 

Why do we define regret as the metric, and not something else?  It could be that this is the simplest and the most flexible and intuitive metric for this problem. Furthermore, we have considered stationary distributions. What about non-stationary ones? There is one detailed work for this with very rigorous theory by [Wei et. al.](https://arxiv.org/pdf/2101.08980).

So within our stationary setup, let us see why the UCB algorithm works. For each bandit we estimate a range $$[a, b]$$ for $$\hat \mu_k(t)$$ such that $$P[\mu_k \in [a, b]] \geq 1 - \delta$$. That is, we want to be able to estimate the properties of the bandits with a certain confidence. Given that we are dealing with Bernoulli variables, we have the following result

$$
    \begin{align*}
        P[\mu \geq \frac{1}{n} \sum Y_i + \epsilon] &\leq e ^{-2n \epislon^2 \\
    \end{align*}
$$

> The proof for the above Hoeffdingâ€™s inequality is to consider the probability nCk mu^k (1 - mu)^(n - k)

