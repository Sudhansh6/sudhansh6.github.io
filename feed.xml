<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://sudhansh6.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sudhansh6.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-29T23:51:51+00:00</updated><id>https://sudhansh6.github.io/feed.xml</id><title type="html">Sudhansh Peddabomma</title><entry><title type="html">Large Language Model Reasoning</title><link href="https://sudhansh6.github.io/blog/Large-Language-Models-Research/" rel="alternate" type="text/html" title="Large Language Model Reasoning"/><published>2024-10-14T00:00:00+00:00</published><updated>2024-10-14T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Large-Language-Models-Research</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Large-Language-Models-Research/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>As a part of this article, we delve into the paradigm of Chain of Though reasoning in Large Language Models. The aim is to highlight the importance of this idea and summarize the main research in this area. The blog should provide enough context for the reader in the field of AI to understand the basic concepts and think about the potential research ideas addressing the limitations of the current models.</p> <h1 id="chain-of-thought-reasoning">Chain of thought Reasoning</h1> <p>Chain of thought (CoT) refers to manifesting the human thought process in large language models by endowing language models with the ability to generate a chain of thought - a coherent series of intermediate reasoning steps.</p> <p>It is hypothesized that CoT prompting helps LLMs to tackle complex arithmetic, commonsense and symbolic reasoning tasks. The following demonstration highlights this improvement.</p> <p><img src="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&amp;w=1920&amp;q=75" alt="COT"/></p> <p>However, there are some limitations with this paradigm of reasoning with the current models.</p> <ul> <li>Small models are unable to improve with CoT prompting. LLMs with more than 100B parameters show performance gains with CoT.</li> <li>The performance improvements are larger with larger models. In other words, the benefits of CoT scale with the size of the models.</li> <li>Sometimes the models arrive at the correct answers with the wrong reasoning. The errors have been classified as <ul> <li><strong>Calculation error</strong> - LLMs are probabilistic models, predicting what token occurs next. So when an LLM tries to do \(3* 25* 8 =\), it does not really calculate the answer but probabilistically guesses the answer which is the next token. This highlights a fundamental limitation in the current architectures of LLMs.</li> <li><strong>Symbol mapping error</strong> - When there are too many variables involved, LLMs sometimes mix up the variables and arrive at the wrong answer. Again, the problem arises from the fundamental architecture flaw highlighted in the previous point.</li> <li>Other than these major errors, the models also have semnatic understanding problems, missing steps, incoherent chain of thought errors</li> </ul> </li> </ul> <h1 id="large-language-models-are-human-level-prompt-engineers"><a href="[[2211.01910] Large Language Models Are Human-Level Prompt Engineers](https://arxiv.org/abs/2211.01910)">Large Language Models are Human-level prompt engineers</a></h1> <p>The motivation of this paper is as follows -</p> <ul> <li> <p><strong>Human effort in prompt engineering</strong> - Crafting effective prompts for LLMs is time-consuming and requires significant human expertise.</p> </li> <li> <p><strong>Optimization challenge</strong> - Primpts greatly influence LLM performance, but users often lack insight into how to optimize them for specific tasks.</p> </li> <li> <p><strong>Scalability</strong> - As LLMs grow in size and capabilities, manuallt designing prompts becomes less deasible for a wide range of applications.</p> </li> <li> <p><strong>Automating promtp design</strong> - There is a growing need to automate the prompt engineering process to enhance LLM usability and performance.</p> </li> <li> <p><strong>Real-world impact</strong> - Applications in diverse domains (e.g., AI chatbots, automated content generation) can benefit from optimized and automated prompts.</p> </li> </ul> <p>This work promposes an <strong>Automatic Prompt Engineer (APE)</strong> - asystem that automates prompt generationg and selection for Large Language Models. This task is treated as a program synthesis task wherein the input-output pairs (natural language questions and answers) are given to the APE, and it has to generate the instruction needed to generate these pairs.</p> <p>In essence, the APE is trying to learn the prompts generated by humans. The framework is as follows -</p> <ol> <li> <p>Instruction Generation. An LLM is used as an ingeerence model where the “instruction candidates” are generated based on a small set of input-output demonstrations</p> <p>Example: The input to APE is of the form -</p> <p><em>Input 1</em> - Forward generation technique</p> <p>”””</p> <p>I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs</p> <p>Input: [ ] Output: [ ]</p> <p>Input: [ ] Output: [ ]</p> <p>…</p> <p>The instruction was &lt;COMPLETE&gt;</p> <p>””””</p> <p><em>Input 2</em> - Reverse generation technique</p> <p>”””</p> <p>I instructed my friend to &lt;INSERT&gt;</p> <p>The friend read the instructions and wrote an output for every one of the inptus. Here are the input-output pairs:</p> <p>Input: [ ] Output: [ ]</p> <p>Input: [ ] Output: [ ]</p> <p>…</p> <p>”””</p> </li> <li> <p>Scoring Instructions. Evaluate each instruction by computing a score that reflects how well the instruction guides the target LLM for the task. This is simply the confidence score associated with the log likelihoods of token generation. The authors consider a <em>moving average</em> score considering the probabilities for a window of tokens.</p> <p>They also consider an <strong>execution accuracy</strong> - the success of an instruction by checking if the model produces the correct output (0-1 loss). However, this cannot. be used for all kinds of instructions.</p> <p>The top $k$-percentile prompts are selected and the rest are discarded.</p> </li> <li> <p>LLM as Resampling Model. They apply an Iterative Monte search method to resample more prompts. The LLM generates semnatically similar instructions variants to improve the top-performing candidates.</p> <p>Once the prompts are generated, the moving average scores are generated for each of the prompts and the better scoring prompts are selected again.</p> </li> </ol> <p>Can APE be used to guide LLMs?</p> <p><img src="/Users/s/Desktop/Projects/Website/assets/img/bc45f7f43ae9ca99bce80bdc9e40d94dcf50dae6.png" alt=""/></p> <p>Although this is a very simple example, the work shows potential in taking such framework forward to work with more complex applications.</p> <p>Another interesting approach is to not generate the prompts from scratch, but to help humans design better prompts. Essentially, augment with context from humans to generate better prompts. On the flipside, RLHF can be used to improve these APE.</p>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[A survey of papers to better understand the workings of Large Language Models.]]></summary></entry><entry><title type="html">Design and Analysis of Algorithms</title><link href="https://sudhansh6.github.io/blog/Design-and-Analysis-of-Algorithms/" rel="alternate" type="text/html" title="Design and Analysis of Algorithms"/><published>2024-09-27T00:00:00+00:00</published><updated>2024-09-27T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Design-and-Analysis-of-Algorithms</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Design-and-Analysis-of-Algorithms/"><![CDATA[<h1 id="greedy-algorithms">Greedy Algorithms</h1> <h2 id="minimum-spanning-tree">Minimum Spanning Tree</h2> <p>Consider a graph \(G\) describe by \(V\) and \(E\) (positive weights). A <strong>spanning tree</strong> of a graph is defined as an edge set \(T \subset E\) such that \((V, T)\) is a tree. A minimum spanning tree is such that \(\sum_{l \in T} l_e\) is minimized.</p> <p>For a graph with \(n\) vertices, there are \(n^{n - 2}\) spanning trees for a complete graph (<strong>Cayley’s formula</strong>).</p> <p>How do we calculate the number of spanning trees for a general graph? <strong>Kirchoff’s theorem</strong> states the following -</p> <ul> <li>Let \(M\) be the adjacency matrix of \(G\)</li> <li>Let \(L - M\) except \(L_{i, i} = -deg(i)\) - This is generally called as <strong>Graph Laplacian</strong></li> <li>Then, #spanning trees is the determinant of any \(m-1\) square sub-matrix (obtained by removing \(i\)th row and column) of \(L\).</li> </ul> <p>Notice how any sub-matrix yields the same value!</p> <h2 id="greedy-idea-1-kruskals-algorithm">Greedy Idea 1: Kruskal’s algorithm</h2> <ul> <li>Sort all the edges with their weights</li> <li>Pick edges as long as they don’t form a cycle</li> </ul> <p>Does this work? If it does, how do we prove it?</p> <p>Firstly, why is it a greedy idea? At each point of the algorithm, we select the current greedy edge (a local minimum) to obtain the minimum spanning tree (a global optimum).</p> <ul> <li> <p><strong>The cut property -</strong> Let \(S \subseteq V\) such that \(S\) and \(V - S\) are non-empty. If \(e\) is an edge across \(S\) and \(V - S\) with the minimum cost, then there always exists a minimum spanning tree with \(e\).</p> <p><strong>Proof.</strong> Exchange argument. If there an MST with \(e’\) across \(S\) and \(V - S\), then replace \(e\) with \(e’\) to obtain another MST.</p> <blockquote> <p>Shouldn’t this argument be more delicate? Why is there a single edge from S to V - S?</p> </blockquote> <p>Essentially, the exchange argument argues replacing a part of the solution improves the solution but does not worsen it.</p> </li> <li> <p>The time complexity of the algorithm comes out to be \(O(m \log m + m \alpha(n))\). The second term in the expression comes from union-find data structures.</p> </li> <li> <p><strong>Correctness of the algorithm</strong> - We shall prove this via induction.</p> <ul> <li>Induction hypothesis - The edges selected in the \(i\)th round of Kruskal’s algorithm can form an MST along with a subset of edges from the remaining edges.</li> <li>Base statement - True for \(i = 0\)</li> <li>Induction step - Cut property</li> </ul> </li> <li> <p><strong>Union-find data structure</strong> - A data structure that supports</p> <ul> <li>Merging elements of two sets into a single set - <code class="language-plaintext highlighter-rouge">union(x, y)</code></li> <li>Checking whether two elements are in the same set - <code class="language-plaintext highlighter-rouge">find(x)</code></li> </ul> <p>efficiently. The amortized time complexity for these operations is \(\alpha(n)\) where \(\alpha(n) \leq 4\) for \(n\) of the order \(2^{2^{2^{2^{16}}}}\). As a result, \(\alpha(n)\) can be regarded as a constant for practical purposes.</p> <p>In our case, the elements are edges and sets represent connected components.</p> </li> </ul> <h2 id="greedy-idea-2-prims-algorithm">Greedy Idea 2: Prim’s Algorithm</h2> <p>Start with any node and expand with the smallest edge connecting to the remaining set of edges. Note that this is different from Kruskal’s algorithm where we sort all the edges and create individual connected components that eventually merge together.</p> <p>The proof for Prim’s algorithm is very similar to that of Kruskal’s. The time complexity is \(O(n^2 + m)\) similar to Djikstra’s algorithm without a data structure. We can maintain a <strong>priority queue</strong> to maintain all edges that come from \(S\) to reduce the time-complexity to \(O((n + m) \log m)\) (without decrease-key). With decrease key and a binary heap, the complexity becomes \(O((n + m) \log n)\). Furthermore, with decrease key and a Fibonacci heap, the complexity reduces to \(O((n\log n + m)\).</p> <h2 id="other-algorithms">Other algorithms</h2> <ul> <li><strong>Reverse deletion</strong> - For every cycle in the original graph and the edge \(e\) with the maximum cost, there always exists an MST without \(e\). Until there are no cycles in the graph, find a cycle and delete the edge with a maximum cost. Note that this algorithm has a higher time complexity since we try and find a cycle for each iteration of the algorithm. How do we implement this?</li> </ul> <h2 id="union-find-data-structure">Union-Find data structure</h2> <p>The idea is to maintain trees with pointers to merge and find elements. The main complication comes while merging the individual sets.</p> <ul> <li> <p>Merging by size (consuming smaller ones by larger sets) - The complexity of merging sets of size \(n\), \(m\) times takes \(O(m \log n)\)</p> </li> <li> <p>To optimize this further, we merge by rank (generalizing the previous approach where rank was simply the size of the set). We add another trick to reduce the amortized time complexity.</p> <ul> <li>Path compression - When <code class="language-plaintext highlighter-rouge">find(x)</code> is called, attach the found elements along the path directly to the root to reduce the path size.</li> </ul> <p>The time complexity then becomes \(O(m \log^* n)\) where \(\log^* n\) is the minimum \(k\) such that \(\log^{(k)} n \leq 1\).</p> </li> </ul> <h1 id="more-greedy-problems-related-to-mst">More Greedy Problems related to MST</h1> <h3 id="k-clustering">\(k\)-clustering</h3> <p>A <strong>maximum spacing</strong> for \(k\)-clustering of \(G =(V, E)\) is defined as</p> <ul> <li>An edge set \(T \subset E\) such that \((V, T)\) has exactly \(k\) connected components</li> <li>The <strong>spacing</strong> is then \(\min d(u, v)\) for \(u, v\) in different connected components</li> <li>The goal is to maximize the spacing</li> </ul> <p>This problem can be solved again with Kruskal’s algotihm to find \(k\)-connected components - perform the <code class="language-plaintext highlighter-rouge">union</code> operation for \(n - k\) times. Why is this correct? WE can show this using a contradiction.</p> <ul> <li>Consider two nodes that lie in the same connected component in the result obtained by Kruskal’s (with spacing \(d’\)). Let them be in different connected components in the optimal solution (with spacing \(d\)). Then,</li> </ul> <h3 id="second-mst">Second MST</h3> <p>A second MST is essentially the spanning tree with the <em>second</em> lowest edge summation cost. How do we find this tree?</p> <ul> <li>Find an MST with weight \(w\)</li> <li>For every edge \(e\) not in the MST, if adding \(e\) yields a cycle in the graph; then remove the largest edge \(e’\) other than \(e\) in the cycle to obtain the second MST</li> <li>The cost of the tree would be \(w + l_e - l_{e’}\)</li> </ul> <p>The time complexity of this algorithm is \(\mathcal O(T_{MST} + mn)\) and can be improved to \(\mathcal O(T_{MST} + m \log n)\) with better data-structures and divide-and-conquer.</p> <p><strong>Lemma.</strong> The second MST only differs by one edge from the MST. Multiple MSTs? <strong>Proof.</strong> Can be shown using contradiction. The idea is that one can move from one spanning tree to another with local changes in the trees. The argument is that you can replace the edges in the second MST with the edges in the MST to obtain a tree with a lower cost. This process can be repeated until there is only one edge that is different from an MST and replacing that would cause the tree to become the MST.</p> <h2 id="more-greedy-problems">More Greedy Problems</h2> <h2 id="needle-in-haystack">Needle in Haystack</h2> <p>Given two strings \(s, t\), decide whether there is a subsequence (need not be contiguous) in \(s\) that matches with \(t\). A naive greedy algorithm is depicted as follows -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">break</span> 
    <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">t</span><span class="p">[</span><span class="n">j</span><span class="p">]:</span>
        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># s[i] is matched with t[j]
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> <p>On first glance, it looks as if this is a very intuitive algorithm. However, there are more intricate details and the proof makes these clearer. The proof relies on the Exchange argument. If there exists a subsequence \(i_1 i_2 \dots i_m\) in \(s\) matching \(t\) (\(\vert t \vert = m\)). If \(i^*_1 &lt; i_1\) is the first index that \(s[i_1] = t[1]\), then \(i^*_1i_2\dots i_m\) also matches \(t\). This way we can find a set of indices \(i^*_1i^*_2 \dots i^*_m\) through the greedy algorithm that gives the correct answer.</p> <p>In general, greedy algorithms can be proven in two general ways</p> <ul> <li>Consider any iteration of the algorithm and show that the decision made by the greedy algorithm is the best and conclude using induction</li> <li>Exchange argument: Pick an optimal solution and gradually change it to the solution produced by the greedy algorithm showing that the optimality is not affected.</li> </ul> <h2 id="matroids">Matroids</h2> <p>A finite matroid \(M\) consists of \((E, I)\) where</p> <ul> <li>\(E\) is a finite set (called the ground set). For example, \(E\) is the set of all edges in the graph \(G = (V, E)\)</li> <li>\(I\) is a collection of subsets of \(E\) (called the independent set). For example, \(I\) consists all subsets of \(S\) of \(E\) such that all the edges in \(s \in S\) form a forest.</li> </ul> <p>\((E, I)\) should satisfy the following properties -</p> <ul> <li>Null set should be in \(I\) - \(\phi \in I\)</li> <li>if \(A \subset B\) and \(B \in I\) then \(A \in I\)</li> <li>If \(A, B \in I\), \(\vert A\vert &gt; \vert B \vert\) then \(\exists e \in A - B\), such that \(B \cup \{e\} \in I\)</li> </ul> <p>Isn’t 2 inclusive of 1 and 3?</p> <p>How does this data structure help us? Suppose we have a graph \(G = (V, E)\) with weights \(c_e \geq 0\). Then, design an algorithm to find an independent set \(S\) that maximizes \(\sum_{e \in S} c_e\). Consider the following algorithm -</p> <ul> <li>Sort \(e\) in decreasing order of weights</li> <li>Let \(S = \phi\). Add \(e\) to \(S\) if \(e\) does not add cycles in \(S\)</li> </ul> <p>This algorithm is very similar to the reverse deletion algorithm and has a time complexity \(\mathcal O(\vert E \vert \log \vert E \vert + T_{check \text{ IS}})\).</p> <p><strong>Lemma.</strong> Let \(e \in E\) have the maximum cost \(c_e\)m then there always exists an IS \(A\) with maximum weight containing \(e\).</p> <p>The proof is very similar to what we have shown with MSTs. This example can be applied to MSTs, and it demonstrates how Matroids can be useful for greedy algorithms.</p> <h2 id="task-scheduling">Task Scheduling</h2> <p>Given \(n\) jobs each with \(t_i\) time to finish and deadlines \(d_i\). Consider that there is a single resource solving tasks sequentially from time \(0\), and a job has to completely finished before moving onto the next one.</p> <p>Suppose in a scheduling algorithm, job \(i\) finishes at time \(f_i\), then the lateness is defined as \(l_i = \max\{0, f_i - d_i\}\). The goal is find an algorithm that minimizes the maximum lateness \(minimize \max_i l_i\).</p> <p>Let us consider a simple case. Suppose there are two jobs with \(d_i \leq d_2\). Note that any scheduling algorithm should not have any idle time between jobs. Why so? If there is idle time, you can always do a job earlier to reduce the lateness. Furthermore, a scheduling algorithm should not have an <em>inversion.</em> That is, if job 1 has deadline earlier than job 2, then it is always optimal to perform job 1 before job 2. This claim can be easily proved using the exchange argument.</p> <h3 id="algorithm">Algorithm</h3> <p>Sort all jobs according to the increasing order of these deadlines \(d_i\), then complete each job without any idle time.</p> <p><strong>Proof.</strong> Generalize the previous two observations to \(n\) jobs.</p> <h2 id="huffman-codes">Huffman Codes</h2> <p>How do we encode an alphabet in binaries to have no ambiguities.</p> <h3 id="prefix-codes">Prefix codes</h3> <p>A prefix code for an alphabet \(T\) is a function \(f:T \to \{0, 1\}^*\), such that for distinct \(x, y \in T\), \(f(x)\) is not a prefix of \(f(y)\).</p> <p>It can be shown that a prefix code gives unique decoding.</p> <p>How do we design an encoding that is most efficient? Let us define efficiency. For every letter \(x\) in \(T\), let its frequency be \(p_x (\sum_{x \in T} p_x = 1)\). Let \(f\) be a prefix code and for every letter \(x \in T\), let \(\vert f(x)\vert\) is the number of bits. The goal is to find a prefix code \(f\) that minimizes the expected number of bits when encoding \(R\) under the frequency \(\{p_x\}\).</p> \[\text{minimize } \sum_{x \in T} p_x \cdot |f(x)|\] <p>It is beneficial to represent prefix codes as a binary tree. Each node has two children: 0 and 1. The paths from the root to other nodes in the tree represent the binary encodings.</p> <ul> <li>For prefix codes, no node of a symbol is an ancestor of node of another symbol (from the alphabet).</li> <li>Another observation is that any optimal prefix code is a full tree (every inner node has two children) - if anode has a single chide, the parent node can itself be used for the symbol deleting the leaf node making the encoding more efficient.</li> <li>There is an optimal tree (prefix code) such that two lower frequent letters are siblings, and are as deep as possible in the tree. This claim can be proved easily with the exchange argument.</li> </ul> <p>With these observations, consider the following algorithm</p> <ul> <li> <p>Initialize each letter \(x\) as a node and label it with \(p_x\)</p> </li> <li> <p>Put all nodes into a min-heap (according to the frequency)</p> </li> <li> <p>While min-heap has atleast two elements</p> <ul> <li> <p>Pop out the two smallest elements \(u, v\) (corresponds to two trees)</p> </li> <li> <p>Combine them to a single tree</p> </li> <li> <p>Push it into the heap, label with \(p_u + p_v\)</p> </li> </ul> </li> </ul> <p>This is the Huffman’s coding algorithm which has a time complexity of \(n \log n\).</p> <h2 id="shannons-source-coding-theorem">Shannon’s source coding theorem</h2> <p>Let \(T\) be an alphabet with frequency \(\{p_x\}\). The entropy of the alphabet is defined as</p> \[H := \sum_{x \in T} p_x \cdot \log \frac{1}{p_x}\] <p>The Shannon’s source coding theorem then states that you cannot send a letter from \(T\) with frequenct \(\{p_x\}\), with expected bits less than \(H\). Huffmman’s encoding gives a solution with expected bits at most \(H + 1\).</p> <p><strong>Important point</strong>. One can suggest to increase the alphabet size with dummy symbols to virtually reduce the value of \(H\) significantly. However, this introduces complexity for encoding algorithms. Therefore, there is a tradeoff with space occupied by encoding and the time for encoding.</p> <blockquote> <p>Even with augmented alphabet, the size of the encoding does not change for the original symbols in the alphabet?</p> </blockquote> <h1 id="binary-search-x-greedy-algorithms">Binary Search X Greedy Algorithms</h1> <p>The basic binary search takes advantage of the monotone structure in arrays to identify elements with certain properties. Any binary search problem can be converted to the following simpler version: For an array \(B\) that has binary elements with all zeros occurring before all ones, find the index of the first occuring \(1\).</p> <p>The binary search algorithm can be simply proved using induction.</p> <p>Let us consider an example - Ternary search. Given an array \(A[1\dots n]\) that is first strictly increasing and then strictly decreasing, dind the largest element. The array \(B[1\dots n - 1]\) is constructed as</p> <ul> <li> \[B[i] = 1 \iff A[i + 1] &gt; A[i]\] </li> <li> \[B[i] = 0 \iff A[i + 1] &lt; A[i]\] </li> </ul> <h2 id="split-array-largest-sum">Split-array largest sum</h2> <h2 id="minimum-fractional-st">Minimum fractional ST</h2> <p>Given an undirected graph \(G = (V, E)\) and each edge has two costs \(a_e, b_e\) both of which are positive, find a spanning tree \(T\) that minimizes</p> \[\frac{\sum_{e \in T} a_e}{\sum_{e \in T} b_e}\] <p>How is this related to binary search? Firstly, we will convert this problem to a decisional version - Given an undirected graph \(G = (V, E)\) and a real number \(U\), decide whether there exists a spanning tree \(T\) such that \(\frac{\sum_{e \in T} a_e}{\sum_{e \in T} b_e} \leq U\).</p> <p>This is equivalent to find a spanning tree such that \(\sum_{e \in T} a_e - U b_e \leq 0\). Construct a new graph with the weights \(a_e - Ub_e\). The reduction is easy to follow.</p> <p>How do we find the monotone structure for binary search? If the decision problem \((G, U)\) is satisfiable, then \((G, U')\) is also satisfiable for any \(U' &gt; U\). Conceptually, assume a function \(B\) (with continuous index) such that \(B[0, S] \to \{0, 1\}\) where \(S\) is an upper bound. \(B(U) = 1\) iuf and only if \((G, U)\) is satisifiable, and \(B\) is monotone.</p> <h1 id="divide-and-conquer">Divide and Conquer</h1> <h2 id="master-theorem">Master Theorem</h2> <p>Consider an algorithm that has the following relationship for running time complexity -</p> \[T(n) = 2T \left(\frac{n}{2}\right) + c n \log^k n \quad (k \geq 0)\] <p>then \(T(n) = \mathcal O(n \log^{k + 1} n)\).</p> <h2 id="closest-point">Closest Point</h2> <h2 id="fast-multiplication">Fast Multiplication</h2> <p>Suppose we have two integers in binary \(a = \sum_{0 \leq 1 \leq n} a_i \cdot 2^i, b = \sum_{0 \leq 1 \leq n} b_i \cdot 2^i\). The goal is to compute \(c = ab = \sum_{0 \leq j &lt; 2n} c_j \cdot 2^j\) where \(c_j = \sum_{0 \leq k \leq j} a_k b_{j - k}$. The naïve brute force approach takes\)\mathcal O(n^2)$ to compute the answer.</p> <p>This question is related to matrix multiplication as well. The naïve algorithm takes $\mathcal O(n^3)$.</p> <h3 id="algorithm-1">Algorithm 1</h3> <p>We segment $a, b$ as follows -</p> <ul> <li> <p>$a = A_1 \cdot 2^{\frac{n}{2}} + A_0$ where $A_0 = \sum_{0 \leq 1 &lt; n/2} a_i \cdot 2^i$ and $A_1 = \sum_{n/2 \leq i &lt; n} a_i \cdot 2^{i - n/2}$</p> </li> <li> <p>$b = B_1 \cdot 2^{\frac{n}{2}} + B_0$ similarly.</p> </li> </ul> <p>Then, $ab = (A_1 \cdot 2^{\frac{n}{2}} + A_0)(B_1 \cdot 2^{\frac{n}{2}} + B_0)$. The strategy then is to do a divide and conquer on these halves to get the final answer.</p> \[ab = A_1 B_1 2^n + (A_0 B_1 + A_1 B_0)2^{n/2} + A_0B_0\] <p>The time complexity is then $T(n) = 4T(\frac{n}{2}) + \mathcal O(n)$. This is essentially \(\mathcal O(n^2)\) that does not give any improvement.</p> <p>This can be optimized further -</p> \[ab = A_1 B_1 2^n + ((A_0 + A_1)(B_0 + B_1) - A_0B_0 - A_1 B_1)2^{n/2} + A_0B_0\] <p>The number of multiplications reduced to 3 - $T(n) = 3T(\frac{n}{2}) + \mathcal O(n)$. Deriving the final expression, $T(n) = cn + cn\frac{3}{2} + \dots + cn\left(\frac{3}{2}\right)^{\log n} = \mathcal(3^{\log n})$.</p> <p>This algorithm can be extended to matrix multiplications as well.</p> \[C = AB = \begin{bmatrix} A_{00}B_{00} + A_{01}B_{10} &amp; A_{00}B_{01} + A_{01} B_{11} \\A_{10}B_{00} + A_{11}B_{10} &amp; A_{10}B_{01} + A_{11}B_{11}\end{bmatrix}\] <p>The naïve algorithm shown above is still $O(n^3)$. Strassen’s algorithm reduces the number of multiplications to \(7\) providing an improvement over the \(\mathcal O(n^3)\) algorithm giving $$\approx \mathcal O(n^{2.81})$.</p> <p>The current state of the art algorithm for matrix multiplication achieves $\mathcal O(n^{2.371552})$. We do not know if there is an algorithm that achieves $\mathcal O (n^{2 + o(1)})$.</p> <h3 id="algorithm-2">Algorithm 2</h3> <p>Multiplication can be seen as a special case of convolution and we can use <strong>Fast Fourier Transform (FFT)</strong> to perform this in $\mathcal O(n \log n)$. The details will be elaborated in the next section.</p> <h2 id="convolution">Convolution</h2> <p>Consider two vectors of the following form -</p> <ul> <li> <p>$a = (a_{n - 1}, a_{n - 2}, \dots, a_2, a_1, a_0)$</p> </li> <li> <p>$b = (b_{n - 1}, b_{n - 2}, \dots, b_2, b_1, b_0)$</p> </li> </ul> <p>The convolution operation $\star$ is defined as</p> \[c = a\star b = (c_{n - 1}, \dots, c_0) \quad \text{ where } c_j = \sum_{0 \leq k &lt; n} a_j b_{(j - k)\mod n}\] <p>Convolution is a generalization of integer multiplication (padding + convolution = multiplication). Also, convolution is a central operation in signal processing - used for blurring images and also to learn features from spatial data.</p> <p>The naïve algorithm can be done in $\mathcal O(n^2)$ time. We can perform convolution using $\mathcal O(n\log n )$ using <strong>Fourier Transform</strong>.</p> <h1 id="fourier-transform">Fourier Transform</h1> <p>Consider the $n$ dimensional vector $a = (a_{n - 1}, a_{n - 2}, \dots, a_2, a_1, a_0)$ and $b = (b_{n - 1}, b_{n - 2}, \dots, b_2, b_1, b_0)$. Let ${e_i}<em>i$ form a unit basis of $$\mathbb R^n$ such that $a = \sum</em>{0 \leq i &lt; n} a_i e_i, b = \sum_{0 \leq i &lt; n} b_i e_i$.</p> <p>Consider another basis $\hat e_i(j) = \omega_n^{ij}$ where $\omega_n = e^{\frac{1\pi \bf{i}}{n}}$ is the $n$-th root of unity. Therefore, $\hat e_i = \frac{1}{\sqrt{n}} \omega_n^{(n - 1)i}, \dots, \omega_n^{2i}, \omega_n^{i}, 1)$.</p> <p>It is easy to check that this is a valid basis. So, again, $a, b$ can be uniquely represented as</p> <ul> <li> <p>$a = \sum_{0 \leq i &lt; n} \hat a_i \hat e_i$, $\hat a_i = \langle a_i, \hat e_i\rangle = \frac{1}{\sqrt{n}} \sum_j a_j \omega_n^{-ij}$</p> </li> <li> <p>$b = \sum_{0 \leq i &lt; n} \hat b_i \hat e_i$</p> </li> </ul> <p>A <strong>Fourier transform</strong> is then defined as - Given ${a_i}<em>{i \in [n]}$, compute $F(a) = {\hat{a_i}}</em>{i \in [n]}$.</p> <p>The <strong>inverse problem</strong> is to find $F^{-1} (\hat a) = {a_i }_{i \in [n]}$. It essentially is a change of basis between ${e_i} \iff {\hat e_i}$.</p> <h2 id="convolution-theorem">Convolution Theorem</h2> <p>Let $a, b$ be two vectors in $\mathbb R^n$; then,</p> \[a \star b = F^{-1} (F(a) \cdot F(b))\] <p>With this claim, convolution can be f=done in $\mathcal O(2T_{FT} + T_{IFT} + n)$.</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[A collection of ideas for design algorithms and analyzing them.]]></summary></entry><entry><title type="html">Design and Analysis of Algorithms</title><link href="https://sudhansh6.github.io/blog/design-and-analysis-of-algorithms/" rel="alternate" type="text/html" title="Design and Analysis of Algorithms"/><published>2024-09-27T00:00:00+00:00</published><updated>2024-09-27T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/design-and-analysis-of-algorithms</id><content type="html" xml:base="https://sudhansh6.github.io/blog/design-and-analysis-of-algorithms/"><![CDATA[<h1 id="greedy-algorithms">Greedy Algorithms</h1> <h2 id="minimum-spanning-tree">Minimum Spanning Tree</h2> <p>Consider a graph \(G\) describe by \(V\) and \(E\) (positive weights). A <strong>spanning tree</strong> of a graph is defined as an edge set \(T \subset E\) such that \((V, T)\) is a tree. A minimum spanning tree is such that \(\sum_{l \in T} l_e\) is minimized.</p> <p>For a graph with \(n\) vertices, there are \(n^{n - 2}\) spanning trees for a complete graph (<strong>Cayley’s formula</strong>).</p> <p>How do we calculate the number of spanning trees for a general graph? <strong>Kirchoff’s theorem</strong> states the following -</p> <ul> <li>Let \(M\) be the adjacency matrix of \(G\)</li> <li>Let \(L - M\) except \(L_{i, i} = -deg(i)\) - This is generally called as <strong>Graph Laplacian</strong></li> <li>Then, #spanning trees is the determinant of any \(m-1\) square sub-matrix (obtained by removing \(i\)th row and column) of \(L\).</li> </ul> <p>Notice how any sub-matrix yields the same value!</p> <h2 id="greedy-idea-1-kruskals-algorithm">Greedy Idea 1: Kruskal’s algorithm</h2> <ul> <li>Sort all the edges with their weights</li> <li>Pick edges as long as they don’t form a cycle</li> </ul> <p>Does this work? If it does, how do we prove it?</p> <p>Firstly, why is it a greedy idea? At each point of the algorithm, we select the current greedy edge (a local minimum) to obtain the minimum spanning tree (a global optimum).</p> <ul> <li> <p><strong>The cut property -</strong> Let \(S \subseteq V\) such that \(S\) and \(V - S\) are non-empty. If \(e\) is an edge across \(S\) and \(V - S\) with the minimum cost, then there always exists a minimum spanning tree with \(e\).</p> <p><strong>Proof.</strong> Exchange argument. If there an MST with \(e’\) across \(S\) and \(V - S\), then replace \(e\) with \(e’\) to obtain another MST.</p> <blockquote> <p>Shouldn’t this argument be more delicate? Why is there a single edge from S to V - S?</p> </blockquote> <p>Essentially, the exchange argument argues replacing a part of the solution improves the solution but does not worsen it.</p> </li> <li> <p>The time complexity of the algorithm comes out to be \(O(m \log m + m \alpha(n))\). The second term in the expression comes from union-find data structures.</p> </li> <li> <p><strong>Correctness of the algorithm</strong> - We shall prove this via induction.</p> <ul> <li>Induction hypothesis - The edges selected in the \(i\)th round of Kruskal’s algorithm can form an MST along with a subset of edges from the remaining edges.</li> <li>Base statement - True for \(i = 0\)</li> <li>Induction step - Cut property</li> </ul> </li> <li> <p><strong>Union-find data structure</strong> - A data structure that supports</p> <ul> <li>Merging elements of two sets into a single set - <code class="language-plaintext highlighter-rouge">union(x, y)</code></li> <li>Checking whether two elements are in the same set - <code class="language-plaintext highlighter-rouge">find(x)</code></li> </ul> <p>efficiently. The amortized time complexity for these operations is \(\alpha(n)\) where \(\alpha(n) \leq 4\) for \(n\) of the order \(2^{2^{2^{2^{16}}}}\). As a result, \(\alpha(n)\) can be regarded as a constant for practical purposes.</p> <p>In our case, the elements are edges and sets represent connected components.</p> </li> </ul> <h2 id="greedy-idea-2-prims-algorithm">Greedy Idea 2: Prim’s Algorithm</h2> <p>Start with any node and expand with the smallest edge connecting to the remaining set of edges. Note that this is different from Kruskal’s algorithm where we sort all the edges and create individual connected components that eventually merge together.</p> <p>The proof for Prim’s algorithm is very similar to that of Kruskal’s. The time complexity is \(O(n^2 + m)\) similar to Djikstra’s algorithm without a data structure. We can maintain a <strong>priority queue</strong> to maintain all edges that come from \(S\) to reduce the time-complexity to \(O((n + m) \log m)\) (without decrease-key). With decrease key and a binary heap, the complexity becomes \(O((n + m) \log n)\). Furthermore, with decrease key and a Fibonacci heap, the complexity reduces to \(O((n\log n + m)\).</p> <h2 id="other-algorithms">Other algorithms</h2> <ul> <li><strong>Reverse deletion</strong> - For every cycle in the original graph and the edge \(e\) with the maximum cost, there always exists an MST without \(e\). Until there are no cycles in the graph, find a cycle and delete the edge with a maximum cost. Note that this algorithm has a higher time complexity since we try and find a cycle for each iteration of the algorithm. How do we implement this?</li> </ul> <h2 id="union-find-data-structure">Union-Find data structure</h2> <p>The idea is to maintain trees with pointers to merge and find elements. The main complication comes while merging the individual sets.</p> <ul> <li> <p>Merging by size (consuming smaller ones by larger sets) - The complexity of merging sets of size \(n\), \(m\) times takes \(O(m \log n)\)</p> </li> <li> <p>To optimize this further, we merge by rank (generalizing the previous approach where rank was simply the size of the set). We add another trick to reduce the amortized time complexity.</p> <ul> <li>Path compression - When <code class="language-plaintext highlighter-rouge">find(x)</code> is called, attach the found elements along the path directly to the root to reduce the path size.</li> </ul> <p>The time complexity then becomes \(O(m \log^* n)\) where \(\log^* n\) is the minimum \(k\) such that \(\log^{(k)} n \leq 1\).</p> </li> </ul> <h1 id="more-greedy-problems-related-to-mst">More Greedy Problems related to MST</h1> <h3 id="k-clustering">\(k\)-clustering</h3> <p>A <strong>maximum spacing</strong> for \(k\)-clustering of \(G =(V, E)\) is defined as</p> <ul> <li>An edge set \(T \subset E\) such that \((V, T)\) has exactly \(k\) connected components</li> <li>The <strong>spacing</strong> is then \(\min d(u, v)\) for \(u, v\) in different connected components</li> <li>The goal is to maximize the spacing</li> </ul> <p>This problem can be solved again with Kruskal’s algotihm to find \(k\)-connected components - perform the <code class="language-plaintext highlighter-rouge">union</code> operation for \(n - k\) times. Why is this correct? WE can show this using a contradiction.</p> <ul> <li>Consider two nodes that lie in the same connected component in the result obtained by Kruskal’s (with spacing \(d’\)). Let them be in different connected components in the optimal solution (with spacing \(d\)). Then,</li> </ul> <h3 id="second-mst">Second MST</h3> <p>A second MST is essentially the spanning tree with the <em>second</em> lowest edge summation cost. How do we find this tree?</p> <ul> <li>Find an MST with weight \(w\)</li> <li>For every edge \(e\) not in the MST, if adding \(e\) yields a cycle in the graph; then remove the largest edge \(e’\) other than \(e\) in the cycle to obtain the second MST</li> <li>The cost of the tree would be \(w + l_e - l_{e’}\)</li> </ul> <p>The time complexity of this algorithm is \(\mathcal O(T_{MST} + mn)\) and can be improved to \(\mathcal O(T_{MST} + m \log n)\) with better data-structures and divide-and-conquer.</p> <p><strong>Lemma.</strong> The second MST only differs by one edge from the MST. Multiple MSTs? <strong>Proof.</strong> Can be shown using contradiction. The idea is that one can move from one spanning tree to another with local changes in the trees. The argument is that you can replace the edges in the second MST with the edges in the MST to obtain a tree with a lower cost. This process can be repeated until there is only one edge that is different from an MST and replacing that would cause the tree to become the MST.</p> <h2 id="more-greedy-problems">More Greedy Problems</h2> <h2 id="needle-in-haystack">Needle in Haystack</h2> <p>Given two strings \(s, t\), decide whether there is a subsequence (need not be contiguous) in \(s\) that matches with \(t\). A naive greedy algorithm is depicted as follows -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">break</span> 
    <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">t</span><span class="p">[</span><span class="n">j</span><span class="p">]:</span>
        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># s[i] is matched with t[j]
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> <p>On first glance, it looks as if this is a very intuitive algorithm. However, there are more intricate details and the proof makes these clearer. The proof relies on the Exchange argument. If there exists a subsequence \(i_1 i_2 \dots i_m\) in \(s\) matching \(t\) (\(\vert t \vert = m\)). If \(i^*_1 &lt; i_1\) is the first index that \(s[i_1] = t[1]\), then \(i^*_1i_2\dots i_m\) also matches \(t\). This way we can find a set of indices \(i^*_1i^*_2 \dots i^*_m\) through the greedy algorithm that gives the correct answer.</p> <p>In general, greedy algorithms can be proven in two general ways</p> <ul> <li>Consider any iteration of the algorithm and show that the decision made by the greedy algorithm is the best and conclude using induction</li> <li>Exchange argument: Pick an optimal solution and gradually change it to the solution produced by the greedy algorithm showing that the optimality is not affected.</li> </ul> <h2 id="matroids">Matroids</h2> <p>A finite matroid \(M\) consists of \((E, I)\) where</p> <ul> <li>\(E\) is a finite set (called the ground set). For example, \(E\) is the set of all edges in the graph \(G = (V, E)\)</li> <li>\(I\) is a collection of subsets of \(E\) (called the independent set). For example, \(I\) consists all subsets of \(S\) of \(E\) such that all the edges in \(s \in S\) form a forest.</li> </ul> <p>\((E, I)\) should satisfy the following properties -</p> <ul> <li>Null set should be in \(I\) - \(\phi \in I\)</li> <li>if \(A \subset B\) and \(B \in I\) then \(A \in I\)</li> <li>If \(A, B \in I\), \(\vert A\vert &gt; \vert B \vert\) then \(\exists e \in A - B\), such that \(B \cup \{e\} \in I\)</li> </ul> <p>Isn’t 2 inclusive of 1 and 3?</p> <p>How does this data structure help us? Suppose we have a graph \(G = (V, E)\) with weights \(c_e \geq 0\). Then, design an algorithm to find an independent set \(S\) that maximizes \(\sum_{e \in S} c_e\). Consider the following algorithm -</p> <ul> <li>Sort \(e\) in decreasing order of weights</li> <li>Let \(S = \phi\). Add \(e\) to \(S\) if \(e\) does not add cycles in \(S\)</li> </ul> <p>This algorithm is very similar to the reverse deletion algorithm and has a time complexity \(\mathcal O(\vert E \vert \log \vert E \vert + T_{check \text{ IS}})\).</p> <p><strong>Lemma.</strong> Let \(e \in E\) have the maximum cost \(c_e\)m then there always exists an IS \(A\) with maximum weight containing \(e\).</p> <p>The proof is very similar to what we have shown with MSTs. This example can be applied to MSTs, and it demonstrates how Matroids can be useful for greedy algorithms.</p> <h2 id="task-scheduling">Task Scheduling</h2> <p>Given \(n\) jobs each with \(t_i\) time to finish and deadlines \(d_i\). Consider that there is a single resource solving tasks sequentially from time \(0\), and a job has to completely finished before moving onto the next one.</p> <p>Suppose in a scheduling algorithm, job \(i\) finishes at time \(f_i\), then the lateness is defined as \(l_i = \max\{0, f_i - d_i\}\). The goal is find an algorithm that minimizes the maximum lateness \(minimize \max_i l_i\).</p> <p>Let us consider a simple case. Suppose there are two jobs with \(d_i \leq d_2\). Note that any scheduling algorithm should not have any idle time between jobs. Why so? If there is idle time, you can always do a job earlier to reduce the lateness. Furthermore, a scheduling algorithm should not have an <em>inversion.</em> That is, if job 1 has deadline earlier than job 2, then it is always optimal to perform job 1 before job 2. This claim can be easily proved using the exchange argument.</p> <h3 id="algorithm">Algorithm</h3> <p>Sort all jobs according to the increasing order of these deadlines \(d_i\), then complete each job without any idle time.</p> <p><strong>Proof.</strong> Generalize the previous two observations to \(n\) jobs.</p> <h2 id="huffman-codes">Huffman Codes</h2> <p>How do we encode an alphabet in binaries to have no ambiguities.</p> <h3 id="prefix-codes">Prefix codes</h3> <p>A prefix code for an alphabet \(T\) is a function \(f:T \to \{0, 1\}^*\), such that for distinct \(x, y \in T\), \(f(x)\) is not a prefix of \(f(y)\).</p> <p>It can be shown that a prefix code gives unique decoding.</p> <p>How do we design an encoding that is most efficient? Let us define efficiency. For every letter \(x\) in \(T\), let its frequency be \(p_x (\sum_{x \in T} p_x = 1)\). Let \(f\) be a prefix code and for every letter \(x \in T\), let \(\vert f(x)\vert\) is the number of bits. The goal is to find a prefix code \(f\) that minimizes the expected number of bits when encoding \(R\) under the frequency \(\{p_x\}\).</p> \[\text{minimize } \sum_{x \in T} p_x \cdot |f(x)|\] <p>It is beneficial to represent prefix codes as a binary tree. Each node has two children: 0 and 1. The paths from the root to other nodes in the tree represent the binary encodings.</p> <ul> <li>For prefix codes, no node of a symbol is an ancestor of node of another symbol (from the alphabet).</li> <li>Another observation is that any optimal prefix code is a full tree (every inner node has two children) - if anode has a single chide, the parent node can itself be used for the symbol deleting the leaf node making the encoding more efficient.</li> <li>There is an optimal tree (prefix code) such that two lower frequent letters are siblings, and are as deep as possible in the tree. This claim can be proved easily with the exchange argument.</li> </ul> <p>With these observations, consider the following algorithm</p> <ul> <li> <p>Initialize each letter \(x\) as a node and label it with \(p_x\)</p> </li> <li> <p>Put all nodes into a min-heap (according to the frequency)</p> </li> <li> <p>While min-heap has atleast two elements</p> <ul> <li> <p>Pop out the two smallest elements \(u, v\) (corresponds to two trees)</p> </li> <li> <p>Combine them to a single tree</p> </li> <li> <p>Push it into the heap, label with \(p_u + p_v\)</p> </li> </ul> </li> </ul> <p>This is the Huffman’s coding algorithm which has a time complexity of \(n \log n\).</p> <h2 id="shannons-source-coding-theorem">Shannon’s source coding theorem</h2> <p>Let \(T\) be an alphabet with frequency \(\{p_x\}\). The entropy of the alphabet is defined as</p> \[H := \sum_{x \in T} p_x \cdot \log \frac{1}{p_x}\] <p>The Shannon’s source coding theorem then states that you cannot send a letter from \(T\) with frequenct \(\{p_x\}\), with expected bits less than \(H\). Huffmman’s encoding gives a solution with expected bits at most \(H + 1\).</p> <p><strong>Important point</strong>. One can suggest to increase the alphabet size with dummy symbols to virtually reduce the value of \(H\) significantly. However, this introduces complexity for encoding algorithms. Therefore, there is a tradeoff with space occupied by encoding and the time for encoding.</p> <blockquote> <p>Even with augmented alphabet, the size of the encoding does not change for the original symbols in the alphabet?</p> </blockquote> <h1 id="binary-search-x-greedy-algorithms">Binary Search X Greedy Algorithms</h1> <p>The basic binary search takes advantage of the monotone structure in arrays to identify elements with certain properties. Any binary search problem can be converted to the following simpler version: For an array \(B\) that has binary elements with all zeros occurring before all ones, find the index of the first occuring \(1\).</p> <p>The binary search algorithm can be simply proved using induction.</p> <p>Let us consider an example - Ternary search. Given an array \(A[1\dots n]\) that is first strictly increasing and then strictly decreasing, dind the largest element. The array \(B[1\dots n - 1]\) is constructed as</p> <ul> <li> \[B[i] = 1 \iff A[i + 1] &gt; A[i]\] </li> <li> \[B[i] = 0 \iff A[i + 1] &lt; A[i]\] </li> </ul> <h2 id="split-array-largest-sum">Split-array largest sum</h2> <h2 id="minimum-fractional-st">Minimum fractional ST</h2> <p>Given an undirected graph \(G = (V, E)\) and each edge has two costs \(a_e, b_e\) both of which are positive, find a spanning tree \(T\) that minimizes</p> \[\frac{\sum_{e \in T} a_e}{\sum_{e \in T} b_e}\] <p>How is this related to binary search? Firstly, we will convert this problem to a decisional version - Given an undirected graph \(G = (V, E)\) and a real number \(U\), decide whether there exists a spanning tree \(T\) such that \(\frac{\sum_{e \in T} a_e}{\sum_{e \in T} b_e} \leq U\).</p> <p>This is equivalent to find a spanning tree such that \(\sum_{e \in T} a_e - U b_e \leq 0\). Construct a new graph with the weights \(a_e - Ub_e\). The reduction is easy to follow.</p> <p>How do we find the monotone structure for binary search? If the decision problem \((G, U)\) is satisfiable, then \((G, U')\) is also satisfiable for any \(U' &gt; U\). Conceptually, assume a function \(B\) (with continuous index) such that \(B[0, S] \to \{0, 1\}\) where \(S\) is an upper bound. \(B(U) = 1\) iuf and only if \((G, U)\) is satisifiable, and \(B\) is monotone.</p> <h1 id="divide-and-conquer">Divide and Conquer</h1> <h2 id="master-theorem">Master Theorem</h2> <p>Consider an algorithm that has the following relationship for running time complexity -</p> \[T(n) = 2T \left(\frac{n}{2}\right) + c n \log^k n \quad (k \geq 0)\] <p>then \(T(n) = \mathcal O(n \log^{k + 1} n)\).</p> <h2 id="closest-point">Closest Point</h2> <h2 id="fast-multiplication">Fast Multiplication</h2> <p>Suppose we have two integers in binary \(a = \sum_{0 \leq 1 \leq n} a_i \cdot 2^i, b = \sum_{0 \leq 1 \leq n} b_i \cdot 2^i\). The goal is to compute \(c = ab = \sum_{0 \leq j &lt; 2n} c_j \cdot 2^j\) where \(c_j = \sum_{0 \leq k \leq j} a_k b_{j - k}$. The naïve brute force approach takes\)\mathcal O(n^2)$ to compute the answer.</p> <p>This question is related to matrix multiplication as well. The naïve algorithm takes $\mathcal O(n^3)$.</p> <h3 id="algorithm-1">Algorithm 1</h3> <p>We segment $a, b$ as follows -</p> <ul> <li> <p>$a = A_1 \cdot 2^{\frac{n}{2}} + A_0$ where $A_0 = \sum_{0 \leq 1 &lt; n/2} a_i \cdot 2^i$ and $A_1 = \sum_{n/2 \leq i &lt; n} a_i \cdot 2^{i - n/2}$</p> </li> <li> <p>$b = B_1 \cdot 2^{\frac{n}{2}} + B_0$ similarly.</p> </li> </ul> <p>Then, $ab = (A_1 \cdot 2^{\frac{n}{2}} + A_0)(B_1 \cdot 2^{\frac{n}{2}} + B_0)$. The strategy then is to do a divide and conquer on these halves to get the final answer.</p> \[ab = A_1 B_1 2^n + (A_0 B_1 + A_1 B_0)2^{n/2} + A_0B_0\] <p>The time complexity is then $T(n) = 4T(\frac{n}{2}) + \mathcal O(n)$. This is essentially \(\mathcal O(n^2)\) that does not give any improvement.</p> <p>This can be optimized further -</p> \[ab = A_1 B_1 2^n + ((A_0 + A_1)(B_0 + B_1) - A_0B_0 - A_1 B_1)2^{n/2} + A_0B_0\] <p>The number of multiplications reduced to 3 - $T(n) = 3T(\frac{n}{2}) + \mathcal O(n)$. Deriving the final expression, $T(n) = cn + cn\frac{3}{2} + \dots + cn\left(\frac{3}{2}\right)^{\log n} = \mathcal(3^{\log n})$.</p> <p>This algorithm can be extended to matrix multiplications as well.</p> \[C = AB = \begin{bmatrix} A_{00}B_{00} + A_{01}B_{10} &amp; A_{00}B_{01} + A_{01} B_{11} \\A_{10}B_{00} + A_{11}B_{10} &amp; A_{10}B_{01} + A_{11}B_{11}\end{bmatrix}\] <p>The naïve algorithm shown above is still $O(n^3)$. Strassen’s algorithm reduces the number of multiplications to \(7\) providing an improvement over the \(\mathcal O(n^3)\) algorithm giving $$\approx \mathcal O(n^{2.81})$.</p> <p>The current state of the art algorithm for matrix multiplication achieves $\mathcal O(n^{2.371552})$. We do not know if there is an algorithm that achieves $\mathcal O (n^{2 + o(1)})$.</p> <h3 id="algorithm-2">Algorithm 2</h3> <p>Multiplication can be seen as a special case of convolution and we can use <strong>Fast Fourier Transform (FFT)</strong> to perform this in $\mathcal O(n \log n)$. The details will be elaborated in the next section.</p> <h2 id="convolution">Convolution</h2> <p>Consider two vectors of the following form -</p> <ul> <li> <p>$a = (a_{n - 1}, a_{n - 2}, \dots, a_2, a_1, a_0)$</p> </li> <li> <p>$b = (b_{n - 1}, b_{n - 2}, \dots, b_2, b_1, b_0)$</p> </li> </ul> <p>The convolution operation $\star$ is defined as</p> \[c = a\star b = (c_{n - 1}, \dots, c_0) \quad \text{ where } c_j = \sum_{0 \leq k &lt; n} a_j b_{(j - k)\mod n}\] <p>Convolution is a generalization of integer multiplication (padding + convolution = multiplication). Also, convolution is a central operation in signal processing - used for blurring images and also to learn features from spatial data.</p> <p>The naïve algorithm can be done in $\mathcal O(n^2)$ time. We can perform convolution using $\mathcal O(n\log n )$ using <strong>Fourier Transform</strong>.</p> <h1 id="fourier-transform">Fourier Transform</h1> <p>Consider the $n$ dimensional vector $a = (a_{n - 1}, a_{n - 2}, \dots, a_2, a_1, a_0)$ and $b = (b_{n - 1}, b_{n - 2}, \dots, b_2, b_1, b_0)$. Let ${e_i}<em>i$ form a unit basis of $$\mathbb R^n$ such that $a = \sum</em>{0 \leq i &lt; n} a_i e_i, b = \sum_{0 \leq i &lt; n} b_i e_i$.</p> <p>Consider another basis $\hat e_i(j) = \omega_n^{ij}$ where $\omega_n = e^{\frac{1\pi \bf{i}}{n}}$ is the $n$-th root of unity. Therefore, $\hat e_i = \frac{1}{\sqrt{n}} \omega_n^{(n - 1)i}, \dots, \omega_n^{2i}, \omega_n^{i}, 1)$.</p> <p>It is easy to check that this is a valid basis. So, again, $a, b$ can be uniquely represented as</p> <ul> <li> <p>$a = \sum_{0 \leq i &lt; n} \hat a_i \hat e_i$, $\hat a_i = \langle a_i, \hat e_i\rangle = \frac{1}{\sqrt{n}} \sum_j a_j \omega_n^{-ij}$</p> </li> <li> <p>$b = \sum_{0 \leq i &lt; n} \hat b_i \hat e_i$</p> </li> </ul> <p>A <strong>Fourier transform</strong> is then defined as - Given ${a_i}<em>{i \in [n]}$, compute $F(a) = {\hat{a_i}}</em>{i \in [n]}$.</p> <p>The <strong>inverse problem</strong> is to find $F^{-1} (\hat a) = {a_i }_{i \in [n]}$. It essentially is a change of basis between ${e_i} \iff {\hat e_i}$.</p> <h2 id="convolution-theorem">Convolution Theorem</h2> <p>Let $a, b$ be two vectors in $\mathbb R^n$; then,</p> \[a \star b = F^{-1} (F(a) \cdot F(b))\] <p>With this claim, convolution can be f=done in $\mathcal O(2T_{FT} + T_{IFT} + n)$.</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[A collection of ideas for design algorithms and analyzing them.]]></summary></entry><entry><title type="html">Tabletop Manipulation Algorithms</title><link href="https://sudhansh6.github.io/blog/Tabletop-Rearrangement/" rel="alternate" type="text/html" title="Tabletop Manipulation Algorithms"/><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Tabletop-Rearrangement</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Tabletop-Rearrangement/"><![CDATA[<h2 id="problem">Problem</h2> <p>Given a table top with various objects, their initial positions and final goal positions, we want to find the algorithm to rearrange the objects with the <strong>least number of actions</strong> and <strong>low buffer space</strong>.</p> <h3 id="buffers">Buffers</h3> <p>If one object is obstructing the goal position of the other, we would want to use <em>buffer space</em> to free-up the goal space. In a brute force manner, we can place everything on the side and put objects in their goals states. We want to optimize the number of actions, and doing this doesn’t do well in that scenario.</p> <p>The problem in the general case reduces to traveling salesman problem or the vertex cover problem, which are both NP-hard. We want to find a good approximation algorithm to get the least amount of actions.</p> <p>We also want real-time execution time because we do not want the algorithm to run for a long time.</p> <h3 id="variations">Variations</h3> <ol> <li> <p>Mobile and mobile base</p> </li> <li> <p>Disk objects and different shape objects</p> </li> <li> <p>stacking vs not stacking objects</p> </li> <li> <p>External buffer or not</p> </li> <li> <p>Single vs multiple tables</p> </li> <li> <p>Overlaps vs non-overlapping objects</p> </li> </ol> <h3 id="search-problem">Search Problem</h3> <p>The problem can be converted to a search problem where the search space is a tree representing all the possibilities of movements of objects (to goal positions) possible with the current configuration. The actions would be of the form “move object ‘i’ to buffer”, “move object ‘i’ to goal position”, etc. At each state, we have at most \(2n\) actions (if there are \(n\) objects on the table) - moving each object to buffer or goal.</p> <p>Obviously, this is a very high-dimensional space, and rudimentary search algorithms would not fare well if used directly.</p> <p>Let us now see some algorithms that try to tackle this.</p> <h2 id="trlb">TRLB</h2> <p><a href="https://arxiv.org/abs/2110.12325">Fast High-Quality Tabletop Rearrangement in Bounded Workspace</a> does the following -</p> <ol> <li> <p>Calculate a primitive plan by assuming there is always a feasible buffer space on the table (assume you have a second table to place objects)</p> </li> <li> <p>Try executing this plan, and assign buffer as we go along the path.</p> </li> <li> <p>If we fail to assign a buffer, we add a node to mark this search path.</p> </li> <li> <p>We repeat the procedure by finding a primitive plan from this stage until we find a feasible path.</p> </li> </ol> <p>The goal of this algorithm is to quickly calculate the solutions but it has a suboptimal traveling cost (example, robot mobile base needs to move a lot). Note that this is a non-deterministic algorithm, since we calculate random primitive plans.</p> <h2 id="orla">ORLA*</h2> <p>Aims to calculate the optimal path without considering the time as a heuristic. It essentially is \(A^*\) adapted to the table-top rearrangement problem - \(f(n) = g(n) + h(n)\) where \(g(n)\) is the travel cost from start to current node and \(h(n)\) expected travel cost from current node to goal.</p> <p>It results in an optimal plan but has a very long execution time. Another point to note is that ORLA* considers a mobile base which not many papers have considered previously.</p> <h2 id="our-research">Our Research</h2> <p>We aim to build an algorithm that gives the optimal solution and also executes fast. We assume mobile base manipulation robot with arbitrary shape objects and no external buffer.</p> <h3 id="approach-1">Approach 1</h3> <p>Perform search in TRLB using MCTS to find the most optimal search plan. Basically, execute TRLB for a fixed time to get multiple feasible paths. TRLB stops as soon as it finds a feasible path, but we execute it until we get a fixed number of possible paths.</p> <h3 id="bit">BIT*</h3> <p>We initially find a suboptimal path using TRLB* - this is fast.</p> <h2 id="future-directions">Future Directions</h2> <ol> <li> <p>Extend the algorithms for multi-agent scenarios where the problem becomes much harder.</p> </li> <li> <p>Learn conflict detection in TRLB or the modified algorithm to do better backtracking - one of the key ideas for combinatorial search algorithms. This can be done via SAT solving - learn conflicts as new constraints. This is particularly important when the number of objects on the table is high with a high density.</p> </li> </ol>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[Various methods for optimal mobile tabletop rearrangement - rearrange objects on the table with the least cost.]]></summary></entry><entry><title type="html">Evaluating interactions in music</title><link href="https://sudhansh6.github.io/blog/Evaluting-Music-with-RL/" rel="alternate" type="text/html" title="Evaluating interactions in music"/><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Evaluting-Music-with-RL</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Evaluting-Music-with-RL/"><![CDATA[<p>How do we identify how well two pieces of music complement each other? From notes, scales, rhythms, chords, time signatures, and many more abstract concepts associated with music, it is an interesting problem to capture the interactions between music pieces. This problem requires concepts from signal processing</p> <p>How is music represented? A Mel spectrogram is used to represent frequencies across time with amplitudes for audio files. A MIDI file is a tabular description of beats, positions, pitch, durations and instruments. These tabular instances can easily be converted to vector representations using one-hot encodings and feature normalization.</p> <p>Using such representations, the paper <a href="https://arxiv.org/abs/2207.06983">Multitrack Music Transformer</a> attempts to capture the interactions with attention mechanisms in a transformer. To understand this better, let us look at some theory.</p> <h3 id="total-information-flow">Total Information Flow</h3> <p>The total information flow is the sum of transfer entropies from \(X\) to \(Y\) and \(Y\) to \(X\).</p> \[T_{X \to Y} + T_{Y \to X} = H(X \vert \bar X) + H(Y \vert \bar Y) - H(XY \vert \bar{XY})\] <p>If the combined sequence \(XY\) does not make sense musically, then it will have a higher entropy, leading to lower total information flow. This concept is explored in depth in this context in this paper - <a href="https://arxiv.org/abs/2402.06810">Evaluating Co-Creativity using Total Information Flow</a>.</p> <h3 id="conditional-entropy">Conditional Entropy</h3> <p>Represented as \(H(Y \vert X)\) it measures how unpredicatble \(Y\) is, given that we have \(X\). How is this useful? A pre-trained music transformer is used to model \(p(X \vert \bar X)\) which represents the probability of a particular <em>token</em> (next part of music) after seeing a particular set of tokens (music till that point).</p> <h2 id="research">Research</h2> <p>Using the features from a MIDI file directly would not yield the best results. It composes of monotonically increasing data (beat), categorical features (instruments) and repeated values (position).</p> <p>This problem can also be posed as a Reinforcement Learning Problem - using total information flow as a reward. For example, <a href="https://arxiv.org/abs/2002.03082">RL-Duet: Online Music Accompaniment Generation Using Deep Reinforcement Learning</a> formulated a Deep RL algorithm for online accompaniment generation. The generation agent learns a policy to generate a musical note (action) based on previously generated context (state). RL has potential for real-time human-machine duel improvisation.</p> <h1 id="rl-duet">RL-Duet</h1> <p>What is the goal? Create an agent that can generate music <em>interactively</em> with a human. Again, this is done via Symbolic MIDI pitch + beat. The earlier approaches used GANs which have large data requirements and usntable training. Another approach, using Gibbs sampling, iteratively modifies the music fragments based on the previous context. However, this approach cannot be done in an online fashion.</p> <p>Typical approaches in reinforcement learning for sequence generation use maximum likelihood estimation. However, to improve the perceptual quality, global coherence and harmony, specific hand-crafted music rules-based rewards work much better.</p> <p>The work in RL-Duet captures the horizontal temporal consistency and vertical harmony relations for the reward function of the RL-agent. The current state is the previously generated context (by both human and the agent) and the action as mentioned before is symbolic MIDI (note and pitch). This involves horizontal view (like linear bi-directional language modeling in NLP), vertical part, joint modeling and hand-crafted rewards.</p>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[Capturing the quantifiers required to comment on how well two pieces of music complement each other.]]></summary></entry><entry><title type="html">Reinforcement Learning in Nuclear Fusion</title><link href="https://sudhansh6.github.io/blog/Control-for-Nuclear-Fusion/" rel="alternate" type="text/html" title="Reinforcement Learning in Nuclear Fusion"/><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Control-for-Nuclear-Fusion</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Control-for-Nuclear-Fusion/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>What is Nuclear fusion? A process where lighter nuclei combine to form heavier atoms. It is of interest to researchers because of its potential to provide clean and long-term energy source. Humans are able to (atleast trying) perform these reactions by carefully controlling plasma through magnetic fields. Plasma is a state of matter where the electrons are stripped from their atoms past a certain temperature. These are extremely hot and require precisely controlled magnetical fields to be stored.</p> <h2 id="fusion-methods">Fusion Methods</h2> <p>There are two methods through which nuclear fusion is being performed currently -</p> <ul> <li> <p>Inertial Confinement Fusion (ICF)</p> <ul> <li>These facilities tend to be very large, and are impractical to scale.</li> </ul> </li> <li> <p>Magnetic Confinement Fusion (MCF)</p> <ul> <li>The goal is to confine hot plasma without touching walls. The device used for this is called as a <strong>tokamak</strong> that uses a central solenoid and induced current to control the magnetic fields.</li> </ul> </li> </ul> <h3 id="tokamak">Tokamak</h3> <p>It’s a sonu shaped device with a central solenoid that generates toroidal and poloidal magnetic fields to confine plasma and maintain high temperatures. Through these fields we want to control the radial position, vertical extent and overall shape of the plasma which is important for the fusion reactions.</p> <p>Recently, KSTAR in South Korea was able to confine plasma for about a minute which is an impressive result for the current sota. In a huge international effort, ITER (International Thermonuclear Experimental Reactor), the world’s largest tokamak is currently being built in Southern France.</p> <h3 id="controls-engineering-process">Controls Engineering Process</h3> <ul> <li> <p><strong>Feedforward stage</strong> - Find the ideal setting for fusion experiment objectives - precompute coil curents, plasma shape, etc.</p> </li> <li> <p><strong>Feedback control stage</strong> - Closed loop control with magnetic measurements to determine the state of the plasma. Maintaining the required state boils down to solving a nonlinear PDE.</p> </li> </ul> <h2 id="better-control">Better control</h2> <p>Tradtional algorithms are able to control the vertical position of the plasma really well but they can be improved for radial control and are a result of intricately designed controllers stiched together. A black-box AI solution shows promise as a single control policy for more robust non-linear control. However, there can be a lack of performance guarantees along with non-interpretability with AI algorithms.</p> <p>To emphasise on safety certifications, there has been works using Lyapunov functions.</p> <h3 id="stability-guarantees">Stability Guarantees</h3> <p><strong>Lyapunov</strong> functions are scalar functions used to verify stability of a system by modeling evolution of system moving from one energy level to another. Recently, work in Neural Lyapunov controls has extended these stability guarantees with the use of neural networks.</p> <h3 id="deep-reinforcement-learning-approaches">Deep Reinforcement Learning Approaches</h3> <p>“<a href="https://www.nature.com/articles/s41586-021-04301-9">Magnetic control of tokamak plasmas through deep reinforcement learning</a>” by Degrace aims to provide a solution for feedback control stage by estimating the current state using various measurements. The challenge arises in modelling such a high-dimensional space - 92 system measurements (poloidal coil currents, flux loops, magnetic probes) and the actions are continuous (19 control coil voltages) and also the absence of a good simulator for this problem.</p> <p>The authors use a simulated environment which is implemented using a Forward FRad-Shafranov solve (FGE). Their algorithm was able to maintain different shapes of the plasma which is not possible with conventional algorithms.</p> <p>Another paper by KSTAR - <a href="https://iopscience.iop.org/article/10.1088/1741-4326/ac79be">Development of an operation trajectory design algorithm for control of multiple 0D parameters using deep reinforcement learning in KSTAR</a> - used <em>TD3</em> (RL algorithm) for feedforward controls. That is, to estimate the desired state of plasma required for the experiments. The states capture “how well the fusion reaction is going”, the safety indices, etc to determine the required controls.</p> <p>Interestingly, instead of a physics-based simulator, they used a data-based simulator trained using an LSTM network on around 5 years of data.</p> <h2 id="current-work">Current Work</h2> <p><strong>Neural Lyapunov control</strong> - Roughly, we want to generate a lyapunov risk formulation for the fusion environment. The challenge is the for the fusion environment, we do not know the system dynamics, and we want to build a neural approximation for estimating this value. This can then be used for the lyapunov risk, which can finally be used for the control algorithms <em>with safety guarantees</em>. In summary, we are trying to improve the above papers by embedding safety guarantees with the methods.</p>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[Introduction to reinforcement learning algorithms for control in nuclear fusion reactors]]></summary></entry><entry><title type="html">Scalable Behavior Planning for Autonomous Driving</title><link href="https://sudhansh6.github.io/blog/Behavior-Planning-for-Autonomous-Driving/" rel="alternate" type="text/html" title="Scalable Behavior Planning for Autonomous Driving"/><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Behavior-Planning-for-Autonomous-Driving</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Behavior-Planning-for-Autonomous-Driving/"><![CDATA[<p>A typical urban scene has numerous people and vehicles with various behaviors paths coupled with rules of the road. Building a framework for predicting behaviors for path planning in such scenes is a difficult task. The frameworks for building algorithms in these scenes involve</p> <ul> <li> <p>Sensors for Perception - RGB, LIDAR, Infra-red</p> </li> <li> <p>Control Algorithms - Motor commands</p> </li> </ul> <p>The planning algorithms have two components to them</p> <ul> <li> <p>Global planning - To plan the coarse waypoints from the source to the destination. Dijkstra’s and A\(^*\) search are typically used for these.</p> </li> <li> <p>Motion planning - To plan the control commands based on the environment between the waypoints. It involves behavior planning and path planning algorithms. Behavior planning involves predicting motion of objects on the road and analysing the traffic signs. The path planning part involves generating multiple trajectories which are pruned down from the perception data (sorting them based on a cost function).</p> </li> </ul> <h2 id="behavior-planning">Behavior Planning</h2> <h3 id="state-diagram-algorithms">State Diagram Algorithms</h3> <p>Some classical approaches for this problem involves state diagrams that have a deterministic policy based on symbolic descriptions of the world. Basically, think of these as complicated <code class="language-plaintext highlighter-rouge">if-else</code> structures. <a href="https://ieeexplore.ieee.org/document/4290200/">Behavior Nets</a> is one of the state-of-the-art approaches in this realm.</p> <p>However, these approaches are prone to errors due to sensor uncertainity, state uncertainiity, uncertain temporal evolution and occlusions. These are corrected using Markov Decision Processes</p> <h3 id="markov-decision-processing">Markov Decision Processing</h3> <p>They involve deterministic or stochastic policy within a framework that models uncertainty by evaluating the future. The MDPs are coupled with a risk-factor to make the optimal strategy the safe strategy. One such approach is listed in <a href="https://ieeexplore.ieee.org/document/6082928">Probabilistic MDP-behavior planning for cars</a>. However, these do not work quite well in dynamic world. Also, in dense scenes the dimensionality becomes very high making this approach unscalable. The number of agent vary as well which cannot be modeled completely using an MDP.</p> <p>In <a href="https://arxiv.org/pdf/2011.04697">Behavior Planning at Urban Intersections through Hierarchical Reinforcement Learning</a> the authors simply choose the nearest $5$ agents to model the algorithm and use an RL algorithm to plan the path. However, this approach is data hungry, requires simulators, has low data coverage and oversimplifies the information in dense scenes.</p> <h3 id="online-algorithms">Online Algorithms</h3> <p>In <a href="https://www.ijcai.org/Proceedings/2017/664">Online Decision-Making for Scalable Autonomous Systems (MODIA)</a> the algorithm decomposes the dynamic scene into small decision problems for each agent in the environment. Each decision problem has a pre-defined MDP based on the class of the agent. After obtaining optimal actions from each of these problems, an <em>executor</em> finds the best action based on a <em>preference</em> function defined on a notion of safety. This way, the algorithm has a linearly-growing complexity and state abstractions which are explainable.</p> <h2 id="status-quo">Status-Quo</h2> <p>Taking a step back, we need to deal with dynamic and complex environments. We need to ensure the algorithms use limited computation power and work with the variety of sensors equipped in the vehicle.</p> <p>We do not have a good behavior-realisitc simulator and sufficient human labor annotations (in academic labs). Without enough data, how do we build algorithms for planning? The AVL Lab at UCSD does the following -</p> <ul> <li> <p>Collect rollouts - Data collection stack for behavior planning - point cloud maps, teleoperation platform that collects high and low-level control</p> </li> <li> <p>Training phase - Offline RL for each decision problem</p> </li> <li> <p>Real-world deployment - Online decision framework from MODIA.</p> </li> </ul> <p>This approach has many limitations. Since the number of states is very large, the state-transition matrix is very sparse, and this is not good for determining the optimal policy. These can be fixed using generative simulators (which do not work quite well cuurently) or bridging large dataset by outsourcing the data collection.</p>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[Discussion on algorithms for predicting behavior patters in dense urban scenes.]]></summary></entry><entry><title type="html">Structural Visualization for Neural Networks</title><link href="https://sudhansh6.github.io/blog/Structural-Visualization-for-Neural-Network/" rel="alternate" type="text/html" title="Structural Visualization for Neural Networks"/><published>2024-04-24T00:00:00+00:00</published><updated>2024-04-24T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Structural-Visualization-for-Neural-Network</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Structural-Visualization-for-Neural-Network/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>The structure of neural networks have been motivated from neurons in brains. Just like how brain has specialized regions for analysing different kinds of data, do neural networks have such mappings? Is there a way we can visually monitor this, analyse patterns and comment on the progress of training of the model?</p> <p>In simpler neural networks, the outputs from each layer can be visualized as linear separating functions. Such interfaces are available on websites like Tensorflow palyground. Other visualization methods like t-SNE, aim to classify data points in high dimensional spaces. Combining such approaches, the paper <a href="https://arxiv.org/abs/1908.02831">Visualizing the PHATE of Neural Networks</a> uses a kernel-based simentionality reduction</p> <blockquote> <p><strong>t-SNE</strong> is a statistical method to visualize high-dimensional data in two or three dimensional maps. The approach involves constructing a probability distribution over pairs of objects such that similar objects have a higher probability. Then, it uses KL-divergence to model a similar distribution in the low-dimensional space for visualization. <a href="https://en.wikipedia.org/wiki/Uniform_manifold_approximation_and_projection">UMAP</a> is another approach for this task. To read in detail refer to the <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">Wiki</a>.</p> </blockquote> <h1 id="multi-layer-perceptrons">Multi-layer Perceptrons</h1> <p>The basic setup boils down to the graph drawing problem where we try to find a layout to better visualize a given graph \(G= (V, E)\). There are two approaches to this problem -</p> <ul> <li> <p><strong>Force-directed layout</strong> - Adds a potential field representing force on edges of the graph to help them spread out</p> </li> <li> <p><strong>Spectral layout</strong> - USes coordinates the eigenveectors of a matric such a the Laplacian derived from the degree matrix and adjacency matrix of the graph.</p> </li> </ul> <h3 id="graph-construction">Graph Construction</h3> <p>Use KNN similarity with spectral layout as the warm start, and then use the force-directed layout to plot the graph. But what do all these mean in the context of latent spaces of a neural network?</p> \[h_i^{(l)} = \phi^{(l)} \left(\sum_{i \in [k]} x_i w_i^{(l)} + b^{(l)}\right)\] <p>Consider the task of classification. What are we trying to visualize? Either the samples or the model weights can be visualized. Let us look at the samples.</p> <h3 id="results">Results</h3> <p>Through out the triaining process, inter-class nodes will gradually move further while the intra-class nodes move closer.</p> <h3 id="experiment---sensitvity-mapping">Experiment - Sensitvity Mapping</h3> <p>Do some embeddings contribute more on the final prediction? We can try visualizing the gradients contributed by the samples at each hidden layer.</p> <h3 id="experiment---neuron-x-sample-mapping">Experiment - Neuron x Sample Mapping</h3> <p>Can we add neurons alongside samples as nodes in the graph? The idea is as follows -</p> <ul> <li> <p>Represent neuron nodes with black nodes and samples nodes as colored ones</p> </li> <li> <p>No connectiones between colored nodes and black nodes</p> </li> <li> <p>A sample node is adjacent to a neuron node if the neuron node</p> </li> </ul> <p>The motivation for this experiment is to figure out if some of the embeddings contribute more on the final prediction.</p> <h1 id="observing-domains-in-generative-models">Observing Domains in Generative Models</h1> <p>Focusing on VAE models, which are characterized with an encoder-decoder architectures to reduce the reconstruction loss (for the decoder) and simialrity loss (for the encoder). The assumption is that any input can be mapped to a lower-dimensional distribution which can then be sampled with a conditional variable to reproduce the high-dimensional input. The similarity loss acts like a regularization term, forcing the distribution learnt by the encoder to match the normal distribution. Depending on the application, the weight of the similarity loss (represented by \(\beta\)) can be varied.</p> <p>Increasing this parameter to a high value causes <em>posterior collapse</em>, reducing the generalizability of the network. Essentially, the network generates only a certain instance/class from the distribution.</p> <h3 id="relevant-work">Relevant Work</h3> <p><a href="https://ar5iv.labs.arxiv.org/html/1812.06775#:~:text=Variational%20Autoencoders%20Pursue%20PCA%20Directions%20%28by%20Accident%29%201,3%20Results%203.1%20The%20problem%20with%20log-likelihood%20">Variational Autoencoders Pursue PCA Directions (by Accident)</a> is one such works that claims that the decoder models will promote orthogonality when transforming the z-embedding to the final output. The question of using this behavior to help us model data better is still an open question.</p> <h2 id="conclusion">Conclusion</h2> <p>These approaches can be used for the following</p> <ul> <li> <p>Learning with trainability - Visualizing the entire course of training instead fthe final model.</p> </li> <li> <p>Learning towards interpretability - Visualizing why models behave in certain ways through the lens of a graph</p> </li> <li> <p>Learning towards robustness - Visualizing scrambled data points</p> </li> </ul>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[Analysing learning patterns in neural networks using graph visualization algorithms.]]></summary></entry><entry><title type="html">Advanced Computer Vision</title><link href="https://sudhansh6.github.io/blog/Advanced-Computer-Vision/" rel="alternate" type="text/html" title="Advanced Computer Vision"/><published>2024-04-06T00:00:00+00:00</published><updated>2024-04-06T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Advanced-Computer-Vision</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Advanced-Computer-Vision/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Vision is a fundamental interface to the world, and it has become a crucial component in the development of intelligent systems. The field has deep and attractive scientific problems, which have been advancing at a rapid pace in the past few years.<br/> In the early days of research, the focus in vision was on engineering “good” features coupled with a optimisation algorithm or a shallow neural network. As the processors became more powerful, the emphasis shifted to end-to-end approaches with inclusion of self-supervision and multi-modal learning paradigms.</p> <p>It is often helpful to breakdown the perception tasks into known-algorithms. For example, in autonomous driving, the tasks include SLAM (visual, Structure from Motion), path planning (lane detection, obstacle detection, 3D localization), Semantic segmentation etc. Similarly, the tasks in augmented reality devices are gaze tracking, material and lighting estimation, head pose estimation, depth estimation, etc.</p> <p>Deep learning has opened new areas of research in vision. Features such as generation of high-quality content, end-to-end training, data-driven priors and highly parallelizable architectures have proven advantageous for many problems in computer vision. However, it is also important to note the limitations of these techniques -</p> <ul> <li>Large scale labeled data is not always available</li> <li>Lack of generalization to unseen domains</li> <li>Good at narrow “classification”, not at broad “reasoning”</li> <li>Lack of interpretability</li> <li>Lack of reliability, security or privacy guarantee</li> </ul> <p>To counter these problems, we typically couple our algorithms with self-supervision, physical modelling, multi-modal learning and <em>foundation models</em>. In the recent years, these techniques have been applied to various problems, and the following are arguably the biggest advances in Computer Vision -</p> <ul> <li>Vision Transformers</li> <li>Vision-Language Models</li> <li>Diffusion Models</li> <li>Neural Rendering</li> </ul> <p>These techniques show promise to solve keystone problems in augmented reality, interactive robotics, and autonomous driving. The course will cover the following these topics, along with other fundamentals required.</p> <ul> <li> <p>Neural Architectures</p> </li> <li> <p>Generative Models</p> </li> <li> <p>Structure from Motion</p> </li> <li> <p>Object Detection</p> </li> <li> <p>image Segmentation</p> </li> <li> <p>Prediction and Planning</p> </li> <li> <p>Inverse Rendering</p> </li> <li> <p>3D GANs</p> </li> <li> <p>Vision-Language</p> </li> </ul> <h3 id="neural-architectures">Neural Architectures</h3> <p>The motivation for an artificial neuron (perceptron), comes from a biological neuron where the output is linear combination of the inputs combined with a non-linear activation function. From here, we develop multi-layer networks which are again motivated from the Hubel and Weisel’s architecture in biological cells.</p> <h3 id="neural-networks">Neural Networks</h3> <p>The simplest neural network is a perceptron represented by \(\sigma (x) = \text{sign}(\sum_i w_i x_i + b)\) where the optimal weight values are obtained using an unconstrained optimization problem. These concepts can be extended for “Linear Regression” and “Logistic Regression” tasks.</p> <p>Non-linearity in neural networks is introduced through <strong>Activation functions</strong> such as</p> <ul> <li>Sigmoid - Have vanishing gradient issues</li> <li>tanh - Centered version of sigmoid</li> <li>ReLU - Simplest non-linear activation, with easy gradient calculation.</li> <li>ELU - Added to prevent passive neurons.</li> </ul> <p>At the <strong>output layer</strong>, we apply a final non-linear function is applied to calculate the <strong>loss</strong> in the predicted output. Typically for <strong>classification problems</strong>, <em>Softmax</em> function is used to map the network outputs to probabilities. One-hot representations are not differentiable, and are hence not used for this task. In image synthesis problems, the output layer usually has \(255*sigmoid(z)\).</p> <p><strong>Theorem (Universal function approximators)</strong>: A two-layer network with a sufficient number of neurons can approximate any continous function to any desired accuracy.</p> <p><strong>Width or Depth?</strong> A wider network needs more and more neurons to represent arbitrary function with high enough precision. A deeper network on the contrary, require few parameters needed to achieve a similar approximation power. However, “overly deep” plain nets do not perform well. This is due to the vanishing gradient problem, wherein we are not able to train deep networks with the typical optimization algorithms.</p> <h3 id="convolution-networks">Convolution Networks</h3> <p>The neural network architecture is modified for images using “learnable kernels” in convolutional neural networks. Each convolution layer consists of a set of kernels that produce feature maps from the input. These feature maps capture the <em>spatial</em> and <em>local</em> relationships in the input which is crucial for images.</p> <p>The <em>induction bias</em> in images is that neighbouring variables are locally correlated. An image need not be 2D, it can consist of multiple channels (RGB, hyperspectral, etc.), and convolutional layers work <em>across all</em> these channels to produce feature maps.</p> <p>In a classical neural network, each pixel in the input image would be connected to every neuron in the network layer leading to <em>many</em> parameters for a single image. Using kernels, we use <em>shared weights</em> across all pixel locations, and this greatly reduces the number of learnable parameters without losing much information.</p> <p>Convolution layers are generally accompanies with <strong>Pooling layers</strong> which do not have any learnable parameters, and are used to reduce the size of the output. These layers are invariant to small (spatial)transformations in the input and help observe a larger <em>receptive field</em> in the next layer. The latter property is important to observe hidden layers in the feature maps.</p> <p><strong>Receptive Field</strong> - It is the area in the input iamge “seen” by a unit in a CNN. Inits with deeper layers will have wider receptive fields whereas wider receptive fields allow more global reasoning across entire image. This way, the pooling leads to more rapid widening of receptive fields. We need \(\mathcal O(n/k)\) layers with \((k \times k)\) convolutional filters to have a receptive field of \(n\) in the input. <em>Dilation layers</em> are used to achieve the same receptive field with \(\mathcal O(\log n)\) layers.</p> <p>However, in practice, the empirical receptive fields in the deeper networks is lower than the theoretical value due to sparse weights.</p> <p>Convolution networks are augmented with <em>dense</em> layers to get the output, to learn from the feature maps.</p> <p>The vanishing gradient problem in deeper networks has been solved using <strong>skip connections</strong> wherein the features from the earlier layers are concatenated with the deeper ones to allow passage of information. This way, we provide the network with the original input allowing it to learn the smaller fluctuations in the input (rather than focusing on learning the input itself).</p> <p>In summary, the key operations in convolutional layers are</p> <p><code class="language-plaintext highlighter-rouge">Input image -&gt; Convlution -&gt; Non-linearity -&gt; Spatial Pooling -&gt; Feature Maps</code></p> <p>CNNs have the above set of operations repeated many times. CNNs have been successful due to the following reasons</p> <ul> <li>Good Abstractions - Hierarchical and expressive feature representations. Conventional image processing algorithms relied on a pyramidal representation of features, and this methodology has also paved its way in CNNs.</li> <li>Good inductive biases - Remarkable in transferring knowledge across tasks. That is, pretrained networks can be easily augmented with other general tasks.</li> <li>Ease of implementation - Can be trained end-to-end, rather than hand-crafted for each task, and they can easily be implemented on parallel architectures.</li> </ul> <p>The key ideas -</p> <ul> <li>Convolutional layers leverage the local connectivity and weight sharing to reduce the number of learnable parameters.</li> <li>Pooling layers allow larger receptive fields letting us capture global features.</li> <li>Smaller kernels limit the number of parameters without compromising the performance much. This design decision comes from preferring deeper networks over wider networks. For example, \((1 \times 1)\) kernels are reduce the dimension in the channels dimension.</li> <li>Skip connections allow easier optimization with greater depth.</li> </ul> <blockquote> <p>Why are (1, 1) kernels useful? Use fewer channels instead?</p> </blockquote> <h1 id="transformers">Transformers</h1> <p>Transformers have shown better results in almost every task that CNNs have shone previously in. CNNs require significant depth or larger kernels to share information between non-local spatial locations (recall receptive fields).</p> <p>Many tasks, such as question-answering, require <em>long-range</em> reasoning and transformers are very good at this. For example, placing objects in augmented reality requires reasoning about light-sources, surface estimation, occlusion/shadow detection, etc. This is the primary intuition behind <strong>attention mechanism</strong> which is representative of foveated vision in humans.</p> <p><img src="../../assets/img/Computer Vision/2024-04-11-12-10-13-image.png" alt=""/></p> <p><strong>Tokens</strong> - A data type than can be understood as a set of neurons obtained from vectorizing patches of an image. Typically need not be vectors, but they can be any structured froup that alows a set of differentiable operations. Note that these tokens in hidden layers might not correspond to pixels or interpretable attributes.</p> <p>The following captures a very good intuition for transformers.</p> <p><em>A transformers acts on tokens similarly as neural network acts on neurons. That is, combining tokens is same as for neurons, except tokens are vectors \(t_{out }= \sum_i w_i t_i\). In neural networks, linear layers are represented by \(x_{out} = W x_{in}\) and \(W\) is data-free, whereas in transformers, \(T_{out} = AT_{in}\), \(A\) depends on the data (attention). Again, non-linearity in neural networks is implemented via functions like ReLU whereas transformers use dense layers for non-linearity (applied token wise).</em></p> <p>The attention layer is a spsecial kind of linear transformation of tokens, wherein the attention function \(A = f(.)\) tells how much importance to pay to each token depending on the input query and other signals. <em>Attention-maps</em> help us visualize the global dependencies in the information. The required information is embedded in some dimension of the token representation. For example, the first dimension can count the number of horses in an iamge, and the bottom 3 dimensions can encode the color of the horse on the right. Attention has this flexibility to different allocations address different parts of a query. They can “attend” to only certain patches <em>which are important to the query</em>. This kind of functionality is difficult with CNNs.</p> <blockquote> <p>Apply embedding and neural network (before CNNs and Transofrmers)? Same number of parameters? Essentially similar thing? Associated higher weight to more related embedding.</p> </blockquote> <h3 id="query-key-value-attention">Query-Key-Value Attention</h3> <p>The mechanisms described previously are implemented by projecting tokens into queries, keys and values. Each of these are a vector of dimensions $p$, where $p &lt; d$. The <strong>query</strong> vector for the question is used to weigh the <strong>key</strong> vector for each token to obtain the <strong>values</strong>. This is done via computing the similarity between query and each key, and then the <em>attention</em> is given by the extent of similarities, normalized with softmax. The output token is obtained by summing all value vectors with weights assigned from the previous calculations. Roughly, the process looks like -</p> \[\left. \begin{align*} q = W_q t \\ K_i = W_k t_i \end{align*} \right\} \implies s_i = q^T k_i \\ a_i = softmax(s_i) \\ t_{out} = \sum a_i v_i\] <p>The purpose of “values” is to ‘project’ back the similarities between queries and keys to the ‘token space’.</p> <p>Also note that, if the patch-size is too large, then we might lose the information within the patches. This is a tradeoff, and the decision is made based on the task at hand. For example, classification may work with large patches but tasks such as segmentation may not.</p> <h4 id="self-attention">Self-Attention</h4> <p>How do we learn implicit representations that can work with general queries? We compute <em>self-attention</em> using the image tokens as the queries - information derived from the image itself. How does this process help us? For example, if we are performing instance segmentation of an image with horses. Then, the concept of ‘horse’ is learned by attending more to tother horse tokens and less to background. Similarly, ‘a particular instance’ is learned by attending less to the tokens from other horses. When we do this for all pairs across \(N\) tokens gives us an \(N \times N\) attention matrix.</p> <p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png" title="" alt="" width="307"/></p> <p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" title="" alt="" width="393"/></p> <p>Typically, this matrix is computed in the <strong>encoder</strong> part of a transformer. This matrix is then used in the decoder with an input query to obtain the required results.</p> <h3 id="encoders">Encoders</h3> <p>An encoder of a transformer typically consists of many identical blocks connected serially with one another. Each such encoder block, computes a self-attention matrix and passes it through a feed-forward neural network, and they need to be applied across all the tokens in the input. Since the embeddings of tokens are independent of one another, each level of encoder blocks can be applied <em>in parallel</em> to all the tokens (patches in vision transformers) to obtain the embeddings. Such parallel computations were not possible in previous models like RNNs, which heavily relied on sequential dependencies.</p> <p>The original transformers paper normalizes the similarities with \(\sqrt{N}\) where \(N\) is the embedding dimension. This allows the gradients to stabilise and gives much better performance. This a simplified view of the mechanisms used in transformers.</p> <p>In addition to these, transformers also use <em>positional encoding</em> to encode the position of the tokens in the input sequence. In the context of images, is encodes where each patch occurs in the image.</p> <p><img src="C:\Users\ITSloaner\AppData\Roaming\marktext\images\2024-04-10-18-07-17-image.png" alt=""/></p> <p>Positional encoding is usually done via sinusoidal functions. Other “learning-based” representations have been explored but they don’t have much different effect. This encoding structure allows extrapolation to sequnce lengths not seen in training.</p> <p>They also have <em>multi-head attention</em> which is equivalent to multiple channels in CNNs. That is, it allows patches to output more than one type of information.</p> <p><img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt=""/></p> <p>This <a href="https://jalammar.github.io/illustrated-transformer/">blog</a> explains these mechanisms in-depth for interested readers. In summary, the functions of the encoder is visualized as</p> <p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png" alt=""/></p> <h3 id="decoders">Decoders</h3> <p>A decoder block is similar to an encoder block, with auto-regressive . The attention values learned in Encoders are used as ‘keys’ in the decoder attention block. This is called as <em>cross attention</em>.</p> <h2 id="vision-transformers">Vision-Transformers</h2> <p>Vision transformers build on the same ideas used in a transformer. Typically, they use only the encoder part of the architecture where patches are extracted from images and are flattened to form the tokens for the attention mechanism. They are usually projected using a linear layer or a CNN before performing the calculations. Apart from this, vision transformers also include an additional <em>class token</em> in the embeddings to help learn the global information across the image.</p> <p>CNNs typically have better inductive bias whereas transformers excel in shorter learning paths for long-range reasoning. However, the cost of self-attention is quadratic in the image size. Also note that, if the patch-size is too large, then we might lose the information within the patches. This is a tradeoff, and the decision is made based on the task at hand. For example, classification may work with large patches but tasks such as segmentation may not.</p> <h3 id="swin-transformers-and-dense-prediction-transformers">Swin Transformers and Dense Prediction Transformers</h3> <p>The vanilla vision transformer is restricted to classification tasks and is not optimal for other tasks like detection and segmentation. Also, as we have noted before, the quadratic complexity limit the number of patches in the image. Some image processing pipelines extract features across different scales of an image whereas the vanilla transformer is restricted to the uniform (coarse) scale.</p> <p>To address these limitations, <strong>swin transformers</strong> bring two key ideas from CNNs</p> <ul> <li> <p>Multi-scale feature maps - Feature maps from one resolution are down sampled to match the size in the next block.</p> <p><img src="assets/2024-06-13-21-25-21-image.png" alt=""/></p> </li> <li> <p>Local connectivity -</p> <ul> <li> <p>Windowed self-attention - Limit the computations by considering a local window (this was actually left as future work in the original attention paper).</p> </li> <li> <p>Shifted window self-attention - Allows windowed self-attention to learn long-range features using “shifts”. Essentially, we move the patches around to bring farther patches close together.</p> </li> </ul> </li> </ul> <p>However, these modifications are not enough for tasks like segmentation, which require reasoning at a pixel-level. Instead, we use something called as a <em>dense prediction transformer</em> (<strong>DPTs</strong>) where we couple the transformer encoder with a convolutional decoder to upsample and predict the required output.</p> <blockquote> <p>CNNs are shift-invariant whereas ViTs are permutation invariant. Why?</p> </blockquote> <p><img src="assets/2024-06-13-21-32-22-image.png" alt=""/></p> <p>At each scale level in the above picture, we <em>reassemble</em> the tokens by concatenating and convolving with appropriate kernels to recover image-like representations in the decoder layers.</p> <h3 id="multimodality">Multimodality</h3> <p>Transformers allowed for easy multi-modal representations by tokenizing data from each modality with its own variant of <em>patches</em>. Works such as VATT have explored merging audio waveforms, text and images.</p> <h1 id="generative-models">Generative Models</h1> <h2 id="discriminative-and-generative-models">Discriminative and Generative Models</h2> <p>Discriminative models (classifiers) learn a many-to-one function \(f\) to learn labels for a given input. A generative model \(g\) maps these labels to the input space, and this function is one-to-many since one label can map to multiple inputs. It is difficult to model a stochastic function. Therefore, a generator model is coupled with a <em>noise vector</em> to construct a deterministic \(g\) with stochastic input \(z\). This variable \(z\) is called a <strong>latent variable</strong> since it is not observed in training data composed of \(\{x, y\}\) pairs.</p> \[\begin{align*} \text{Discriminative Models learn } P(Y \vert X) \\ \text{Generative Models learn } P(X, Y) \\ \text{Conditional Generators learn } P(X \vert Y) \\ \end{align*}\] <p>Let us focus on image generative models. Then, each dimension of the latent variable can encode the various characteristics of the image essentially allowing us to generate a wide variety of images. The labels \(y\) for images need not be classification labels, but textual descriptions can also be used for supervision.</p> <p>To understand how these models work, consider the case of unconditional generative models. The goal is, given the real data \(x\), to generate synthetic data \(\hat x\) that <em>looks like</em> the real data. How do we quantify ‘looks like’?</p> <ul> <li> <p>We can try and match some marginal statistics - mean, variance, edge statistics, etc of the real data. Such measures are very useful in techniques for texture synthesis. For example, <a href="https://www.cns.nyu.edu/labs/heegerlab/content/publications/Heeger-siggraph95.pdf">Heeger-Bergen texture synthesis [1995]</a> uses an iterative process starting from the Gaussian noise and matches intensity histograms across different scales. Such design choices are used in modern techniques like <a href="https://en.wikipedia.org/wiki/StyleGAN">StyleGANs</a> and <a href="https://en.wikipedia.org/wiki/Diffusion_model">Diffusion models</a>.</p> </li> <li> <p>Have a high probability of success under a model fit to real-data (A discriminator).</p> </li> </ul> <p>The key-challenge in these models is novelity in the generation to ensure generalization.</p> <p>Generative models are classified into the following approaches -</p> <ul> <li> <p><strong>Energy-based models</strong> - Learn a scoring function \(s:X \to R\) that scores real-samples with a high score which represents energy/probability. During generation, return samples which have a high score. This paradigm is particularly useful for applications like anomaly detection.</p> </li> <li> <p><strong>Sampling from Noise</strong> - Learn a generative function \(g: Z \to X\) without needing to learn a probability density function. These methods explicitly model the data distribution, and techniques like GANs and Diffusion Models come under this regime.</p> </li> </ul> <h2 id="densityenergy-based-models">Density/Energy based models</h2> <p>Learn a scoring function \(s:X \to R\) that scores real-samples with a high score which represents energy/probability. During generation, return samples which have a high score. This paradigm is particularly useful for applications like anomaly detection.</p> <p>In these methods, we produce \(p_\theta\) fit to the training data by optimizing \(\arg\min_{p_\theta} D(p_{\theta}, p_{data})\). Since we don’t know \(p_{data}\) explicitly (we only have access to samples \(x \sim p_{data}\)), we minimize the <strong>KL-divergence</strong> to reduce the distance between the distributions using the samples.</p> \[\begin{align*} p_\theta^* &amp;= \arg\min_{p_\theta} KL(p_{data} \vert\vert p_\theta)\\ &amp;= \arg\max_{p_\theta} \mathbb E_{x\sim p_{data}}[\log p_\theta] - \mathbb E_{x\sim p_{data}}[\log p_{data}] \\ &amp;= \arg\max_{p_\theta} \mathbb E_{x\sim p_{data}}[\log p_\theta] \end{align*}\] <p>Intuitively, this way of optimization increases the density where the models are observed.</p> <p>The energy-based models have a slight modification wherein the energy function \(E_\theta\) is unnormalized unlike the probability density function \(p_\theta\). Indeed, \(p_\theta\) can be determined as</p> \[\begin{align*} p_\theta &amp;= \frac{e^{-E_\theta}}{Z(\theta)}, \\ &amp;\text{ where } Z_\theta = \int_x e^{-E_{\theta}(x)} dx \end{align*}\] <p>This formulation is called as <em>Boltzmann or Gibbs</em> distribution. Learning energy function is convenient since normalization is intractable since we cannot see all the samples in the distribution. What is the motivation to use exponential functions?</p> <ol> <li> <p>It arises naturally in physical statistics</p> </li> <li> <p>Many common distributions (Normal, Poisson, …) are modeled using exponential functions.</p> </li> <li> <p>Want to handle large variations in probability</p> </li> </ol> <p>Although we don’t have the probabilities, we can still compare different samples using ratios of their energies.</p> <p>After obtaining the energy distribution, how do we use it to sample in these approaches? Sampling approaches <strong>Markov Chain Monte Carlo</strong> (MCMC) only require relative probabilities for generating data. We can also find data points that maximize the probability.</p> \[\nabla_x \log p_\theta(x) = -\nabla_x E_\theta(x)\] <p>On the other hand, where would be prefer probability formulation over energy? Certain systems like safety in autonomous driving require absolute quantifiable probabilities rather than relative scores.</p> <p>In probability modeling, the probability of space where no samples are observed is automatically pushed down due to normalization. However, in energy based models, we need an explicit negative term to push energy up where no data points are observed. To do so, we set up an iterative optimization where the gradient of the log-likelihood function naturally decomposes into contrastive terms</p> \[\begin{align*} \nabla_\theta \mathbb E_{x \sim p_{data}} [\log p_\theta(x)] &amp;= \frac{1}{N}\nabla_\theta\sum_{i = 1}^N (\underbrace{-E_\theta(x^{(i)})}_{\text{ data samples}} + \underbrace{E_\theta(\hat x^{(i)})}_{\text{model samples}}) \\ &amp;= - \mathbb E_{x \sim p_{data}} [\nabla_\theta \mathbb E_\theta (x)] + \mathbb E_{x \sim p_\theta} [\nabla_\theta \mathbb E_\theta(x)] \end{align*}\] <h4 id="sampling">Sampling</h4> <p>We randomly initialize \(\hat x_0\) at \(t = 0\). Then, we repeat the following</p> <ol> <li> <p>Let \(\hat x' = \hat x_t + \eta\)$ where \(\eta\) is some noise</p> </li> <li> <p>If \(\mathbb E_\theta(\hat x') &lt; \mathbb E_\theta(\hat x_t)\), then choose \(\hat x_{t + 1} = \hat x'\).</p> </li> <li> <p>Else choose \(\hat x_{t + 1} = \hat x'\) with probability \(e^{(\mathbb E_\theta(\hat x_t) - \mathbb E_\theta(\hat x'))}\)</p> </li> </ol> <p>In practice, this algorithm takes a long time to converge. A variant of this called <strong>Langevin MCMC</strong> uses the gradient of the distribution to accelerate the sampling procedure</p> <ol> <li> <p>Choose \(q(x)\) an easy to sample prior distribution.</p> </li> <li> <p>Repeat for a fixed number of iterations \(t\)</p> \[\hat x_{t + 1} \sim \hat x_t + \epsilon \nabla_x \log p_\theta(\hat x_t) - \sqrt{2\epsilon} z_t\] <p>where \(z_t \sim \mathcal N(0, I)\). When \(\epsilon \to 0\), and \(t \to \infty\), we have \(\hat x_t \sim p_\theta\)</p> </li> </ol> <h2 id="sampling-from-noise">Sampling from Noise</h2> <h3 id="diffusion-models">Diffusion Models</h3> <p>The intuition for these models builds from our previous approach. It is hard to map pure noise to \(x \sim N(0, I)\) to structured data, but it is very easy to do the opposite. To readers familiar with autoregressive models, where we remove one pixel of information at a time, diffusion models generalise this notion further. Diffusion models have two steps in training -</p> <p>The forward process in these models involves adding noise to the image \(x_0\) over many time steps. At time \(t\), we add noise \(\epsilon_t\) to obtain \(x_{t + 1}\) from \(x_{t}\). The noise addition is modeled with respect to the following equation</p> \[x_t = \sqrt{(1 - \beta_t) x_{t - 1}} + \sqrt{\beta_t}\epsilon_t, \quad \epsilon_t \sim N(0, I)\] <p>If this process is repeated for a large number of times, it can be shown that the final output simulates white noise. Now, in the learning part, we train a predictor to learn denoising from \(x_t\) to \(x_{t - 1}\). That is, given our model \(f_\theta\)</p> \[\hat x_{t - 1} = f_\theta(x_t , t)\] <h3 id="forward-noising">Forward Noising</h3> <p>Given an image \(x_0 \sim q(x)\), we essentially add the following Gaussian noise in \(T\) time steps</p> \[q(x_t \vert x_{t - 1}) = \mathcal N(x_t ; \sqrt{1 - \beta} x_{t - 1}, \beta_t I)\] <p>The term \(\beta_t\) is referred to as the schedule and \(0 &lt; \beta_t &lt; 1\). Typically, we set it to a small value in the beginning and do linear increments with time. The above process is a Markov’s process, resulting in the following property</p> \[q(x_{1:T} \vert x_0) = \prod_{t = 1}^T q(x_t \vert x_{t - 1})\] <p>Instead of a slow step-by-step process, training uses samples from arbitrary time step. To decrease the computations, we use the following properties of Gaussian distributions</p> <ul> <li> <p>Reparameterization trick: \(\mathcal N(\mu, \sigma^2)= \mu + \sigma \epsilon\) where \(\epsilon \sim \mathcal N(0, I)\)</p> </li> <li> <p>Merging Gaussians \(\mathcal N(0, \sigma_1^2 I)\) and \(\mathcal N(0, \sigma_2^2 I)\) is a Gaussian of the form\(\mathcal N(0, (\sigma_1^2 + \sigma_2^2) I)\)</p> </li> </ul> <p>Define \(\alpha_t = 1 - \beta_t\), and \(\bar \alpha_t = \prod_{i = 1}^t \alpha_i\), we can now sample \(x_t\) at an arbitrary time step</p> \[\begin{align*} x_t &amp;= \sqrt{\alpha_t} x_{t - 1} + \sqrt{1 - \alpha_t}\epsilon_{t - 1} \\ &amp;= \sqrt{\alpha_t\alpha_{t - 1}} x_{t - 1} + \sqrt{1 - \alpha_t\alpha_{t - 1}}\epsilon_{t - 1} \\ &amp;\dots \\ &amp;= \sqrt{\bar \alpha_t}x_0 + \sqrt{1 - \bar \alpha_t}\epsilon \end{align*}\] <p>When we schedule \(\beta_t\) such that \(\beta_1 &lt; \beta_2 &lt; \dots, \beta_T\) so that \(\bar \alpha_1 &gt; \dots, &gt; \bar \alpha_T\), such that \(\bar \alpha_T \to 0\), then</p> \[q(x_T \vert x_0) \approx \mathcal N(x_T; 0, I)\] <p>The intuition is that the diffusion kernel is a Gaussian</p> \[q(x_t) = \int q(x_0, x_t) dx_0 = \int q(x_0) q(x_t \vert x_0) dx_0\] <p>There are theoretical bounds showing a relation between the number of steps and overfitting of the model to the distribution. There are no such guarantees for VAEs.</p> <p>The increasing \(\beta_t\) schedule sort of accelerates the diffusion process wherein we simulate white noise with few iterations. However, the stable diffusion paper to generate images chose the cosine learning schedule which gave better results.</p> <h3 id="reverse-denoising">Reverse Denoising</h3> <p>The goal is to start with noise and gradually denoise to generate images. We start with \(x_T \sim \mathcal N(0, I)\) and sample form \(q(x_{t - 1} \vert x_t)\) to denoise. When \(\beta_t\) is small enough, we can show that this quantity is a Gaussian. However, estimating the quantity is difficult since it requires optimization over the whole dataset.</p> <p>Therefore, we learn a model \(p_\theta\) to reverse the process</p> \[P_\theta(x_{t - 1} \vert x_t) = \mathcal N(x_{t - 1}; \mu_\theta(x_t, t), \Sigma)\] <p>#</p> <p>Essentially, the training objective is to maximise log-likelihood over the data distribution. A variational lower bound can be derived and is used in practice</p> \[\mathbb E_{q(x_0)}[-\log p_\theta(X_0)] \leq \mathbb E_{q(x_0) q(x_{1:T}\vert x_0)} \left[-\log \frac{p_\theta (x_{0:T})}{q(x_{1:T} \vert x_0)}\right]\] <p>Then, this decomposes into</p> \[\sum_{t &gt; 1} D_{KL} (q(x_{t - 1} \vert x_t, x_0) \vert\vert p_\theta(x_{t - 1} \vert x_t)) + \text{other terms}\] <p>The idea is that $q(x_{t - 1} \vert x_t, x_0)$ is tractable even though $q(x_{t - 1} \vert x_t)$ is not. That is because</p> \[\begin{align*} q(x_{t - 1} \vert x_t, x_0) &amp;= \mathcal N(x_{t - 1}; \tilde \mu_t(x_t, x_0), \tilde \beta_t I) \\ \text{where } \tilde \mu_t(x_t, x_0) &amp;= \frac{\sqrt{\bar \alpha_{t - 1}}}{1 - \bar \alpha_t}x_0 + \frac{\sqrt{1 - \beta_t} ( 1- \bar\alpha_{t - 1})}{1- \bar\alpha_{t - 1}}x_t \\ &amp;=\tilde \beta_t = \frac{1 - \bar \alpha_{t - 1}}{1 - \bar \alpha_t} \beta_t \end{align*}\] <h3 id="generative-adversarial-networks">Generative Adversarial Networks</h3> <p>In these architectures, a generator network tries to fool the discriminator by generating real-looking images. In contrast, a discriminator network tries to distinguish between real and fake images. GANs don’t produce as good images as diffusion models, but the concept of adversarial learning is a crucial concept in many fields. The framework looks like this -</p> <p><img src="../../assets/img/Computer%20Vision/2024-04-24-17-21-56-image.png" alt=""/></p> <p>The objective function for a GAN is formulated as a mini-max game - the generator tries to maximize the loss function whereas the discriminator tries to reduce it.</p> \[L = \min_{\theta_g} \max_{\theta_d} [\mathbb E _{x \sim p_{data}} \log D_{\theta_d} (x) + \mathbb E_{z \sim p(z)} \log (1 - D(G_{\theta_g}(z)))]\] <p>The training is done alternately, performing gradient ascent on the generator and descent on the discriminator.</p> <h1 id="object-detection">Object Detection</h1> <p>Simply put, the goal is identify objects in a given image. The evolution of algorithms for object detection is summarized as</p> <ul> <li> <p>HoG + SVM - Sliding window mechanism for feature detection.</p> </li> <li> <p>Deformable Part Models (DPMs) -</p> </li> <li> <p><strong>CNN</strong> - The vanilla approaches with CNNs proposed sliding a window across the image and use a classifier to determine if the window contains an object or not (one class is background). However, this is computationally expensive because we need to consider windows at several positions and scales. Following this, <strong>region proposals</strong> were designed to find blob-like regions in the image that could be an object. They do not consider the class and have a high rate of false positives which are filtered out in the next stages. Such class of methods are called <strong>multi-stage CNNs</strong> (RCNN, Fast-RCNN, Faster-RCNN) and are covered in detailed below. On the other hand, the sliding window approach has been optimised using anchor-boxes and this class of detectors are referred to as <strong>single-stage CNNs</strong> (YOLO series) which have been discussed in the next section. For more details, interested readers could also refer to <a href="https://sudhansh6.github.io/blog/Object-Detection/">my blog</a>.</p> </li> <li> <p><strong>Transformers</strong> - These methods are inspired from CNNs and have a transformer back-end. Examples include DETR and DINO discussed in later sections.</p> </li> <li> <p>Self-supervision</p> </li> <li> <p>Open vocabulary</p> </li> </ul> <h2 id="multi-stage-cnns">Multi-stage CNNs</h2> <h3 id="rcnn">RCNN</h3> <p>For the region proposal network, an ImageNet pre-trained model (ResNet or like) is used. Since it has to be fine-tuned for detection, the last layer is modified to have 21 object classes (with background) instead of 1000 and is retrained for positive and negative regions in the image. The regions are then</p> <h3 id="fastrcnn">FastRCNN</h3> <h3 id="fasterrcnn">FasterRCNN</h3> <p>Uses a Region Proposal Network (RPN) after the last convolutional layer. The RPN is trained to produce region proposals directly without any need for external region proposals. After RPN, the RoI Pooling and an upstream classifier are used as regressors similar to FastRCNN.</p> <h4 id="region-proposal-network">Region Proposal Network</h4> <p>The network does the following -</p> <ul> <li> <p>Slide a small window on the feature map</p> </li> <li> <p>A small network to do classification - object or no-object</p> </li> <li> <p>A regressor to give the bounding box</p> </li> </ul> <h4 id="anchors">Anchors</h4> <p>Anchors are a set of reference positions on the feature map</p> <p>3 dimensions with aspect ratio, 3 different scales of boxes,</p> <h4 id="training">Training</h4> <h4 id="non-maximal-supression">Non-Maximal Supression</h4> <p>Since, multiple anchor boxes can map to an object, we iteratively choose the highest scoring box among the predictions and supp ress the predictions that have high IoU with the currently chosen box.</p> <h2 id="transformer-based-architectures">Transformer based Architectures</h2> <h3 id="detr">DETR</h3> <p>Faster R-CNN has many steps, handcrafted architecture and potentially non-differentiable steps. In contrast, DETR was proposed - an end-to-end transformer based architecture for object detection. The motivation was to capture all the human-designed optimization parts of the pipeline into one black-box using a transformer.</p> <p><img src="../../assets/img/Computer%20Vision/2024-04-24-17-53-05-image.png" alt=""/></p> <p>Why do we want such an end-to-end architecture? They are more flexible to diverse data, capturing large datasets, finetuning, etc.</p> <p>In this architecture, a CNN backbone like ResNet extracts the features which are passed through the encoder to obtain latent representations. These are then used with a decoder along with object queries in the form of cross-attention to give bounding boxes as the output.</p> <p>After obtaining the bounding boxes, the loss is appropriately calculated by matching the boxes to the correct labels. It is our task to assign a ground truth label to the predictions</p> <blockquote> <p>Hungarian??</p> </blockquote> <p>The decoder takes object queries as input,</p> <p>500 epochs</p> <p>encoder is segmented, decoder is just getting bounding boxes</p> <h3 id="dino">DINO</h3> <p>lagging in scale variant objects.</p> <p>contrastive loss acting for negative samples</p> <p>YOLOX</p> <h2 id="single-shot-detectors">Single Shot-Detectors</h2> <p>The motivation for these networks is to infer detections faster without much tradeoff in accuracy.</p> <p><strong>Anchor boxes</strong></p> <p>no ambiguous IoU</p> <p>Hard negative mining</p> <h3 id="you-only-look-once-yolo">You Only Look Once (YOLO)</h3> <p>Divide into grids</p> <p>anchor boxes in grid and class probability map</p> <h3 id="yolo-x">YOLO-X</h3> <p>No anchored mechanism - hyperparameter tuning, more predictions, less generalization (OOD fails)</p> <p>Detection head decoupling, no anchor boxes, SimOTA</p> <p>predict size and centers rather than top-left</p> <p>mosaicing and mixing</p> <h3 id="pyramid-feature-extractions">Pyramid Feature Extractions</h3> <p>RFCN</p> <h1 id="semantic-segmentation">Semantic Segmentation</h1> <p>The goal in this task is much more fine-tuned wherein we assign class pixel-wise. We want locally accurate boundaries as compared to finding bounding boxes in object detection. The approaches require global reasoning as well.</p> <p>A naive approch would be to classify a pixel by considering the patch around the pixel in the image. This is very expensive computationally and does not capture any global information.</p> <p>Another approach is to consider image resolution convolutions (purely convolutions with stride 1, no pooling) and maining the image resolution to produce the output. This still does not work quite well, and they also consume more memory.</p> <p>Taking classification networks as the motications, some networks try pooling/striding to downsample the features. This architecture corresponds to the encoder part of the model, and the decoder then upsamples the image using transposed convolution, interpolation, unpooling to recover the spatial details in the original size. The convolutions can go deeper without occupying a lot of memory.</p> <p>How do we upsample?</p> <ul> <li> <p>Transposed convolution to upsample. The operation is shown below -</p> <table> <tbody> <tr> <td>![Transpose Convolution for Up-Sampling Images</td> <td>Paperspace Blog](https://blog.paperspace.com/content/images/2020/07/conv.gif)</td> </tr> </tbody> </table> <p>However, the outputs are not very precise. Combine global and local information.</p> </li> <li> <p>U-Net</p> </li> <li> <p>Unpooling followed by convolutions (simpler implementation of transposed convolution)</p> <p>The <strong>max-unpooling</strong> operation does the following -</p> <p>Why is this better? Lesser memory</p> </li> </ul> <p>In all the architectures above, once you downsample the image, you lose a lot of spatial information and the network uses a significant amount of parameters learning the upsampling process to represent the final output. To help with this, we can do the following</p> <ul> <li> <p>Predict at multiple scales and combine the predictions</p> </li> <li> <p><strong>Dilated convolutions</strong> - These are used to increase the receptive field without downsampling the image too much -</p> <p><img src="https://th.bing.com/th/id/R.4992be8ec58775d0f6f963c2ae7129b3?rik=orAxXCOkxWt5dw&amp;pid=ImgRaw&amp;r=0" alt="Animations of Convolution and Deconvolution — Machine Learning Lecture"/></p> <p>It’s multi-scale and also captures full-scale resolution. The idea of dilated convolution is very important in other tasks as well - It prevents the loss of spatial information in downsampling tasks. However, there are gridding artifacts and higher memory consumptions with Dilated networks.</p> <p><strong>Degridding solutions</strong></p> <ul> <li> <p>Add convolution layers at end of the network with progressively lower dilation</p> </li> <li> <p>Remove skip connections in new layers, can propogate gridding artefacts because skip connections transfer high-frequency information.</p> </li> </ul> </li> <li> <p>Skip-connections</p> </li> <li></li> </ul> <h2 id="deeplab-v3">DeepLab v3+</h2> <h2 id="segformer">Segformer</h2> <h1 id="instance-segmentation">Instance Segmentation</h1> <h2 id="mask-rcnn">Mask RCNN</h2> <p>Predict eh</p> <h1 id="panoptic-segmentation">Panoptic Segmentation</h1> <p>Panoptic segmentation extends the idea of instance segmentation even further to segment objects beyond a fixed set of classes.</p> <p>#</p> <h1 id="universal-segmentation">Universal Segmentation</h1> <p>Specialized architectures for semantic</p> <h2 id="maskformer">MaskFormer</h2> <p>The architecture contains a strong CNN backbone for the encoder along with a query and pixel decoder. The query decoder essentially takes $n$ inputs to generate $n$ masks. The outputs from these layers are passed through an MLP to get binary masks for different classes.</p> <h2 id="masked2former">Masked2Former</h2> <p>Uses a masked attention layer instead of cross-attention to provide faster convergence</p> <h3 id="limitations">Limitations</h3> <p>Needs to be trained on speicfic datasets, struggles with small objects and large inference time.</p> <p>Hasn’t been extended to video segmentation and panoptic segmentation.</p> <h2 id="mask-dino">Mask DINO</h2> <p>Does both detection and segmentation. A Mask prediction branch along with box prediction in DINO.</p> <h3 id="unified-denoising-training">Unified Denoising Training</h3> <h1 id="foundation-models">Foundation Models</h1> <p>Models that have been trained on simple tasks which have good performance in the pipelines of other tasks for <strong>zero-shot transfer</strong> capabilities. Such models were first used in Natural Language Processing and using these in vision is typicalled hindered by unavailability of labeled data.</p> <h2 id="segment-anything">Segment Anything</h2> <p>A model developed by Meta with zero-shot segmentation capabilities. The task this model tries to solve is to allow for interactive and automatic use with zero-shot generatlization and re-use. The model allows for flexibile prompts with real-time usage and is ambiguity-aware. The model also has a data-engine which is used to generate data through the model.</p> <h3 id="promptable-segmentation">Promptable segmentation</h3> <p>The model generates a valid mask for a given prompt (even if ambiguous) as input. How is the ambiguity resolved? Ambiguous cases arise when the mutiple objects lie on top of each other.</p> <p><img src="../../assets/img/Computer%20Vision/2024-05-03-17-21-39-image.png" alt=""/></p> <p>The image encoder is heavy - a pretrained ViT with masked autoencoder. The design choice for the masking involved masking around 75% of the patches. Unlike NLP tasks, vision tasks rely heavily on spatial information and neighbor patches can be reconstructed without much effort. Also, NLP tasks can get away with a simple MLP for decoders but vision taks require strong decoders.</p> <p>The model allows for prompts using points, box or text. The decoder has self-attention on the primpts followed by cross-attention with image and MLP for non-linearity to get masks. The model also estimates the IoU to rank masks.</p> <p>The emphasis of the model was also to make it universal. Since the encoder is heavy, the embeddings are precomputed after which the prompt encoder and mask decoder work quite fast.</p> <h3 id="data-engine">Data Engine</h3> <p>There is no large-scale dataset available for training a segmentation model of this scale. Initially, the model has been trained iteratively on available datasets with several rounds of human correction. Following this, in the semi-automatic stage, the diversity of outputs (increasing ambiguity in overlapping objects) is improved by human refinement. Finally, in the fully automatic stage, prompting is introduced and masks with high IoU are preserved followed by NMS.</p> <h3 id="zero-shot-transfer-capabilities">Zero-shot transfer capabilities</h3> <p>The idea is to fine-tune the model for specific tasks like edge-derection, object proposals and isntance segmentation.</p> <h1 id="future-trajectory-prediction">Future Trajectory Prediction</h1> <p>This problem arises in many applications, particularly in autonomous driving. Given traffic participants and scene elements which are inter-dependent, we need to predict trajectories with scene awareness and multimodality. Humans are able to predict up to 10 seconds in highway scenes and plan their trajectory effectively whereas the SOTA models have a large gap in performance.</p> <p>More concretely, the algorithm takes a video sequence as an input, and needs to predict <strong>probabilistic</strong> trajectories which are diverse, have 3D awareness, capture semantics interactions and focus on <strong>long-term rewards</strong>. There are many components in these solutions including 3D localization module, generative sampling modules, semantic scene modules and scene aware fusion modules with encoders and decoders.</p> <p>#</p> <h3 id="representation-framework">Representation Framework</h3> <p>The past observed trajectories are represented by \(X = \{X_{i, t - l + 1}, \dots, X_{i, t}\}\) using we which we wish to predict future trajectory \(Y_i = \{Y_{i, t + 1}, \dots, Y_{i, t + \delta}\}\). The parameter \(\delta\) represents how far ahead we want to look in the future.</p> <p>The <em>sample generation module</em> produces future hypotheses \hat Y, and then a ranking module assigns a reward to each hypothesis considering long-term rewards. Following this, the refinement step calculates the displacement \(\Delta Y_t\) for the selected trajectory.</p> <p>Focusing on the sample generation module, the goal is to estimate posterior \(P(Y \vert X, I)\). Earlier, RNNs and other deterministic function maps from \(\{X, I\}\) to Y have been designed to address this. The problem with training in this approach is that the ground truth has a single future trajectory whereas the sampler predicts a distribution. How do we compare the two?</p> <h2 id="principal-component-analysis">Principal Component Analysis</h2> <p>Principal component analysis (PCA) is a linear dimensionality reduction technique where data is linearly transformed onto a new coordinate system such that the directions (principal components) capturing the largest variation in the data can be easily identified. PCA works well when the data is near a <strong>linear manifold</strong>* in high dimensional spaces.</p> <p>CAn we approximate PCA with a Network? Train a network with a bottleneck hidden layer, and try to make the output the same as the input. Without any activations, the neural network simply performs least-squares minimization which essentially is the PCA algorithm.</p> <p>Adding non-linear activation functions would help the network map non-linear manifolds as well. This motivates for autoencoder networks.</p> <h2 id="autoencoders">Autoencoders</h2> <p>Network with a bottleneck layer that tries to reconstruct the input. The decoder can map latent vectors to images essentially helping us reconstruct images from a low-dimension. However, this is not truly generative yet since it learns a one-to-one mapping. An arbitrary latent vector can be far away from the learned latent space, and sampling new outputs is difficult.</p> <h2 id="--variational-autoencoder">### Variational Autoencoder</h2> <p>This network extends the idea in autoencoders and trains the network to learn a Gaussian distribution (parametrized by \(\mu, \sigma\)) for the latent space. After learning, say, the normal distribution, sampling is quite easy to generate new samples representative of the trianing dataset using the decoder.</p> <p>To train this network, a distribution loss, like the KL-divergence is added in the latent space. But how do we backpropagate through random sampling? The reparametrization trick!</p> <h3 id="conditional-variational-autoencoder">Conditional Variational Autoencoder</h3> <p>The encoder and decoder networks are now conditioned on the class variable.</p> <h2 id="conditional-diffusion-models">Conditional Diffusion Models</h2> <p>Gradient of log, score function, conditioning at the cost of diversity - train a classifier and a diffusion model together in principle it’s fine. Potential issues - classifier can’t learn from noisy images and gradient of the classifier is not meaningful.</p> <h2 id="classifier-free-guidance">Classifier-Free Guidance</h2> <p>learn unconditional diffusion model \(p_\theta(x)\) and use the conditional model \(p_\theta(x \vert y)\) for some part of the training. this would not require any external classifier for training.</p> <h2 id="ctg---language-guided-traffic-simulation-via-scene-level-diffusion">CTG++ - Language Guided Traffic Simulation via Scene-Level Diffusion</h2> <p>Testing autonomous agents in the real-world scenarios is expensive. Alternately, researchers aim to build good simulator that are realistic and can be controlled easily.</p> <p>Why not RL-based methods? Designed long-term reward functions is difficult for such scenarios. The goal here is in a generation regime rather than problem solving. Also, the focus here is on controllability rather than realism. For realism, the methods would be more data-driven.</p> <h3 id="trajectory-planning">Trajectory Planning</h3> <p>Recent works have proposed diffusion-models for planning algorithms using classifier-guidance. <em>Diffuser</em> for example, is one such work where the authors generate state action pairs which are guided using a reward function to generate paths in a maze.</p> <h3 id="traffic-simulation-methods">Traffic simulation methods</h3> <p>Early works for this task rrelied on rule-based algorithms which were highly controllable but not very realistic.</p> <h3 id="ctg">CTG</h3> <p>One of the first workd for diffusion models for planning and conditional guidance for control with STL-based loss. Howeber, it modelled each agent independently which caused issues.</p> <p>CTG++ uses <strong>scene-level control</strong></p> <h3 id="problem-formulation">Problem Formulation</h3> <p>The state is given by the locations, speeds and angle of \(M\) agents whose past history and local semantic maps are given. We aim to learn a distribution of trajectories given this information.</p> <p>The encoder represents the state action pairs in an embedded space on which temporal and cross attention on a diffusion based loss for training/</p> <h3 id="inference">Inference</h3> <p>The model uses LLMs to generate code for guidance function.</p> <h3 id="limitations-and-future-work">Limitations and future work</h3> <p>CTG++ is able to generate more realistic example, but is still far-off from realism. There are no ablation studies or emperical evaluations with complicated scenarios. Also, the multi-agent modeling can be improved for scalability.</p> <p>In conclusion, trajectory prediction requires generative models which are able to distinugish between feasible and infeasible trajectories. Variational Auto-Encoders have shown good performance for this task, and the current works aim to explore the scope of diffusion models in these tasks.</p> <h2 id="leapfrog---stochastic-trajectory-prediction">LeapFrog - Stochastic Trajectory Prediction</h2> <p>Similar to CTG++, this model aims to simulate real-world traffic scenarios. The authors aim for real-time predictions and better prediction accuracy. Leapfrog initializer skips over several steps of denoising and uses only few denoising steps to refine the distribtuion.</p> <h3 id="system-architecture">System Architecture</h3> <p>Leapfrog uses physics-inspired dynamics to reduce the number of steps required for the denoising process. The leapfrog initializer estimates mean trajectory for backbone of pericition, variance to control the prediction diversity and K samples simultaneously.</p> <h3 id="limitations-1">Limitations</h3> <p>The inference speed improved dramatically and achieves state of the art performance on the datasets. The model’s efficiency is higher for lower dimensional data and requires more work for scaling to higher dimensions.</p> <h1 id="structure-from-motion">Structure from Motion</h1> <p>The problem statement for structure from motion is - given a set of unordered or ordered set of images, estimate the relative positions of the cameras and recover the 3D structure of the world, typically as point clouds. In scenarios like autonomous driving, the images are ordered and the emphasis is on real-time performance.</p> <h2 id="feature-detection">Feature Detection</h2> <p>The first step involves identifying matching features across images to determine the camera movements. In unordered feature matching, the images to be compared are identified using vocabulary based retrieval methods to reduce the complexity from \(\mathcal O(n^2)\) to \(\log\) complexity.</p> <p>In the canonical coordinate system, the camera axis passes through the origin, and the \(k\)-axis points away from the camera. The \(i\)-axis and \(j\)-axis are parallel to the image plane. The projection of 3D points in the pixel space involves a non-linear operation -</p> \[(x, y, z) \to (-d\frac{x}{z}, -d\frac{y}{z})\] <p>where \(d\) is the distance of the camera plane from the origin. For computational reasons, we want to convert these to a linear transformation. This is done via homogenous point representations -</p> \[\underbrace{(\frac{x}{w}, \frac{y}{w})}_\text{Euclidean} \to\underbrace{(x, y, w)}_\text{Homogenous}\] <p>Such representations are useful for ray marching operations. The camera transformation then becomes</p> \[\begin{bmatrix}-d &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; -d &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix} \begin{bmatrix}x \\ y \\ z \\ 1\end{bmatrix} = \begin{bmatrix}-dx \\ -dy \\ z\end{bmatrix}\] <p>which represents \((-d\frac{x}{z}, -d\frac{y}{z})\) in Euclidean space. The matrix above can be decomposed as</p> \[\underbrace{\begin{bmatrix}-d &amp; 0 &amp; 0 \\ 0 &amp; -d &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}}_{K}\underbrace{\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix}}_\text{projection}\] <p>Here, we have considered a simple model for the camera intrinsics matrix \(K\) whose general form is</p> \[\begin{bmatrix}-d_x &amp; s &amp; c_x \\ 0 &amp; -d_y &amp; c_y \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\] <p>where \(\alpha = \frac{d_x}{d_y}\) is the aspect ratio (1 unless pixels are not square), \(s\) is the skew, \(c_x, c_y\) represent the translation of the camera origin wrt world origin.</p> <h3 id="coordinate-systems">Coordinate Systems</h3> <p>We need to determine the transformations between the world and camera coordinate systems. Since camera is a rigid body, the transformation is represented by a translation and a rotation. Considering these, the projection matrix becomes</p> \[\Pi = K \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0\end{bmatrix}\begin{bmatrix}R &amp; 0 \\ 0 &amp; 0 \end{bmatrix} \begin{bmatrix} T &amp; - c \\ 0 &amp; 0\end{bmatrix}\] <h2 id="problem-formulation-1">Problem Formulation</h2> <p>Given the projections \(\Pi_i X_i \sim P_{ii}\), we aim to minimize a non-linear least squares of the form \(G(R, T, X)\) -</p> \[G(X, R, T) = \sum_{i = 1}^m \sum_{j = 1}^n w_{ij} \cdot \left\|P(X_i, R_j, t_j) - \begin{bmatrix}u_{i, j} \\v_{i, j}\end{bmatrix} \right\|^2\] <p>This problem is called as <strong>Bundle Adjustment</strong>. Since this is an non-linear least squares, Levenberg-Marquardt is a popular choice to solve this. In theory, given enough number of points, we should get a unique solution. However, practically, we require a very good initialization to navigate the high-dimensional space. Also, outliers in the feature detection can deteriorate the performance of the model drastically.</p> <h2 id="tricks-for-real-world-performance">Tricks for Real-world performance</h2> <p>Basically, we aim to make the feature detection very robust to remove all the possible outliers. Towards that, we note the following background</p> <h4 id="fundamental-matrix">Fundamental Matrix</h4> <p>Given pixel coordinates \(x_1, x_2\) of a point \(x\) in two camera views, we define a fundamental matrix \(F\) such that \(x_1Fx_2 = 0\). The fundamental matrix is unique to the cameras and their relative configuration. It is given by</p> \[\begin{align*} F &amp;= K_2^{-T} E K_1^{-1} \\ E &amp;= [t]_{\times} R \end{align*}\] <p>where \(R\) and \([t]_\times\)represent the relative base transformations given by</p> \[[t]_X = \begin{bmatrix} 0 &amp; -t_z &amp; t_y \\ t_z &amp; 0 &amp; -t_x \\ -t_y &amp; t_x &amp; 0\end{bmatrix}\] <p>The matrix \(E\) is called as the essential matrix which has similar properties as the fundamental matrix and is also independent of the camera parameters.</p> <p>The fundamental matrix is scale-invariant and has rank \(2\). Therefore, it has \(7\) degrees of freedom implying that we just need \(8\) correctly configured points (\(x\) and \(y\)) across the two pixel spaces to compute the fundamental matrix. We pose this problem as solving \(Af = 0\) - minimizing \(\|Af\|\) using SVD (to directly operate on the rank). This procedure is popularly known as the \(8\)-point algorithm. Furthermore, we can also use this to find the camera parameters by using more points.</p> <p>The essential matrix can be derived from \(F\) using \(RQ\) decomposition to obtain skew-symmetric and orthogonal matrices \([t]_\times\) and \(R\). These matrices are what we were looking to determine - the relative configuration of the cameras with respect to each other. Now, triangulation can be used to locate \(x\) in the 3D space. However, this may not work since the projection lines may not intersect in 3D space.</p> <p>To filter outliers, we will use the fundamental matrix as a model - it is a geometric property which all points have to abide by. However, the fundamental matrix itself is constructed from the detected features, which can be noisy. We need to design a robust method using heuristics to filter the outliers. We use a population-voting metric to choose the best model.</p> <h3 id="ransac">RANSAC</h3> <p>Suppose we are finding the best-fit line for a given set of points. For every pair of points, we determine the corresponding line and count the number of outliers for that particular line. The best model would then be the on which has the minimum number of outliers. This idea can be extended to any model, and it can be used to calculate the best fundamental matrix. The algorithm is as follows -</p> <ol> <li>Randomly choose \(s\) samples - typically \(s\) is the minimum sample size to git a model (7 for a Fundamental matrix)</li> <li>Fit a model to these samples</li> <li>Count the number of inliers that approximately fit the models.</li> <li>Repeat \(N\) times</li> <li>Choose the model with the largest set of inliers.</li> </ol> <p>How do we choose \(N\)? It depends on the ratio of outliers to the dataset size, the minimum number of samples \(s\) for the model and confidence score. We get</p> \[N = \frac{\log(1 - p)}{\log(1 - (1 - \epsilon)^s)}\] <p>Where \(\epsilon\) is the proportion of outliers and \(p\) is the confidence score.</p> <h3 id="bundle-adjustment">Bundle Adjustment</h3> <p>Assuming we have a good \(R, t\) robust to outliers with the above methods, we can now solve the least-squares problem to determine the scene geometry.</p> <h2 id="minimal-problems-in-computer-vision">Minimal Problems in computer Vision</h2> <h3 id="threepoint-absolute-pose-estimation">Three—point absolute pose estimation</h3> <p>Assume the intrinsics of the camera are known. Determine the camera pose from given 3D-2D correspondences. Minimal case is 3 points: 6 degrees of freedom, 2 constraints per point \((u, v)\). Given \(p_1, p_2, p_3\) in world coordinates, find positions In camera coordinates. Equivalently, determine how to move camera such that corresponding 2D-3D points align.</p> <h3 id="real-time-sfm-steady-state">Real-time SFM: Steady state</h3> <p>Suppose we have point-cloud and poses available at frame \(k\), find the pose in frame \(k + 1\).</p> <h4 id="option-1">Option 1</h4> <p>Find correspondences between consecutive frames, estimate the essential matrix and find \(R, t\). Keep repeating this as frames are read. 5 point estimation</p> <h4 id="option-2">Option 2</h4> <p>Estimate correspondences between point cloud and frame - 3 point absolute pose. Challenges - inliers calculations, which points to match. RANSAC maybe easier here. Option 1 is narrow baseline triangulation.</p> <h3 id="drift-problem">Drift Problem</h3> <p>Absolute scale is undeterminable. Why does scale matter? Noise and RANSAC. External measurements to fix this.</p> <h3 id="loop-closure">Loop Closure</h3> <p>Retrieval mechanism</p> <h2 id="lighting">Lighting</h2> <p>Unlike material properties and geometry, lighting is an external factor that is very complicated to infer. Lighting in a scene can. Be categorised as</p> <ul> <li>Local illumination - Li</li> <li>Global illumination</li> </ul> <h2 id="shape">Shape</h2> <p>Normals Explicit representations Implicit representations - SDF</p> <h1 id="inverse-rendering">Inverse Rendering</h1> <h2 id="physg">PhySG</h2> <p>Geometry - SDF, Lighting - Environment map, Material (BRDF) Sphere Tracing! Monte Carlo integration for BRDFs - too slow. PhySG - spherical Gaussians</p> <h1 id="neural-fields">Neural Fields</h1> <p>The central question associated with neural fields is “what can be seen?”. That is, how can we measure what we see and how to model it for further computations? This question is important to not only the researchers in Computer Graphics but also in a field called Plenoptic Functions. It is basically the bundle of all light rays in a scene - a 5D function parametrized by position (\(x, y, z\)) and viewing direction (\(\theta, \phi\)). Neural Fields aim to approximate this function for a scene using Neural networks. Our core problem is that we have 2D slides and we need to estimate a 5D function. This problem is referred to as Image-based Rendering. It was taken into serious consideration in literature with the advent of the digital Michaelangelo project by Stanford, wherein researchers reconstructed the 3D models of all the sculptures in the Louvre museum. A closely related problem called “view synthesis” aims to generate images from views not present in the training data. In this problem, the 5D function is simplified to a 4D light field (rays passing through 3D space). Images are then captured from multiple view points, and <strong>novel view</strong> synthesis is done via interpolation with the closest rays. The earliest work in virtual reality by Sutherland in 1968, which talks about view synthesis and the ideas introduced in this paper are still relevant till date. The problem also gained interest in the graphics community - Chen et. al in SIGGRAPH 1993 talks about representing 3D models using images. However, researchers viewed view-synthesis as a small part of overall 3D scene understanding.</p> <h2 id="perfect-3d-understanding">Perfect 3D understanding</h2> <p>Research then aimed to solve this problem rather than view synthesis. How is it different? In view syntheis, there is no information regarding the geometry of the scene itself. We want to infer the structure of the scene as well. So, how do we estimate the geometry of the scene?</p> <h3 id="image-depth-warping">Image-Depth Warping</h3> <p>As we have seen earlier, one of the first approaches for this problem was to perform feature detection and matching to triangulate points in 3D space. This core approach is being used in Depth from Stereo, multi-view stereo, 3D reconstruction, SLAM and so on. However, the output from these techniques is a <em>point cloud</em> which is sparse and contains “holes” which are not useful for the “structure of the scene”.</p> <h4 id="surface-reconstruction">Surface reconstruction</h4> <p>Another subfield of graphics/vision tries to estimate the surface from these points clouds. There were methods using Local Gaussians, Local RBF functions, Local Signed Distance functions, etc. The famous approaches for this problem is Poisson surface reconstruction and Neural Signed Distance functions. In a way, Gaussian splatting is a surface reconstruction technique.</p> <h3 id="space-carving">Space Carving</h3> <p>The underlying idea is motivatede from how humans create a sculpture - start out with a block, and carve/chip out regions that are not consistent with the camera viewpoints. However, this does not work well if there are not enough view points of the object (imaging back-projection problem in tomography).</p> <h3 id="using-depth-data">Using Depth data</h3> <p>Suppose we have the depth information of each pixel as well - in such scenario, dealing with occlusions and “best-view planning” for 3D reconstruction becomes quite easy.</p> <h3 id="the-slump">The slump</h3> <p>Between 2005 and 2014, there wasn’t much work done in the area of view synthesis, but many researchers focused on solving sub-problems in 3D scene understanding. There were papers for single-view depth estimation with a variety of diverse approaches. Along with these advances, the camera technologies also improved rapidly. Multi-array cameras were improved to hand-held cameras with much better resolution. The sensors to capture depth also improved - structured light was used in the past which did not work well in real-life scenarios. In contrast, we now have LIDARs to capture depth accurately and instantly. Given these advances, large-scale datasets came into place. KITTI dataset is one such famous dataset still being used as a benchmark for autonomous driving applications today. On the other front, GPUs improved exponentially, and the deep learning architectures grew better - ImageNet, Resnet, and other advances. With all these advances together, the focus on 3D reconstruction was on the rise again. Approaches using deep CNN networks, sparial transformer networks, etc were being applied to these problems. One key idea introduced in spatial transformer networks is <strong>differentiable samplers</strong>, which is an important property approaches aim to have now too.</p> <h2 id="3d-understanding-with-modern-tools">3D understanding with Modern Tools</h2> <p>Since monocular reconstruction did not yield good results due to disocclusion problems, works focused on RGB\(\alpha\) planes wherein the depth was captured by <strong>multi-plane images</strong>. To get the image from a novel view point, <strong>alpha compositing</strong> is used to render the image. At this point, since we have perfect depth and color data from a viewpoint, we should be able to generate new views with ease. However, the novel views that can be generated are limited by single view data. Then, the idea of using multiple images of a scene started entering the literature.</p> <blockquote> <p>What is the difference between depth map and MPI? Both capture the depth of the scene. Fast forward into the future, NeRFs started using multiplane images for each pixel rather than a single image.</p> </blockquote> <h2 id="neural-radiance-fields">Neural Radiance Fields</h2> <p>Alpha-composting is used to generate the value of each pixel in the rendering image, along with which, continuous volume rendering is done to represent the scene itself -</p> \[C_0 = \int C_t \sigma_t \exp\left(-\int \sigma_u du\right) dt\] <p>which is discretized as</p> \[C_0 = \sum_i C_i \alpha_i \prod_j (1 - \alpha_j)\] <p>Here, \(\sigma_t\) and \(\alpha\) are related to the transmittance/opacity of the pixel. The idea is that we shoot out a ray corresponding to a pixel in the camera image, sample points on the ray (ray-marching) to estimate their opacities and color. Neural networks are used to calculate the opacity for each spatial point and color value depending on the spatial location and viewing direction. The color depends on viewing direction because of BRDF functions that vary based on the viewing direction (image specular surfaces). Using the positions and viewing directions directly does not yield good results. Why? It is difficult to estimate high-frequency parameters required for images from these low-dimensional values. Neural networks tend to estimate low-frequency functions over high-frequency functions. To solve this, these coordinates are mapped to a high-dimensional space using <strong>positional encoding</strong>. This addition yields much superior results. Since the whole framework is completely differentiable, it is easy to find the optima minimizing the error across multiple camera views. NeRF was a very influential paper, enabling applications in new fields such as text-to-3D and it embodies the perfect combination of the right problem, at the right time, with the right people.</p> <h1 id="mip-nerf-360">Mip-NeRF 360</h1> <p>The original paper has two major assumptions -</p> <ul> <li>Bounded scenes</li> <li>Front-facing scenes - camera views are limited to certain directions</li> </ul> <p>The outputs from NeRF had a lot of aliasing issues when these assumptions are broken. Instead of assuming rays shooting out from the camera, Mip-Nerf shoots out conical frustrums with multivariate Gaussians to represent 3D volumes in the scenes. This solves the problem of front-facing scenes. To solve the problem of unbounded scenes, Mip-NeRF 360 uses “parameterization” to warp the points outside a certain radius using Kalman Filtering. Essentially, farther points are warped into a non-Euclidean space which is a tradeoff - for applications in SLAM and robotics, the geometry may be precisely needed which is sort of lost in such transformations. To speed up training in unbounded scenes, the authors proposed “distillation”, wherein sampling is done in a hierarchical manner to identify regions with higher opacities. These higher opacity regions are then used to estimate the color in the finer-sampled regions. To supervise the training of “density finding network”, the idea of distribution matching in multi-resolution histograms is used to formulate a loss function. Given limited number of camera, it is difficult to estimate all the parameters correctly - causing artefacts in the reconstructions. To solve this, the authors use a <strong>distortion regularizer</strong> to clump together points with higher densities. This acts as a prior to resolve some ambiguities in the scene.</p> <ul> <li></li> </ul> <h2 id="instant-ngp">Instant-NGP</h2> <h3 id="graphics-primitives">Graphics Primitives</h3> <p>A form of representation for objects in the real-world. For example, an image is a graphics primitive that maps 2D pixels to RGB values.</p> <p>So what is the motivation to have neural fields represent 3D volumes over voxels or point clouds? The latter are explicit representations which take up a lot of space!</p> <h3 id="nerfs">NeRFs</h3> <p>We have seen that high-frequency encoding is used to effectively represent high-frequency features. This is a form of non-parametric encoding.</p> <p>Parametric encodings on the other hand are more complicated, and can be used to learn feature vectors.</p> <h3 id="multi-resolution-hash-encoding">Multi-resolution hash-encoding</h3> <p>The scene is divided into multiple grids - the corners of each cube are hashed and stored in a hash table. For any point outside this lattice, we simply use a linear combination based on distances - this ensures continuity and differentiability.</p> <p>What exactly is hashed? The high-dimensional vector embedding for each 3D point is stored. Furthermore, the authors implement a low-level CUDA kernel to realise fast matrix multiplications.</p> <p>This implementation greatly reduces the training time and reduces the memory used. It takes days to train NeRFs, and with this the model could be trained in a couple of seconds.</p> <h3 id="method-agnostic">Method Agnostic!</h3> <p>This idea is not only for NeRFs but can be used for other primitives like Gigapixel images, Neural SDFs, etc.</p> <h3 id="weaknesses">Weaknesses</h3> <ul> <li> <p>Rely on neural networks to resolve hash collisions - cause microstructure artifacts</p> </li> <li> <p>Hand-crafter hash function</p> </li> <li> <p>May not be robust to real world noise like flossy surfaces or motion blur.</p> </li> </ul> <h1 id="3d-generative-models">3D Generative Models</h1> <p>Taking motivation from 2D generation, we use noise-sampling based approaches to build 3D scenes. While our 2D models are able to generate very high quality results, 3D models aren’t able to match these outputs. The first limitation is due to the unavailability of data (Objaverse-XL 10M vs LAION 5B). The dataset itself has very simple models, so it is difficult to build highly-detailed models.</p> <h2 id="pretrained-2d-diffusion-models">Pretrained 2D Diffusion Models</h2> <p>How about <a href="https://arxiv.org/abs/2212.00774">Lifting Pretrained 2D Diffusion Models for 3D Generation</a>? After the advent of NeRFs, this approach became feasible. The problem of creating a 3D model can be distilled to updated view-dependent 2D images!</p> <p>To this end, people tried using diffusion models for the generative capabilities. An optimal denoiser should understand the image structure. What does this mean? The model needs to understand that there is a low-dimensional image-manifold in a high-dimensional space. The noising process takes the image out of this manifold whereas the denoising process tries to bring it back to this space - A projection function.</p> <p>An important question arises here - How does the denoiser know <em>which direction</em> to project on? The models typically project to the <strong>weighted mean</strong> (Gaussian likelihood based) direction of the training samples. This is known as <strong>mean shifting</strong> - used widely in clustering methods.</p> <p>How is this relevant to 3D generation? When we start with white noise, the initial direction would be towards the mean of all training samples. <a href="https://openreview.net/pdf?id=k7FuTOWMOc7">Elucidating the design space of Diffusion-Based Generative Models</a> examines this property in detail. The mean-shift essentially generates more ‘likely’ samples.</p> <p>Start out with a 3D blob, add Gaussian noise (to take it to the space where the diffusion model has been trained on) and then denoise it. The noise function used is</p> \[\partial \theta = \sum_i \mathbb E[w(\sigma) (\hat \epsilon_\phi (x_{c_i} + \sigma \epsilon) - \epsilon) \frac{\partial x_c}{\partial \theta}]\] <p>Alternately, <a href="https://arxiv.org/abs/2209.14988">DreamFusion: Text-to-3D using 2D Diffusion</a> tries to optimize using a KL divergence loss function but further derivation shows that it is equivalent to the above loss function. This noise function is called as <strong>Score Distillation Sampling (SDS)</strong>.</p> <p>However, these still don’t yield good results. Here, researchers realised that unconditional distribution is much harder than conditional generation. Unconditional generation is still an open problem.</p> <p>For conditional generation, text prompts are able to generate results with higher-fidelity. Authors in DreamFusion added some more tricks wherein they decomposed the scene into geometry, shading, and albedo to improve the results.</p> <h3 id="mode-seeking">Mode-seeking</h3> <p>The score distillation sampling function (mean-shifting) has a mode-seeking behavior. This behavior is not necessarily good - it causes artefacts like saturated colors, lower diversity and fuzzy geometry. To alleviate these issues, there has been a new loss function crafted called <strong>variational score distillation</strong>.</p> <h3 id="improvements">Improvements</h3> <ul> <li> <p><em>Speed</em> - Sampling speed can be improved using Gaussian Splatting</p> </li> <li> <p><em>Resolution</em> - Resolution can be improved using Coarse-to-Fine refinement</p> </li> </ul> <h2 id="alternative-approaches">Alternative Approaches</h2> <h3 id="multi-view-2d-generative-models">Multi-view 2D Generative Models</h3> <p>Fine-tune a 2D diffusion or any other generative model to generate multiple views of the same object. We pass in a single view of the object, and the generative model generates multiple views. This approach is explored in the paper <a href="https://zero123.cs.columbia.edu">Zero-1-to-3: Zero-shot One Image to 3D Object</a>. Then, NeRF or Gaussian splatting can be used to create the 3D models. The problem becomes “Images-to-3D” which is a much more easier problem.</p> <p>The recovered geometry may not have very good - the 2D models do not understand the geometry of the scene quite well.</p> <h3 id="multi-view-25d-generative-models">Multi-view 2.5D Generative Models</h3> <p>Along with the RGB images, we could also estimate the normals to estimate the geometry better. This method is implemented in <a href="https://arxiv.org/abs/2310.15008">Wonder3D: Single Image to 3D using Cross-Domain Diffusion</a>, and it obtains better results.</p> <hr/> <p>At this point, the Objaverse-XL dataset was released, and people tried to train image to 3D directly - <a href="https://arxiv.org/abs/2311.04400">LRM: Large Reconstruction Model for Single Image to 3D</a>.</p> <p>However, the issue with this approach is that since the dataset is object-centric and we want a more general model! Capturing such a general dataset is quite difficult.</p> <p>An alternative idea could be to use videos as a dataset. Such an idea is explored in . Also, video generation is an achievable task with the current models - <a href="https://openai.com/index/sora/">Sora</a>.</p> <p>People are still figuring out other ways to solve this general problem, and it is a very lucrative field - hundreds of papers in the past year!</p> <p>Let us see some more papers which tried to address other issues in this problem.</p> <p>#</p> <h2 id="prolific-dreamer">Prolific Dreamer</h2> <p>As mentioned before, Google first released Dream Fusion for this problem. It used Imagen and Classifier-free guidance, but it did have the limitations mentioned before.</p> <p>To improve on this performance, the authors of Prolific Dreamer modified the SDS Loss to something known as <strong>Variation Distillation Score</strong> - our goal is match the generated distribution with the distribution formed by multi-view images of a scene.</p> <p>This is a difficult task as it is a sparse manifold in a complex high-dimensional space.</p> <p>Do we have a metric to quantitatively analyse these models? <strong>T3 Bench</strong> is a benchmark score that checks the text-3D model alignment and the quality of the result itself.</p> <h2 id="reconfusion">ReconFusion</h2> <p>ZeroNVS is a modification over Zero-1-2-3 that does not require any pretraining on 3D models to generate models. However, this paper along with other approaches during this time required heavy pretrained models, with high computational requirements and scene specific fine-tuning. Along with these, they also had floater artifacts and inconsistencies in multi-view generation.</p> <p>PixelNerf is one of the state-of-the-art 3D reconstruction papers that does not require dense training data because it relies on underlying pixel structure. The idea is to use this scene representation with latent diffusion models to address the limitations of the previous papers.</p> <p>How do we train NeRF and Diffusion models simultaneously?</p> <ul> <li> <p>We first have a reconstruction loss for the NeRF part wherein we sample a novel view and use some image similarity loss like L2 to optimize the network.</p> </li> <li> <p>Then, we have a sampling loss for the diffusion model that has LPIPS and a component for multi-view consistency</p> </li> </ul> <p>Interestingly, the authors choose DDIM for the diffusion model over stochastic sampling (probably helps with multi-view consistency).</p> <p>Also, they use a trajectory based novel view sampling to further maintain consistency across views.</p> <p>The resultant method is able to reconstruct a consistent 3D model even with inconsistent 2D views!</p> <h1 id="vision-and-language">Vision and Language</h1> <p>The idea is to use an LLM agent to use language as a means to solve complex visual workflows. Along with human-curated high-level concepts, these can solve planet-scale problems. Robots can work on more general tasks - physically grounding them on images.</p> <p>The main problem in realizing these models is aligning text and image concepts.</p> <h2 id="pre-training-tasks-generative">Pre-training Tasks: Generative</h2> <p>GPT uses <strong>causal modeling</strong> whereas BERT uses <strong>masked modeling</strong>. In the latter method, the words from the input data is masked out, and the model must predict the missing words. This allows global supervision allowing the model to look at both past and future data.</p> <p>In context of images, we mask out some patches in the image and the model has to predict the remaining patches. This sort of an approach is displayed in <strong>Segment Anything Model (SAM)</strong>.</p> <p>However, some types of tasks may not support such a paradigm - text generation in a conversation. This is where causal modeling is used - the model is only allowed to look at the past data.</p> <p>How do we do supervision in this case? It is similar to what we do in auto-regressive models for generating images and for generating text for images, it is similar to MLE.</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Introduction Vision is a fundamental interface to the world, and it has become a crucial component in the development of intelligent systems. The field has deep and attractive scientific problems, which have been advancing at a rapid pace in the past few years. In the early days of research, the focus in vision was on engineering “good” features coupled with a optimisation algorithm or a shallow neural network. As the processors became more powerful, the emphasis shifted to end-to-end approaches with inclusion of self-supervision and multi-modal learning paradigms. It is often helpful to breakdown the perception tasks into known-algorithms. For example, in autonomous driving, the tasks include SLAM (visual, Structure from Motion), path planning (lane detection, obstacle detection, 3D localization), Semantic segmentation etc. Similarly, the tasks in augmented reality devices are gaze tracking, material and lighting estimation, head pose estimation, depth estimation, etc. Deep learning has opened new areas of research in vision. Features such as generation of high-quality content, end-to-end training, data-driven priors and highly parallelizable architectures have proven advantageous for many problems in computer vision. However, it is also important to note the limitations of these techniques - Large scale labeled data is not always available Lack of generalization to unseen domains Good at narrow “classification”, not at broad “reasoning” Lack of interpretability Lack of reliability, security or privacy guarantee To counter these problems, we typically couple our algorithms with self-supervision, physical modelling, multi-modal learning and foundation models. In the recent years, these techniques have been applied to various problems, and the following are arguably the biggest advances in Computer Vision - Vision Transformers Vision-Language Models Diffusion Models Neural Rendering These techniques show promise to solve keystone problems in augmented reality, interactive robotics, and autonomous driving. The course will cover the following these topics, along with other fundamentals required. Neural Architectures Generative Models Structure from Motion Object Detection image Segmentation Prediction and Planning Inverse Rendering 3D GANs Vision-Language Neural Architectures The motivation for an artificial neuron (perceptron), comes from a biological neuron where the output is linear combination of the inputs combined with a non-linear activation function. From here, we develop multi-layer networks which are again motivated from the Hubel and Weisel’s architecture in biological cells. Neural Networks The simplest neural network is a perceptron represented by \(\sigma (x) = \text{sign}(\sum_i w_i x_i + b)\) where the optimal weight values are obtained using an unconstrained optimization problem. These concepts can be extended for “Linear Regression” and “Logistic Regression” tasks. Non-linearity in neural networks is introduced through Activation functions such as Sigmoid - Have vanishing gradient issues tanh - Centered version of sigmoid ReLU - Simplest non-linear activation, with easy gradient calculation. ELU - Added to prevent passive neurons. At the output layer, we apply a final non-linear function is applied to calculate the loss in the predicted output. Typically for classification problems, Softmax function is used to map the network outputs to probabilities. One-hot representations are not differentiable, and are hence not used for this task. In image synthesis problems, the output layer usually has \(255*sigmoid(z)\). Theorem (Universal function approximators): A two-layer network with a sufficient number of neurons can approximate any continous function to any desired accuracy. Width or Depth? A wider network needs more and more neurons to represent arbitrary function with high enough precision. A deeper network on the contrary, require few parameters needed to achieve a similar approximation power. However, “overly deep” plain nets do not perform well. This is due to the vanishing gradient problem, wherein we are not able to train deep networks with the typical optimization algorithms. Convolution Networks The neural network architecture is modified for images using “learnable kernels” in convolutional neural networks. Each convolution layer consists of a set of kernels that produce feature maps from the input. These feature maps capture the spatial and local relationships in the input which is crucial for images. The induction bias in images is that neighbouring variables are locally correlated. An image need not be 2D, it can consist of multiple channels (RGB, hyperspectral, etc.), and convolutional layers work across all these channels to produce feature maps. In a classical neural network, each pixel in the input image would be connected to every neuron in the network layer leading to many parameters for a single image. Using kernels, we use shared weights across all pixel locations, and this greatly reduces the number of learnable parameters without losing much information. Convolution layers are generally accompanies with Pooling layers which do not have any learnable parameters, and are used to reduce the size of the output. These layers are invariant to small (spatial)transformations in the input and help observe a larger receptive field in the next layer. The latter property is important to observe hidden layers in the feature maps. Receptive Field - It is the area in the input iamge “seen” by a unit in a CNN. Inits with deeper layers will have wider receptive fields whereas wider receptive fields allow more global reasoning across entire image. This way, the pooling leads to more rapid widening of receptive fields. We need \(\mathcal O(n/k)\) layers with \((k \times k)\) convolutional filters to have a receptive field of \(n\) in the input. Dilation layers are used to achieve the same receptive field with \(\mathcal O(\log n)\) layers. However, in practice, the empirical receptive fields in the deeper networks is lower than the theoretical value due to sparse weights. Convolution networks are augmented with dense layers to get the output, to learn from the feature maps. The vanishing gradient problem in deeper networks has been solved using skip connections wherein the features from the earlier layers are concatenated with the deeper ones to allow passage of information. This way, we provide the network with the original input allowing it to learn the smaller fluctuations in the input (rather than focusing on learning the input itself). In summary, the key operations in convolutional layers are Input image -&gt; Convlution -&gt; Non-linearity -&gt; Spatial Pooling -&gt; Feature Maps CNNs have the above set of operations repeated many times. CNNs have been successful due to the following reasons Good Abstractions - Hierarchical and expressive feature representations. Conventional image processing algorithms relied on a pyramidal representation of features, and this methodology has also paved its way in CNNs. Good inductive biases - Remarkable in transferring knowledge across tasks. That is, pretrained networks can be easily augmented with other general tasks. Ease of implementation - Can be trained end-to-end, rather than hand-crafted for each task, and they can easily be implemented on parallel architectures. The key ideas - Convolutional layers leverage the local connectivity and weight sharing to reduce the number of learnable parameters. Pooling layers allow larger receptive fields letting us capture global features. Smaller kernels limit the number of parameters without compromising the performance much. This design decision comes from preferring deeper networks over wider networks. For example, \((1 \times 1)\) kernels are reduce the dimension in the channels dimension. Skip connections allow easier optimization with greater depth. Why are (1, 1) kernels useful? Use fewer channels instead? Transformers Transformers have shown better results in almost every task that CNNs have shone previously in. CNNs require significant depth or larger kernels to share information between non-local spatial locations (recall receptive fields). Many tasks, such as question-answering, require long-range reasoning and transformers are very good at this. For example, placing objects in augmented reality requires reasoning about light-sources, surface estimation, occlusion/shadow detection, etc. This is the primary intuition behind attention mechanism which is representative of foveated vision in humans. Tokens - A data type than can be understood as a set of neurons obtained from vectorizing patches of an image. Typically need not be vectors, but they can be any structured froup that alows a set of differentiable operations. Note that these tokens in hidden layers might not correspond to pixels or interpretable attributes. The following captures a very good intuition for transformers. A transformers acts on tokens similarly as neural network acts on neurons. That is, combining tokens is same as for neurons, except tokens are vectors \(t_{out }= \sum_i w_i t_i\). In neural networks, linear layers are represented by \(x_{out} = W x_{in}\) and \(W\) is data-free, whereas in transformers, \(T_{out} = AT_{in}\), \(A\) depends on the data (attention). Again, non-linearity in neural networks is implemented via functions like ReLU whereas transformers use dense layers for non-linearity (applied token wise). The attention layer is a spsecial kind of linear transformation of tokens, wherein the attention function \(A = f(.)\) tells how much importance to pay to each token depending on the input query and other signals. Attention-maps help us visualize the global dependencies in the information. The required information is embedded in some dimension of the token representation. For example, the first dimension can count the number of horses in an iamge, and the bottom 3 dimensions can encode the color of the horse on the right. Attention has this flexibility to different allocations address different parts of a query. They can “attend” to only certain patches which are important to the query. This kind of functionality is difficult with CNNs. Apply embedding and neural network (before CNNs and Transofrmers)? Same number of parameters? Essentially similar thing? Associated higher weight to more related embedding. Query-Key-Value Attention The mechanisms described previously are implemented by projecting tokens into queries, keys and values. Each of these are a vector of dimensions $p$, where $p &lt; d$. The query vector for the question is used to weigh the key vector for each token to obtain the values. This is done via computing the similarity between query and each key, and then the attention is given by the extent of similarities, normalized with softmax. The output token is obtained by summing all value vectors with weights assigned from the previous calculations. Roughly, the process looks like - \[\left. \begin{align*} q=W_q t \\ K_i=W_k t_i \end{align*} \right\} \implies s_i=q^T k_i \\ a_i=softmax(s_i) \\ t_{out} = \sum a_i v_i\] The purpose of “values” is to ‘project’ back the similarities between queries and keys to the ‘token space’. Also note that, if the patch-size is too large, then we might lose the information within the patches. This is a tradeoff, and the decision is made based on the task at hand. For example, classification may work with large patches but tasks such as segmentation may not. Self-Attention How do we learn implicit representations that can work with general queries? We compute self-attention using the image tokens as the queries - information derived from the image itself. How does this process help us? For example, if we are performing instance segmentation of an image with horses. Then, the concept of ‘horse’ is learned by attending more to tother horse tokens and less to background. Similarly, ‘a particular instance’ is learned by attending less to the tokens from other horses. When we do this for all pairs across \(N\) tokens gives us an \(N \times N\) attention matrix. Typically, this matrix is computed in the encoder part of a transformer. This matrix is then used in the decoder with an input query to obtain the required results. Encoders An encoder of a transformer typically consists of many identical blocks connected serially with one another. Each such encoder block, computes a self-attention matrix and passes it through a feed-forward neural network, and they need to be applied across all the tokens in the input. Since the embeddings of tokens are independent of one another, each level of encoder blocks can be applied in parallel to all the tokens (patches in vision transformers) to obtain the embeddings. Such parallel computations were not possible in previous models like RNNs, which heavily relied on sequential dependencies. The original transformers paper normalizes the similarities with \(\sqrt{N}\) where \(N\) is the embedding dimension. This allows the gradients to stabilise and gives much better performance. This a simplified view of the mechanisms used in transformers. In addition to these, transformers also use positional encoding to encode the position of the tokens in the input sequence. In the context of images, is encodes where each patch occurs in the image. Positional encoding is usually done via sinusoidal functions. Other “learning-based” representations have been explored but they don’t have much different effect. This encoding structure allows extrapolation to sequnce lengths not seen in training. They also have multi-head attention which is equivalent to multiple channels in CNNs. That is, it allows patches to output more than one type of information. This blog explains these mechanisms in-depth for interested readers. In summary, the functions of the encoder is visualized as Decoders A decoder block is similar to an encoder block, with auto-regressive . The attention values learned in Encoders are used as ‘keys’ in the decoder attention block. This is called as cross attention. Vision-Transformers Vision transformers build on the same ideas used in a transformer. Typically, they use only the encoder part of the architecture where patches are extracted from images and are flattened to form the tokens for the attention mechanism. They are usually projected using a linear layer or a CNN before performing the calculations. Apart from this, vision transformers also include an additional class token in the embeddings to help learn the global information across the image. CNNs typically have better inductive bias whereas transformers excel in shorter learning paths for long-range reasoning. However, the cost of self-attention is quadratic in the image size. Also note that, if the patch-size is too large, then we might lose the information within the patches. This is a tradeoff, and the decision is made based on the task at hand. For example, classification may work with large patches but tasks such as segmentation may not. Swin Transformers and Dense Prediction Transformers The vanilla vision transformer is restricted to classification tasks and is not optimal for other tasks like detection and segmentation. Also, as we have noted before, the quadratic complexity limit the number of patches in the image. Some image processing pipelines extract features across different scales of an image whereas the vanilla transformer is restricted to the uniform (coarse) scale. To address these limitations, swin transformers bring two key ideas from CNNs Multi-scale feature maps - Feature maps from one resolution are down sampled to match the size in the next block. Local connectivity - Windowed self-attention - Limit the computations by considering a local window (this was actually left as future work in the original attention paper). Shifted window self-attention - Allows windowed self-attention to learn long-range features using “shifts”. Essentially, we move the patches around to bring farther patches close together. However, these modifications are not enough for tasks like segmentation, which require reasoning at a pixel-level. Instead, we use something called as a dense prediction transformer (DPTs) where we couple the transformer encoder with a convolutional decoder to upsample and predict the required output. CNNs are shift-invariant whereas ViTs are permutation invariant. Why? At each scale level in the above picture, we reassemble the tokens by concatenating and convolving with appropriate kernels to recover image-like representations in the decoder layers. Multimodality Transformers allowed for easy multi-modal representations by tokenizing data from each modality with its own variant of patches. Works such as VATT have explored merging audio waveforms, text and images. Generative Models Discriminative and Generative Models Discriminative models (classifiers) learn a many-to-one function \(f\) to learn labels for a given input. A generative model \(g\) maps these labels to the input space, and this function is one-to-many since one label can map to multiple inputs. It is difficult to model a stochastic function. Therefore, a generator model is coupled with a noise vector to construct a deterministic \(g\) with stochastic input \(z\). This variable \(z\) is called a latent variable since it is not observed in training data composed of \(\{x, y\}\) pairs. \[\begin{align*} \text{Discriminative Models learn } P(Y \vert X) \\ \text{Generative Models learn } P(X, Y) \\ \text{Conditional Generators learn } P(X \vert Y) \\ \end{align*}\] Let us focus on image generative models. Then, each dimension of the latent variable can encode the various characteristics of the image essentially allowing us to generate a wide variety of images. The labels \(y\) for images need not be classification labels, but textual descriptions can also be used for supervision. To understand how these models work, consider the case of unconditional generative models. The goal is, given the real data \(x\), to generate synthetic data \(\hat x\) that looks like the real data. How do we quantify ‘looks like’? We can try and match some marginal statistics - mean, variance, edge statistics, etc of the real data. Such measures are very useful in techniques for texture synthesis. For example, Heeger-Bergen texture synthesis [1995] uses an iterative process starting from the Gaussian noise and matches intensity histograms across different scales. Such design choices are used in modern techniques like StyleGANs and Diffusion models. Have a high probability of success under a model fit to real-data (A discriminator). The key-challenge in these models is novelity in the generation to ensure generalization. Generative models are classified into the following approaches - Energy-based models - Learn a scoring function \(s:X \to R\) that scores real-samples with a high score which represents energy/probability. During generation, return samples which have a high score. This paradigm is particularly useful for applications like anomaly detection. Sampling from Noise - Learn a generative function \(g: Z \to X\) without needing to learn a probability density function. These methods explicitly model the data distribution, and techniques like GANs and Diffusion Models come under this regime. Density/Energy based models Learn a scoring function \(s:X \to R\) that scores real-samples with a high score which represents energy/probability. During generation, return samples which have a high score. This paradigm is particularly useful for applications like anomaly detection. In these methods, we produce \(p_\theta\) fit to the training data by optimizing \(\arg\min_{p_\theta} D(p_{\theta}, p_{data})\). Since we don’t know \(p_{data}\) explicitly (we only have access to samples \(x \sim p_{data}\)), we minimize the KL-divergence to reduce the distance between the distributions using the samples. \[\begin{align*} p_\theta^* &amp;= \arg\min_{p_\theta} KL(p_{data} \vert\vert p_\theta)\\ &amp;= \arg\max_{p_\theta} \mathbb E_{x\sim p_{data}}[\log p_\theta] - \mathbb E_{x\sim p_{data}}[\log p_{data}] \\ &amp;= \arg\max_{p_\theta} \mathbb E_{x\sim p_{data}}[\log p_\theta] \end{align*}\] Intuitively, this way of optimization increases the density where the models are observed. The energy-based models have a slight modification wherein the energy function \(E_\theta\) is unnormalized unlike the probability density function \(p_\theta\). Indeed, \(p_\theta\) can be determined as \[\begin{align*} p_\theta &amp;= \frac{e^{-E_\theta}}{Z(\theta)}, \\ &amp;\text{ where } Z_\theta = \int_x e^{-E_{\theta}(x)} dx \end{align*}\] This formulation is called as Boltzmann or Gibbs distribution. Learning energy function is convenient since normalization is intractable since we cannot see all the samples in the distribution. What is the motivation to use exponential functions? It arises naturally in physical statistics Many common distributions (Normal, Poisson, …) are modeled using exponential functions. Want to handle large variations in probability Although we don’t have the probabilities, we can still compare different samples using ratios of their energies. After obtaining the energy distribution, how do we use it to sample in these approaches? Sampling approaches Markov Chain Monte Carlo (MCMC) only require relative probabilities for generating data. We can also find data points that maximize the probability. \[\nabla_x \log p_\theta(x) = -\nabla_x E_\theta(x)\] On the other hand, where would be prefer probability formulation over energy? Certain systems like safety in autonomous driving require absolute quantifiable probabilities rather than relative scores. In probability modeling, the probability of space where no samples are observed is automatically pushed down due to normalization. However, in energy based models, we need an explicit negative term to push energy up where no data points are observed. To do so, we set up an iterative optimization where the gradient of the log-likelihood function naturally decomposes into contrastive terms \[\begin{align*} \nabla_\theta \mathbb E_{x \sim p_{data}} [\log p_\theta(x)] &amp;= \frac{1}{N}\nabla_\theta\sum_{i = 1}^N (\underbrace{-E_\theta(x^{(i)})}_{\text{ data samples}} + \underbrace{E_\theta(\hat x^{(i)})}_{\text{model samples}}) \\ &amp;= - \mathbb E_{x \sim p_{data}} [\nabla_\theta \mathbb E_\theta (x)] + \mathbb E_{x \sim p_\theta} [\nabla_\theta \mathbb E_\theta(x)] \end{align*}\] Sampling We randomly initialize \(\hat x_0\) at \(t = 0\). Then, we repeat the following Let \(\hat x' = \hat x_t + \eta\)$ where \(\eta\) is some noise If \(\mathbb E_\theta(\hat x') &lt; \mathbb E_\theta(\hat x_t)\), then choose \(\hat x_{t + 1} = \hat x'\). Else choose \(\hat x_{t + 1} = \hat x'\) with probability \(e^{(\mathbb E_\theta(\hat x_t) - \mathbb E_\theta(\hat x'))}\) In practice, this algorithm takes a long time to converge. A variant of this called Langevin MCMC uses the gradient of the distribution to accelerate the sampling procedure Choose \(q(x)\) an easy to sample prior distribution. Repeat for a fixed number of iterations \(t\) \[\hat x_{t + 1} \sim \hat x_t + \epsilon \nabla_x \log p_\theta(\hat x_t) - \sqrt{2\epsilon} z_t\] where \(z_t \sim \mathcal N(0, I)\). When \(\epsilon \to 0\), and \(t \to \infty\), we have \(\hat x_t \sim p_\theta\) Sampling from Noise Diffusion Models The intuition for these models builds from our previous approach. It is hard to map pure noise to \(x \sim N(0, I)\) to structured data, but it is very easy to do the opposite. To readers familiar with autoregressive models, where we remove one pixel of information at a time, diffusion models generalise this notion further. Diffusion models have two steps in training - The forward process in these models involves adding noise to the image \(x_0\) over many time steps. At time \(t\), we add noise \(\epsilon_t\) to obtain \(x_{t + 1}\) from \(x_{t}\). The noise addition is modeled with respect to the following equation \[x_t = \sqrt{(1 - \beta_t) x_{t - 1}} + \sqrt{\beta_t}\epsilon_t, \quad \epsilon_t \sim N(0, I)\] If this process is repeated for a large number of times, it can be shown that the final output simulates white noise. Now, in the learning part, we train a predictor to learn denoising from \(x_t\) to \(x_{t - 1}\). That is, given our model \(f_\theta\) \[\hat x_{t - 1} = f_\theta(x_t , t)\] Forward Noising Given an image \(x_0 \sim q(x)\), we essentially add the following Gaussian noise in \(T\) time steps \[q(x_t \vert x_{t - 1}) = \mathcal N(x_t ; \sqrt{1 - \beta} x_{t - 1}, \beta_t I)\] The term \(\beta_t\) is referred to as the schedule and \(0 &lt; \beta_t &lt; 1\). Typically, we set it to a small value in the beginning and do linear increments with time. The above process is a Markov’s process, resulting in the following property \[q(x_{1:T} \vert x_0) = \prod_{t = 1}^T q(x_t \vert x_{t - 1})\] Instead of a slow step-by-step process, training uses samples from arbitrary time step. To decrease the computations, we use the following properties of Gaussian distributions Reparameterization trick: \(\mathcal N(\mu, \sigma^2)= \mu + \sigma \epsilon\) where \(\epsilon \sim \mathcal N(0, I)\) Merging Gaussians \(\mathcal N(0, \sigma_1^2 I)\) and \(\mathcal N(0, \sigma_2^2 I)\) is a Gaussian of the form\(\mathcal N(0, (\sigma_1^2 + \sigma_2^2) I)\) Define \(\alpha_t = 1 - \beta_t\), and \(\bar \alpha_t = \prod_{i = 1}^t \alpha_i\), we can now sample \(x_t\) at an arbitrary time step \[\begin{align*} x_t &amp;= \sqrt{\alpha_t} x_{t - 1} + \sqrt{1 - \alpha_t}\epsilon_{t - 1} \\ &amp;= \sqrt{\alpha_t\alpha_{t - 1}} x_{t - 1} + \sqrt{1 - \alpha_t\alpha_{t - 1}}\epsilon_{t - 1} \\ &amp;\dots \\ &amp;= \sqrt{\bar \alpha_t}x_0 + \sqrt{1 - \bar \alpha_t}\epsilon \end{align*}\] When we schedule \(\beta_t\) such that \(\beta_1 &lt; \beta_2 &lt; \dots, \beta_T\) so that \(\bar \alpha_1 &gt; \dots, &gt; \bar \alpha_T\), such that \(\bar \alpha_T \to 0\), then \[q(x_T \vert x_0) \approx \mathcal N(x_T; 0, I)\] The intuition is that the diffusion kernel is a Gaussian \[q(x_t) = \int q(x_0, x_t) dx_0=\int q(x_0) q(x_t \vert x_0) dx_0\] There are theoretical bounds showing a relation between the number of steps and overfitting of the model to the distribution. There are no such guarantees for VAEs. The increasing \(\beta_t\) schedule sort of accelerates the diffusion process wherein we simulate white noise with few iterations. However, the stable diffusion paper to generate images chose the cosine learning schedule which gave better results. Reverse Denoising The goal is to start with noise and gradually denoise to generate images. We start with \(x_T \sim \mathcal N(0, I)\) and sample form \(q(x_{t - 1} \vert x_t)\) to denoise. When \(\beta_t\) is small enough, we can show that this quantity is a Gaussian. However, estimating the quantity is difficult since it requires optimization over the whole dataset. Therefore, we learn a model \(p_\theta\) to reverse the process \[P_\theta(x_{t - 1} \vert x_t) = \mathcal N(x_{t - 1}; \mu_\theta(x_t, t), \Sigma)\] # Essentially, the training objective is to maximise log-likelihood over the data distribution. A variational lower bound can be derived and is used in practice \[\mathbb E_{q(x_0)}[-\log p_\theta(X_0)] \leq \mathbb E_{q(x_0) q(x_{1:T}\vert x_0)} \left[-\log \frac{p_\theta (x_{0:T})}{q(x_{1:T} \vert x_0)}\right]\] Then, this decomposes into \[\sum_{t &gt; 1} D_{KL} (q(x_{t - 1} \vert x_t, x_0) \vert\vert p_\theta(x_{t - 1} \vert x_t)) + \text{other terms}\] The idea is that $q(x_{t - 1} \vert x_t, x_0)$ is tractable even though $q(x_{t - 1} \vert x_t)$ is not. That is because \[\begin{align*} q(x_{t - 1} \vert x_t, x_0) &amp;= \mathcal N(x_{t - 1}; \tilde \mu_t(x_t, x_0), \tilde \beta_t I) \\ \text{where } \tilde \mu_t(x_t, x_0) &amp;= \frac{\sqrt{\bar \alpha_{t - 1}}}{1 - \bar \alpha_t}x_0 + \frac{\sqrt{1 - \beta_t} ( 1- \bar\alpha_{t - 1})}{1- \bar\alpha_{t - 1}}x_t \\ &amp;=\tilde \beta_t = \frac{1 - \bar \alpha_{t - 1}}{1 - \bar \alpha_t} \beta_t \end{align*}\] Generative Adversarial Networks In these architectures, a generator network tries to fool the discriminator by generating real-looking images. In contrast, a discriminator network tries to distinguish between real and fake images. GANs don’t produce as good images as diffusion models, but the concept of adversarial learning is a crucial concept in many fields. The framework looks like this - The objective function for a GAN is formulated as a mini-max game - the generator tries to maximize the loss function whereas the discriminator tries to reduce it. \[L = \min_{\theta_g} \max_{\theta_d} [\mathbb E _{x \sim p_{data}} \log D_{\theta_d} (x) + \mathbb E_{z \sim p(z)} \log (1 - D(G_{\theta_g}(z)))]\] The training is done alternately, performing gradient ascent on the generator and descent on the discriminator. Object Detection Simply put, the goal is identify objects in a given image. The evolution of algorithms for object detection is summarized as HoG + SVM - Sliding window mechanism for feature detection. Deformable Part Models (DPMs) - CNN - The vanilla approaches with CNNs proposed sliding a window across the image and use a classifier to determine if the window contains an object or not (one class is background). However, this is computationally expensive because we need to consider windows at several positions and scales. Following this, region proposals were designed to find blob-like regions in the image that could be an object. They do not consider the class and have a high rate of false positives which are filtered out in the next stages. Such class of methods are called multi-stage CNNs (RCNN, Fast-RCNN, Faster-RCNN) and are covered in detailed below. On the other hand, the sliding window approach has been optimised using anchor-boxes and this class of detectors are referred to as single-stage CNNs (YOLO series) which have been discussed in the next section. For more details, interested readers could also refer to my blog. Transformers - These methods are inspired from CNNs and have a transformer back-end. Examples include DETR and DINO discussed in later sections. Self-supervision Open vocabulary Multi-stage CNNs RCNN For the region proposal network, an ImageNet pre-trained model (ResNet or like) is used. Since it has to be fine-tuned for detection, the last layer is modified to have 21 object classes (with background) instead of 1000 and is retrained for positive and negative regions in the image. The regions are then FastRCNN FasterRCNN Uses a Region Proposal Network (RPN) after the last convolutional layer. The RPN is trained to produce region proposals directly without any need for external region proposals. After RPN, the RoI Pooling and an upstream classifier are used as regressors similar to FastRCNN. Region Proposal Network The network does the following - Slide a small window on the feature map A small network to do classification - object or no-object A regressor to give the bounding box Anchors Anchors are a set of reference positions on the feature map 3 dimensions with aspect ratio, 3 different scales of boxes, Training Non-Maximal Supression Since, multiple anchor boxes can map to an object, we iteratively choose the highest scoring box among the predictions and supp ress the predictions that have high IoU with the currently chosen box. Transformer based Architectures DETR Faster R-CNN has many steps, handcrafted architecture and potentially non-differentiable steps. In contrast, DETR was proposed - an end-to-end transformer based architecture for object detection. The motivation was to capture all the human-designed optimization parts of the pipeline into one black-box using a transformer. Why do we want such an end-to-end architecture? They are more flexible to diverse data, capturing large datasets, finetuning, etc. In this architecture, a CNN backbone like ResNet extracts the features which are passed through the encoder to obtain latent representations. These are then used with a decoder along with object queries in the form of cross-attention to give bounding boxes as the output. After obtaining the bounding boxes, the loss is appropriately calculated by matching the boxes to the correct labels. It is our task to assign a ground truth label to the predictions Hungarian?? The decoder takes object queries as input, 500 epochs encoder is segmented, decoder is just getting bounding boxes DINO lagging in scale variant objects. contrastive loss acting for negative samples YOLOX Single Shot-Detectors The motivation for these networks is to infer detections faster without much tradeoff in accuracy. Anchor boxes no ambiguous IoU Hard negative mining You Only Look Once (YOLO) Divide into grids anchor boxes in grid and class probability map YOLO-X No anchored mechanism - hyperparameter tuning, more predictions, less generalization (OOD fails) Detection head decoupling, no anchor boxes, SimOTA predict size and centers rather than top-left mosaicing and mixing Pyramid Feature Extractions RFCN Semantic Segmentation The goal in this task is much more fine-tuned wherein we assign class pixel-wise. We want locally accurate boundaries as compared to finding bounding boxes in object detection. The approaches require global reasoning as well. A naive approch would be to classify a pixel by considering the patch around the pixel in the image. This is very expensive computationally and does not capture any global information. Another approach is to consider image resolution convolutions (purely convolutions with stride 1, no pooling) and maining the image resolution to produce the output. This still does not work quite well, and they also consume more memory. Taking classification networks as the motications, some networks try pooling/striding to downsample the features. This architecture corresponds to the encoder part of the model, and the decoder then upsamples the image using transposed convolution, interpolation, unpooling to recover the spatial details in the original size. The convolutions can go deeper without occupying a lot of memory. How do we upsample? Transposed convolution to upsample. The operation is shown below - ![Transpose Convolution for Up-Sampling Images Paperspace Blog](https://blog.paperspace.com/content/images/2020/07/conv.gif) However, the outputs are not very precise. Combine global and local information. U-Net Unpooling followed by convolutions (simpler implementation of transposed convolution) The max-unpooling operation does the following - Why is this better? Lesser memory In all the architectures above, once you downsample the image, you lose a lot of spatial information and the network uses a significant amount of parameters learning the upsampling process to represent the final output. To help with this, we can do the following Predict at multiple scales and combine the predictions Dilated convolutions - These are used to increase the receptive field without downsampling the image too much - It’s multi-scale and also captures full-scale resolution. The idea of dilated convolution is very important in other tasks as well - It prevents the loss of spatial information in downsampling tasks. However, there are gridding artifacts and higher memory consumptions with Dilated networks. Degridding solutions Add convolution layers at end of the network with progressively lower dilation Remove skip connections in new layers, can propogate gridding artefacts because skip connections transfer high-frequency information. Skip-connections DeepLab v3+ Segformer Instance Segmentation Mask RCNN Predict eh Panoptic Segmentation Panoptic segmentation extends the idea of instance segmentation even further to segment objects beyond a fixed set of classes. # Universal Segmentation Specialized architectures for semantic MaskFormer The architecture contains a strong CNN backbone for the encoder along with a query and pixel decoder. The query decoder essentially takes $n$ inputs to generate $n$ masks. The outputs from these layers are passed through an MLP to get binary masks for different classes. Masked2Former Uses a masked attention layer instead of cross-attention to provide faster convergence Limitations Needs to be trained on speicfic datasets, struggles with small objects and large inference time. Hasn’t been extended to video segmentation and panoptic segmentation. Mask DINO Does both detection and segmentation. A Mask prediction branch along with box prediction in DINO. Unified Denoising Training Foundation Models Models that have been trained on simple tasks which have good performance in the pipelines of other tasks for zero-shot transfer capabilities. Such models were first used in Natural Language Processing and using these in vision is typicalled hindered by unavailability of labeled data. Segment Anything A model developed by Meta with zero-shot segmentation capabilities. The task this model tries to solve is to allow for interactive and automatic use with zero-shot generatlization and re-use. The model allows for flexibile prompts with real-time usage and is ambiguity-aware. The model also has a data-engine which is used to generate data through the model. Promptable segmentation The model generates a valid mask for a given prompt (even if ambiguous) as input. How is the ambiguity resolved? Ambiguous cases arise when the mutiple objects lie on top of each other. The image encoder is heavy - a pretrained ViT with masked autoencoder. The design choice for the masking involved masking around 75% of the patches. Unlike NLP tasks, vision tasks rely heavily on spatial information and neighbor patches can be reconstructed without much effort. Also, NLP tasks can get away with a simple MLP for decoders but vision taks require strong decoders. The model allows for prompts using points, box or text. The decoder has self-attention on the primpts followed by cross-attention with image and MLP for non-linearity to get masks. The model also estimates the IoU to rank masks. The emphasis of the model was also to make it universal. Since the encoder is heavy, the embeddings are precomputed after which the prompt encoder and mask decoder work quite fast. Data Engine There is no large-scale dataset available for training a segmentation model of this scale. Initially, the model has been trained iteratively on available datasets with several rounds of human correction. Following this, in the semi-automatic stage, the diversity of outputs (increasing ambiguity in overlapping objects) is improved by human refinement. Finally, in the fully automatic stage, prompting is introduced and masks with high IoU are preserved followed by NMS. Zero-shot transfer capabilities The idea is to fine-tune the model for specific tasks like edge-derection, object proposals and isntance segmentation. Future Trajectory Prediction This problem arises in many applications, particularly in autonomous driving. Given traffic participants and scene elements which are inter-dependent, we need to predict trajectories with scene awareness and multimodality. Humans are able to predict up to 10 seconds in highway scenes and plan their trajectory effectively whereas the SOTA models have a large gap in performance. More concretely, the algorithm takes a video sequence as an input, and needs to predict probabilistic trajectories which are diverse, have 3D awareness, capture semantics interactions and focus on long-term rewards. There are many components in these solutions including 3D localization module, generative sampling modules, semantic scene modules and scene aware fusion modules with encoders and decoders. # Representation Framework The past observed trajectories are represented by \(X = \{X_{i, t - l + 1}, \dots, X_{i, t}\}\) using we which we wish to predict future trajectory \(Y_i = \{Y_{i, t + 1}, \dots, Y_{i, t + \delta}\}\). The parameter \(\delta\) represents how far ahead we want to look in the future. The sample generation module produces future hypotheses \hat Y, and then a ranking module assigns a reward to each hypothesis considering long-term rewards. Following this, the refinement step calculates the displacement \(\Delta Y_t\) for the selected trajectory. Focusing on the sample generation module, the goal is to estimate posterior \(P(Y \vert X, I)\). Earlier, RNNs and other deterministic function maps from \(\{X, I\}\) to Y have been designed to address this. The problem with training in this approach is that the ground truth has a single future trajectory whereas the sampler predicts a distribution. How do we compare the two? Principal Component Analysis Principal component analysis (PCA) is a linear dimensionality reduction technique where data is linearly transformed onto a new coordinate system such that the directions (principal components) capturing the largest variation in the data can be easily identified. PCA works well when the data is near a linear manifold* in high dimensional spaces. CAn we approximate PCA with a Network? Train a network with a bottleneck hidden layer, and try to make the output the same as the input. Without any activations, the neural network simply performs least-squares minimization which essentially is the PCA algorithm. Adding non-linear activation functions would help the network map non-linear manifolds as well. This motivates for autoencoder networks. Autoencoders Network with a bottleneck layer that tries to reconstruct the input. The decoder can map latent vectors to images essentially helping us reconstruct images from a low-dimension. However, this is not truly generative yet since it learns a one-to-one mapping. An arbitrary latent vector can be far away from the learned latent space, and sampling new outputs is difficult. ### Variational Autoencoder This network extends the idea in autoencoders and trains the network to learn a Gaussian distribution (parametrized by \(\mu, \sigma\)) for the latent space. After learning, say, the normal distribution, sampling is quite easy to generate new samples representative of the trianing dataset using the decoder. To train this network, a distribution loss, like the KL-divergence is added in the latent space. But how do we backpropagate through random sampling? The reparametrization trick! Conditional Variational Autoencoder The encoder and decoder networks are now conditioned on the class variable. Conditional Diffusion Models Gradient of log, score function, conditioning at the cost of diversity - train a classifier and a diffusion model together in principle it’s fine. Potential issues - classifier can’t learn from noisy images and gradient of the classifier is not meaningful. Classifier-Free Guidance learn unconditional diffusion model \(p_\theta(x)\) and use the conditional model \(p_\theta(x \vert y)\) for some part of the training. this would not require any external classifier for training. CTG++ - Language Guided Traffic Simulation via Scene-Level Diffusion Testing autonomous agents in the real-world scenarios is expensive. Alternately, researchers aim to build good simulator that are realistic and can be controlled easily. Why not RL-based methods? Designed long-term reward functions is difficult for such scenarios. The goal here is in a generation regime rather than problem solving. Also, the focus here is on controllability rather than realism. For realism, the methods would be more data-driven. Trajectory Planning Recent works have proposed diffusion-models for planning algorithms using classifier-guidance. Diffuser for example, is one such work where the authors generate state action pairs which are guided using a reward function to generate paths in a maze. Traffic simulation methods Early works for this task rrelied on rule-based algorithms which were highly controllable but not very realistic. CTG One of the first workd for diffusion models for planning and conditional guidance for control with STL-based loss. Howeber, it modelled each agent independently which caused issues. CTG++ uses scene-level control Problem Formulation The state is given by the locations, speeds and angle of \(M\) agents whose past history and local semantic maps are given. We aim to learn a distribution of trajectories given this information. The encoder represents the state action pairs in an embedded space on which temporal and cross attention on a diffusion based loss for training/ Inference The model uses LLMs to generate code for guidance function. Limitations and future work CTG++ is able to generate more realistic example, but is still far-off from realism. There are no ablation studies or emperical evaluations with complicated scenarios. Also, the multi-agent modeling can be improved for scalability. In conclusion, trajectory prediction requires generative models which are able to distinugish between feasible and infeasible trajectories. Variational Auto-Encoders have shown good performance for this task, and the current works aim to explore the scope of diffusion models in these tasks. LeapFrog - Stochastic Trajectory Prediction Similar to CTG++, this model aims to simulate real-world traffic scenarios. The authors aim for real-time predictions and better prediction accuracy. Leapfrog initializer skips over several steps of denoising and uses only few denoising steps to refine the distribtuion. System Architecture Leapfrog uses physics-inspired dynamics to reduce the number of steps required for the denoising process. The leapfrog initializer estimates mean trajectory for backbone of pericition, variance to control the prediction diversity and K samples simultaneously. Limitations The inference speed improved dramatically and achieves state of the art performance on the datasets. The model’s efficiency is higher for lower dimensional data and requires more work for scaling to higher dimensions. Structure from Motion The problem statement for structure from motion is - given a set of unordered or ordered set of images, estimate the relative positions of the cameras and recover the 3D structure of the world, typically as point clouds. In scenarios like autonomous driving, the images are ordered and the emphasis is on real-time performance. Feature Detection The first step involves identifying matching features across images to determine the camera movements. In unordered feature matching, the images to be compared are identified using vocabulary based retrieval methods to reduce the complexity from \(\mathcal O(n^2)\) to \(\log\) complexity. In the canonical coordinate system, the camera axis passes through the origin, and the \(k\)-axis points away from the camera. The \(i\)-axis and \(j\)-axis are parallel to the image plane. The projection of 3D points in the pixel space involves a non-linear operation - \[(x, y, z) \to (-d\frac{x}{z}, -d\frac{y}{z})\] where \(d\) is the distance of the camera plane from the origin. For computational reasons, we want to convert these to a linear transformation. This is done via homogenous point representations - \[\underbrace{(\frac{x}{w}, \frac{y}{w})}_\text{Euclidean} \to\underbrace{(x, y, w)}_\text{Homogenous}\] Such representations are useful for ray marching operations. The camera transformation then becomes \[\begin{bmatrix}-d &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; -d &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix} \begin{bmatrix}x \\ y \\ z \\ 1\end{bmatrix} = \begin{bmatrix}-dx \\ -dy \\ z\end{bmatrix}\] which represents \((-d\frac{x}{z}, -d\frac{y}{z})\) in Euclidean space. The matrix above can be decomposed as \[\underbrace{\begin{bmatrix}-d &amp; 0 &amp; 0 \\ 0 &amp; -d &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}}_{K}\underbrace{\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix}}_\text{projection}\] Here, we have considered a simple model for the camera intrinsics matrix \(K\) whose general form is \[\begin{bmatrix}-d_x &amp; s &amp; c_x \\ 0 &amp; -d_y &amp; c_y \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\] where \(\alpha = \frac{d_x}{d_y}\) is the aspect ratio (1 unless pixels are not square), \(s\) is the skew, \(c_x, c_y\) represent the translation of the camera origin wrt world origin. Coordinate Systems We need to determine the transformations between the world and camera coordinate systems. Since camera is a rigid body, the transformation is represented by a translation and a rotation. Considering these, the projection matrix becomes \[\Pi = K \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0\end{bmatrix}\begin{bmatrix}R &amp; 0 \\ 0 &amp; 0 \end{bmatrix} \begin{bmatrix} T &amp; - c \\ 0 &amp; 0\end{bmatrix}\] Problem Formulation Given the projections \(\Pi_i X_i \sim P_{ii}\), we aim to minimize a non-linear least squares of the form \(G(R, T, X)\) - \[G(X, R, T) = \sum_{i = 1}^m \sum_{j = 1}^n w_{ij} \cdot \left\|P(X_i, R_j, t_j) - \begin{bmatrix}u_{i, j} \\v_{i, j}\end{bmatrix} \right\|^2\] This problem is called as Bundle Adjustment. Since this is an non-linear least squares, Levenberg-Marquardt is a popular choice to solve this. In theory, given enough number of points, we should get a unique solution. However, practically, we require a very good initialization to navigate the high-dimensional space. Also, outliers in the feature detection can deteriorate the performance of the model drastically. Tricks for Real-world performance Basically, we aim to make the feature detection very robust to remove all the possible outliers. Towards that, we note the following background Fundamental Matrix Given pixel coordinates \(x_1, x_2\) of a point \(x\) in two camera views, we define a fundamental matrix \(F\) such that \(x_1Fx_2 = 0\). The fundamental matrix is unique to the cameras and their relative configuration. It is given by \[\begin{align*} F &amp;= K_2^{-T} E K_1^{-1} \\ E &amp;= [t]_{\times} R \end{align*}\] where \(R\) and \([t]_\times\)represent the relative base transformations given by \[[t]_X = \begin{bmatrix} 0 &amp; -t_z &amp; t_y \\ t_z &amp; 0 &amp; -t_x \\ -t_y &amp; t_x &amp; 0\end{bmatrix}\] The matrix \(E\) is called as the essential matrix which has similar properties as the fundamental matrix and is also independent of the camera parameters. The fundamental matrix is scale-invariant and has rank \(2\). Therefore, it has \(7\) degrees of freedom implying that we just need \(8\) correctly configured points (\(x\) and \(y\)) across the two pixel spaces to compute the fundamental matrix. We pose this problem as solving \(Af = 0\) - minimizing \(\|Af\|\) using SVD (to directly operate on the rank). This procedure is popularly known as the \(8\)-point algorithm. Furthermore, we can also use this to find the camera parameters by using more points. The essential matrix can be derived from \(F\) using \(RQ\) decomposition to obtain skew-symmetric and orthogonal matrices \([t]_\times\) and \(R\). These matrices are what we were looking to determine - the relative configuration of the cameras with respect to each other. Now, triangulation can be used to locate \(x\) in the 3D space. However, this may not work since the projection lines may not intersect in 3D space. To filter outliers, we will use the fundamental matrix as a model - it is a geometric property which all points have to abide by. However, the fundamental matrix itself is constructed from the detected features, which can be noisy. We need to design a robust method using heuristics to filter the outliers. We use a population-voting metric to choose the best model. RANSAC Suppose we are finding the best-fit line for a given set of points. For every pair of points, we determine the corresponding line and count the number of outliers for that particular line. The best model would then be the on which has the minimum number of outliers. This idea can be extended to any model, and it can be used to calculate the best fundamental matrix. The algorithm is as follows - Randomly choose \(s\) samples - typically \(s\) is the minimum sample size to git a model (7 for a Fundamental matrix) Fit a model to these samples Count the number of inliers that approximately fit the models. Repeat \(N\) times Choose the model with the largest set of inliers. How do we choose \(N\)? It depends on the ratio of outliers to the dataset size, the minimum number of samples \(s\) for the model and confidence score. We get \[N = \frac{\log(1 - p)}{\log(1 - (1 - \epsilon)^s)}\] Where \(\epsilon\) is the proportion of outliers and \(p\) is the confidence score. Bundle Adjustment Assuming we have a good \(R, t\) robust to outliers with the above methods, we can now solve the least-squares problem to determine the scene geometry. Minimal Problems in computer Vision Three—point absolute pose estimation Assume the intrinsics of the camera are known. Determine the camera pose from given 3D-2D correspondences. Minimal case is 3 points: 6 degrees of freedom, 2 constraints per point \((u, v)\). Given \(p_1, p_2, p_3\) in world coordinates, find positions In camera coordinates. Equivalently, determine how to move camera such that corresponding 2D-3D points align. Real-time SFM: Steady state Suppose we have point-cloud and poses available at frame \(k\), find the pose in frame \(k + 1\). Option 1 Find correspondences between consecutive frames, estimate the essential matrix and find \(R, t\). Keep repeating this as frames are read. 5 point estimation Option 2 Estimate correspondences between point cloud and frame - 3 point absolute pose. Challenges - inliers calculations, which points to match. RANSAC maybe easier here. Option 1 is narrow baseline triangulation. Drift Problem Absolute scale is undeterminable. Why does scale matter? Noise and RANSAC. External measurements to fix this. Loop Closure Retrieval mechanism Lighting Unlike material properties and geometry, lighting is an external factor that is very complicated to infer. Lighting in a scene can. Be categorised as Local illumination - Li Global illumination Shape Normals Explicit representations Implicit representations - SDF Inverse Rendering PhySG Geometry - SDF, Lighting - Environment map, Material (BRDF) Sphere Tracing! Monte Carlo integration for BRDFs - too slow. PhySG - spherical Gaussians]]></summary></entry><entry><title type="html">Mathematics for Finance</title><link href="https://sudhansh6.github.io/blog/Mathematics-for-Finance/" rel="alternate" type="text/html" title="Mathematics for Finance"/><published>2024-04-01T00:00:00+00:00</published><updated>2024-04-01T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Mathematics-for-Finance</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Mathematics-for-Finance/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>We will discuss mathematics for options, pricing methods, and optimizing the performance of a portfolio. We will cover discrete (finite probability spaces, tree models) as well as continuous math (Brownian Motion, Black-Scholes formula, Martingale theory).</p> <p>The theory in the course revolves around the “No Arbitrage theorem” which is closely related to one-price principle. Similarly, the idea of one-price principle theorizes that a set of investments with the same net outcome, albeit with different transactions, should have the same price.</p> <h2 id="financial-markets">Financial Markets</h2> <p>A <em>financial market</em> consists of tradable products such as stocks, bonds, currencies and indices. The market consists of two parties - the buyers and the sellers, who buy and sell these products respectively trying to make an <em>arbitrage</em> from the transactions. Formally, an <em>arbitrage opportunity</em> is a chance for a risk-free profit.</p> <h3 id="odds-and-arbitrage">Odds and Arbitrage</h3> <p>Let us consider the example of a horse race, where \(P(\text {S wins}) = 3/4\) and \(P(\text{W wins}) = 1/4\). How do we calculate the odds against \(W\) winning? It is given by \(P(\text{W wins})/P(\text{W loses}) = 3\). This situation is called \(3\text{-to-}1\) odds. That means, a dollar bet \(\$ 1\) brings \(\$ 3\) if \(W\) wins. Similarly, for \(S\), it brings \(\$ 0.3\). These bets are an example of a fair game, the net reward \(\frac{1}{4} (3) + \frac{3}{4}(-1)\) is \(0\).</p> <p>A <em>bookmaker</em> sets the odds for such scenarios. Consider an other example where the odds are \(9\text{-to-}5\) against \(W\) and \(2\text{-to-}5\) against \(S\). We claim that this results in an arbitrage for the bookmaker. To illustrate this, consider the bets \(\$ 10\) on \(S\) and \(\$ 5\) on \(W\). The payoff for the bookmaker when \(S\) wins is \(5 - 10\frac{2}{5} = 1\). When \(W\) wins, the profit is \(10 - 5\frac{9}{5} = 1\). A bookmaker typically decides the odds after the bets are made so that arbitrage is made in any scenarios.</p> <p>Ideally, one wants to make profit from such games. How do we do this? In the first example, we considered the probabilities and calculated the odds. To decide the odds for arbitrage, we do the reverse. In the above scenario, given the odds, the probabilities are \(P(S) \approx 71\%\) and \(P(W) \approx 36\%\). The probabilities add up to slightly above \(100\%\), which makes up the arbitrage for the bookmaker.</p> <h2 id="contingent-claim">Contingent Claim</h2> <p>A <strong>derivative</strong> or a <strong>contingent claim</strong> is a security whose value depends on the value of an underlying asset. Forward contracts, futures contracts and options are examples of such securities. Interestingly, the markets around the world also use derivatives whose underlying asset is also a derivative. Such securities are called as <em>structured products</em>.</p> <p>With such contrived products, it is important to choose a <em>fair price</em> to ensure there are no arbitrage opportunities in the market. To do so, we introduce the required mathematical notation - \(X\) is a real-valued random variable defined on a probability space \((\Omega, \mathcal F, P)\) and \(\mathcal G\) is a sub-\(\sigma\)-algebra of \(\mathcal F\). For unfamiliar readers, in probability theory, a probability space consists of three elements -</p> <ul> <li>A sample space \(\Omega\), which is the set of all possible outcomes. For example, the sample space for a dice roll is \(\Omega = \{1, 2, 3, 4, 5, 6\}\).</li> <li>An event space or a set of events is represented by \(\mathcal F\). For example, the events constituting an odd dice roll is \(\mathcal F = \{1, 3, 5\}\).</li> <li>A probability function \(P\), assigns a probability (a number between \(0\) and \(1\)) to each event in the event space.</li> </ul> <p>The term \(\sigma\)-algebra on a set \(X\) refers to a nonempty collection \(\Sigma\) of subsets of \(X\) closed under complement, countable unions, and countable intersections. The pair \((X, \Sigma)\) is called a <strong>measurable space</strong>.</p> <p>In general, we model the price of a contract with a random variable \(X\) that evolves across time with \(X(w) \geq 0\) (all the values for the random variable are positive). For example, for the <em>call option</em> described in #options, we have</p> \[X(w) = (S(w)_t - K)^+\] <p>Similarly, for a <em>put</em> option, where the holder has the right to sell a certain asset at a fixed price, we get</p> \[X(w) = (K - S_T)^+\] <p>There can be other kinds of <em>structured products</em> like</p> <ul> <li> \[X = \max(S_1, \dots, S_T)\] </li> <li> \[X = \left(\frac{1}{T} \sum_{t = 1}^T S_t - K\right)^+\] </li> </ul> <p>In the above definitions, the variables \(w, S_t\) capture the evolution of price across time, and are described in the later section. The key takeaway is that, we are trying to price a contingent claim \(X\) - find \(C_0(X)\)</p> <h3 id="forwards-contract">Forwards Contract</h3> <p>A <strong>forward constract</strong> is an agreement to buy or sell an asset at a <em>certain</em> future time for a <em>certain</em> price. Consider the following example,</p> <ul> <li>\(A\) agrees at time \(t = 0\) to sell one share at time \(T\) for \(\$ F\). We assume that \(A\) buys the share at \(t = 0\) at price \(S_0\).</li> <li>\(B\) agrees at time \(t = 0\) to buy one share at time \(T\) for \(\$ F\)</li> </ul> <p>We assume that \(A, B\) don’t invest their own money, and borrow money from the bank to perform transactions. Given such a scenario, how do we decide a <em>fair price</em> \(F\)? To simplify the calculations, we assume the following -</p> <ul> <li>No transaction costs or dividends.</li> <li>Market is liquid - every transaction has a buyer and seller available.</li> <li>Investor is <em>small</em> compared to the market. That is, the action of the investor is not momentous enough to change the price of the stock in the market.</li> <li>Short selling/borrowing of stock is allowed.</li> </ul> <p>\(B\) can invest the amount in the bank for a <em>continuous compound</em> interest rate \(r\) during this term. The claim is that \(F = S_0 e^{rT}\) where \(S_0\) is the price of the share at \(t = 0\). To analyze this, we consider the following cases -</p> <ul> <li> \[F &gt; S_0 e^{rT}\] <p>\(A\) sells the share for \(\$ F\) and repays the loan of \(\$ S_0\) with interest - \(\$ S_0 e^{rT}\). The profit \(A\) gets is \(F - S_0 e^{rT}\)</p> </li> <li> \[F &gt; S_0e^{rT}\] <p>\(B\) short sells one share for \(\$ S_0\) and invests \(\$ S_0\) in the bank. The money in the bank will grow to \(S_0 e^{rT}\) at \(t = T\), and \(B\) buys one share at \(F\). The profit for \(B\) here is \(S_0 e^{rT} - F &gt; 0\)</p> </li> </ul> <p>So the fair price is \(S_0 e^{rT}\) where neither \(A\) nor \(B\) can take advantage.</p> <p><em>Note.</em> A <strong>futures contract</strong> is similar to a forward contract but is traded on a financial exchange. They typically have a delivery month rather than a delivery date, and are followed by a settlement procedure called <em>marking to market</em>.</p> <h3 id="options">Options</h3> <p>An <strong>option</strong> is a contract which gives the holder of the option the right, but not the obligation, to buy other sell a given security at a given price (called the <em>strike</em> price) within a fixed time period \([0, T]\). A <strong>call option</strong> gives the option holder the right to buy at the given price, whereas the <em>put option</em> gives the option holder the right to sell at the given price.</p> <h1 id="european-call-options">European Call Options</h1> <p>A <em>European option</em> can only be exercised by the holder of the option at the expiration time \(T\) (unlike an American Option, which will be discussed in the later sections). We have the following equation for profit of a <em>call option</em> holder, assuming the option is free -</p> \[P = (S_T - K)^+ = \begin{cases} S_T - K &amp; S_T &gt; K \\ 0 &amp; S_T \leq K \end{cases}\] <p>How do we decide the <em>fair price</em> at \(t = 0\) for the option? We need to use probability results to model the price movement to decide this. In discrete time settings, we use different models like Cox, Ross and Rubenstein. In continuous time frame, we use Brownian motion using the Black-Scholes model. In both cases, there exists a unique fair price. However, if the product is slightly more complicated, then there is no unique fair price.</p> <h2 id="binomial-model">Binomial Model</h2> <p>Also known as <strong>Cox-Ross-Rubenstein Model (CRR)</strong> is a discrete time model for a financial market defined for \(t = 0, 1, \dots, T\). At each time instant the model considers the following two types of assets</p> <h4 id="bonds">Bonds</h4> <p>Considered as a risk-free asset, a <em>bond</em> yields at a constant rate of return \(r \geq 0\) over each time period. The price is modeled as \(B_t = B_0 (1 + r)^t\) for \(t = 0, 1, \dots, T\). \(B_0\) is typically assumed as \(1\).</p> <h4 id="stocks">Stocks</h4> <p>Stocks are considered as risk-models where the price evolution is captured using binomial random variables. We consider the model \(S_t = s_o \zeta_1 \dots \zeta_{t_1} = S_{t - 1} \zeta_t\) where \(S_0 &gt; 0\) is the constant initial price. That is, the stock price process is modeled as an <em>exponential random walk</em>. In this model, we assume \(P(\zeta_t = u) = p\) and \(P(\zeta_t = d) = 1- p\) where \(0 &lt; d &lt; 1 + r &lt; u\) where \(d, u\) are the down and up movements respectively. That is, \(S_{t + 1} = S_t u\) represents the price of the stock if the price has moved upward at time \(t\).</p> <p>Concretely, the probability space \((\Omega, \mathcal F, P)\) is such that \(\Omega\) is the finite set of \(2^T\) possible outcomes for the values of stock price \((T + 1)\)-tuple (for \(S_0, S_1, \dots, S_T)\), \(\mathcal F\) is the \(\sigma\)-algebra consisting of all possible subsets of \(\Omega\), and \(P\) is the probability measure on \((\Omega, \mathcal F)\) associated with the Bernoulli probability \(p\).</p> <p>Let the field \(\mathcal F_0 = \{ \phi, \Omega\}\) and</p> \[\begin{align*} \mathcal F_1 &amp;= \sigma(S_1) \\ &amp;= \{ \phi, \Omega, \{w: S_1(w) = S_0u \}, \{w: S_1(w) = S_0 d\}\} \\ &amp;= \sigma(\{S_1 = S_0u\}), \{S_1 = S_0d\}) \end{align*}\] <p>This is a partition of \(\Omega\). Similarly, for \(t = 2\), we get</p> <p>\(\mathcal F_1 = \{ \phi, \Omega, \{uu, ud\}, \{dd, du\}\}\). Extrapolating this,</p> \[\mathcal F_t = \sigma(S_1, \dots, S_t) = \sigma(\zeta_2, \dots, \zeta_t)\] <p>We get \(\#(\mathcal F)_t = 2^t\). This evolution can be represented using a binomial tree of the following form -</p> \[\begin{align*} &amp;\boxed{} \\ \boxed{d} &amp;\quad \boxed{u} \\ \boxed{dd} \quad \boxed {du} &amp;\quad \boxed{ud} \quad \boxed{uu} \\ &amp;\dots \end{align*}\] <h2 id="trading-strategies-and-fair-pricing">Trading Strategies and Fair Pricing</h2> <p>A strategy \(\phi\) in stocks and bonds consists of sequence of random variable pairs \(\phi = \{(\alpha_t, \beta_t); t = 1, \dots, T\}\) where \(\alpha_t\) represents the holdings of a stock and \(\beta_t\) represents the holdings of a bond on day \(t\) (The unit for time can be taken as minutes, hours, etc., but we consider days). We also have the following assumptions</p> <ol> <li>Predictability: \(\alpha_t, \beta_t \in \mathcal F_{t - 1}, t = 1, \dots, T\)</li> <li>Self-financing: \(\alpha_t S_t + \beta_t B_t = \alpha_{t + 1} S_t + \beta_{t + 1}B_t\). That is, there is no influx or outflux of money asides from the changes in the underlying assets. We have some initial investment and assume that \(\alpha_1, \beta_1\) are \(\mathcal F_0\)-measurable are constant.</li> </ol> <p>The value of the portfolio at time \(t\) is given by \(V_t(\phi) = \alpha_t S_t + \beta_t B_t, t = 1, \dots, T\) and \(V_0(\phi) = \alpha_1 S_0 + \beta B_0 = \alpha_1 S_1 + \beta_1\) We can now define the concept of <strong>arbitrage</strong> formally.</p> <h3 id="arbitrage">Arbitrage</h3> <p>An arbitrage is a trading strategy \(\phi\) such that \(V_0(\phi) = 0\) and \(V_t(\phi) \geq 0\). That is, we don’t have any initial investment and get a profit at time \(t = T\). However, note that \(V_t(\phi)\) is a random variable, so the inequality is assumed to be a point-wise inequality wherein \(V_T(\phi)(w) \geq 0, \forall w \in \Omega\). This is a stronger assumption than \(P(V_T(\phi) &gt; 0) &gt; 0 \equiv E(V_T(\phi)) &gt; 0 \equiv V_T(\phi)(w) &gt; 0\) for some \(w \in \Omega\). Notice that the last equivalence does not have any probability term in the expression. This equivalence is important to note since it makes the definition invariant of the underlying probability assumptions.</p> <h3 id="hedge">Hedge</h3> <p>A hedging or a replicating strategy \(\phi\) has the property \(V_T(\phi) = X\). That is, to price a contingent claim \(X\), if we find a trading strategy \(\phi\) with cost (initial investment) \(V_0(\phi)\), such that \(V_T(\phi) = X\), then \(V_0(\phi)\) is a fair price of \(X\) by the one-price principle. Essentially, with the price “\(V_0(\phi)\)” we bought an opportunity that resulted in a value of “\(V_T(\phi)\)” at time \(T\). If a contingent claim \(X\) has the same value at time \(T\), then its fair price should be \(V_0(\phi)\) at \(t = 0\).</p> <p>Using this as a motivation, let us price a European call option. Our approach is as follows - an investment in the underlying stock and a bond should result in the same outcome as an investment in the option itself. The movements in the option will be correlated with those of the underlying stock, allowing us to correctly determine a fair price at every time instant.</p> <h4 id="hedging-for-t--1">Hedging for \(T = 1\)</h4> <p>Consider the following example, where we try to hedge \(X\) at \(t = 1\). That is, we are trying to find a strategy \(\phi = (\alpha_1, \beta_1)\) such that \(V_1(\phi) = \alpha_1 S_1 + \beta_1 B_1\equiv X\).</p> <p>We have two cases based on the stock movement -</p> <ul> <li> \[\alpha_1 S_0 u + \beta_1 (1 + r) = X^u\] </li> <li> \[\alpha_1 S_0 d + \beta_1 (1 + r) = X^d\] </li> </ul> <p>Equivalently, \(\alpha_1 S_0(u - d) = X^u - X^d\)</p> \[\begin{align*} \alpha_1 &amp;= \frac{X^u - X^d}{u - d}\frac{1}{S_0} \\ \beta_1 &amp;= \frac{uX^d - d X^u}{u - d}\frac{1}{B_1} \end{align*}\] <p>What do the above expressions mean? Investing \(\alpha_1\) in stock and \(\beta_1\) in bond at the end of day \(0\) will result in same price as \(X\) on day \(1\). Manipulating the expressions further,</p> \[\begin{align*} V_0 (\phi) &amp;= \alpha_1 S_0 + \beta_1 \\ &amp;= \frac{X^u - X^d}{u- d} + \frac{uX^d - d X^u}{u - d}\frac{1}{1 + r} \\ &amp;= \frac{1}{1 + r}\left\{X^u \left(\frac{1 + r - d}{u - d}\right) + X^d \left(\frac{u - 1 - r}{u - d}\right)\right\} \\ &amp;= \frac{1}{1 + r} \mathbb E^* (X) \end{align*}\] <p>The above expression is reminiscent of an expectation formulation where \(p^* = \frac{1 + r - d}{u - d}\) and \(0 &lt; p^* &lt; 1\). Here, the set of discounted stock prices \(\{S_0, S_1/(1 + r)\}\) form a \(p^*\)-martingale.</p> <p>What is a Martingale? A <strong>Martingale</strong> is a sequence of random variables representing a stochastic process for which, at a particular time, the conditional expectation of the next value in the sequence is equal to the present value, regardless of all prior values.</p> <p>In the above case, we have</p> \[\begin{align*} \mathbb E^* (S_1/(1 + r) \vert \mathcal F_0) &amp;= S_0 u \cdot p^* + S_0 d \cdot (1 - p)^* \\ &amp;= S_0 \implies \mathbb E^*(S_1) = S_0(1 + r) \end{align*}\] <p>It is important to realize that computing expectations under \(p^*\) is a mathematical device. We are not assuming that the stock price actually moves according to this probability. That is, \(p^*\) may be unrelated to the subjective probability \(p\) that we associate with the binomial model for movements in the stock price.</p> <p><strong>Theorem 1 (T = 1)</strong> <em>For a given \(X\), the value \(\mathbb E^*(X)/(1 + r)\) is the non-arbitrage price of \(X\) at \(t = 0\).</em> <em>Proof.</em> Let \(C_0\) is the market price of the contingent claim \(X\) and \(V_0 = \mathbb E^*(X)/(1 + r) = V_0(\bar \phi)\) where \(\bar \phi = (\bar \alpha_1, \bar \beta_1)\) (the values we calculated before). Consider the following cases -</p> <ol> <li> <p>Say \(C_0 &gt; V_0\) To make an arbitrage in this situation, we sell \(X\) for \(C_0\). We invest the amount \(V_0\) in the stock and amount \(C_0 - V_0 &gt; 0\) in bonds at \(t = 0\). At time \(t = 1\), we get \(V_1(\bar \phi) = X\) yielding \((C_0 - V_0)(1 + r) &gt; 0\) as profit from the bond.</p> </li> <li> <p>Say \(C_0 &lt; V_0\) We do the opposite from above, where we buy the asset \(X\). Short sell \(\bar \phi\) for \(V_0\), and invest \(C_0\) in \(X\) and \(V_0 - C_0\) in the bond. We get the profit \((V_0 - C_0)(1 + r)\) at \(t = 1\).</p> </li> <li> <p>Say \(C_0 = V_0\) Here, we can show that no one can make an arbitrage. Consider the strategy in stock, bond and contingent claim \(\psi = (\alpha_1, \beta_1, \gamma_1)\). We need to argue that no arbitrage can be made with any such strategy. We have,</p> </li> </ol> \[\begin{align*} V_0(\psi) &amp;= \alpha_1 S_0 + \beta_1 + \gamma_1 C_0 \\ V_1(\psi) &amp;= \alpha_1 S_1 + \beta B_1 + \gamma_1 X \\ \implies \mathbb E^*(V_1(\psi)) &amp;= \mathbb E^*(\alpha_1 S_1 + \beta_1 B1 + \gamma_1 X) \\ &amp;= \alpha_1 \mathbb E^*(S_1) + \beta_1 (1 + r) + \gamma_1 \mathbb E^*(X) \end{align*}\] <p>We can distribute the expectation this way due to the linearity property. Recall that, we typically assume \(B_0 = 1\) for brevity. Continuting the calculations,</p> \[\begin{align*} \mathbb E^*(V_1(\psi)) &amp;= \alpha_1 S_1(1 + r) + \beta_1(1 + r) + \gamma_1 V_0 (\bar \phi)(1 + r) \\ &amp;= \alpha_1 S_1(1 + r) + \beta_1(1 + r) + \gamma_1 C_0 (1 + r) \\ &amp;= (1 + r) V_0 (\psi) \end{align*}\] <p>If \(V_0(\psi) = 0\), then \(\mathbb E^*(V_1(\psi)) = 0\), and if \(V_1(\psi) \geq 0\), then \(P^*(V_1(\psi) = 0) = 1\). That is, \(V_1(\psi) = 0\) meaning no arbitrage can be made with any strategy \(\psi\).</p> <h4 id="general-t">General \(T\)</h4> <p>This analysis can be extended to derive the hedging strategy a general \(t &gt; 1\). Given an \(X\), find \(\phi = \{(\alpha_t, \beta_t): t = 1, \dots, T\}\) such that \(V_T(\phi) = X\) (resulting in \(C_0 = V_0(\phi)\)). To derive such a strategy, we work backwards in time starting from \(V_T(\phi) = X\). This way, we can also calculate the intermediate price - the price to enter the market at some intermediary time \(t\). It is helpful to think of the tree visualization for the price evolution.</p> \[\begin{align*} V_T(\phi) = \alpha_T S_T + \beta_T B_T = X \end{align*}\] <p>conditional on \(\mathcal F_{T - 1}\). Focusing on \(T - 1\) and \(T\), the analysis is similar to the previous case wherein \(X^U = X(w_1 \dots w_{T - 1} u)\) and \(X^d = X(w_1 \dots w_{T - 1} d)\). Doing so we get</p> \[\left. \begin{align*} \alpha_T = \frac{X^u - X^d}{u - d}\frac{1}{S_{T - 1}} \\ \beta_T = \frac{uX^d - dX^u}{u - d}\frac{1}{(1 + r)^T} \end{align*} \right\} \in \mathcal F_{T - 1}\] <p>Using the property of Martingales,</p> \[\begin{align*} \mathbb E^* (X \vert \mathcal F_{T - 1}) = \mathbb E^* [V_T(\phi) \vert \mathcal F_{T - 1}] &amp;= \mathbb E^*[\alpha_T S_T + \beta_T B_T \vert \mathcal F_{T - 1}] \\ &amp;= \alpha_T \mathbb E^*(S_T \vert \mathcal F_{T - 1}) + \beta_T B_T \\ &amp;= \alpha_T \left(p^* S_{T - 1} u + (1 - p^*) S_{T - 1} d\right) + \beta_T B_T \\ &amp;= \alpha_T S_{T - 1}(1 + r) + \beta_T B_T \end{align*}\] <p><em>Note.</em> \(\alpha_T, \beta_T\) are constants with respect to the <em>condition</em> \(\mathcal F_{T - 1}\), and that is why the linearity property can be used.</p> <p>Continuing the previous calculations</p> \[\begin{align*} \mathbb E^*(V_T \vert \mathcal F_{T - 1}) &amp;= \mathbb E^*(\alpha_T S_T + \beta_T B_T \vert \mathcal F_{T - 1}) \\ &amp;= \alpha_T S_{T - 1}(1 + r) + \beta_T B_{T - q}(1 + r) \\ &amp;= (1 + r) \left[\alpha_T S_{T - 1} + \beta_T B_{T - 1}\right] \\ &amp;= (1 + r) \left[\alpha_{T - 1} S_{T - 1} + \beta_{T - 1} B_{T - 1}\right] &amp;\because \text{ self-financing} \\ &amp;= (1 + r) V_{T - 1}(\phi) \end{align*}\] <p>As a result, we have a recursive property</p> \[\begin{align*} V_{t - 1}(\phi) &amp;= \frac{1}{1 + r} \mathbb E^*(V_t(\phi) \vert \mathcal F_{t - 1}) \\ \alpha_t &amp;= \frac{V_t^u - V_t^d}{u - d} \frac{1}{S_{t - 1}} \\ \beta_t &amp;= \frac{uV^d_t - dV^u_t}{u - d} \frac{1}{B_{t - 1}} \end{align*}\] <p>for \(t = T, \dots, 1\)</p> <p>The above relation can also be alternately derived using the <strong>Tower law</strong> to relate the value with \(V_t(\phi)\) for a general \(t\).</p> <blockquote> <p>Tower Law: If \(X\) is a random variable whose expected value \(E ⁡( X )\) is defined, and \(Y\) is any random variable on the same probability space, then \(E(X) = E (E (X \vert Y ) )\), i.e., the expected value of the conditional expected value of \(X\) given \(Y\) is the same as the expected value of \(X\).</p> </blockquote> <p>Then, we get \(\begin{align*} V_t(\phi) &amp;= \frac{1}{1 + r} \mathbb E^*(V_{t + 1} (\phi) \vert \mathcal F_t) \\ &amp;= \frac{1}{1 + r} \mathbb E^*\left(\frac{1}{1 + r} \mathbb E^*(V_{t + 2}(\phi) \vert \mathcal F_{t + 1} ) \vert \mathcal F_t\right) \\ &amp;= \frac{1}{(1 + r)^2} \mathbb E^*\left(\mathbb E^*\left(V_{t + 1} \vert \mathcal F_{t + 1}\right \vert \mathcal F_{t} \right) \\ &amp;= \frac{1}{(1 + r)^2} \mathbb E^*(V_{t + 2} \vert \mathcal F_t) \end{align*}\)</p> <p>Consequently,</p> \[\begin{align*} V_T(\phi) &amp;= \frac{1}{(1 + r)^{T - t}} \mathbb E^*(V_T (\phi) \vert \mathcal F_t) \\ &amp;= \frac{1}{(1 + r)^{T - t}} \mathbb E^*(X \vert \mathcal F_t) \\ V_0 (\phi) &amp;= \frac{1}{(1 + r)^T} \mathbb E^*(X) \end{align*}\] <h3 id="conditional-expectation-of-martingales">Conditional Expectation of Martingales</h3> <p>Given \((\Omega, \mathcal D, P)\) with \(g \subset \mathcal F\), \(X \in L'(\mathcal F)\) (the space of random variables that are integrable with respect to the \(\sigma\)-algebra \(\mathcal F\)) then \(Y = \mathbb E(X \vert g)\) is the random variable such that</p> <ol> <li> \[Y \in L'(g)\] </li> <li>\(\mathbb E(YZ) = \mathbb E(XZ)\) for all \(Z\) that are \(g\)-measurable.</li> </ol> <p>where \(L'\) represents integrability. The above property is the general statement for the law of general statician.</p> <p><em>Special case.</em> When \(\Omega\) is finite (\(\#(\Omega) &lt; \infty\)), then \(\mathcal F = \{ \text{all subsets of } \Omega\}\) and \(\mathcal G = \sigma(G_1, \dots, G_n)\) form a partition (\(G_i \cap G_j = \phi, \cup_{j = i}^n G_j = \Omega\)) of \(\Omega\) with \(P(G_k) &gt; 0\).</p> <p>A random variable \(Z\) is \(g\)-measurable iff \(Z(w) = \sum_{k = 1}^n \mathbb 1_{G_k} (w) \cdot c_k \equiv Z(w) = c_k \text{ if } w \in G_k\) where \(c_k\) is the cardinality of \(G_k\).</p> <p>Then,</p> \[\mathbb E[X \vert \mathcal G](w) = \sum_{k = 1}^n \left(\mathbb 1_{G_k}(w) \cdot E[X \vert G_k]\right)\] <p>From Bayes’ rule</p> \[\mathbb E(X \vert G_k) = \mathbb E(X . \mathbb 1_{G_k}) /P(G_k) = \mathbb E(X; G_k)/P(G_k)\] <p>Using the above results, we get</p> <ol> <li> <p>\(X \to \mathbb E(X \vert Y)\) is linear &amp; positive (\(X \geq 0 \implies \mathbb E(X \vert g) \geq 0\))</p> </li> <li> \[\mathbb E(\mathbb E (X \vert g)) = \mathbb E(X)\] </li> <li> \[\mathbb E(XZ \vert g) \equiv E(X \vert y) Z\] </li> <li>If \(\mathbb E \in g\), then \(E(X \vert g) = X\) and \(X \perp\!\!\!\perp g\), then \(\mathbb E(X \vert Y) = \mathbb E(X)\).</li> </ol> <p><strong>Exercise.</strong></p> <p>Show that \(E( \cdot \vert \mathcal G)\) is a \(\perp\) projection where \(d(X, \tilde X) = \sqrt{\mathbb E(X - \tilde X)^2}\)</p> <h3 id="tower-property">Tower Property</h3> <p>For any \(\mathcal G \subset \mathcal H ( \subset \mathcal F)\), we have \(\mathbb E[\mathbb E[X \vert \mathcal H] \vert \mathcal G] = \mathbb E[X \vert \mathcal G]\).</p> <p>What does \(\mathcal G \subset \mathcal H\) mean? In the discrete case where \(\mathcal G = \sigma(G_1, \dots, G_n)\) and \(\mathcal H = \sigma(H_1, \dots, H_m)\), we have \(G_k = \dot \cup_{j \in J(k)} H_j\) where \(J(1),\dots, J(n)\) is a partition of \(\{1, \dots, m\}\).</p> <p>Now, to show the tower property,</p> \[\begin{align*} \mathbb E[X \vert G_k] &amp;= \underbrace{\mathbb E[X; G_k]}_{P(G_k)} \\ &amp;= \sum_{j \in J(k)} \underbrace{\mathbb E(X; H_j)}_{P(H_j)}P(H_j \vert G_k) \\ &amp;= \sum_{j \in J(k)} \mathbb E(X \vert H_j)P(H_j \vert G_k) \end{align*}\] <p>In the trading strategy context, we have \(\mathcal F_0 \subset \mathcal F_1 \subset \dots \subset \mathcal F_n\).</p> <h3 id="martingales">Martingales</h3> <p>We now introduce Martingales formally - A Martingale is a sequence of random variables \(\{M_0, \dots, M_T\}\) such that</p> <ol> <li> <p><em>Adapted:</em> \(M_t \in \mathcal F_t\), for all \(t\)</p> </li> <li> <p><em>Integrable:</em> \(\mathbb E(\vert M_t\vert) &lt; \infty\) for all \(t\)</p> </li> <li> <p>\(\mathbb E(M_t \vert \mathcal F_{t - 1}) = M_{t - 1}\), \(1 \leq t \leq T\). From Tower law, \(\mathbb E[M_T \vert \mathcal F_s] = M_s\).</p> </li> </ol> <p>Let us now revert to our discussion on trading strategies with these formal definitions in mind.</p> <p>As we have seen before in #General \(T\), \(\phi\) that satisfied \(V_t^*(\phi) := V_t(\phi)/(1 + r)^t\) is a \(p^*\)-martingale, where \(p^* = \frac{1 + r - d}{u - d}\). A special case of this is \(S^*_t\) is a \(p^*\)- martingale. We have the following theorem,</p> <p><strong>Theorem 2. (General \(T\))</strong> \(V_0\) is the arbitrage-free price of \(V\)</p> <p><em>Proof.</em> We follow a similar structure as before,</p> <ol> <li> <p>\(C_0 \neq V_0\), then arbitrage is available</p> </li> <li> <p>\(C_0 = V_0\), we have for any strategy \(\psi = \{(\alpha_t, \beta_T, \gamma_1): t = 1, \dots, T\}\), \(V_0(\psi) = 0, V_T(\psi) \geq 0\) due to self-financing. Let \(\bar \phi = \{(\bar \alpha_t, \bar \beta_T): t = 1, \dots, T\}\) be the hedge for \(X\). Then, \(V_T(\bar \phi) = X\) and \(V_0(\bar \phi) = V_0\).</p> \[\begin{align*} V_0 (\psi) &amp;= \alpha_1 S_0 + \beta + \gamma_1 C_0 \\ V_T(\psi) &amp;= \alpha_T S_t + \beta_T B_T + \gamma_1 X \end{align*}\] <p>Continuing the calculations,</p> \[\begin{align*} \mathbb E^*(V^*_T(\psi)) &amp;= \mathbb E^*(\alpha_T S^*_T + \beta_T + \gamma_1 X^*) \\ &amp;= \mathbb E^*(V_T(\phi)) + \gamma_1 \mathbb E^*(X^*) \\ &amp;= V_0(\phi) + \gamma_1 C_0 \\ &amp;= V_0 (\psi) = 0 \implies V_T(\psi) = 0 \end{align*}\] <p>Therefore, no arbitrage is possible in this case.</p> </li> </ol> <p>Let us consider a numerical example to understand this better - Consider the call option \(X = (S_T - K)^+\), where \(T = 4, u = 1.2, d = 0.8, r = 0\) (\(p^* = 0.5\)). Let \(S_0 = 50\) and \(K = 40\). What is the fair price of \(X\)?</p> <p>Then, we have the following evolution for \(S_T\),</p> \[\begin{align*} &amp;50\\ 40&amp;\quad 60\\ 32\quad &amp;48\quad 72\\ 25.6\quad 38.4&amp;\quad 57.6\quad 86.4\\ 20.48\quad 30.72\quad &amp;46.08\quad 69.12\quad 103.68 \end{align*}\] <p>Consequently, the tree for \(V_0\) is</p> \[\begin{align*} &amp;13.54\\ 1&amp;\quad 21.66\\ 1.52\quad &amp;10.32\quad 32\\ 0\quad 3.04&amp;\quad 17.6\quad 46.4\\ 0\quad\quad 0 \quad &amp;6.08\quad 29.12 \quad 63.68 \end{align*}\] <p>We construct the tree recursively, starting from the bottom-most level. At time \(T\), we know \(V_T\), and then we use \(V_t^u p^* + V_t^d(1 - p^*)\) to get \(V_{t - 1}(1 + r)\).</p> <h2 id="call-put-parity">Call-Put Parity</h2> <p>A call option is represented as \(C_T = (S_T - K)^+\) and a put option is represented as \(P_T = (K - S_T)^+ = (S_T - K)^-\).</p> <p>For any \(b\), we have \(b^+ - b^- = b\) and \(b^+ + b^- = b\). In other words, the payoffs in call and put options are related to each other as</p> \[\begin{align*} C_T - P_T = S_T - K \\ C_T + K = S_T + P_T \end{align*}\] <p><strong>Claim.</strong> For \(t &lt; T\), we have</p> \[C_t + \frac{K}{(1 + r)^{T - t}} = S_t + P_t\] <p>This result can be proved using <strong>One price principle</strong>. Think of each side of the equation as separate investments</p> <ol> <li> <p>First investment has a call option and bonds with \(K\).</p> </li> <li> <p>Second investment has a put option and one share of the stock</p> </li> </ol> <p>We can see that at time \(T\), the values become equal \(C_T + K = S_T + P_T\)</p> <h1 id="american-options">American Options</h1> <p>In American Options, we have \(Y_0, Y_1, \dots, Y_T\) random payouts where each \(Y_t \in \mathcal F_T\). The holder of the options can take the payment at any time \(t \in [0, T]\) of choice. That is , unlike European options, American options have the choice of payment at any intermediate time. For example, consider a call option \(Y_t = (S_t - K)^+\) for \(t = 0, \dots, T\).</p> <p>Formally, we have the <strong>stopping time</strong> property represented as \(\mathcal T: \Omega \to \{0, 1, \dots, T\}\) such that \(\{\mathcal T \leq t\} = \{w \in \Omega: \mathcal T(W) \leq t\} \in \mathcal F_t, \forall t = 0, 1, \dots, T\). This is equivalent to saying, \(\{T = t\} \in \mathcal F_t, \forall t\).</p> <p>For example, consider \(Z_0, Z_1, \dots, Z_T\), we have \(\mathcal T:= \min \{t: Z_T \in B\} \wedge T\) where \(A \wedge b = \min(a, b)\). Then, \(\{\mathcal T = t\} = \{Z_) \not\in B, Z_1 \not\in B, \dots, Z_{t - 1} \not \in B, Z_t \in B\} \in \mathcal F_t\).</p> <h2 id="optional-stopping-theorem">Optional Stopping Theorem</h2> <p>Consider a stopping time \(\tau\) and a Martingale \((M_t)\), we have \(\mathbb E(M_\tau) = \mathbb E(M_0)\). Essentially, in a fair game, the average payout from exiting the option at an intermediate time is the same as the average payout till time \(t= T\).</p> <p>Here \(M_\tau\) is a random variable of the form \(M_{\tau(w)}(w)\) which we have represented in a singular form.</p> <p>Is the converse true? That is, a sequence of random variables have the optional stopping property where expectations are the same for the beginning and an intermediate time. Then do they constitute a Martingale? Yes, we have</p> \[\begin{align*} M_T &amp;= M_0 + \sum_{t = 1}^T (M_t - M_{t - 1}) \\ M_\tau &amp;= M_0 + \sum_{t = 1}^T (M_t - M_{t - 1}) \mathbb 1_{\{t \leq \tau\}} \end{align*}\] <p>The event \(\{t \leq \tau\}\) is equivalent to \(\{\tau \leq t - 1\}^c\). This allows us to get back the notion of \(\mathcal F_{t - 1}\)</p> \[\begin{align*} \mathbb E(M_\tau) &amp;= \mathbb E(M_0) + \sum_{t = 1}^T \mathbb E[(M_t - M_{t - 1} \mathbb 1_{\{t \leq \tau\}}] \\ &amp;= \mathbb E(M_0) + \sum_{t = 1}^T \mathbb E[\mathbb E[(M_t - M_{t - 1} \mathbb 1_{\{t \leq \tau\}} \vert \mathcal F_{t - 1}]] \\ &amp;= \mathbb E(M_0) + \sum_{t = 1}^T \mathbb E[\mathbb E[(M_t - M_{t - 1} \vert \mathcal F_{t - 1}] \mathbb 1_{\{t \leq \tau\}}] \\ &amp;= \mathbb E(M_0) \end{align*}\] <p>Let us consider an example of an American Put option to understand this better -</p> \[Y_t = (K - S_t)^+\] <p>where \(T = 1, u = 2, d = 0.5, r = 0.2, S_0 = 100, K = 150\). The value for \(p^*\) is \(7/15\). Then, we have the following tree evolution for \(S_t\)</p> \[\begin{align*} &amp;100 \\ 50 &amp;\quad 100 \\ 25 \quad 100 &amp;\quad 100 \quad 400 \end{align*}\] <p>The payment \(Y_t\) looks like</p> \[\begin{align*} &amp;50 \\ 100^{1} &amp;\quad 0 \\ 125 \quad 50 &amp;\quad 50 \quad 0 \end{align*}\] <p>How do we calculate a fair price in this case? We introduce the notion of a <strong>super-martingale</strong>. In comparison, in European call options, we get for \(V_t\)</p> \[\begin{align*} &amp;41.98 \\ 75^{1} &amp;\quad 22.\bar 2 \\ 125 \quad 50 &amp;\quad 50 \quad 0 \end{align*}\] <p>From the seller’s perspective, we have \(U_t\), and we use something called as a <strong>necessary wealth</strong> process - calculating the amount of money the seller requires at every node to pay the option holder. To do so, we need to consider the maximum of all possible payouts at every node. In the node labeled \(1\) above, if the buyer exits the option at \(t = 1\), then the seller needs a payout of \(100\). However, if the buyer exits at time \(t = 2\), the seller requires atleast \(75\) at \(t = 1\) (They can invest the \(75\) in the same stock using short sell to get the required payouts). Considering these scenarios, the seller requires \(100\) at the \(1\) node. Doing such a process for every node, we get \(U_t\)</p> \[\begin{align*} &amp;53.09 \\ 100 &amp;\quad 22.\bar 2 \\ 125 \quad 50 &amp;\quad 50 \quad 0 \end{align*}\] <p>We can see that the price of this option is slightly higher than European since the buyer has the choice of exiting the contract at any intermediate time.</p> <p>To formally describe the process we did above, we consider the following <em>recursion</em></p> \[\begin{align*} U_{t - 1} &amp;= \max \left(\frac{1}{1 + r} \mathbb E^*(U_t \vert \mathcal F_{t - 1}), Y_{t - 1}\right) \quad t = T, T - 1, \dots, 1 \\ U^*_{t - 1} &amp;= \max\left(\mathbb E^*(U^*_t \vert \mathcal F_{t - 1}), Y^*_{t - 1}\right) \\ &amp;\geq \mathbb E^*(U^*_t \vert \mathcal F_{t - 1}) \end{align*}\] <p>The variables \(U^*_{t} = U_t (1 + r)\) are called as <strong>super-martingales</strong>.</p> <p>Let us now delve deeper into the <em>necessary wealth</em> argument.</p> <h2 id="necessary-wealth">Necessary Wealth</h2> <p>We have \(Y = \{Y_0, \dots, Y_T\}\) representing the random variable for the payouts at each time \(t\). We formulated that</p> \[\begin{align*} U_T := Y_T \\ U_{t - 1} := \max\left(\frac{1}{1 + r} \mathbb E^*(U_t \vert \mathcal F_{t - 1}), Y_{t - 1}\right) \end{align*}\] <p>for \(t = T, T - 1, \dots, 1\). For each \(U_{t - 1}\),</p> \[\begin{align*} U_{t - 1} &amp;\geq \frac{1}{1 + r} \mathbb E^*(U_T \vert \mathcal F_{t - 1}) \\ U^*_{t - 1} &amp; \geq \mathbb E^*(U_t^* \vert \mathcal F_{t - 1}) \end{align*}\] <p>\(\{U_T^*: t = 0, 1, \dots, T\}\) form a \(p^*\)-supermartingale. Also note that, \(U^*_t \geq Y^*_t\) for \(t = 0, 1, \dots, T\). It is called as a <strong>Snell Envelope</strong> of \(\{Y_t\}\).</p> <p>Consider the probability space \((\Omega, \mathcal F, P)\) with the event spaces \((\mathcal F_t)_{t = 0}^T\). Then, we have the following claim -</p> <p><strong>Claim.</strong> There does not exist a supermartingale \((Z_t)\) such that</p> <ol> <li> <p>\(Z_t \geq Y_t\), for all \(t\)</p> </li> <li> \[Z_t = \max_{t \leq \tau \leq T} \mathbb E(Y_\tau \vert \mathcal F_t)\] </li> <li> <p>The maximum in the above equation is attained at \(\tau^*(t) = \min(s \geq t: Z_s = Y_S) \wedge T\)</p> </li> <li>The random variables \(Z_{T \wedge \tau^*(0)}\) for \(t = 0, 1, \dots, T\) is a martingale.</li> </ol> <p><strong><em>Proof.</em></strong> Define \(Z_t := \max_{t \leq \tau \leq T } \mathbb E(Y_T \vert \mathcal F_t)\). We can compute \(Z_t\) since it is a finite optimization problem. Since \(\{Y_t\}\) form a martingale, we get</p> \[Z_t \geq \mathbb[Y_T \vert \mathcal F_t] = Y_t\] <p>We use this result to show that \(Z_t\) form a supermartingale. Consider the stopping time \(\sigma\) between \(t\) and \(T\). Then, for attaining maximum</p> \[\begin{align*} Z_t &amp;= \mathbb E[Y_\sigma \vert F_t ]\\ \mathbb E[Z_t \vert \mathcal F_{t - 1}] &amp;= \mathbb E[\mathbb E[Y_\sigma \vert \mathcal F_t ] \vert \mathcal F_{t - 1}] \\ &amp;= \mathbb E[Y_\sigma \vert \mathcal F_{t - 1}] \leq Z_{t - 1} \end{align*}\] <p>How do we now show that this is minimal? \(Z_t\) being minimal implies that if \(Z_t\) is a supermarginale majorant of \(\{Y_t\}\), then \(X_t \geq Z_t\) for all \(t\).</p> \[\begin{align*} X_t(w) \geq Y_t(w) \forall t, \forall w \\ \implies X_{\tau(w)} (w) \geq Y_{\tau(w)}(w) , \forall w\end{align*}\] <p>Then, equivalently, \(X_\tau \geq Y_\tau\) for \(t \leq \tau \leq T\).</p> \[\begin{align*} X_t &amp;\geq \mathbb E(X_t \vert \mathcal F_t) \geq \mathbb E[Y_T \vert \mathcal F_t] \\ &amp;\geq \mathbb E[Y_T \vert \mathcal F_t], \forall t \end{align*}\] <p>Resulting in \(X_t \geq Z_t\). So far, we have shown (1) and (2) in the claim. Also, in the above proof, we use the following property. Let \(X_t\) be a supermartingale ifor \(t \leq \tau \leq T\). Then \(\mathbb E(X_\tau \vert \mathcal F_t) \leq X_t\). The proof is left as an exercise.</p> <p>How do we show (3) and (4) in the claim. For (4), we simply need to check the martingale property</p> \[\begin{align*} \mathbb E[Z_{(t + 1) \wedge \bar \tau} - Z_{t \wedge \bar \tau} \vert \mathcal F_t] &amp;\stackrel{?}{=} 0 \\ &amp;= \mathbb E[\mathbb 1_{t &lt; \bar \tau}(Z_{(t + 1) \wedge \bar \tau} - Z_{t \wedge \bar \tau} \vert \mathcal F_t] \\ &amp;= \mathbb E[\mathbb 1_{t &lt; \tau} (Z_{t + 1} = Z_t) \vert \mathcal F_t] \\ &amp;= \mathbb 1_{t &lt; \bar \tau} \left(\mathbb E(Z_{t + 1} \vert F_t) - Z_t\right) \\ \end{align*}\] <p>where \(\bar \tau(w) = \tau^*(0) (w)\). Now, rewriting the definition of \(Z_t\), we get</p> \[Z_t = \max \left(\mathbb E(Z_{T + 1} \vert \mathcal F_t, Y_t\right)\] <p>meaning for \(t &lt; \bar \tau \implies Z_t &gt; Y_t\) (from definition of \(\bar \tau\) in (3)) implying \(Z_t = \mathbb E(Z_{t + 1} \vert \mathcal F_t)\). This proves the required property.</p> <p>The last argument is to show that, given a supermartingale \(\tilde Z\) with \(\tilde Z_t \geq Y_t\) for all \(t\), then \(\tilde Z_t = Z_t\) for all \(t\). This proves uniqueness.</p> <p>We have</p> \[\begin{align*} \tilde Z_{t - 1} &amp;= \max(Y_{t - 1}, \mathbb E(\tilde Z \vert \mathcal F_{t - 1})) \\ &amp;\leq \max(Y_{t - 1}, \mathbb E(Z_t \vert \mathcal F_{t -1})) \\ &amp;\leq \max(Y_{t - 1}, Z_{t - 1}) = Z_{t - 1} \end{align*}\] <p>We have already shown that \((Z_t)_{t - 0}^T\) is the least supermartingale majorant of \((Y_t)_{t = 0}^T\). This proves uniqueness.</p> <h2 id="pricing-an-american-call">Pricing an American Call</h2> <p>Can we find a \(\phi^*\) such that \(V_t(\phi^*) \geq U_t\). We had derived the following before</p> \[\begin{align*} \tilde \alpha_t = \frac{U^u_t - U^d_t}{u - d} \frac{1}{S_{t - 1}} \\ \tilde \beta_t = \frac{uU^d_t - dU^u_t}{u - d} \frac{1}{B_{t - 1}} \end{align*}\] <p>An important to thing to note here is that these values are random variables and are measurable under \(\mathcal F_{t - 1}\). However, this strategy will not be self-financed due to the super-martingale property. The value of the portfolio is</p> \[\tilde \alpha_t S_t + \tilde \beta_t B_t \equiv U_t\] <p>Then we have,</p> \[\begin{align*} U_{t - 1} &amp;= Y_{t - 1} \vee \left(\frac{1}{1 + r} \mathbb E^*(U_t \vert \mathcal F_{t - 1})\right)\\ \tilde \delta_t &amp;:= U_{t - 1} - \frac{1}{1 + r} \mathbb E^*(U_t \vert \mathcal F_{T - 1}) \geq 0 \\ &amp;\because \tilde \delta_t \geq 0 \Leftrightarrow Y_{t - 1} &gt; \frac{1}{1 + r} \mathbb E^*(U_T \vert \mathcal F_{t - 1}) \end{align*}\] <p>Now, consider the following strategy</p> \[\begin{align*} \alpha^*_t &amp;= \tilde \alpha_t \\ \beta_t^* &amp;= \tilde \beta_t + \sum_{s = 1}^t \tilde \delta_s/B_{s - 1} \geq \tilde \beta_t \end{align*}\] <p>This trading strategy \(\phi^* = \{(\alpha^*, \beta^*): t = 1, \dots, T\}\) is a <strong>super-hedging strategy</strong> with \(V_t(\phi^*) \geq U_t\). We introduced \(\tilde \delta_t\) to make it a self-financing strategy.</p> <p>Define \(\bar \tau = \bar \tau(0) = \min(T: U_t = Y_T) \wedge T\). Then, \(U_{t - 1} &gt; Y_{t - 1}\) and \(\tilde \delta_t = 0\) for \(t = 1, \dots, \bar \tau(w)\). Consequently, \(V_{t \wedge \tau}(\phi^*) = U_{t \wedge \tau}\), and \(U^*_{t \wedge \tau} = \frac{U_{t \wedge \tau}}{(1 + r)^{t \wedge \tau}}\) forming a martingale.</p> <h2 id="arbitrage-1">Arbitrage</h2> <p>For a trading strategy \(\psi\), we have \(V_0(\psi) = 0\) and \(V_T(\psi) \geq 0\). Alternately, \(P(V_T(\psi) &gt; 0) &gt; 0\). In an American option, there is an asymmetry between the buyers and the sellers. The buyer has to choose only one time \(t \leq T\) to exit the option whereas the seller has to safeguard against an arbitrage in all possibilities.</p> <p>The <strong>seller’s arbitrage</strong> for a strategy \(\phi^s\) (in stocks and bonds) is then defined as</p> \[\begin{align*} V_0(\phi^s) &amp;= C_0 \\ \forall \tau V_\tau(\phi^s) &amp;\geq Y_\tau \\ &amp;&gt; \quad \quad \text{sometimes} \end{align*}\] <p>The seller sells an option for \(C_0\) and makes an investment of \(C_0\) in \(\phi_s\). Then an arbitrage for the seller is</p> \[V_T = (V_\tau(\phi^s) - Y_\tau)(1 + r)^{T - \tau}\] <p>where \(\tau\) is the time when the buyer exits the option. This value is always greater than or equal to \(0\) with a non-zero probability that it is strictly greater than \(0\).</p> <p>On the other hand, the <strong>buyer’s arbitrage</strong> for a strategy \(\phi_b\) is defined as</p> \[\begin{align*} V_0(\phi^b) &amp;= -C_0 \\ \exists \tau V_\tau(\phi^b) + Y_\tau &amp;\geq 0 \\ &amp;&gt; \quad \quad \text{sometimes} \end{align*}\] <p>The arbitrage at time \(T\) is \((V_\tau(\phi^b) + Y_\tau)(1 + r)^{T - \tau}\).</p> <p>What would be the good time to exercise the option? In the following sections, we’ll see that \(\tau = \bar \tau\) is the optimal time to exit the option.</p> <h3 id="example">Example</h3> <p>Consider a put option \(Y_T = (K - S_t)^+\) where \(K = 150, S_0 = 100, u = 2, d = 0.5, r = 0.2, T = 2\). We get \(U_0 = 53.09\) and \(Y_0 = 50\). Then, the super-hedging strategy is given by</p> \[\begin{align*} \end{align*}\] <p>To summarise, for a payout random variable \(Y = \{Y_0, \dots, Y_T\}\), we have \(U_t\) as the necessary wealth at each time \(t\). Then, the super-hedging strategy \(\phi^*\) has the following properties -</p> <ul> <li> <p>\(V^*_t(\phi^*)\) is a martingale</p> </li> <li> <p>\(V^*(\phi^*) \geq U^*_t \geq Y^*_t\) for all \(t\)</p> </li> </ul> <p>We define \(\bar \tau\) such that \(\bar \tau = \min\{t \vert U_t = Y_t) \wedge T\). With this definition, we have</p> \[V_t^*(\phi^*) = U_t^*\] <p>for \(t = 0, 1, \dots, \bar \tau\) and \(V_0(\phi^*) = U_0\).</p> <p><strong>Theorem.</strong> \(U_0\) is the no-arbitrage price for American call option based on \(Y\).</p> <p><em>Proof.</em></p> <ol> <li> <p>\(C_0 &gt; U_0\), then there’s arbitrage for the seller. The seller sells the claim for \(C_0\) and buys \(\phi^*\) for \(U_0\). The remaining amount, \(C_0 - U_0\) is invested in bonds. Then, for this strategy \(\phi^s\), we have</p> \[\begin{align*} V_t(\phi^s) &amp;= V_t(\phi^*)+ (C_0 - U_))B_t \\ V_\tau(\phi^s &amp;= V_\tau(\phi^*) + \underbrace{(C_0 - U_0)B_\tau}_{&gt;0} \\ &amp;\geq U_\tau + \dots \\ &amp;\geq Y_\tau + \dots \end{align*}\] </li> <li> <p>\(C_0 &lt; U_0\), we have a buyer’s arbitrage for the strategy \(\phi^b\)</p> \[\phi^b = \begin{cases} -U_0 &amp; \text{in } -\phi^* \\ U_0 - C_0 &amp; \text{in bond} \end{cases}\] <p>Then,</p> \[\begin{align*} V_0(\phi^b) &amp;= -C_0 \\ V_\tau(\phi^b) &amp;= - V_\tau(\phi^*) + (U_0 - C_0)B_\tau \end{align*}\] <p>Using \(\tau = \bar \tau\),</p> \[\begin{align*} V_{\bar \tau}(\phi^b) + Y_{\bar \tau} &amp;= -V_{\bar \tau}(\phi^*) + (U_0 - C_0)B_{\bar \tau} + Y_{\bar \tau} \\ &amp;= (U_0 - C_0) B_{\bar \tau} &gt; 0 \end{align*}\] </li> <li> <p>\(C_0 = U_0\). We need to show separately that there’s no seller’s and buyer’s arbitrage</p> <ol> <li> <p>No seller’s arbitrage - Suppose \(\phi^s\) is a seller’s arbitrage strategy. We have</p> \[V_0(\phi^s) = C_0, V_\tau(\phi^s) \geq Y_\tau\] <p>Note that all elements need not be strictly greater for an arbitrage. Computing the expectation,</p> \[\mathbb E^*(Y^*_\tau) &lt; \mathbb E^*(V^*_\tau(\phi^s)) = V_0(\phi^s) = U_0 = C_0\] <p>for all stopping times \(\tau\). However,</p> <blockquote> <p>why strictly less than in the above equation?</p> </blockquote> \[\mathbb E^*(Y^*_\tau) = \mathbb E^*(V_\tau^*(\phi^s)) = V_0(\phi^s) = C_0\] <p>This shows that no seller’s arbitrage is possible.</p> </li> <li> <p>No buyer’s arbitrage - Suppose \(\phi^b\) is such that</p> \[\begin{align*} V_0(\phi^b) &amp;= -C_0 \\ \exists \tau_0 V_{\tau_0} (\phi^b) + Y_{\tau_0} &amp;\geq 0 \quad (&gt; 0 \text{ sometimes}) \end{align*}\] <p>Then, computing the expectation</p> \[\begin{align*} \mathbb E^*(V^*_\tau(\phi^b)) &amp;= V_0(\phi^b) = -C_0 \\ \mathbb E^*(V_\tau^*(\phi^b) + Y_\tau^*) &amp;= -C_0 + \mathbb E^*(Y^*_\tau) \\ &amp;\leq -C_0 + \mathbb E^*(U^*_\tau) \\ &amp; \leq -C_0 + U_0 = 0 \end{align*}\] <p>This shows that no buyer’s arbitrage is possible. \(\square\)</p> </li> </ol> </li> </ol> <h3 id="example-1">Example</h3> <p>Consider anMerican Call \(S_0 = 4, r = 0.25, u = 2, d = 0.5, K = 4, T = 3\). This yields \(p^* = 0.5\). We have the following</p> \[S_t \implies \begin{align*} &amp;4 \\ 2 &amp;\quad 8 \\ 1 \quad &amp;4 \quad 16 \\ 0.5 \quad 2 &amp;\quad 8 \quad 32 \end{align*}\] \[Y_t \implies \begin{align*} &amp;0 \\ 0 &amp;\quad 4 \\ 0 \quad &amp;0 \quad 12 \\ 0 \quad 0 &amp;\quad 0 \quad 28 \end{align*}\] \[U_t = V_t \implies \begin{align*} &amp;\frac{64}{5} \\ \frac{16}{25} &amp;\quad \frac{144}{25} \\ 0 \quad &amp;\frac{8}{5} \quad \frac{64}{5} \\ 0 \quad 0 &amp;\quad 4 \quad 28 \end{align*}\] <h2 id="american-call-put-parity">American Call-Put Parity</h2> <p>The relation between call and put options in general can be shown using <strong>Jensen’s inequality</strong>. We have</p> \[\begin{align*} \mathbb E(\phi(X)) &amp;\geq \phi(\mathbb E(X)) \\ \mathbb E(\phi(X \vert \mathcal G)) &amp;\geq \phi(\mathbb E(X | \mathcal G)) \end{align*}\] <p>The payout functions for both call and put options are convex. We aim to show that \(U_t = V_t\) for \(t = T, T - 1, \dots, 1, 0\), using induction. The statement is true for \(t = T\).</p> <p>ASsume \(U_s = V_s\) for \(s = t, t + 1, \dots, T\), then</p> \[\begin{align*} U_{t - 1} &amp;= \max \left(Y_{t - 1} , \frac{1}{1 + r} \mathbb E^*(U_t \vert \mathcal F_{T - 1})\right)\\ &amp;= \max \left(Y_{t - 1} , \frac{1}{1 + r} \mathbb E^*(V_t \vert \mathcal F_{T - 1})\right)\\ \end{align*}\] <p>Substituting \(V_t = \frac{1}{(1 + r)^{T - \tau}} \mathbb E^*(V_t \vert \mathcal F_t)\),</p> \[\begin{align*} \mathbb E^*(V_t \vert \mathcal F_{t - 1}) &amp;= \frac{1}{(1 + r)^{T - t}} \mathbb E^*(V_T \vert \mathcal F_{t - 1}) \\ &amp;= \frac{1}{(1 + r)^{T - t}} \mathbb E^*((S_T - K)^+ \vert \mathcal F_{T - 1}) \\ &amp;\geq \frac{1}{(1 + r)^{T - t}} \left(\mathbb E^*(S_T \vert \mathcal F_{t - 1}) - K\right)^+ &amp; \because \text{Jensen's inequality}\\ &amp;= \frac{1}{(1 + r)^{T - t}} ((1 + r)^{T - t + 1}S_{t - 1} - K)^+ \\ &amp;= (1 + r)\left( S_{t - 1} - \frac{K}{(1 + r)^{T - t + 1}}\right)^+ \\ &amp;\geq (1 + r) (S_{t - 1} - K)^+ \\ &amp;= (1 + r) Y_{t - 1} \end{align*}\] <p>Using this result above, we have</p> \[\begin{align*} U_{t - 1} &amp;= \max \left(Y_{t - 1} , \frac{1}{1 + r} \mathbb E^*(V_t \vert \mathcal F_{T - 1})\right)\\ &amp;= \frac{1}{1 + r} \mathbb E^*(V_t \vert \mathcal F_{T - 1}) = V_{t - 1} \end{align*}\] <h1 id="finite-market-models">Finite Market Models</h1> <p>The binomial model we have discussed previously is an example of a finite market model. These structures can be generalised to markets with multiple stocks, bonds and contingent claims. The price evolution need not be binary We shall derive results for pricing similarly like before for this general market. To do the analysis, the notation for probability space, portfolio, value, trading strategies and arbitrage has to be defined -</p> <h4 id="probability-space">Probability Space</h4> <p>Again, we consider a finite probability space \((\Omega, \mathcal F, P)\), that is \(\#(\Omega)&lt; \infty)\) with \(\mathcal F = \mathcal P(\Omega)\) (the power set) and \(P(A) = \sum_{w \in A} P(w), A \in \mathcal F\).</p> <p>We shall assume that every event has a non-zero probability. If there are events with zero-probability, they are not relevant to our discussion and we can simply prune these points.</p> <p>A <strong>filtration</strong> \(\mathcal F_0 \subset \mathcal F_1 \subset \cdots \subset \mathcal F_T\) represents the history of the market or the evolution of prices of assets in the market. Typically, \(\mathcal F_T = \mathcal F\).</p> <h4 id="assets-in-portfolio">Assets in portfolio</h4> <p>Now in this setup, we consider the assets - asset 0 bond and assets \(1, 2, \dots, d\) stocks. At any time \(t\), the prices are given by</p> \[S_t = (S^0_t, S^1_t, \dots, S^d_T), t = 0, 1, \dots, T\] <p>Where \(S^0_t\) is the bond-price and not random and \(S_t^U\) for \(i &gt; 0\) represent the stock prices.</p> <p>Each sequence \(\{S_t^i\}_{t = 0}^T\) is \(\mathcal F_t measurable\), that is \((S_t)\) is adapted to the filtration.</p> <h4 id="trading-strategy">Trading strategy</h4> <p>After defining the prices and the structure of the portfolio, we define a <em>trading strategy</em> for this market -</p> \[\phi = \{\phi_t: t = 1, \dots, T\}\] <p>where \(\phi_t = (\phi_t^0, \phi_t^1, \dots \phi_t^d)\) is a vector of holdings representing the assets held at time \(t\). That is, \(\phi_t^I\) is the number of shares of asset \(i\) to be held on day \(t\).</p> <p>The basic assumptions with our trading strategies are</p> <ul> <li>The trading strategy is predictable - \(\phi_t^i \in \mathcal F_{t - 1}\) for all \(i\) and \(t = 1, 2, \dots, T\).</li> <li>Self-financing - \(\phi_t \cdot S_t = \phi_{t + 1} \cdot S_t\) (note that these are vector dot products \(\phi_t \cdot S_t = \sum_{i = 1}^d \phi_t^i S_t^i\)).</li> </ul> <h4 id="value-of-the-portfolio">Value of the portfolio</h4> <p>The value of the portfolio at time \(t = 0\) is given by \(V_0(\phi) = \phi_1 \cdot S_0\) and for time \(t &gt; 0\), we have \(V_t(\phi) = \phi_t \cdot S_t\). For this market, the change in the value of portfolio across a day is given by \(\Delta V_t(\phi)\)</p> \[\begin{align*} \Delta V_t(\phi) &amp;:= V_t(\phi) - V_{t - 1}(\phi) \\ &amp;= \phi_t \cdot S_t - \phi_{t - 1}\cdot S_{t - 1} \\ &amp;= \phi_t \cdot (S_t - S_{t - 1}) \\ &amp;= \phi_t \cdot \Delta S_t \end{align*}\] <p>So the value at any time \(t\) can now be defined as</p> \[\begin{align*} V_t(\phi) &amp;= V_0(\phi) + \sum_{u = 1}^t \sum_{i = 0}^d \phi_u^i \Delta S_u^i \quad \quad\because \quad \text{Telescoping sum} \\ &amp;=V_0(\phi) + \sum_{u = 1}^t \phi_u \cdot \Delta S_u \end{align*}\] <p>for \(t = 0, 1, \dots, T\)</p> <p>To help us derive results in the later sections, we define the following notation for <strong>discounted stock prices</strong> -</p> \[\begin{align*} S_t^* &amp;= S_t /S^0_t \\ &amp;= (1, S_t^{1*}, \dots, S_t^{d*}) \end{align*}\] <p>It essentially divides the stock prices with the bond price at time \(t\). Similarly we can define discounted value of a portfolio.</p> <p>We can derive that the telescopic sum property still holds with the discounted values for the portfolio. That is,</p> \[V_T^*(\phi) = V_0^*(\phi) + \sum_{u = 1}^t \phi_u \cdot \Delta S_u^*\] <p>where \(\Delta S_u^* = S_u^* - S_{u - 1}^*\). This property will be useful for extracting Martingales from the price evolution tree. Specifically, we have the following lemma</p> <p><strong>Lemma 1</strong> If \((M_t: t = 0, 1, \dots, T)\) is a martingale and \((H_1,H_2, \dots, H_T)\) is a predictable sequence of random variables then</p> \[X_t := \sum_{u = 1}^T H_u \Delta M_u\] <p>is a martingale!</p> <h4 id="arbitrage-2">Arbitrage</h4> <p>The expressions are very similar to before wherein an arbitrage opportunity (risk-free profit) is a trading strategy \(\phi\) such that</p> \[\begin{align*} V_0(\phi) &amp;= 0 \\ V_T(\phi) &amp;\geq 0 \\ P(V_T(\phi) &gt; 0) &amp;&gt; 0 \end{align*}\] <p>The last condition essentially ensures that there is atleast one case where the profit is non-zero. Previously, we had used expectation to ensure this using the \(p^*\) model derived using the binomial model. However, we want a general scenario here and an expectation requires an associated probability distribution.</p> <p>The market is set to be <strong>viable</strong> if there are no arbitrage opportunities. We can show that the binomial model considered previously is viable if \(d &lt; 1 + r &lt; u\). So, when is a market non-viable? Consider \(\Omega = \{w_1, w_2\}, T = 1, d = 2\) with the stock price evolution \(S_0^0 = S_1^0 = 1\), \(S_0^1 = S_0^2 = 1\), and</p> <table> <thead> <tr> <th> </th> <th>( w_1 )</th> <th>( w_2 )</th> </tr> </thead> <tbody> <tr> <td>( S_1 )</td> <td>1</td> <td>2</td> </tr> <tr> <td>( S_2 )</td> <td>1</td> <td>3</td> </tr> </tbody> </table> <p>The trading strategy \(\phi = \{(0, -1, 1)\}\) has \(V_1(\phi) \geq 0\) in all entries and \(&gt;0\) in at least one entry.</p> <p>Notice the subtleties in this example. Previously, in the CRR model, we focused on finding an arbitrage-free pricing. We ensured there is no arbitrage in the market with this design. However, considering a trading strategy with only stocks in a bullish market, there is always an opportunity for arbitrage.</p> <p>For trading strategies with only stocks, the problem of finding a trading strategy boils down to solving some linear inequalities.</p> <h2 id="martingale-measure">Martingale Measure</h2> <p>As noted earlier, we will assume that \(P(w) &gt; 0\) or each \(w \in \Omega\). For a new measure \((\Omega, \mathcal F, Q)\), we say that \(Q\) is equivalent to \(P\) (\(Q \sim P\)) if for all \(A \in \mathcal F\)</p> \[Q(A) &gt; 0 \iff P(A) &gt; 0\] <p>With this infrastructure, we are aiming to find a new probability \(Q\) (like \(p^*\) in CRR model) to prove properties in a market.</p> <p><strong>Definition.</strong> A probability \(Q\) on \((\Omega, \mathcal F)\) is a <strong>martingale measure</strong> provided \(\{S_T^{i*}: t = 0, 1, \dots, T\}\) is a \(Q-martingale\) for each stock \(I = 1, \dots, d\).</p> <p>An <strong>equivalent martingale measure</strong> is a probability \(Q\) such that \(Q \sim P\) and \(Q\) is a martingale measure. With these tools, we now define the first fundamental theorem of asset pricing.</p> <h2 id="first-fundamental-theorem-of-asset-pricing">First Fundamental Theorem of Asset Pricing</h2> <p>A market is viable if and only if there exists at least one Equivalent Martingale Measure (EMM).</p> <p>To understand this better, consider the previous example of a market that is not viable. Suppose we have a martingale measure \(Q\), then</p> \[\mathbb E^Q(S_i^1) = S_0^i\] <p>for \(I = 1, 2\) using the martingale property. Letting \(a = Q(w_1), b = Q(w_2)\), we get the linear equations</p> \[\left. \begin{align*} a + 2b &amp;= 1\\ a + 3b &amp;= 1 \end{align*} \right\} b = 0\] <p>Since, $P(w_2) &gt; 0 \not\imply Q(w_2) &gt; 0$$, there cannot exist an Equivalent Martingale Measure in this market.</p> <h4 id="proof">Proof</h4> <p>(\(\Longleftarrow\)) Let \(Q\) be an EMM and \(\phi\) is a strategy making an arbitrage. That is, \(V_0(\phi) = 0, V_T(\phi) \geq 0\)</p> <p>Since \(\{S_t\}_{t = 1}^T\) is a martingale under \(Q\), the sequence \(\sum_{i = 1}^t \phi_u^I \cdot \Delta S_u^{i*}\) for \(t = 0, 1, \dots, T\) is a \(Q\)-martingale.</p> \[\begin{align*} V^*_T(\phi) &amp;:= V_T(\phi)/S_T^0 \\ &amp;= V_0^* + \sum_{u = 1}^T \phi_u \cdot \Delta S_u^* \\ &amp;= 0 + \sum_{u - 1}^T \sum_{i = 1}^d \phi_u^I \cdot \Delta S_u^{i*} \end{align*}\] <p>\(V_t^*\) is also a martingale under \(Q\), implying \(\mathbb E^Q(V_T^*(\phi)) = \mathbb E^Q(V_0^*(\phi)) = 0\). Since \(P \sim Q\), \(P(V_T(\phi) &gt; 0) = 0\).</p> <p>\((\implies)\) Now, we show the opposite direction, that is for a viable market, there exists an EMM.</p> <p>We have \(\#(\Omega) &lt; \infty\), where \(\Omega = \{w_1, w_2, \dots, w_n\}\). Consider a random variable \(X: \Omega \to \mathbb R\), then we have \((X(w_1), X(w_2), \dots, X(w_n) \in \mathbb R^n)\) and a distribution \(Q\) with probabilities \((Q(w_1), \dots, Q(w_n))\). Then, we have</p> \[\mathbb E^Q(V^*_T(\phi)) = \sum_{k = 1}^n Q(w_k) V^*_T(\phi) (w_k) = Q \cdot V^*_T(\phi)\] <p>It suffices to show that there exists a \(Q\) such that \(Q \perp \{V_T^*(\phi): \phi \text{ is a trading strategy with } V_0(\phi) = 0\} \subset \mathbb R^n\}\).</p> <p>We have seen before that as a result of a telscopric sum formulation, \(V^*_T(\phi) = V_0^*(\phi) + \sum_{u = 1}^T \phi_u \cdot \Delta S_u^*\).</p> <p><strong>Lemma 2</strong></p> <p>Suppose a \(M = (M_1, \dots, M_n)\) is an adapted sequence such that</p> \[\mathbb E\left(\sum_{u = 1}^T q_u \cdot \Delta M_u\right) = 0; \quad \Delta M_k = M_k = M_{k - 1}\] <p>for all predictable \((q_1, \dots, q_T)\), then \(M\) is a martingale.</p> <p><em>Proof.</em> To show, \(\mathbb E(M_t \vert \mathcal F_{t - 1}) = M_{t - 1}\) for \(t = 1, 2, \dots, T\). Equivalently, \(\mathbb E(M_t \cdot \mathbb 1_A) = \mathbb E(M_{t - 1} \cdot \mathbb 1_A)\) for all \(A \in \mathcal F_{t - 1}\).</p> <p>If \(X, Y\) are both \(\mathcal G\)-measurable and \(\mathbb E(X \cdot \mathbb 1_A) = \mathbb E(Y \cdot \mathbb 1_A)\) for all \(A \in \mathcal G\), then \(P(X = Y) = 1\) (Consider \(\{X &lt; Y\} = A\)).</p> <p>In our context \(X = \mathbb E(M_t \vert \mathcal F_{t - 1})\) and \(Y = M_{t - 1}\), leading to \(\mathbb (\Delta M_t \cdot \mathbb 1_A) = 0\). Fixing \(t\) and \(A \in \mathcal F_{t - 1}\), let</p> \[q_u = \begin{cases} \mathbb 1_A &amp; u = t \\ 0 &amp; u \neq t \end{cases}\] <p>Consequently,</p> \[\begin{align*} \sum_{u = 1}^T q_u \cdot \Delta M_u = \mathbb 1_A \cdot (M_t - M_{t - 1}) \end{align*}\] <p><strong>Lemma 3</strong> If we have, \(q_t^i \in \mathcal F_{t - 1}\) for \(i = 1, \dots, d\) amd \(t = 1, \dots, T\), then for any \(c \in \mathbb R\), there exists \(\phi^0_t\) for \(t = 1, \dots, T\) such that</p> \[\phi = \{(\phi_t^0, \phi_t^1, \dots, \phi_t^d): t = 1, \dots, T\}\] <p>is a <strong>self-financing</strong> trading strategy and \(V_0(\phi) = c\).</p> <p><strong>Proposition.</strong> Suppose \(Q \sim P\), such that \(\mathbb E^Q(V_T^*(\phi)) = 0\) for all trading strategies \(\phi\) with \(V_0(\phi) = 0\), then \(Q\) is an EMM. That is, we need to show \(S^{1*}_t\) is a Q-martingale. From lemma 2, it is equivalent to showing \(\mathbb E^Q(\sum_{u = 1}^T q_u \cdot \Delta S_u^{1*}) = 0\) for all \((q_1, \dots, q_T)\). Consider the following strategy</p> \[\phi^i_t = \begin{cases} q_t &amp; i = 1 \\ 0 &amp; i = 2, 3, \dots, d \\ \phi_t^0 \text{ from Lemma 3} \end{cases}\] <p>Then,</p> \[\begin{align*} \sum_{u = 1}^T q_u \Delta M_u &amp;= \mathbb 1_A (M_t - M_{t - 1}) \\ V^*_T(\phi) &amp;= \sum_{i = 0}^d \sum_{u = 1}^T (\phi_u^i \cdot S_u^{i*}) \\ &amp;= \sum_{u = 1}^T q_u \cdot \Delta S_u^{1*} \end{align*}\] <p>Now, for all trading strategies with the self-financing property, the set \(L:= {V_T^*(\phi): \phi \text{ with } V_0(\phi) = 0} \subset \mathbb R^n\) is a linear subspace of \(\mathbb R^n\). Similarly, consider the set \(D := \{Y \in \mathbb R^n : Y \geq 0 \text{ with } Y \neq 0\}\).</p> <p>Our goal boils down to show that a viable market exists if and only if \(L \cap D = \phi\). It is equivalent to show \(L \cap F = \phi\) for \(F:= \{Y \in D: \sum_{i = 1}^n Y_k = 1\}\).</p> <h4 id="assume-viability--">Assume viability -</h4> <p>We have noted that \(L\) is a subspace of \(\mathbb R^n\). Also, \(F\) is compact and convex.</p> <p><strong>Lemma.</strong> <strong><em>Separating Hyperplane Theorem</em></strong> For a vector subspace \(L \subseteq \mathbb R^n\) and a compact convex set \(F\), if \(L \cap F = \phi\), then there exists a \(\zeta \in \mathbb R^n\), \(\zeta \neq 0\) such that</p> \[\begin{align*} L &amp;\subset \{t: t \cdot \zeta = 0\} \\ F &amp;\subset \{y: y \cdot z &gt; 0\} \end{align*}\] <p><em>Proof.</em> Let \(G:= F - L = \{f - l: f \in F, l \in L\}\), then \(G\) is convex and closed and \(0 \not \in G\). We choose the vector \(\zeta \in G\) that minimizes \(\|x\|\), \(x \in G\). Now,</p> \[\beta := \inf \{\|x \|: x \in G \}\] <p>then for any sequence \(x_n \in G\) such that \(\|x_n\|\) converges to \(\beta\) (\(\|x_n\| \searrow \beta\)) , using the parallelogram law</p> \[\begin{align*} \|x_n - x_m\|^2 &amp;= 2(\|x_n\|^2 + \|x_m\|^2) - 4\left\|\frac{x_n + x_m}{2}\right\|^2 \\ \end{align*}\] <table> <tbody> <tr> <td>The terms \(|x_n|^2, |x_m|^2\) converge to \(\beta^2\) and the average $$\left</td> <td>\frac{x_n + x_m}{2}\right</td> <td>^2\(goes to\)\beta^2\(leading to\)|x_n - x_m|^2 \to 0\(for\)m, n \to \infty\(and\)x_n \to \zeta\(. Therefore,\){x_n}\(form a Cauchy sequence convergent to some\)\zeta\(. Since\)G\(is closed, this shows the existence of such\)\zeta \in G$$.</td> </tr> </tbody> </table> <p>Now, for any \(x \in G\), \(x \cdot \zeta \geq \|\zeta\|^2\). How do we show this? Consider any arbitrary vector formed by \(x, \zeta\), \(\|\alpha x + (1 - \alpha)\zeta\|^2 - \|\zeta\|^2 \geq 0\) (a quadratic in \(\alpha\), and \(0 \leq \alpha \leq 1\)). For \(\alpha \to 0\), we get</p> \[-2 \|\zeta^2\| + 2(x\cdot \zeta) \geq 0\] <p>Now, in particular, since \(0 \in L\), if \(f \in G\), then \(f \in F\). Consequently, if \(f \in F\), then for any \(\lambda \in \mathbb R, l \in L\), \(f - \lambda l \in G\). We get</p> \[f\cdot \zeta \geq \lambda (l \cdot \zeta) + \|\zeta\|^2\] <p>With the limits of \(\lambda\), we can conclude that \(l \cdot \zeta = 0\). \(\square\)</p> <p>How does this theorem help us show \(L \cap F = \phi\) for a viable market? Consider the EMM \(Q(w_k) = \frac{\zeta_k}{c} &gt; 0\). \(c = \sum_{i = 1}^n \zeta_i\). For a fixed \(k\), consider</p> \[Y_i = \begin{cases} 1 &amp; i = k \\ 0 &amp; i \neq k \end{cases}\] <p>Then for \(Y \in F\), \(Y \cdot \zeta &gt; 0\). Let \(\phi\) be a trading strategy with \(V_0(\phi) = 0\), then</p> \[\begin{align*} \mathbb E^Q(V_t^*(\phi)) &amp;= Q\cdot V^*_T(\phi) \\ &amp;= \frac{1}{c} \left[\zeta \cdot \underbrace{V_t^*(\phi)}_{\in L}\right] = 0 \end{align*}\] <h3 id="completeness-of-a-market">Completeness of a Market</h3> <p>A merket is complete if for every contingent claim \(X \in \mathcal F_T\), there exists a trading strategy \(\phi\) that can be replicate the claim with \(V_T(\phi) \equiv X\).</p> <h2 id="second-fundamental-theorem-of-asset-pricing">Second Fundamental Theorem of Asset Pricing</h2> <p>Assuming the market is viable, then the market is complete if and only if there is a unique EMM for the market.</p> <p><em>Proof.</em></p> <ol> <li> <p>\((\implies)\) Assume that the market is complete. Let \(P_1^*, P_2^*\) be the distinct EMMs. Then for a \(A \in \mathcal F_T\), define \(X:= \mathbb 1_A\). Due to completeness, \(\exists \phi\) with \(V_T(\phi) = \mathbb 1_A\).</p> \[\begin{align*} P_i^*(A) &amp;= \mathbb E_i^*(\mathbb 1_A) \\ &amp;= \mathbb E_1^*(V_T(\phi)) = \frac{S^0_T}{S^0_0} \cdot V_0(\phi) \end{align*}\] <p>Therefore, \(P_1^*(A) = P_2^*(A)\) for all \(A \in \mathcal F_T\) implying that \(P_1^* = P_2^*\).</p> </li> <li> <p>(\(\Longleftarrow\) ) Assuming that there is a unique EMM, we need to show that the market is complete. We do this by proving the contrapositive - a market is incomplete implies that there is no unique EMM for the market. We define</p> \[L:= \{V_T^*(\phi): \phi \text{ self-financing trading strategies }\}\] <p>Now, we show that the market is incomplete if and only if \(L \subsetneq \mathbb R^n\) and \(\exists z \neq 0\) such that \(z \perp L\). Consider the EMM \(Q\) for a viable market. Then, define \(\tilde Q(w_k) = Q(w_k) + c z_k\) for some \(c &gt; 0\). Then,</p> \[\sum_{k = 1}^n \tilde Q(w_k) = \sum_{k = 1}^n Q(w_k) + c (z \cdot \bf{1}) = 1 + 0\] <p>Is \(\bf{1} \in L\)? Yes, the strategy of investing in one bond yields a discounted value of \(1\) at \(T\).</p> </li> <li></li> </ol> <p>    Now, it is left to show that there exists a \(c\) such that \(\tilde Q(w_k) &gt; 0\). Consider the following formulation</p> \[c:= \frac{1}{2 \max_{1 \leq k \leq n} \left(\frac{\|z_k\|}{Q(w_k)}\right)}\] <p>Then,</p> \[\begin{align*} \frac{1}{2c} &amp;= \max_k \left(\frac{\|z_k\|}{Q(w_k}\right) \\ Q(w_k) &amp;\geq 2c \|z_k\| \\ \tilde Q(w_k) &amp;= Q(w_k) + cz_k \\ &amp;\geq 2c \|z_k\| + cz_k &gt; 0 \end{align*}\] <p>Finally, to show that \(\tilde Q\) is an EMM,</p> \[\begin{align*} \mathbb E^{\tilde Q}(V_T^*(\phi)) &amp;\stackrel{?}{=} 0 \quad \text{ if} V_0(\phi) = 0\\ &amp;= \mathbb E^Q (V_T^*(\phi)) + c z \cdot V_T^*(\phi) \quad \because z \perp L \\ &amp;= 0 \end{align*}\] <p>This proves the second fundamental theorem of asset pricing.</p> <p>Note that if \(Q_1, Q_2\) are two EMMs, then a linear combination of these \(\alpha Q_1(A) + (1 - \alpha) Q_2(A)\) for \(A \in \mathcal F\) and \(0&lt;\alpha&lt;1\) is also an EMM.</p> <h3 id="example-2">Example</h3> <p>For \(T = 1, \Omega = \{w_1, w_2,w_3\}\), let \(S_1^0 = S_0^0 = 1\), \(S_0^1 = 2\) and \(S_1^1 = \begin{pmatrix}1 &amp; 3 &amp; 5\end{pmatrix}^T\). To check if the market is viable, we try and solve for an EMM on this market - Let \(Q = (a, b, c)\), then</p> \[\begin{align*} a + b + c = 1, a &gt; 0,b&gt;0,c&gt;0\\ a + 3b + 5c = 2 \end{align*}\] <p>Solving, we get</p> \[Q^{(c)} = \left(\frac{1}{2} + c, \frac{1}{2} - 2c, c\right)\] <p>for \(0 &lt; c &lt; \frac{1}{4}\). This shows that the market is viable. Also, the market is not viable since there are multiple EMMs possible.</p> <p>For a contingent claim \(X\), we have</p> \[\begin{align*} \mathbb E^{(c)}(X^*) &amp;= \left(\frac{1}{2} + c\right)x_1 + \left(\frac{1}{2} - 2c\right)x_2 + cx_3 \\ &amp;= \frac{x_1 + x_2}{2} + c (x_1 + x_3 - 2x_2) \end{align*}\] <p>That means, when \(x_1 + x_3 - 2x_2\) is \(0\), then the expectation is independent of \(c\) - \(X\) can be replicated in some cases even if the market is incomplete!</p> <p>Let us try an replicate such an \(X\). Consider the bond holdings \(\alpha\) and stock holdings \(\beta\).</p> \[\begin{align*} \alpha S_1^1 + \beta S_1^0 &amp;= X \\ \alpha \begin{pmatrix}1 \\ 3 \\ 5\end{pmatrix} + \beta \begin{pmatrix}1 \\ 1 \\ 1\end{pmatrix} &amp;= \begin{pmatrix}x_1 \\ x_2 \\ x_3\end{pmatrix} \end{align*}\] <p>\(X\) is replicable iff \(X \in \text{span}(\begin{pmatrix}1 &amp; 3 &amp; 5\end{pmatrix}, \begin{pmatrix}1 &amp; 1 &amp; 1\end{pmatrix}) \equiv X \perp \begin{pmatrix}1 &amp; -2 &amp; 1\end{pmatrix}\). This again yields \(x_1 + x_3 - 2x_2\).</p> <p><strong>Theorem.</strong> In a viable and complete market, the number V_0(\phi) is the no-arbitrage price of a European Call Clain X = V_T(\phi).</p> <p><em>Note.</em> We have seen this before in the context of a binomial model, but this theorem is for a general market with discrete possibilities. In our previous proof, we did not use any binomial properties but rather showed the result using martingale properties. The same proof follows for this as well.</p> <p>How do we conclude such results in a general case? Even when the market is incomplete, how do we find contingent claims which can be replicated?</p> <p>Consider the set \(M = \{Q: Q \text{ is an EMM}\} \neq \phi\). Then for any measure in this set,</p> \[\begin{align*} \mathbb E^Q(V_T^*(\phi)) &amp;= V_0^*(\phi) \\ &amp; = \frac{V_0(\phi)}{S_0^0} \end{align*}\] <p>Now, for a contingent claim X, we have</p> \[\begin{align*} V_+(X)&amp;:= \inf \{V_0(\phi):V_T(\phi)\geq X\} \\ &amp;= \min \{V_0(\phi):V_T(\phi)\geq X\} \\ \hline \mathbb E^Q(X^*) &amp;\leq \mathbb E^Q(V_T^*(\phi)) \sout{S_T^0} = V_0(\phi) \frac{\sout{S_T^0}}{S_0^0} \\ &amp;\leq \frac{1}{S_0^0} V_+(X) \end{align*}\] <p>Similarly,</p> \[\begin{align*} V_-(X)&amp;:= \sup \{V_0(\phi):V_T(\phi)\leq X\} \\ &amp;= \max \{V_0(\phi):V_T(\phi)\leq X\} \\ \hline \\ V_-(X) &amp;\leq S_0^0 \mathbb E^Q(X^*) \leq V_+ (X) \end{align*}\] <p>Suppose \(X\) is replicable, then \(X = V_T(\tilde \phi)\) for some \(\tilde \phi\). Then,</p> \[\begin{align*} V_0(\tilde\phi) \geq V_+(X), V_0(\tilde\phi) \leq V_-(X) \\ V_-(X) \leq V_+(X) \end{align*}\] <p>leading to \(V_+(X) = V_-(X)\)!</p> <p>Does the converse hold? In the previous example, say we have a contingent claim \(X = \begin{pmatrix}1 &amp; 9 &amp; 25\end{pmatrix} = (S_1^1)^2\). Now consider a strategy \(\phi = (\alpha, \beta)\), we have</p> \[\begin{align*} V_0(\phi) &amp;= 2\alpha + \beta \\ V_1^1(\phi) &amp;\leq X \quad \text{ for } V_-(X) \\ \alpha \begin{pmatrix} 1 \\ 3 \\5 \end{pmatrix} + \beta \begin{pmatrix} 1 \\ 1 \\ 1\end{pmatrix} &amp;\leq \begin{pmatrix}1 \\9 \\ 25\end{pmatrix} \end{align*}\] <p>Solving these inequalities, we get \(V_-(X) = 5\), and similarly, \(V_+(X) = 7\). Let us check if \(X\) can be replicated</p> \[\mathbb E^{(c)}(X) = 5 + 8c, 0 &lt; c&lt; \frac{1}{4}\] <p>It is not possible for the expectation to be independent of \(c\). Also note that the range of the above expression is exactly \((5, 7)\).</p> <p>We have seen that \(V_+(X) \geq V_-(X)\) for a contingent claim \(X\).</p> <ul> <li> <p>If \(C_0 &gt; V_+(X)\) then there is a seller’s arbitrage. How? There exists a strategy \(\phi\) such that \(V_T(\phi) \geq X\) and \(V_0(\phi) = V_+(X) &lt; C_0\).</p> </li> <li> <p>Similarly. when \(C_0 &lt; V_-(X)\) there is a buyer’s arbitrage.</p> </li> </ul> <p><strong>Theorem.</strong> A contingent claim \(X\) is replicable <strong>if and only if</strong> \(V_+(X) = V_-(X)\)</p> <p><em>Proof.</em></p> <ul> <li> <p>(\(\implies\)) We saw that previously</p> </li> <li> <p>(\(\Longleftarrow\)) Assume \(V_+(X) = V_-(X)\). Let \(\phi^+\) be a trading strategy such that \(V_T(\phi^+) \geq X\) and \(\phi^-\) is such that \(V_T(\phi^-) \geq X\). However, \(V_0(\phi^+) = V_+(X) = V_-(X) = V_0(\phi^-)\) and consequently \(V_T(\phi^+) - V_T(\phi^-) \geq 0\). Let \(Q \in M\) be any martingale measure -</p> \[\begin{align*} \mathbb E^Q(V_T^*(\phi^+) - V_T^*(\phi^-)) &amp;= V_0^*(\phi^+) - V_0^*(\phi^-) \\ &amp;= (V_0(\phi^+) - V_0(\phi^-))/S_0^0 = 0 \end{align*}\] <p>Therefore, \(V_T(\phi^+) \equiv V_T^(\phi^-) \equiv X\) implying that \(\phi^+\) can be used to replicate \(X\).</p> <p>Suppose \(X\) is not replicable, then there is \(\phi\) such that \(V_T(\phi) \leq X\) and \(V_0(\phi) = V_+(X)\).</p> <p><strong>Claim.</strong> \(P(V_T(\phi) &lt; X) &gt; 0\). If it is equal to \(0\), then \(V_T(\phi) \equiv X\).</p> <p>So, any market price for \(X\) with \(C_) \not \in (V_-(X), V_+(X))\) yields an arbitrage.</p> </li> </ul> <p><strong>Theorem.</strong> \(X\) is replicable <strong>if and only if</strong> \(\mathbb E^Q(X)\) is the same for all \(Q \in M\).</p> <p><em>Proof</em>.</p> <ul> <li> <p>\((\implies)\) \(X = V_T(\phi)\) then</p> \[\mathbb E^Q(X) = \mathbb E^Q(V_T(\phi)) = V_0(\phi) \frac{S_T^0}{S_0^0}\] </li> <li> <p>\((\Longleftarrow)\) Assume for contradition \(X\) is not replicable. As we’ve constructed before, let \(L = \{V_T(\phi): \phi \text{ is a trading strategy}\}\). Since \(X\) is not replicable, \(X \not \in L\). Let \(X = X_0 + Z\) such that \(X_0 \in L\) and \(Z \neq 0, \perp L\). For any \(Q \in M\), we define \(\tilde Q(w_k) = Q(w_k) + c Z_k\). Again, we choose a \(c\) such that \(\tilde Q &gt; 0\) and \(\tilde Q \in M\). Then, since \(\mathbb E^Q(X)\) is the same for all \(Q \in M\), we have</p> \[\begin{align*} \mathbb E^Q(X) &amp;= \mathbb E^{\tilde Q}(X) \\ Q \cdot X &amp;= \tilde Q \cdot X = Q \cdot X + c Z \cdot X \\ &amp;\therefore Z \cdot X = 0 \end{align*}\] <p>implying that \(X \in L\) giving a contradiction.</p> </li> </ul> <h3 id="dual-formulae">Dual Formulae</h3> <p>As a result of the above theorem, we can give alternate formulae to \(V_+(X)\) and \(V_-(X)\) (dual comes from linear programming)</p> \[\begin{align*} V_+(X) &amp;= \sup \{\mathbb E^Q(X): Q \in M\} \\ V_-(X) &amp;= \inf \{\mathbb E^Q(X) : Q \in M\} \end{align*}\] <h1 id="random-walk">Random Walk</h1> <p>In this section, we shall extend our theory beyond finite market models, wherein the price evolution is described using a random walk.</p> <p>Let \(X_n = \xi_1 + \dots + \xi_n\) where \(\mathbb E(\xi_k) = \mu\) and \(\xi\) are iid for \(n = 0, 1, 2, \dots\). Let \(\mathcal F_n = \sigma(\xi_1, \dots, \xi_n)\) and \(\mathcal F_n \perp \!\!\! \perp \xi+{n + k}\) for \(k \geq 1\). Then, consider set of random variables \(M_n = X_n - n \mu\).</p> <p><strong>Claim.</strong> \(M_n\) is a martingale with respect to \(\mathcal F_n\).</p> <p><em>Proof.</em></p> \[\begin{align*} \mathbb E[M_{n + 1} \vert \mathcal F_n] &amp;= \mathbb E[X_{n + 1} \vert \mathcal F_n] - (n + 1)\mu\\ &amp;= \mathbb E[X_n + \xi_{n + 1} \vert \mathcal F_n] - (n + 1)\mu\\ &amp;= X_n + \mu - (n + 1)\mu = M_n \end{align*}\] <p>Assuming that \(var(\xi_k) = \sigma^2\) is finite, consider \(Y_n = M_n^2 - n\sigma^2\).</p> <p><strong>Claim.</strong> \(Y_n\) is a martingale with respect to \(\mathcal F_n\) when \(\mu = 0\)</p> <p><em>Proof.</em></p> \[\begin{align*} \mathbb E[Y_{n + 1} \vert \mathcal F_n]&amp;= \mathbb E[M^2_{n + 1} \vert \mathcal F_n] - (n + 1)\sigma^2\\ &amp;= \mathbb E[(M_n + \xi_{n + 1})^2 \vert \mathcal F_n] - (n + 1)\sigma^2\\ &amp;= M_n^2 + M_n \mu + \sigma^2 - (n + 1)\sigma^2 = Y_n \end{align*}\] <h2 id="optional-stopping">Optional Stopping</h2> <p>Let \(\tau\) be a random variable representing stopping time. Then, we have</p> \[\mathbb E[M_\tau] = \mathbb E(M_0)\] <p>provided \(\tau(w) \leq N\). That is, \(M_{n \wedge \tau}\) is a martingale..</p> <h3 id="single-random-walk">Single Random Walk</h3> <p>We have the following setup</p> \[X_n = x + \xi_1 + \dots + \xi_n\] <p>where \(\xi_k\) are i.i.d. Let \(P(\xi= 1) = p\) and \(P(\xi_k = -1) = q = 1 - p\) where \(0 &lt; p &lt; 1\). We then have</p> \[\mu = 2p - 1, \sigma^2 = 1 - (2p - 1)^2\] <p>Let \(\tau_b := \min(n \geq 0: X_n = b)\) and \(\tau = \min(\tau_a, \tau_b)\).</p> <p><strong>Claim.</strong> Suppose \(p &gt; \frac{1}{2}\) and \(\mu &gt; 0\) then</p> \[\begin{align*} P_x(\tau_b &lt; \infty) = 1 \quad b \geq x \\ P_x(\tau_a &lt; \infty) &lt; 1\quad a &lt; x \end{align*}\] <p><em>Proof.</em> We construct a martingale of the form \(M_n = \alpha^{X_n} \beta^n\) for some \(\alpha &gt; 0\).</p> <p>For \(\alpha &gt; 0\), we let \(\frac{1}{\beta} = \mathbb E(\alpha^{\xi_k}) = \alpha p + \frac{1}{\alpha}(1 - p)\). Then,</p> \[\begin{align*} \mathbb E(M_{n + 1} \vert \mathcal F_n) &amp;= \alpha^{X_n} \beta^{n + 1} \mathbb E(\alpha^{\xi_{n + 1}} \vert \mathcal F_n) \\ &amp;= \alpha^{X_n} \beta^{n + 1} \frac{1}{\beta} = \alpha^{X_n} \beta_n = M_n \end{align*}\] <p>That concludes the proof.</p> <p><strong>Claim.</strong> When \(p \geq \frac{1}{2}\), then \(P_x(\tau_b &lt; \infty) = 1\) where \(\tau_a = \min(n \geq 0: X_n = a)\) (\(=\infty\) if no such \(n\)). Note that we are not dealing with finite timeframe anymore.</p> <p>The graph of \(\beta\) looks like</p> <p><em>Proof.</em> So for \(b \geq X\), we choose \(0 &lt; \alpha &lt; 1\) such that \(\beta &gt; 1\), then</p> \[\begin{align*} \alpha^X &amp;= \mathbb E_X(M_{n \wedge \tau_b}) \\ &amp;= \mathbb E_X(\alpha^{X_{n \wedge \tau_b}} \beta^{n \wedge \tau_b}) \\ &amp;\geq \mathbb E_X (\alpha^b \beta^{n \wedge \tau_b}) \geq \alpha^b \mathbb E_x(\beta^{n \wedge \tau_b}; \tau_b = \infty) \\ &amp;= \alpha^b \beta^n P_X(\tau_b = \infty) \end{align*}\] <p>Letting \(n \to \infty\), we infer \(P_X(\tau_b = \infty) = 0\).</p> <p><strong>Claim.</strong> When \(p &gt; \frac{1}{2}\), then \(P_x(\tau_a &lt; \infty) = \left(\frac{1}{p}\right)^{x - a}\).</p> <p><em>Proof.</em> For \(\frac{q}{p} &lt; \alpha &lt; 1\), \(0 &lt; \beta&lt; 1\)</p> \[\begin{align*} \alpha^X &amp;+ \mathbb E_X(M_{n \wedge \tau_a} ) \\ &amp;= \mathbb E_X(\alpha^{X_n}\beta^n ; n &lt; \tau_a) + \mathbb E_X(\alpha^{X_{\tau_a} \beta^{\tau_a}}; n \geq \tau_a) \\ &amp;\leq \alpha^a \beta^n P_X(n &lt; \tau_a) + \alpha^a\mathbb E_X (\beta^{\tau_a}; \tau_a \leq n ) \\ \end{align*}\] <p>The second term can be simplified using the <strong>Monotone Convergence Theorem</strong> -</p> \[\begin{align*} \mathbb E_X (\beta^{\tau_a}; \tau_a &lt; n) &amp;= \mathbb E_X(\beta^{\tau_a}; \tau_a &lt; \infty) \\ &amp;=\alpha^{x - a} \end{align*}\] <p>Letting \(\alpha \searrow q/p\), \(\beta \nearrow 1\) and</p> \[\left(\frac{q}{p}\right)^{x - a} = \mathbb E_x( 1; \tau_a &lt; \infty) = \mathbb P_X(\tau_a &lt; \infty)\] <p>In summary, we have for \(p &gt; \frac{1}{2}\)</p> \[\begin{align*} P_X(\tau_a &lt; \infty) &amp;= \left(\frac{q}{p}\right)^{x - a} &lt; 1\\ P_X(\tau_b &lt; \infty) &amp;= 1 \end{align*}\] <p>For \(p = \frac{1}{2}\),</p> \[P_X(\tau_b &lt; \infty) = P_X(\tau_a &lt; \infty) = 1\] <p>So irrespective of \(p\), \(P_X(\tau_a \wedge \tau_b &lt; \infty) = 1\).</p> <h4 id="example-3">Example</h4> <p>When \(P_X(\tau_a &lt; \tau_b) = \frac{\rho^b - \rho^x}{\rho^b - \rho^a}\) where \(M_n = \left(\frac{q}{p}\right)^{X_n} = \rho\) (\(p \neq q\))</p> <p>Note that \(M_n = X_n\) when \(p = q\).</p> <p>Let \(\tau = \tau_a \wedge \tau_b\) when \(p \neq q\),</p> \[\rho^X = \mathbb E_X(M_{n \wedge \tau})\] <p>Using the <strong>Dominated Convergence Theorem</strong>,</p> \[\rho^X = \mathbb E_X(M_\tau)\] <p>and we get</p> \[\begin{align*} \rho^X &amp;= \rho^A P_X(\tau_a &lt; \tau_b) + \rho^b P_x(\tau_b &lt; \tau_a) \\ 1 &amp;= P_X(\tau_a &lt; \tau_b) + P_X(\tau_b &lt; \tau_a) \end{align*}\] <h4 id="example-4">Example</h4> <p>Let \(M_n = X^2 - n\), with \(M_0 = X^2\). Then, for \(p = \frac{1}{2}, \sigma^2 = 1, \mu = 1\), \(\tau = \tau_a \wedge \tau_b\), we have</p> \[\begin{align*} X^2 &amp;= \mathbb E_X (X^2_{n \wedge \tau}) - \mathbb E(n \wedge \tau) \\ X^2 + \mathbb E(n \wedge \tau) &amp;= \mathbb E_X (X^2_{n \wedge \tau}) \\ \end{align*}\] <p>Letting \(n \to \infty\) and using Monotone convergence theorem on the left and Dominated convergence theorem on the right, we get</p> \[\begin{align*} X^2 + \mathbb E_X(\tau) &amp;= \mathbb E_X(X_\tau^2) \\ X^2 + \mathbb E_X(\tau) &amp;= a^2\frac{b - x}{b - a} + b^2 \frac{x - a}{b - a}\\ \end{align*}\] <p>resulting in \(\mathbb E_X(\tau_a \wedge \tau_b) = (X - a)(b - X)\).</p> <h1 id="brownian-motion-wiener-process">Brownian Motion (Wiener Process)</h1> <p>Brownian motion is defined as a stochastic process defined on continuous time - \(W = (W_t, t \geq 0)\). It is defined on a probability space \((\Omega, \mathcal F, P)\) and has the following properties -</p> <ul> <li> <p>The initial value is assumed to be \(W_0 = 0\)</p> </li> <li> <p><strong>Independent increments</strong> - Given a set of times \(t_0, \dots, t_n\) such that \(0 = t_0 &lt;t_1 &lt; \cdots &lt; t_n\) then \(W_{t_1} - W_{t_0}, \dots, W_{t_{n}} - W_{t_{n - 1}}\) are independent.</p> </li> <li> <p><strong>Gaussian increments</strong> - For \(0 \leq s \leq t\), \(W_t - W_s \sim \mathcal N(0, t - s)\).</p> </li> <li> <p>For a fixed \(\omega \in \Omega\), the function \(t \to W_t(\omega)\) is continuous.</p> </li> </ul> <p>Equivalently, a Brownian motion is a stochastic process \((w_t: t \geq 0)\) is a Gaussian process with \(\mathbb E(W_t) = 0, cov(W_s, W_t) = s \wedge t\) for all \(s, t\) and \(t \to W_t\) is continuous.</p> <p>This is equivalent to the third property, for \(s &lt; t\),</p> \[\begin{align*} \mathbb E[W_sW_t] &amp;= \mathbb E[W_s(W_s + (W_t - W_s)] \\ &amp;= \mathbb E[W_s^2] + \mathbb E[W_s(W_t - W_s)] \\ \end{align*}\] <p>since \(W_s - W_0\) is a Gaussian distribution and independent increments property,</p> \[\begin{align*} \mathbb E[W_sW_t] &amp;= \mathbb E[W_s^2] + \mathbb E[W_s(W_t - W_s)] \\ &amp;= s^2 + \mathbb E[W_s]\mathbb E[W_t - W_s] = s^2 \end{align*}\] <h3 id="symmetries">Symmetries</h3> <p>Brownian motion is a unique stochastic process that has some symmetry properties. That is, transformations that yield a Brownian motion when applied on a Brownian motion. Let \((W_t)\) be a Brownian motion, then the so are the following</p> <ul> <li> <p><strong>Scaling</strong> - \(\frac{1}{\sqrt{c}} W_{ct}\) for \(c&gt; 0\)</p> </li> <li> <p><strong>Reflection</strong> - \(-W_t\)</p> </li> <li> <p><strong>Time Inversion</strong> - \(tW_{1/t}\) with \(\lim_{t \to 0} tW_{1/t} = 0\)</p> </li> <li> <p><strong>Time Shift</strong> - \(\tilde W_t = W_{t + s} - W_s\), \(t\geq 0\) for a fixed \(s\). This comes in handy to define Markov processes.</p> </li> <li> <p><strong>Time Reversal</strong> - For a \(T &gt; 0\), \(\tilde W_t = \tilde W_{T - t} - W_T\) for \(0 \leq t \leq T\).</p> </li> </ul> <h3 id="more-properties">More Properties</h3> <ul> <li> <p>Define the sigma field \(\mathcal F_s:=\sigma(W_u : 0 \leq u \leq s)\), then \((W_{t + s} - W_s)_{t \geq 0} \perp\!\!\!\perp F_s\) - This is the Markov Property for Brownian Motion.</p> <p>Brownian paths are not smooth</p> \[W_{t + h - W_t} = \mathcal O(\sqrt{h})\] <p>roughly whereas if \(f\) is a \(C'\) function,</p> <p>then \(f(t + h) - f(t) = \mathcal O(h)\). \(\sqrt{h} \gg h\) as \(h \to 0\). A Brownian motion path has a property called <em>fractal</em> and it is nowhere differentiable.</p> <p>Also,</p> \[\sum_{k = 1}^{2^n} [W(k2^{-n} t) - W((k - 1)2^{-n} t]^2 \to t\] <p>as \(n \to \infty\). For a smooth function \(f\), doing the same thing yields \(2^{-n}t \to 0\). This observation forms one of the fundamental building blocks for stochastic calculus.</p> </li> <li> <p>Another peculiar property of Brownian motion has is -</p> <p>$$ \begin{align*} \frac{W_t}{t} &amp;\to 0 <br/> \lim_{t \to \infty} \sup W_t &amp;= \infty \</p> </li> </ul> <p>\lim_{t \to \infty} \inf W_t &amp;= -\infty <br/> \end{align*} $$</p> <ul> <li> <p>Since Brownian motion is basically the continuous version of random walks, it has some Martingale properties. Specifically, the following processes are martingales</p> \[\begin{align*} W_t \\ W_t^2 - t \\ \exp\left(zW_t - z^2t /2\right) \text{ for } z \in \mathbb C \end{align*}\] <p>The last result comes from the properties of moment generating functions - for a random variable \(X \sim \mathcal N(0, 1)\), we get \(\mathbb E(e^{zX}) = e^{z^2/2}\). Then,</p> \[\begin{align*} \mathbb E(e^{zW_t} \vert \mathcal F_s) &amp;= \mathbb E(e^{zWs} e^{z(W_t - W_s)} \vert \mathcal F_s) \\ &amp;= e^{zW_s} \mathbb E(e^{z(W_t - W_s)}) = e^{zW_s} e^{z^2 (t - s)/2} \end{align*}\] </li> <li> <p>Wiener also studied the integration properties for Brownian motion</p> \[\begin{align*} M_t^f = \int_0^t f(u) dW_u = W_t f(t) - \int_0^t W_u f'(u) du \end{align*}\] <p>for a \(f \in C'\) and non-random. The above expression comes from <em>integration by parts</em> and is called as the <strong>Wiener’s integral</strong>. <u>Also, $$M_t^f$$ is a martingale</u> (This is easy to show).</p> </li> <li> <p><strong>Doob’s inequality</strong> - Let \((M_t)\) be a martingale with right-continuous paths. Then, for each \(t &gt; 0\),</p> \[\mathbb E[\sup_{0 \leq s \leq t} M_s^2] \leq 4\mathbb E[M_t^2]\] </li> <li> <p><strong>Passage times</strong> - Let \(\tau_b = \min(t: W_t = b)\), then</p> \[\mathbb E_x(e^{-\alpha \tau_b}) = \exp(-\sqrt{2\alpha} \vert b - x\vert)\] <p>where \(\mathbb E_x\) means we use \(x + W_t\) (we are changing the initial mean value of \(W_t\)).</p> <p><em>Proof.</em> Fix \(\lambda &gt; 0\) and \(x &lt; b\). Since \(\exp(\lambda W_{t \wedge \tau_b} - \lambda^2(t \wedge \tau_b)/2)\) is a martingale, and is \(\leq e^{\lambda b}\), we get</p> \[\begin{align*} e^{\lambda x} &amp;= \mathbb E_x(e^{\lambda W_{t \wedge \tau_b} - \lambda^2/2 (t \wedge \tau_b)}) \\ &amp;\text{as } t\to \infty \implies \mathbb E_x(e^{\lambda W_{\tau_b} - \lambda^2/2\tau_b}) \quad \because \text{Dominated Convergence Theorem} \\ &amp;= e^{\lambda b} \mathbb E_x(e^{-\lambda^2/2 \tau_b}) \end{align*}\] <p>Taking \(\lambda = \sqrt{2\alpha}\), we get the statement of the theorem. Observe that when \(\alpha \to 0\), the RHS becomes \(1\). Therefore, \(P_x(\tau_b &lt; \infty) = 1\) as \(\alpha \to 0\). That is, Brownian motion will eventually get to any level!</p> </li> <li> <p><strong>Wiener’s integral</strong> - Formally,</p> \[M_t^f := f(t) W_t - \int_0^t w_s f'(s)ds\] <p>It is useful to know the distribution of this process. Thinking of the above integral as Riemann’s sum, we can see that \(M_t^f\) has the distribution \(\mathcal N(0, \sigma^2)\). The variance is calculated as</p> \[\begin{align*} \mathbb E[(M_t^f)^2 ] &amp;= f(t)^2 t + \underbrace{\int_0^t \int_0^t (u \wedge v) f'(u) f'(v) du dv}_{(2)} - \underbrace{2f(t) \int_0^t uf'(u) du}_{(3)} \\ (2) &amp;\implies 2\int_0^t \left(\int_0^u f'(v) dv \right) u f'(u) du \quad\because \text{ by symmetry}\\ &amp;= 2\int_0^t [f(t) - f(u) ]u f'(u) du \\ (2) + (3) &amp;\implies -2\int_0^t uf(u)f'(u) du \\ &amp;= -t f(t)^2 + \int_0^t f(u)^2 du \\ \mathbb E[(M_t^f)^2 ] &amp;= \int_0^t [f(u)]^2 du \end{align*}\] <p>This is a very useful result. In fact, using two different functions -</p> \[M_t^f M_t^g - \int_0^t f(u)g(u) du\] <p>is also a martingale! An easier way to show this would be using <strong>polarization</strong> - manipulate \((M_t^f + M_t^g )^2 - (M_t^f - M_t^g)^2\). Also, \(M_t^f \pm M_t^g = M_t^{f \pm g}\).</p> </li> </ul> <h3 id="donskers-invariance-principle">Donsker’s Invariance Principle</h3> <p>A discrete random walk can be represented as \(S_n = \xi_1 + \cdots + \xi_n\). When \(n\) becomes very large, the magnitude of the random walk scales by \(\frac{1}{\sqrt{n}}\). This is due to the Central Limit Theorem - \(\lim_{n \to \infty} S_n/\sqrt{n} \sim \mathcal N(0, 1)\). It is a useful fact to note while visualizing the random walk to fit the evolution appropriately in space. Also,</p> \[W_n(t) = \begin{cases} \frac{S_k}{\sqrt{n}} &amp; t = \frac{k}{n}, k = 0, 1, \dots, n \\ \text{linearly interpolate otherwise}\end{cases}\] <p>Again, from the CLT, \(\lim_{n \to \infty} W_n(t) \to \mathcal N(0, t)\). The above formulation takes care of appropriately representing large random walks in a bounded space (probabilistically). Alternately, \(W_n\) can be understood as a function from the space \(C([0, 1] \to \mathbb R)\) - continuous functions from \([0, 1]\) to \(\mathbb R\).</p> <p>Now, we are interested in performing operations \(F\) on these random walks -</p> \[\lim_{n \to \infty} \mathbb E[F(W_n)] \to \mathbb E[F(W)]\] <p>for all bounded continious functions \(F: C \to \mathbb R\) (maps each walk \(W_n\) to a real number). Here, \(W\) is a standard Brownian motion. All the above equation is saying is that the discretized formulation \(W_n\) is equivalent to a standard Brownian motion for practical purposes when \(n\) is large.</p> <p>For example, let \(F(x) = \int_{0}^1 x(t) dt\). Then,</p> \[\begin{align*} \vert F(x) - F(y) \vert &amp;\leq \int_0^1 \vert x(t) - y(t) \vert dt \\ &amp;\leq \max_t \vert x(t) - y(t) \vert = \|x - y\|_\infty \end{align*}\] <h3 id="binomial-model-as-a-random-walk">Binomial Model as a Random Walk</h3> <p>We have \(S_t = S_{t - 1} \cdot \xi_t\) where \(P(\xi_t = 1) = p, P(\xi_t = -1) = 1 - p\). How do we model this as a difference equation?</p> \[\begin{align*} S_t - S_{t - 1} &amp;= S_{t - 1} (\xi_t - 1) \\ \equiv \frac{dS_t}{S_t} &amp;= \xi_t - 1 \end{align*}\] <p>The RHS should be equated to \(a \cdot dt + b \cdot dW_t\) for extending it to random walks. The change in the stock price is represented as a composition of a constant value and a random variable. Interpreting this as an integral, we get the expression</p> \[\begin{align*} S_t - S_0 = \int_0^t S_u (a) du + \int_0^t S_u b dW_u \end{align*}\] <p>In this expression, we know how to evaluate \(S_u(a)du\) (integrate with respect to time, sum of values as time evolves), but how do we make sense of \(S_u b dW_u\)? We evaluate the above expression using <strong>Ito’s Stochastic Integral</strong>.</p> <blockquote> <p>Intuition - If you think about stock price, it has an inherent drift and randomness. Using this intuition, we get</p> \[d S_t = \mu S_t dt + \sigma S_t dW_t\] <p>The first integral with respect to \(du\) captures the drift whereas the second term captures the randomness.</p> </blockquote> <h2 id="itos-stochastic-integral">Ito’s Stochastic Integral</h2> <p>How do we calculate the integral \(\int_0^t Y_s dW_s\)? From Wiener’s analysis, we have seen that this expression is a martingale. One approach to evaluate the expression is to it as a combination of Riemman’s sums of step functions -</p> <ol> <li> <p>Let \(Y_t = 1_{(u, v]}(s) \cdot H\) where \(H \in L^2(\mathcal F_u)\)</p> \[M_t = \int_0^t Y_s dW_s = \begin{cases} 0, &amp;t \leq u \\ H(W_t - W_u), &amp;u \leq t \leq v \\ H(W_v - W_u), &amp; t \geq v \end{cases} = H(W_{t \wedge v} - W_{t \lor u})\] <p><strong>Claim.</strong> \(M\) is a <strong>continuous</strong> <strong>martingale</strong> and \(\mathbb E[M_t^2] = \mathbb E[\int_0^t Y_s^2 ds]\)</p> <p><em>Proof.</em> The continuous part is trivial since \(W_{t}\) is continuous. We need to show that \(\mathbb E[M_t \vert \mathcal F_s] \substack{?}{=} M_s\) for \(s&lt; t\)</p> \[\begin{align*} \mathbb E[M_t \vert F_s] &amp;= \mathbb E[H(W_t - W_u) \vert F_s] \\ &amp;= \mathbb E[HW_t \vert F_s] - E[HW_u \vert F_s] \\ &amp;= HW_s - HW_u = M_s \end{align*}\] <p>due to the martingale property of \(W_t\). Finally, for the second moment, we have</p> \[\begin{align*} \mathbb E(M_t^2) &amp;= \mathbb E[H^2(W_{t \wedge v} - W_{t \wedge u})] \\ &amp;= \begin{cases} 0 &amp; t \leq u \\ \mathbb E[H^2(W_t - W_u)^2], &amp; u \leq t \leq v \\ \mathbb E[H^2(W_v - W_u)^2], &amp; t\geq u \end{cases} \\ &amp;= \begin{cases} 0 &amp; t \leq u \\ \mathbb E[H^2](t - u), &amp; u \leq t \leq v \\ \mathbb E[H^2](v - u), &amp; t\geq u \end{cases} \\ &amp;= \mathbb E[H^2(t \wedge u - t \wedge u)] = \mathbb E\left[\int_0^t H^2 1_{(u, v]} (s)ds\right] \end{align*}\] </li> <li> <p>Now, we need to add such \(Y_k\)’s for the required interval (Riemman sum). \(Y_s = \sum_{k = 1}^n 1_{(u_k, v_k]} (s) H_k\) and \(H_k \in L^2 (\mathcal F_{u_k})\) -</p> \[\int_0^t Y_s dW_s := \sum_{k = 1}^n \int_0^t Y_s^{(k)} dW_s\] <p>Since \(Y_s\) is a continuous martingale in \(t\). Also, the <strong>Ito’s Isometry</strong> property states that</p> \[\mathbb E((\int_0^t Y_s dW_s)^2) = \mathbb E(\int_0^t Y_s^2 ds)\] <p>This is easy to show considering non-overlapping intervals. <em>Hint</em>. Consider conditional expectation using Tower law.</p> <p>We have seen this in context of Wiener integrals, but the integrand was considered to be deterministic in that case. Here, we generalize the notion by considering integrands that can be stochastic processes as well.</p> </li> <li> <p><strong>Progressively Measurable</strong> - Consider a finite segment of time \(0 \leq t \leq T\). Then, a stochastic process \(X_t(\omega)\) can be viewed as a function \(\Omega \times [0, T] \to \mathbb R\). Then, the integral \(\int_{0}^t X_s(\omega)ds\) can be viewed as a marginal integral of a joint function. The fact that \(\int_0^t X_s(\omega) ds \in \mathcal F_s\) (the integral is measurable) comes from Fubini’s theorem in Measure theory. Specifically, the process \(X_t(\omega)\) is on the domain \(\Omega \times [0, t]\) and is consequently measurable on \(\mathcal F_t \otimes \mathcal B[0, t]\). Here \(\mathcal B\) refers to <strong>Borel Measurability</strong>. \((X_t)\) is said to be <strong>adapted</strong> to \((\mathcal F_t)\)</p> <p><strong>Claim.</strong> If \(X_t\) is <em>adapted</em> and \(t \to X_t(\omega)\) is left-continuous for all \(\omega \in \Omega\), then \(X\) is progressively measurable.</p> <p>In the previous approach to integrate stochastic processes, we considered \(\mathcal L_s\) class of integrands. That is, we assumed that the integrands can be written as a sequence of step functions. Now, we generalise this notion for any integrands that are progressive measurable</p> \[\mathcal L = \{Y: Y \text{ is progressively measurable and } \mathbb E\left(\int_0^T Y_S^2ds\right) &lt; \infty\}\] <p>A key point to note is that \(\mathcal L_s\) is dense in \(\mathcal L\) (we will not be proving this). That means, given \(Y \in \mathcal L\), there exists a sequence of integrands \((Y^{(n)})_{n \geq 1}\) belonging to \(\mathcal L_s\) such that</p> \[\lim_{n \to \infty} \mathbb E\left[\int_0^T (Y_s - Y_s^{(n)})^2 ds\right] \to 0\] <p>Since each \((Y^{(n)})\) is continuous and a martingale, we will try to show that \(M_t^{(n)} = \int_0^t Y_s^{(n)} dW_s\) is a Cauchy sequence converging to the required integral of \(Y_s\) (note that \((Y^{(n)})\) is Cauchy) -</p> \[\begin{align*} \mathbb E\left[ M_t^{(n)} - M_t^{(m)}\right] &amp;= \mathbb E\left[\int_0^t [Y_s^{(n)} - Y_s^{(m)}]\right] \\ &amp;\leq \mathbb E\left[\int_0^T [Y_s^{(n)} - Y_s^{(m)}]\right] \to 0 \end{align*}\] <p>Therefore, \((M_t^{(n)})_{n \geq 1}\) is Cauchy in \(\mathcal L^2(\mathcal F_t)\). Consequently, there exists \(M_t \in \mathcal L^w(\mathcal F_t)\) such that \(M_t^{(n)} \to M_t\). Then, using Doob’s inequality</p> \[\begin{align*} \mathbb E\left(\sup_{0 \leq t \leq T} \vert M_t^{(n)} - M_t^{(m)}\vert^2 \right) \leq 4 \mathbb E[(M_T^{(n)} - M_T^{(m)})^2] \end{align*}\] <p>we can say that \((M_t)_{0 \leq t \leq T}\) is a path-continuous martingale and \(\mathbb E[M_t^2] = \mathbb E[\int_0^t Y_s^2 ds]\) for all \(t \in [0, T]\) from Ito’s isometry.</p> </li> <li> <p>An alternate way to view the integral is to formulate it as a Riemann integral. If \(Y \in \mathcal L\) is such that \(s \to Y_s(w)\) is left-continuous for all \(\omega \in \Omega\) then</p> \[\int_0^t Y_s dW_s = \lim_n \sum_{k = 0}^{n - 1} Y_{\frac{k}{n}t}\left(W_{\frac{k + 1}{n} t} - W_{\frac{k}{n} t}\right)\] <p>is also a limiting sum of Riemann sums (converges in \(L^2\)). If the above limit converges, then \(Y_s \in \mathcal L^2\).</p> <p><strong>Example</strong> - Consider \(\int_0^t W_s dW_s\) . \(W \in \mathcal L\). How?</p> \[\mathbb E\left[\int_0^T W_s^2 ds \right] = \int_0^T \mathbb E(W_s^2) ds = \int_0^T sds &lt; \infty\] </li> </ol> <p>        from Fubini’s theorem. Also, since \(W\) is         progressive, it is integrable.</p> <h3 id="quadratic-variation">Quadratic Variation</h3> <p>Quadratic variation of a Brownian motion is an important tool in stochastic calculus. Simply put, it is defined as</p> \[\lim_{n \to \infty} Q_t^{(n)} = \lim_{n \to \infty} \sum_{k = 0}^{n - 1} (W_{\frac{k + q}{n} t} - W_{\frac{k}{n}t})^2 = t\] <p>Since the increments are independent are from the distribution \(\mathcal N(0, \frac{t}{n})\), the variance of \(Q_t^{(n)}\) is</p> \[\mathbb E((Q_t^{(n)})^2 ) = 3n \left(\frac{t}{n}\right)^2 + n(n - 1)\left(\frac{t}{n}\right)^2\] <p>implying \(Var(Q_t^{(n)}) = \frac{2}{n} t^2 \to 0\) as \(n \to \infty\).</p> <p>Going back to our example, writing the integral as Riemann sums -</p> \[\begin{align*} W_t^2 &amp;= \sum_{k = 0}^{n - 1} (W^2_{\frac{k + 1}{n} t} - W^2_{\frac{k }{n} t}) \quad \because W_0^2 = 0\\ &amp;= 2 \sum_{k =0}^{n - 1} W_{\frac{k}{n}t} (W_{\frac{k + 1}{n} t} - W_{\frac{k }{n} t}) + Q_t^{(n)}\quad \because \text{telescopic sum} \\ &amp;= 2\int_0^t W_S dW_s + t \end{align*}\] <p>since the above is left-continuous, letting \(n \to \infty\), we get the above expression.</p> <p>We have derived</p> \[W_t^2 - t = \int_0^t 2W_s dW_s\] <p>We can check Ito’s isometry using this.</p> <p>Also, the above expression in reminiscent of the fundamental theorem of calculus. Using this, we now state the following theorem</p> <p><strong>Theorem (Ito’s Formula/Lemma)</strong> For a function \(f \in C^2\) and \(\vert f'(x) \vert \leq c_0 e^{c_1\vert x \vert}\) for constants \(c_0, c_1\),</p> \[f(W_t) = f(W_0) + \int_0^t f'(W_s)dW_s + \frac{1}{2} \int_0^t f''(W_S) ds\] <p>This forms the basis for the fundamental theorem for stochastic calculus.</p> <h2 id="localization">Localization</h2> <p>We define the set</p> \[\mathcal L_{loc} := \{Y: Y \text{ is progressive and } P\left(\int_0^t Y_s^2ds &lt; \infty\right) = 1\}\] <p>This is a weaker condition than \(\mathbb E[\int_0^t Y_S^2 ds] &lt; \infty\), and it allows us to generalize the notion of \(\int_0^t Y_s dWs\) to more integrands.</p> <p>Given a \(Y \in \mathcal L_{loc}\), we define the stop times \(\tau(n) = \inf \{t: \int_0^t Y_s^2 ds &gt; n\} \wedge T\). Then,</p> \[\cap_n \{\tau(n) &lt; T\} \subset \left\{\int_0^T Y_s^2 ds = \infty\right\}\] <p>so \(P(\tau(n) &lt; T) \to 0\) as \(n \to \infty\). Furthermore, defining \(Y_s^{(n)} = \mathbb 1_{(0, \tau(n)]} (s) Y_s\), we get</p> <p>\(\int_0^T (Y_s^{(n)})^2 ds \leq n^2\) implying that \(Y^{(n)} \in \mathcal L\). Also, the martingale \(\int_0^t Y_s^{(n)} dW_s\) is well-defined for each \(n = 1, 2, \dots\).</p> <p>They are also consistent - For \(m &lt; n\), \(\tau(m) &lt; \tau(n)\) and</p> \[\int_0^t Y_s^{(m)} dW_s = \int_0^t Y_s^{(n)} dW_s\] <p>if \(t \in [0, \tau(m)]\).</p> <p>Since \(P(\tau(n) &lt; T) \to 0\) as \(n \to \infty\), and on \(\cup_n \{\tau(n) = T\}\), we have</p> \[I_t = \lim_n \int_0^t Y_s^{(n)}dW_s, 0 \leq t \leq T\] <p>and \(I_{t \wedge \tau(n)} = \int_0^t Y_s^{(n)} dW_s\) is a martingale.</p> <p>This process \(I\), which we use as our definition of \(\int_0^t Y_s dW_s\) is an instance of a <strong>local martingale</strong>.</p> <p>A <strong>local martingale</strong> is an adapted process \(M = (M_t)\) for which there is an increasing sequence \((\sigma(n))\) of stopping times such that \(P(\sigma(n) &lt; T) \to 0\) as \(n \to \infty\) and \(M_{t \wedge \sigma(n)}\) is a martingale.</p> <p>When the martingales and local martingales have continuous paths, \((\sigma(n))\) can be taken to be \(\sigma(n) = \inf(t: \vert M_t \vert n) \wedge T\). In particular it can always be arranged that \(\vert M_{t \wedge \sigma(n)} \vert \leq K_n\) for all \(t\).</p> <h2 id="itos-process">Ito’s Process</h2> <p>An Ito’s process is a continuous adapted process \(X = (X_t)\) of the form</p> \[X_t = X_0 + \int_0^t Y_s dW_s + \int_0^t b_s ds, 0 \leq t \leq T\] <p>where \(Y \in \mathcal L_{loc}\) and \(b = (b_s)\) is progressive with \(\int_0^T \vert b_s\vert ds &lt; \infty\).</p> <p>Revisiting the notion of quadratic variation, the <strong>quadratic variation of</strong> \(X\) is formally defined as \(\langle X\rangle\) -</p> \[\langle X\rangle_t = \int_0^t Y_s^2 ds, 0 \leq t \leq T\] <p>For a non-random continuous function \(g\), the process \(G_t = \int_0^t g(s)ds\) has \(0\) quadratic variation.</p> <p><strong>Theorem (Ito Formula II)</strong> Let \(f: \mathbb R \times [0, T]\) be of class \(C^{2, 1}\). If \(X\) is an Ito’s process as defined above, then so is \(f(X_t, t)\) with</p> \[f(X_t,t) = F(X_0, 0) + \int_0^t f_x(X_s, s)dX_s + \int_0^t f_t(X_s, s)ds + \frac{1}{2} \int_0^t f_{xx} (X_s, s_ d)\langle X\rangle_s\] <p>where \(dX_s = Y_s dW_s + B_s ds\) and \(d\langle X\rangle_s = Y^2_s ds\).</p> <h3 id="example-5">Example</h3> <p>Consider the case of Stochastic exponential,</p> <p>For a \(Y \in \mathcal L_{loc}\) we define \(M_t = \int_0^t Y_s dW_s\) (local martingale). Define</p> \[Z_t = \exp(M_t - \frac{1}{2} \langle M\rangle_t), 0\leq t\leq T\] <p>By Ito’s formula for \(f(x) = e^x\) and Ito’s process \(X_t = M_t - \frac{1}{2} \langle M\rangle_t\), we have</p> \[Z_t = 1 + \int_0^t z_s dM_s \equiv dZ_t = Z_t dM_t \;\&amp;\; Z_0 = 1\] <p>Again, \(Z\) is a local martingale.</p> <ul> <li> <p>Since \((Z_t)\) is a strictly positive local martingale, it’s a supermartingale (<strong>Fatou’s Lemma</strong>)</p> </li> <li> <p>If \(\vert Y_s(w)\vert \leq K\) for all \(s, w\), then \(Z\cdot Y \in \mathcal L\) and \(Z\) is a true martingale.</p> </li> </ul> <h1 id="black-scholes-model">Black-Scholes Model</h1> <p>As usual, consider two assets to be traded given by</p> <ul> <li>Bond</li> </ul> \[\begin{align*} B_t &amp;= e^{rt}, r &gt; 0 \\ dB_t &amp;= rB_t dt, B_0 = 1 \end{align*}\] <ul> <li>Stock</li> </ul> \[\begin{align*} S_t &amp;= S_0 \exp(\sigma W_t + (\mu - \sigma^2/2)t)\\ dS_t &amp;= S_t (\sigma dW_t + \mu dt) \quad \text{From Ito's formula} \end{align*}\] <p>Note that \(\sigma W_t + \mu t\) is a Brownian motion with <strong>volatility</strong> \(\sigma &gt; 0\) and <strong>drift</strong> \(\mu \in \mathbb R\)</p> <p>A <strong>trading strategy</strong> is a pair \(\phi = (\alpha, \beta)\) of progressive processes \((\alpha_t)\), \((\beta_t)\) such that \(\int_0^T \alpha_s^2 ds &lt; \infty\) and \(\int_0^T \vert \beta_s\vert ds &lt; \infty\) (they are <em>progressively measurable</em>). \(\alpha\) represents the holdings in stock and \(\beta\) represents the holdings in bond. At any time \(t\),</p> \[V_t(\phi) = \alpha_t S_t + \beta_t B_t\] <p>For a trading strategy to be <strong>self-financing</strong>,</p> \[V_t(\phi) - V_0(\phi) = \int_0^t \alpha_u dS_u + \int_0^t \beta_u dB_u, 0 \leq t\leq T\] <p>where \(dS_u = S_u(\sigma dW_u + \mu du)\) and \(dB_u = re^{ru}du\). In particular, \(V_t(\phi)\) is an Ito process.</p> <p>The importance of \(\alpha\) being progressively measurable comes in handy here, since we want \(\sigma \alpha S_u \in \mathcal L_{loc}\) . That is because \(S_u\) is positive, \(\int_0^T \alpha_u^2 S_u^2 du &lt; \infty \iff \int_0^T \alpha_u^2 du &lt; \infty\)</p> <p>Once again, we define the discounted processes</p> \[\begin{align*} V_t^*(\phi) &amp;= V_t(\phi) /B_t = V_t e^{-rt} \\ S_t^*(\phi) &amp;= S_t(\phi) /B_t = S_t e^{-rt} \\ \end{align*}\] <p>and consequently,</p> \[V_t^*(\phi) - V_0(\phi) = \int_0^t \alpha_u dS^*_u\] <h4 id="product-rule-for-itos-calculus">Product Rule for Ito’s Calculus</h4> <p>Suppose we have two Ito’s processes \(X_t = X_0 + \int_0^t H_s dWs + \int_0^T bs_ds\) + \(Y_t = Y_0 + \int_0^t K_s dWs + \int_0^T cs_ds\), then their product is also an Ito’s process</p> \[d(X_tY_t) = X_t dY_t + Y_t dX_t + d\langle X,Y\rangle_t\] <p>where \(\langle X, Y \rangle_t = \int_0^t H_sK_s ds\) is the quadratic covariation.</p> <h3 id="arbitrage-3">Arbitrage</h3> <p>An arbitrage is a self-financing trading strategy \(\phi\) is such that \(V_0(\phi) = 0\) and \(V_T(\phi) \geq 0\) with \(P(V_T(\phi) &gt; 0)&gt; 0\).</p> <p>The binomial model is viable and complete. However, it is easy to create arbitrage opportunities in Black-Scholes model -</p> \[\begin{align*} I_t &amp;= \int_0^t \frac{1}{\sqrt{T - s}} dW_u, 0\leq t &lt; T \\ \langle I \rangle_t &amp;= \log\frac{T}{T - t} \nearrow \infty \text{ as } t \nearrow T \end{align*}\] <p>The quadratic variation represents the energy of the martingale in a way. We further define \(\tau_q = \inf (t: I_t = a) \wedge T\) for some \(a &gt; 0\) (\(P(\tau_a &lt; T) = 1\)). Now, the trading strategy is given by</p> \[\begin{align*} \alpha_t &amp;= \mathbb 1_{t \leq T} \frac{1}{\sqrt{T - t}} \frac{1}{S_t} \\ \beta_t &amp;= I_{t \wedge \tau_a} - \alpha_t s_t \end{align*}\] <p>Let \(r = 0, B_t = 1\) for simplicity. Then,</p> \[\begin{align*} V_t(\phi) = I_{t \wedge \tau_a} \end{align*}\] <p>implying \(V_0(\phi) = , V_T(\phi) = I_{\tau_a} = a &gt; 0\).</p> <h3 id="equivalent-martingale-measure">Equivalent Martingale Measure</h3> <p>When \(\mu = 0\), we have \(S_t = S_0 e^{\sigma W_t - \sigma^2t/2}\) is a martingale. Then, \(P\) itself is an EMM. When \(\mu \neq 0\), we need to find a \(Q\) on \((\Omega, \mathcal F_T)\) that is equivalent to \(P\). How do we check equivalence here? \(Q(A) &gt; 0 \iff P(A)&gt; 0\) for all \(A \in \mathcal F_t\).</p> <h4 id="girsanovs-theorem">Girsanov’s Theorem</h4> <p>Let \(H \in \mathcal L_{loc}\) and \(M_t = \int_0^t H_S dW_s\) and \(\langle M \rangle_t = \int_0^t H_s^2 ds\), then</p> \[\Lambda_t := \exp(M_t - \frac{1}{2} \langle M\rangle_t) = 1 + \int_0^t \Lambda_s dM_s\] <p>is <strong>continuous in</strong> \(t\) <strong>, positive, local martingale and a supermartingale</strong>. We have \(\mathbb E(\Lambda_t) \leq 1\), \(t \in [0, T]\). The process is a martingale when \(\mathbb E(\Lambda_t) = 1\).</p> <p>Now, the theorem states that suppose \(\mathbb E(\Lambda_t) = 1\) for all \(t \in [0, T]\) (\(\iff \mathbb E(\Lambda_T) = 1 \iff \Lambda_t\) is a martingale ). We define</p> \[Q(A) := \mathbb E^P (\mathbb 1_A \Lambda_T), A \in \mathcal F_T\] <p>then \(Q \sim P\) and</p> \[N_t := \int_0^t Y_s dW_s\] <p>for \(Y \in \mathcal L_{loc}\) is a local martingale with respect to \(P\) and</p> \[\tilde N_t = N_t - \int_0^t Y_s H_s ds\] <p>is a local martingale with respect to \(Q\).</p> <p>Finally, \(\langle N \rangle_t = \langle \tilde N\rangle_t\). In particular, consider</p> \[\tilde W_t = W_t - \int_0^t H_s ds\] <p>then \(\tilde W_s\) is a local martingale with respect to \(Q\) and \(\langle \tilde W \rangle = t\). It can be shown that any process with quadratic variation equating to \(t\) is a Brownian motion!</p> <p>Using this theorem, we try and define a martingale for \(S_t^*\)</p> \[S_t^* = S_0 \exp (\sigma W_t + (\mu - r - \sigma^2/2)t)\] <p>Let \(H_t = \theta = \frac{\mu - r}{\sigma}\), then \(S^*\) is a martingale with respect to \(Q\).</p> <p>By Ito’s formula,</p> \[\frac{dS^*_t}{S_t^*} = \sigma\left\{dW_t + \underbrace{\frac{\mu - r}{\sigma}}_{\theta} dt \right\}\] <p>Using Girsanov’s theorem with \(H_s = - \theta\), \(P^* = \Lambda_t P\) is an EMM.</p> \[S_t^* = S_0 + \int_0^t S_u^* d \tilde W_u\] <p>so \(S_t^*\) is a local martingale. Furthermore,</p> \[\begin{align*} \mathbb E^*((S_t^*)^2) &amp;= \mathbb E^*(\exp(2\sigma \tilde W_t - \sigma^2 t))S_0^2. \\ &amp;= e^{4\sigma^2 t/2 - \sigma^2 t}S_0^2 = S_0^2 e^{\sigma^2t} \end{align*}\] <p>Consequently,</p> \[\mathbb E^*[\int_0^T (S_t^*)^2 dt] = S_0^2 \int_0^T e^{\sigma^2 t}dt = S_0^2 \frac{e^{\sigma^2T} - 1} {\sigma^2} &lt; \infty\] <p>implying that \(S_t^* \in \mathcal L_{loc}(P^*)\) and \((S_t^*)\) is a \(P^*\)-martingale.</p> <p><strong>Corollary</strong> If \(\phi\) is a self-financing trading strategy, then \(V_t^*(\phi)\) is a \(p^*\)-local martingale.</p> <p><em>Proof.</em></p> \[\begin{align*} V_t^*(\phi) &amp;= V_0(\phi) + \int_0^t \alpha_u dS_u^* \\ &amp;= V_0(\phi) + \int_0^t (\alpha_u S_u^*) d\tilde W_u \end{align*}\] <p>and \(\alpha_uS_u^* \in \mathcal L_{loc}(P^*)\) by path continuity.</p> <p><strong>Definition.</strong> A self-financing trading strategy is <strong>admissible</strong> provided \(V_t^*(\phi)\) is a \(P^*\)-martingale (not just a local martingale).</p> <p>Also, if \(\mathbb E^*[\int_0^T \alpha_s^2 (S_s^*)^2ds &lt; \infty\), then \(\phi\) is admissible.</p> <h2 id="martingale-representation-theorem">Martingale Representation Theorem</h2> <p>If \((M_t)\) is a \(P^*\)-martingale (such that \(t \to M_t\) is right continuous) then there exists a unique \(\eta = (\eta_t)_{0 \leq t \leq T} \in \mathcal L_{loc}\) such that</p> \[M_t = M_0 + \int_0^t \eta_s d\tilde W_s, 0 \leq t \leq T\] <p><strong>Corollary.</strong> Given \(X \in L^1(\mathcal F_T, P^*)\), there is a unique admissible \(\phi\) such that \(\mathbb E^*(X^* \vert \mathcal F_t) = V_t^*(\phi)\) for \(0 \leq t\leq T\) (\(X^* = e^{-rT} X\)).</p> <p><em>Proof.</em> Define a right continuous \(P^*\)-martingale \(M\) by</p> \[M_t := \mathbb E^*(X^* \vert \mathcal F_t)\] <p>By the martingale representation theorem, there exists a unique \(\eta\) such that</p> \[M_t = M_0 + \int_0^t \eta_u d \tilde W_u = M_0 + \int_0^t \alpha_u dS_u^*\] <p>where \(\alpha_u := \eta_u/S_u^* \in \mathcal L_{loc}\). Define \(\beta_t := M_t - \alpha_t S_t^*\) which is continuous. Then, \(\phi := (\alpha_t, \beta_t)\) is a self-financing trading strategy and</p> \[V_t^*(\phi) = \alpha_t S_t^* + \beta_t = M_t\] <p>is a martingale and hence, admissible. Also,</p> \[V_0(\phi) = M_0 = \mathbb E^*[X^* \vert \mathcal F_0] = \mathbb E^*[X^*]\] <p>Therefore, <strong>each</strong> \(X \in L^1(P^*)\) <strong>is replicable</strong>!</p> <h2 id="pricing-a-european-contingent-claim">Pricing a European Contingent Claim</h2> <p><strong>Theorem.</strong> The unique no arbitrage price of the European claim \(X \in L^1 (P^*)\) is \(V_t(\phi) = e^{-r(T - t)} \mathbb E^*(X \vert \mathcal F_t)\).</p> <p>The European call option is given by \(X = (S_T - K)^*\)</p> <p>Let us compute \(\mathbb E^*(X^* \vert \mathcal F_t)\) - Firstly, we have</p> \[S_T^* = S_t^* \exp(\sigma(\tilde W_T - \tilde W_t) - \frac{\sigma^2}{2} (T - t))\] <p>and \(\tilde W_t - \tilde W_t \sim \mathcal N(0, T - t) \perp\!\!\!\perp_{p^*} \mathcal F_t\). Renaming \(\tilde W_T - \tilde W_t\) as \(\sqrt{T - t} Z\) where \(Z \sim \mathcal N(0, 1)\) -</p> \[\begin{align*} \mathbb E^*((S^*_T - K^*)^+ \vert \mathcal F_t) &amp;= E[(S_t^*\exp(\sigma Z\sqrt{T - t} - \frac{\sigma^2}{2}(T - t)) - K^*)^+] \\ \mathbb E^*[g(S_T^*) \vert \mathcal F_t]&amp;= \mathbb E[g(xe^{\alpha z - \alpha^2/2}) \vert \mathcal F_t] \end{align*}\] <p>where \(x = S_t^*\) and \(\alpha = \sigma \sqrt{T - t}\). Let us work with this expression. Letting \(\phi(z) = \frac{1}{\sqrt{2\pi}}e^{-z^2/2}\) we get \(\mathbb E^*[g(S^*_T) \vert \mathcal F_t] = f(S_t^*, t)\) where</p> \[f(x, t) = \int_{-\infty}^{\infty} g(xe^{\alpha z - \alpha^2/2})\phi(z) dz\] <p>Now,</p> \[g(xe^{\alpha z - \alpha^2/2}) = \begin{cases} xe^{\alpha z - \alpha^2/2} - K^* &amp; z &gt; l \\ 0 &amp; z \leq l \end{cases}\] <p>where \(l = \frac{L}{\alpha} + \frac{\alpha}{2}\). and \(L = \log(K^*/x)\). Then, we get \(f(x, t) = (1) + (2)\) where</p> \[\begin{align*} (1) &amp;= x\int_l^\infty e^{\alpha z - \alpha^2 /2} \phi(z) dz \\ &amp;= x \int_{\tilde l} ^\infty \frac{1}{\sqrt{2\pi}} e^{-y^2/2}dy = x \Phi(\frac{\alpha}{2} - \frac{L}{\alpha}) \\ (2) &amp;= -K^*\int_l^\infty \phi(z) dz \\ &amp;= -K^* \Phi(-l) \end{align*}\] <p>Finally, we get</p> \[C^*(t) = f(x, t) = x \Phi\left(\frac{\log{(x/K^*)}}{\sigma \sqrt{T - t}} + \frac{\sigma \sqrt{T - t}}{2}\right) - K^* \Phi\left(\frac{\log(x/K^*)}{\sigma \sqrt{T - t}} - \frac{\sigma \sqrt{T - t}}{2}\right)\] <p>Let us now try to replicate the option using a trading strategy -</p> <p>We have seen that</p> \[V_t(\phi) = e^{rt} f(S_t^*, t)\] <p>We have</p> \[f(x, t) = x \Phi\left(\frac{\log{(x/K^*)}}{\sigma \sqrt{T - t}} + \frac{\sigma \sqrt{T - t}}{2}\right) - K^* \Phi\left(\frac{\log(x/K^*)}{\sigma \sqrt{T - t}} - \frac{\sigma \sqrt{T - t}}{2}\right)\] <p>where \(X = (S_t - K)^+, K^* = Ke^{-rT}, \Phi =\) standard normal CDF. Here, \(f\) is smooth in the sense that it is twice differentiable in \(S_t^*\) and differentiable in \(t\). Using Ito’s formula, since \(V_t^*(\phi)\) is a martingale,</p> \[V_t^*(\phi) = V_0^*(\phi) + \int_0^t \underbrace{\frac{\partial f}{\partial x} (S_u^*, u)}_{\alpha_u} dS_u^* + \cdots\] <p>The remaining integral terms cancel out to \(0\)</p> <blockquote> <p>Why?</p> </blockquote> <p>Consequently,</p> \[\begin{align*} \alpha_t &amp;= \frac{\partial f}{\partial x} (S_t^*, t) = \Phi\left(\frac{\log(S_t^*/K^*)}{\sigma \sqrt{T - t}} + \frac{\sigma \sqrt{T - t}}{2}\right) \\ \beta_t &amp;= V_t^*(\phi) - \alpha_t S_t^* \end{align*}\] <p>We can check that when \(S_T &gt; K, \alpha_t \to 1\), and \(S_T &lt; K, \alpha_t \to 0\).</p> <p>Let \(X^* = g(S_T^*)\), then, \(V_T^*(\phi) = f(S_t^*, t) = \mathbb E^*(g(S_t^*) \vert \mathcal F_t)\) and \(\lim_{t \uparrow T} f(x, t) = g(x)\)</p> <p>Also,</p> \[\frac{\partial f}{\partial t} + \frac{1}{2} \sigma^2 x^2 \frac{\partial^ f}{\partial x^2} \equiv 0\] <p>for \((x, t) \in (0, \infty) \times [0, T)\). This PDE has a unique solutions when \(g\) has certain properties.</p> <h2 id="black-scholes-formula">Black-Scholes Formula</h2> <p>Consider the market with the assets \(S_t = (S_t^0, S_t^1, \dots, S_t^d)\) where \(S_t^0 = B_t = e^{\int_0^t r_s ds}\) and \((r_s)_{0 \leq s \leq T}\) is a progressive process (\(r_s \geq 0, \int_0^t r_s ds &lt; \infty\)). This formulation maintains \(\frac{d S_t^0}{S_t^0} = r_t dt\).</p> <ul> <li> <p>The stocks have drifts \(\mu_t^1 , \dots, \mu_t^d\) which are progressive processes and \(\int_0^T \vert \mu_t^i \vert dt &lt; \infty\) for all \(i\).</p> </li> <li> <p>The volatilities are represented as a matrix \(\sigma^T = \begin{bmatrix} \sigma^1 &amp;\sigma^2 &amp; \dots &amp; \sigma^d \end{bmatrix}\) (\(d \times n\)) where \(\sigma_t^{ij}\) is progressive and \(\int_0^T \|\sigma_t^i\|^2 dt &lt; \infty\).</p> </li> <li> <p>\(W = (W^1, \dots, W^n)\) are independent 1-dimensional standard Brownian Motions.</p> </li> </ul> <p>Using these elements, the price process is defined as</p> \[S_t^i = S_0^i \exp\left(\int_0^t \sigma_u^i \cdot dW_u - \frac{1}{2} \int_0^t \vert \sigma_u^i \vert ^2 du + \int_0^t \mu_u^i du\right)\] <p>Then, the using the basic definitions, a trading strategy is \(\phi_t = (\phi_t^0, \dots, \phi_t^d)\) and</p> \[\begin{align*} V_t(\phi) &amp;= \phi_t \cdot S_t\\ &amp;= V_0(\phi) + \int_0^t \phi_u \cdot dS_u \quad \because \text{self financing} \\ \equiv V_t^*(\phi) - V_0(\phi) &amp;= \sum_{i = 1}^d \int_0^t \phi_u^i dS_u^{i, *} \end{align*}\] <p>Also, given any strategy \(\tilde \phi = (\tilde \phi_t^1, \dots, \tilde \phi_t^d)\) which is progressive and integrable and \(c \in \mathbb R\), representing the initial holdings, there exists \(\phi_t^0\) such that</p> <p>\(\phi_t = (\phi_t^0, \tilde \phi_t^1, \dots, \tilde \phi_t^d)\) is a self-financing trading strategy \(V_0(\phi) = c\).</p> <hr/> <p>A tame trading strategy \(\phi\) there exists \(c &gt; 0\) , \(V_t^*(\phi) \geq -c\), \(\forall t \in [0, T]\).</p> <p>Consequently, \(V_t^*(\phi) + c \implies V_t^*(\phi)\) is a \(Q\)-supermartingale.</p> <p><strong>Proposition</strong> - If there is an ELMM \(Q\) then no tame trading strategy can be an arbitrage. <em>Proof.</em> Suppose you have a tame trading strategy with \(V_T^*(\phi) \geq 0\) and \(V_0^*(\phi) = 0\). Due to supermartingale property,</p> \[\begin{align*} E^Q(V_T^*(\phi)) &amp;\leq \mathbb E^Q(V_0^*(\phi)) \\ &amp;= V_0(\phi) = 0 \end{align*}\] <p><strong>Theorem. (FFTAP)</strong></p> <ul> <li>No free lunch with vanishing risk\(\iff\) there exists an ELMM</li> </ul> <p>ELMM via Girsanov’s theorem - For any \(Q \sim P\) there exists a \(\Lambda_t\), such that \(\left. \frac{dP}{DQ} \right\vert_{\mathcal F_t} = \Lambda_t\). This is equivalent to saying</p> \[Q(A) = \mathbb E^P(\mathbb 1_A \Lambda_t)\] <p>For all \(A \in \mathcal F_t\). Here, \(\Lambda_t\) is a \(P\)-martingale. We want the form \(\Lambda_t = \exp(M_t - \frac{1}{2}\langle M\rangle_t))\) for the Girsanov’s theorem. Consider the following - Using Ito’s theorem</p> \[d(\log(\Lambda_t)) = \frac{1}{\Lambda_t}{d \Lambda_t} + \frac{1}{2} \frac{1}{\Lambda_t^2} d\langle\Lambda\rangle_t\] <p>Resembling that \(dM_t = \frac{1}{\Lambda_t} d\Lambda_t\).</p> <p>Using Martingale representation theorem, \(M_t = - \int_0^t \theta_s \cdot dW_s\). With \(Q = \exp(M_T - \frac{1}{2} \langle M\rangle_T)P\), Girsanov’s theorem gives us</p> \[\tilde W_t = W_t + \int_0^t \theta_s ds\] <p>is a \(Q\)-Brownian motion. Going back to our Black-Scholes model,</p> \[S_t^{i,*} = S_0^i \exp(\int_0^t \sigma_s^i\cdot dW_s - \frac{1}{2} \int_0^t \vert \sigma_s^i\vert^2 ds + \int_0^t (\mu_s^i - r_s) ds)\] <p>We want to choose \(\theta\) such that \(\tilde W_s\) captures the drift term. That is, let \(-\sigma_s^i \cdot \theta_s + \mu_s^i - r_s = 0\), then</p> \[S^{i,*}_t = S_0^I \exp(\int_0^t \sigma_s^i \cdot d \tilde W_s - \frac{1}{2} \int_0^t \vert \sigma_s^i \vert^2 ds)\] <p>Simply put, we are trying to find a \(Q\)-martingale given \(\sigma_s^i, \mu_s^i, r_s\) for all \(\omega, s\) in terms of \(\theta_s\). In other words, we are checking if \(\mu_s - r_s {\bf 1}\) is in the column span of \(\sigma_s\). We are working our way backwards to find an ELMM \(Q\) using the Black-Scholes model. Using \(\theta_s\), we get \((\Lambda)_t\). But when is this a \(P\)-martingale? A super-martingale with same initial and final expectations is a martingale.</p> <p><strong>Novikov’s condition</strong> - If \(\mathbb E^p[e^{\frac{1}{2} \langle M\rangle_T}] &lt; \infty\), then \(\Lambda_t = \exp(M_t - \frac{1}{2} \langle M\rangle_t)\) for all \(0 \leq t \leq T\) is a \(P\)-martingale.</p> <p><strong>Theorem.</strong> If \(n = d\), and \(\sigma_s\) is invertible for all \(\omega, s\) and \(\mathbb E^p [e^{\frac{1}{2} \in_0^t \vert \theta_s\vert^2 ds}] &lt; \infty\), then \(Q\) is an ELMM.</p> <p>With that, we now state the <strong>Second Fundamental Theorem of Asset Pricing</strong> for continuous models -</p> <p><strong>Theorem (SFTAP)</strong>. Suppose there exists an ELMM \(P^*\). Then, the market is complete if the ELMM is unique. By complete, we mean that for all contingent claims \(X^* \in L^1 (P^*)\), there is a self-financing trading strategy such that \(X = V_T(\phi)\).</p> <p><em>Proof.</em></p> <p>\((\implies)\) Let \(Q\) be another ELMM, then for \(A \in \mathcal F_T\), we define \(\mathbb 1_A = X^* \in L^1(P^*)\). That is, \(X = S_T^0 \mathbb 1_A\) (let \(S_0^0 = 1\)).</p> \[\begin{align*} X^* &amp;= V_T^*(\phi) \\ \mathbb E^*(V_T^*(\phi)) &amp;= \mathbb E^*(V_0^*(\phi)) = V_0(\phi) = P^*(A) \end{align*}\] <p>Also, \(V_t^*(\phi), 0\leq t\leq T\) is a \(Q\)-local martingale</p> \[V_t^* = V_0(\phi) + \int_0^t \phi_s dS_s^*\] <p>Since the claim is replicable and \(X^* \geq 0\), \(V_t^*(\phi) = \mathbb E^*(X^* \vert \mathcal F_t) \geq 0\). Since \(V_t^*(\phi)\) is a \(Q\)-local martingale, it is also a \(Q\)-supermartingale -</p> \[P^*(A) =V_0(\phi) \geq \mathbb E^Q(V_T^*(\phi)) = \mathbb E^Q(X^*) = Q(A)\] <p>We have \(P^*(A) \geq Q(A)\) for all \(A \in \mathcal F_T\). Also, \(1 - P^*(A) \geq 1 - Q(A)\) as well, implying \(P^* = Q\).</p> <p>\((\Longleftarrow)\) Assume \(P^*\) is the only ELMM. Let \(X^* \in L^1(P^*)\). The martingale</p> \[M_t = \mathbb E^*(X^* \vert \mathcal F_T), 0 \leq t \leq T\] <p>is right-continuous (continuous). From the martingale representation theorem, there is a unique progressive process \(\eta_u = (\eta_u^1, \dots, \eta_u^d)\) such that</p> \[M_t = M_0 + \int_0^t \eta_u \cdot dS_u^*\] <p>Now, correspondingly, there exists a unique self-financing trading strategy \(\phi = (\phi_t^0, \eta_t^1, \dots, \eta_t^d)\) such that \(V_0(\phi) = M_0\) and</p> \[X^* = V_0(\phi) + \int_0^T \phi_u \cdot dS_u^*\] <blockquote> <p>How does the proof conclude here?</p> </blockquote> <hr/> <p>From Girsanov’s theorem, we have seen that</p> \[S_t^{i,*} = S_0^i \exp\left(\int_0^t \sigma_s^i \cdot d\tilde W_ + \int_0^t \vert \sigma_s^i \vert^2 ds + \int_0^t \mu_s^i dst\right)\] <p>where \(\sigma_s^i\) is the \(i\)th row of \(\sigma_s\) which itself is a \((d \times n)\) matrix. We typically assume \(d \leq n\) meaning that we want enough randomness to represent the stocks.</p> <p>Then, we have the following result -</p> <p><strong>Lemma.</strong> The market is complete <strong>iff</strong> \(d = n\) and \(\sigma(w)\) is invertible</p> <p><em>Proof.</em></p> <p>\((\Longleftarrow)\) This is trivial to show</p> <p>\((\implies)\) Let \(k_t(w) \in \mathcal n(\sigma_t(w))\) where \(\mathcal n\) represents the nullspace operation. Also, let \(\|k_t(w) \|\leq 1\) and \(\|k_t(w) \| &gt; 0\) if \(\mathcal n(\sigma_t(w)) \neq \{0\}\). Then, let</p> \[X = S_T^0 (1 + \int_0^T k_u \cdot d \tilde W_u)\] <p>Since the second term is always a local martingale, and \(\|k\|\leq 1\), it’s norm is finite and therefore is a martingale. Also, \(\mathbb E^*(X^*) = 1\).</p> <p>The market being complete means, \(X^* = 1 + \int_0^T \phi_u \cdot dS_u^*\) for some trading strategy \(\phi\). Equating both the equations, we get</p> \[\begin{align*} \int_0^T \left(k_t - \sum_{i = 1}^d \phi_t^i S_t^{i, *} \sigma_t^i\right) \cdot d\tilde W_t &amp;= 0 \\ \int_0^T \left\vert k_t - \sum_{i = 1}^d \phi_t^i S_t^{i, *} \sigma_t^i\right\vert^2 &amp;= 0 \\ k_t = \sum_{i = 1}^d \phi_t^i S_t^{i, ^*} \sigma_t^i \end{align*}\] <p>implying that the vector \(k_t(w)\) is in the column space of \(\sigma_t(w) ^T\). For a matrix \(B\), the orthogonal subspace of \(\mathcal R(B^{T})\) denoted by \([\mathcal R(B^T)]^{\perp} = \mathcal n(B)\). Therefore, \(k_t(w) \perp \mathcal n(\sigma_t(w))\) implying that \(k_t(w) = 0\)!</p> <p>How do we conclude \(n = d\)? If \(d &lt; n\), then the matrix would not be full rank, implying \(d = n\).</p> <hr/> <p><strong>Theorem</strong> If a market is viable and complete, then the no-arbitrage price of \(X\) (assuming \(X\) is integrable \(\in L^1(P^*)\)) at time \(t\) is</p> \[V_t(\phi) = S_t^0 \mathbb E^*(X^* \vert \mathcal F_t)\] <p>The proof is similar to the ones we have shown before, and this theorem completes the whole picture of a continuous market.</p> <hr/> <p>Consider the Black-Scholes model where we had</p> \[X^* = g(S_T^*)\] <p>We had calculated \(\mathbb E^*(g(S_T^*) \vert \mathcal F_t) = f(S_t^*, t)\) where</p> \[\begin{align*} f(x, t) &amp;= \int_{-\infty}^\infty g(x\exp(\alpha z - \alpha^2/2)), x&gt; 0\\ &amp;= \int_0^\infty g(y) \phi\left(\frac{\log(y/s)}{\alpha} + \frac{\alpha}{2}\right) \frac{dy}{\alpha y} \end{align*}\] <p>where \(\alpha = \sigma \sqrt{T - t}\) and \(\phi(z) = \frac{1}{\sqrt{2\pi}} e^{-z^2/2}\). Assume \(\vert g(y) \vert \leq C \vert y\vert^p\) for some \(p, C &gt; 0\). That is, we want the contingent claim to have a polynomial growth.</p> <p>We have \((x, t) \in (0, \infty) \times [0, T)\), and letting \(y = x\exp(\alpha z - \alpha^2/2)\) we get</p> \[\frac{\partial f}{\partial t} + \frac{1}{2} \sigma^2 x^2 \frac{\partial^2 f}{\partial x^2} = 0\] <p>Let us delve into an example to understand this better - consider the calculation for \(\mathbb E[W_T^4 \vert \mathcal F_t]\) Note that this process is a martingale. Now, we present two methods to perform this calculation -</p> <h4 id="method-1">Method 1</h4> <p>We say that this expression is equivalent to evaluating a martingale function - \(= f(W_t, t)\). We need the property \(= f(W_t, t) = \mathbb E[W_T^4 \vert \mathcal F_t]\), and we choose \(f(x, t) = x^4 + 6x^2(T - t) + 3(T - t)^2\) so that it has the following property -</p> \[\begin{align*} f_x &amp;= 4x^2 + 12x(T - t) \\ f_{xx} &amp;= 12x^2 + 12(T - t) \\ f_t &amp;= -6x^2 - 6(T - t) \end{align*}\] <p>We observe that \(f_t + \frac{1}{2} f_{xx} = 0\). From Ito’s formula</p> \[f(W_t,t) = f(0, 0) + \int_0^t f_x dW_s + \underbrace{\int_0^t f_t ds + \int_0^r \frac{1}{2} f_{xx} ds}_{= 0}\] <p>Therefore, \(f(W_t, t)\) is a local martingale. We can check that the integrand \(f_{xx} = 4W_s^3 + 2W_s(T - s)\) does not integrate to infinite (is a finite value), and consequently, \(f(W_t, t)\) is a martingale.</p> <p>We see that \(M_T = W_T^4\). Let the process \(N_t =\mathbb E[W_T^4 \vert \mathcal F_t]\) - this by itself is also a martingale. Consider the following lemma -</p> <p><strong>Lemma.</strong> Suppose \(M_t\) and \(N_t\) are martingales with right continuous paths and \(M_t = N_T\) a.s., then \(P(M_t = N_t, \forall t \in [0, T]) = 1\).</p> <p><em>Proof.</em> \(M_t = \mathbb E[M_T \vert \mathcal F_t] = \mathbb E[N_t \vert \mathcal F_t] = N_t\)</p> <p>This implies that \(M_t\) and \(N_t\) agree on all rational times (the way we define martingales for Brownian motions). However, since the processes are right continuous , we get</p> \[M_t(w) = \lim_{s \downarrow t, s \in Q} M_s(w) = \lim_{s \downarrow t, s \in Q}N_s(w) = N_t(w)\] <p>for an arbitrary \(t \in [0, T]\).</p> <h4 id="method-2">Method 2</h4> <p>Consider the Ito’s formula for \(W_t^4\)</p> \[\begin{align*} W_t^4 &amp;= \int_0^t 4W_s^2 dWs + \int_0^t 6W_s^2 ds \\ \mathbb E[W_u^2 \vert \mathcal F_t] &amp;= W_t^2 + u - t, t &lt; u \\ \mathbb E[W_T^2 \vert \mathcal F_t] &amp;= \underbrace{\int_0^t 4W_s^2 dWs}_{\text{Martingale}} + \int_0^t 6W_s^2 ds + \mathbb E\left[\int_0^T 6W_s^2 ds \vert \mathcal F_t\right] \\ &amp;= W_t^4 + \int_t^T 6\mathbb E[W_s^2 \vert \mathcal F_t] \quad \because \text{Fubini's theorem} \\ &amp;= W_t^4 + 6\int_t^T [W_t^2 + s - t] ds \\ &amp;= W_t^4 + 6W_t^2(T - t) + 3(T - t)^2 \end{align*}\] <p>Both boil down to the same sort of an expression that satisfy the heat equation! But how do we guess the \(f(., .)\) in the first method?</p> <hr/> <p>Okay, now suppose we have an arbitrary function \(g\) and we have to calculate \(\mathbb E[g(W_T) \vert \mathcal F_t]\). We can do the following -</p> \[\begin{align*} \mathbb E[g(W_t + (W_T - W_t)) \vert \mathcal F_t ] = E[g(x + z\sqrt{T - t})]\vert_{x = W_t} \end{align*}\] <p>Since \(W_T - W_t\) is independent of \(\mathcal F_t\) and \(W_t\) is known at \(\mathcal F_t\), we can remove the conditionality in the expectation!</p> <p>Then, we say \(\mathbb [g(x + \sqrt{T - t} x]\vert_{x = W_t} = f(W_t, t)\). For \(g(x) = x^4\), we can expand the binomial expression to get \(f\). Furthermore,</p> \[\begin{align*} f(x, t) = \int_{-\infty}^{\infty} g(y) p_{T - t} (x, y) dy \end{align*}\] <p>where \(p_u(x, y) = \frac{1}{\sqrt{2\pi u}} e^{(y - x)^2/2u}\) . This satisfies the heat equation, namely</p> \[\frac{\partial p_u}{\partial u} = \frac{1}{2} \frac{\partial^2 p_u}{\partial x^2}, \quad \forall y\] <p>and consequently we can check that the <strong>dual</strong> is satisfied for \(f\) as well!</p> \[\frac{\partial f}{\partial u} + \frac{1}{2} \frac{\partial^2 f}{\partial x^2}= 0\] <p>resulting in \(f(W_t, t)\) always being a martingale!</p> <p>This works for any \(g\) that does not grow too fast (polynomial growth - sufficient not necessary), and it does not have anything to do with smoothness of \(g\) either!</p> <p>Also, \(\lim_{t \uparrow T} f(x, t) = g(x)\) as long as \(g\) is continuous at \(x\).</p> <hr/> <p>This indeed is a very powerful result. Let us see how it can be used. Consider another example - Suppose we have \(\mu = r = 0\), and a \(\sigma\) for a price process \(S_t\) of the form \(S_t = S_0 \exp\left(\sigma W_t - \sigma^2 t/2\right)\). Let the contingent claim be</p> \[X = g\left(\underbrace{\int_0^t S_t dt}_{I_T}\right)\] <p>That is, the contingent claim is on the total value is a function of the total value accumulated till time \(T\).</p> <p>Now, \(\mathbb E[g(I_T) \vert \mathcal F_t]\) is calculated as -</p> \[\begin{align*} I_T &amp;= I_t + \int_t^T S_0 \exp(\sigma W_u - \sigma^2 u /2)du \\ &amp;= I_t + S_0\exp(\sigma (W_t + (W_u - W_t)) - \sigma^2 u /2)du \quad \because \text{similar trickas before} \\ &amp;= I_t + S_0 e^{\sigma W_t} \int_0^T e^{\sigma (W_u - W_t)} e ^{-\sigma^2 u} du \\ &amp;= I_t + S_0e^{\sigma W_t} \int_0^{T - t} e^{\sigma \hat W_s} e^{-\sigma^2 s/2} ds \quad \because \hat W_s = W_{t + s} - W_t, s = u - t \\ &amp;= I_t + S_t \underbrace{\int_0^{T - t} e^{\sigma \hat W_s } e^{-\sigma^2 s/2}}_{\perp\!\!\! \perp\mathcal F_t} \end{align*}\] <p>So, \(\mathbb E[g(I_t) \vert \mathcal F_t] = \psi(S_t, I_t, t)\) where \(\psi(x, y, t) = \mathbb E[g(y + x \int_0^{T - t} S_s)]\) if \(S_0 = 1\). The closed form expression depends on \(g\) in this case. Again, using Ito’s formula on \(\psi(S_t, I_t,t)\) we get</p> \[\psi(S_t, I_t, t) = \psi(S_0, 0, 0) + \int_0^t \psi_i'(S_u, I_u, u)dS_u + \int_0^t \psi_2' S_u du + \int_0^t \psi_3'di + \frac{1}{2} \int_0^t \psi_1'' \sigma^2 S_u^2 du\] <p>Using this formulation, we will now look at <strong>Asian Call Options</strong></p> <h1 id="asian-call-option">Asian Call Option</h1> <p>The contingent claim is given by</p> \[X = \left(\frac{1}{T} \int_0^T S_u du - K\right)^+\] <p>and obtain \(g(y) = (\frac{Y}{T} - K)^+\). The no-arbitrage price is given by the martingale</p> \[\begin{align*} V_T &amp;= \mathbb E\left[\left(\frac{1}{T} I_T - K\right)^+ {\Huge \vert} \mathcal F_t\right] \\ &amp;= \mathbb E\left[\left(\frac{1}{T} I_t + \frac{1}{T} \int_t^T S_u du- K\right)^+ {\Huge \vert} \mathcal F_t\right] \end{align*}\] <p>We’ll assume \(\mu = r = 0\) for simplicity. For stock price processes we have seen that</p> \[S_T = S_0 \exp(\sigma W_t - \frac{\sigma^2 T}{2}) = S_t \exp(\sigma(W_T - W_t) - \frac{\sigma^2}{2}(T - t))\] <p>So the idea is to factor out \(S_t\) this way in the expectation as well</p> \[\begin{align*} V_T &amp;= S_t \mathbb E\left[\left(\frac{1}{T} \int_t^T \frac{S_u}{S_t}du - \left(\frac{K - \frac{1}{T}I_t}{S_t}\right)\right)^+ {\Huge \vert} \mathcal F_t\right] \\ &amp;= S_t \phi(X_t, t) \end{align*}\] <p>where</p> \[\begin{align*} X_t &amp;= \frac{K - \frac{1}{T}I_t}{S_t} \\ \phi(x, t) &amp;= \mathbb E\left[\left(\frac{1}{T} \int_0^{T - t} \hat S_u du - x\right)^+\right] \\ \hat S_u &amp;= \exp(\sigma(W_{u + t} - W_t) - u\sigma^2/2) \end{align*}\] <p>The expectation of this integral is not trivial. Therefore, we resort to representing the equations as a differential equation. We know \(V_t = S_t\phi(X_t, t)\) is a martingale from the way it has been constructed. Let us use Ito’s formula to represent it. Furthermore, it can be shown that \(X_t = S_t^{-1} (K - \frac{1}{T} I_t)\) is an Ito’s process. Why? \(I_t\) is an Ito’s process and \(S_t^{-1} = S_0^{-1} \exp(-\sigma W_t + \sigma^2t/2)\) is also an Ito’s process - multiplying Ito’s processes gives an Ito process. Using Ito’s formula -</p> \[\begin{align*} d(S_t^{-1}) &amp;= \frac{1}{S_t} \left(\sigma dW_t + \sigma^2 t\right)\\ dX_t&amp;= -S_t^{-1} \frac{1}{T} S_t dt + (k - \frac{1}{T} I_t ) d(S_t^{-1}) \end{align*}\] <p>The quadratic variation of \(X_t\) is \(0\), and therefore the product rule does not have any covariation terms.</p> <p>Finally, we get</p> \[\begin{align*} dV_t = S_t \left[\phi_t - \frac{1}{T} \phi_x + \frac{\sigma^2}{2} X_t^2 \phi_{xx}\right]dt + \text{martingale terms} \end{align*}\] <p>We have</p> \[\begin{align*} \phi(x, T) = (-x)^+ = x^{-} \\ \phi_t - \frac{1}{T} \phi_x + \frac{\sigma^2 x^2}{2}\phi_{xx} = 0 \end{align*}\] <p>The stochastic differential equation comes from the fact that \(V_t\) is a martingale, and with this we have essentially captured the process in terms of a single spatial random variable. Previously, we had</p> \[\mathbb E(g(I_T) \vert \mathcal F_t) = \psi(S_t, I_t, t)\] <p>where</p> \[\psi_t + x \psi_y + \frac{\sigma^2 x^2}{2} \psi_{xx} = 0 \\ \psi(x, y, T) = \left(\frac{y}{T} - K\right)^+\] <p>In fact, with brutal exercise, we can show that</p> \[\psi(x, y, y) = x \phi\left(\frac{TK - y}{xT}, t\right)\] <p>Note that we were able to do such a simplification for a specific nature of \(g\) - here \(g\) is a straight-line function (\(g_k(cy) = cg_{k/c}(y)\)). Something similar can be done when \(g(y) = y^p\) as well.</p> <h2 id="geometric-asian-call">Geometric Asian Call</h2> <p>A continual version of geometric mean is given by</p> \[G_T = \exp\left(\frac{1}{T}\int_0^T \log S_t dt \right)\] <p>So, the contingent claim is given by</p> \[\begin{align*} X &amp;= \left(\exp\left(\frac{1}{T} \int_0^T \log S_u du\right) -K \right)^+ \\ \text{Also}&amp; \log S_u = \log S_0 + \sigma W_u - \frac{\sigma^2u}{u} \end{align*}\] <p>Once again, assume \(\mu = r = 0\). The no-arbitrage price is given by</p> \[\mathbb E[X \vert \mathcal F_t] = \mathbb E\left[\left[S_0\exp\left(\frac{\sigma}{T} J_t + \frac{\sigma}{T}\int_t^TW_u du - \frac{\sigma^2 T}{4}\right) - K\right]^+ {\Huge \vert} \mathcal F_t\right]\] <p>where \(J_t = \int_0^t W_u du\). Then,</p> \[\begin{align*} \mathbb E[X \vert \mathcal F_t] = \mathbb E\left[\left[S_0\exp\left(\frac{\sigma}{T} J_t + \frac{\sigma}{T}(T-t) W_t + \frac{\sigma}{T}\int_0^{T - t}\hat W_s ds - \frac{\sigma^2 T}{4}\right) - K\right]^+ {\Huge \vert} \mathcal F_t\right] \end{align*}\] <p>where \(\hat W_s = W_{t + s} - W_t\) and \(\hat J_{T - t} = \int_0^{T - t} \hat W_u du \sim \mathcal N(0, (T - t)^3/3)\)</p> <p>The above expression is then equal to \(\psi(W_t, J_t, t)\)</p> <p>where</p> \[\psi(x, y, y) = \mathbb E\left[\left(S_0 \exp\left(\frac{\sigma}{T} y + \frac{\sigma}{T}(T - t) x - \frac{\sigma^2T}{4} + \frac{\sigma}{T} \hat J_{T - t}\right) - K \right)^+\right]\] <p>this is very similar to the expectation we have in Black-Scholes - \(\mathbb E[(x \exp(\alpha x - \frac{\alpha^2}{2}) - K)^+]\) and the final expression can be derived from there.</p> <hr/> <p>Consider another example where \(X = g(\max_{0 \leq t \leq T} S_t)\) , then the no-arbitrage price is</p> \[\begin{align*} \mathbb E[X \vert \mathcal F_t] &amp;= Y_t = f(\underbrace{Z_t}_{\text{Ito}}, \underbrace{A_t}_{\text{adapted and r.c.}}, t)\\ &amp;\implies dY_T = f_1' dZ_t + f_2' dA_t + f_3' dt + f''d \langle Z\rangle -- (1) \end{align*}\] <p>where \(A_t = \max_{0 \leq u \leq t} S_u\) (the Lebesgue measure of this set is 0).</p> \[\begin{align*} A_T &amp;= A_t \vee S_t \left(\max_{u \leq u \leq T} \frac{S_u}{S_t}\right) \\ &amp;= A_t \vee S_t \max_{0 \leq v \leq T- t}\exp(\underbrace{\sigma W_{t + v} - \sigma W_t}_{\sigma \hat W_v} - \frac{\sigma^2}{2}(v))\\ &amp;= A_t \vee S_t \underbrace{\hat A_{T - t}}_{\perp \!\!\! \perp \mathcal F_t} \end{align*}\] <p>So, we get \(V_t = \psi(S_t, A_t, t) = \mathbb E[A_t \vee (S_t \hat A_{T - t})]\) (note that \(\mathcal F_t\) goes away).</p> <p>Using Ito’s formula (equation (1) above) ,</p> \[\begin{align*} dV_t = \underbrace{\psi_1' dS_t}_{\text{martingale}} + \psi_2' dA_t + \psi_3' dt + \psi'' \sigma^2 S^2 dt \end{align*}\] <p>\(dV_t\) should be a martingale by definition, therefore, \(\psi_2' dA_t + \psi_3' dt + \psi'' \sigma^2 S^2 dt = 0\). Since \(dA_t\) and \(dt\) are independent</p> \[\begin{align*} \frac{\partial \psi }{\partial A_t} = 0 \text{ when } dA_t = 0 &amp;\implies S_t = A_t \implies \{(S_t, A_t, t): S_t \leq A_t ,0\leq t\leq T\} \\ \frac{\partial \psi}{\partial t} + \frac{1}{2} \sigma^2 x^2 \frac{\partial^2 \psi}{\partial x^2} = 0&amp;, x&lt;y \end{align*}\] <p>Generally, such PDEs are solved by curated guesses, and there is no algorithmic procedure to determine these.</p> <hr/> <p><strong>Lemma:</strong> \(X, Y\) are independent martingales, then \(X_tY_t\) is a martingale. Applies to local martingale as well</p> <p><em>Proof.</em> Let \(W = (W^1, \dots, W^n)\) be an \(n\)-dimensional Brownian motion.</p> \[\begin{align*} M_t &amp;= \int_0^t H_s \cdot DdW \\ N_t &amp;= \int_0^t K_s \cdot dW \\ \langle M_t, N_T \rangle &amp;:= \int_0^t \sum_i (H_s^i K_s^i) ds \\ &amp;= \int_0^t H_s \cdot K_s ds \end{align*}\] <p>To show \(\mathbb E[X_t Y_t \cdot 1_C] = \mathbb E[X_s Y_s \cdot 1_C]\). Let \(\mathcal F_t^X = \sigma(X_S: s \leq t), \mathcal F_t^Y = \sigma(Y_s: s \leq t)\). Then \(\mathcal F_t^{X, Y} = \sigma(\mathcal F_t^X, \mathcal F_t^Y)\)</p> <p>This is true for \(C = A \cap B\) for \(A \in \mathcal F_s^X, B \in F_S^Y\). From Monotone Class Theorem, if such a relation is true on the Pi class (closed under intersection), then it is true for all \(C\).</p> <p><strong>Corollary.</strong> \(\langle X, Y \rangle \equiv 0\)</p> <hr/> <p>With these results, we can now work with multiple stocks (\(d\)) using the Black-Scholes equation with underlying \(n\)-Brownian motion.</p> <p>Consider this example - -\(d = 1, n = 2, r = 0\). We have \(dS_t = S_t(\beta dt + g(Z_t)dW_t^1), S_0 &gt; 0\) where \(dZ_t = Z_t (\gamma dt + \delta dW_t^2), Z_0 = 1\), \(\beta, \gamma \in \mathbb R, \delta &gt; 0, g(x) \geq \epsilon &gt; 0\).</p> <p><strong>Claim.</strong> There exists at least one ELMM but this market is not complete.</p> <p><em>Proof.</em> Choose a martingale</p> \[M_T^\theta := -\int_0^t \frac{\beta}{g(Z_s)} dW_s' + \theta W_t^2\] <p>for \(\theta \in \mathbb R\). The density process is chosen as \(\Lambda_t = \exp(M_t^\theta - \frac{1}{2} \langle M^\theta \rangle_t)\) as suggested by the Girsanov’s theorem. Now, we need to show two things - \(M_t^\theta\) is a martingale and \(\Lambda_t\) satisfied Novikov’s condition.</p> <p>\(M_t^\theta\) is a local martingale because the individual terms in the expression are martingales. The quadratic variation of \(M_t^\theta\) is given by</p> \[\begin{align*} \langle M^\theta \rangle_t &amp;= \int_0^t \frac{\beta^2}{g(Z_s)^2}ds + \theta^2 t \\ &amp;\leq \frac{\beta^2}{\epsilon^2}t + \theta ^2 t \end{align*}\] <p>Since the quadratic variation of \(M_t^\theta\) is bounded by a constant, \(M_t^\theta\) is a martingale. Also, from Novikov’s condition, \(\Lambda_t\) is a martingale as well.</p> <p>Now, according to Girsanov’s theorem,</p> \[\begin{align*} \tilde W_t^1 &amp;= W_t^1 - \langle W^1, M^\theta\rangle_t \\ &amp;= W_t^1 + \int_0^t \frac{\beta}{g(Z_s)}ds \end{align*}\] <p>is a \(Q^\theta\)-Brownian motion.</p> <p>From the previous result, \(\tilde W^2_t = W_t^2 - \theta t\) is a \(Q\)-Brownian motion.</p> \[\begin{align*} dS_t &amp;= S_t (\beta dt- g(Z_t)dW') \\ &amp;= S_t g(Z_t) d \tilde W'_t \end{align*}\] <p>So, \(S_t\) is a \(Q^\theta\)-local martingale, implying that \(Q^\theta\) is a ELMM.</p> <p>Now, to show that the market is not complete, we’ll explicitly construct a martingale that cannot be replicated. Letting \(\theta = 0\), we get \(\tilde W_T^2 = W_T^2\). To replicate it, we have</p> \[\begin{align*} \tilde W_t^2 = \int_0^t \alpha_u dS_u \\ \end{align*}\] <p>Let us now calculate the quadratic variation -</p> \[\begin{align*} \langle \tilde W^2 \rangle_t &amp;= t \quad \because Q\text{-Brownian motion} \\ &amp;= \langle \tilde W^2, \tilde W^2 \rangle_t = \langle \tilde W^2, \int_0^t \alpha_u S_u g(Z_s) d \tilde W_u' \rangle _t \\ &amp;= \int_0^t \alpha_u S_u g(Z_u) d \langle \tilde W^2, \tilde W^1 \rangle_u = 0 \end{align*}\] <p>because \(\langle \tilde W^2, \tilde W' \rangle_t = \langle W^2, W^1 \rangle_t = 0\). This is a contradiction, and therefore, the market cannot be replicated.</p> \[\sim fin \sim\]]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Discussion on Discrete and continuous pricing models for Options, No-arbitrage pricing and the mathematics involved.]]></summary></entry></feed>