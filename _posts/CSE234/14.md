In summary, inter-operator parallelism assigns different operators of the computational graph to different devices and executed in a pipelined fashion. 

![](/assets/img/2025-01-06-data-systems-for-ml/17405378678788.jpg)

Furthermore, imbalanced stages cause further pipeline bubbles, and RL-based/optimization-based automatic stage partitioning techniques are being explored. 

### Intra-op parallelism

Let us understand some core ideas to implement techniques in this paradigm. 

Suppose we are parallelizing one operator. 
1. Element-wise operations - The operations involved are typically independent of one another and can be parallelized across devices. 
    ![](/assets/img/2025-01-06-data-systems-for-ml/17405380474272.jpg)
    The splitting can be done based on the optimal parameters for the underlying hardware.
2. Matrix multiplication - It is a 3-level nested for-loop. We have the following variations
    ![](/assets/img/2025-01-06-data-systems-for-ml/17405381689985.jpg)
    ![](/assets/img/2025-01-06-data-systems-for-ml/17405381942988.jpg)
    ![](/assets/img/2025-01-06-data-systems-for-ml/17405382663939.jpg)
3. 2D convolution - It is a 7-level nested for-loop as we’ve seen before. 
    ![](/assets/img/2025-01-06-data-systems-for-ml/17405385743018.jpg)

Now, we consider how we parallelize multiple operators together. Note that data parallelism can be seen as a special case of intra-op parallelism. We will consider two main types of matrix multiplication parallelization strategies. 

![](/assets/img/2025-01-06-data-systems-for-ml/17405388302958.jpg)

Different parallelization strategies for operators require different partitioning format of the same tensor. For example, with type 1 matrix multiplications, a ReLU operator following matmul would not require any repartitioning. However, in Type 2, we would require an all-gather operation.

![](/assets/img/2025-01-06-data-systems-for-ml/17405389681048.jpg)

So given these variations, our goal is to minimize the node costs (computation and communication) and the edge costs (repartitioning communication). There have been various solutions using manual design, randomized search, dynamic programming and integer linear programming. We discuss some important projects. 

**Model-specific strategies**
1. **AlexNet** - A convolution layer was split across 2 GPUs. It improved the accuracy by 2%, because the training improved on using 2 GPUs. 
2. **Megatron-LM** - A large language model with 8.3B parameters that outperforms SOTA results! They noted the following - performing Type 2 matmul followed by Type 1 matmul results in only 1 all-reduce operations for both forward and backward passes. 
    ![](/assets/img/2025-01-06-data-systems-for-ml/17405394028552.jpg)

1. **GShard MoE** - A multi-language translation model with 600B parameters that outperforms SOTA. 
    ![](/assets/img/2025-01-06-data-systems-for-ml/17405394573494.jpg)
    Deepseek used this kind of parallelization, and so they optimized all-to-all operations in their pipeline. These implementations were shared as part of their open-source week event. 
    
**Systems for Intra-op parallelism**
2. **ZeRO Optimizer** - The authors noticed that data parallelism replicates gradients, optimizer states and model weights on all devices. So, they partitioned gradients, optimizer states and model weights too! Here is a summary - 
    ![](/assets/img/2025-01-06-data-systems-for-ml/17405396936446.jpg)
    The memory cost in the above table is per device. ZeRO Stage 3 is part of PyTorch through `FSDP` (Fully Sharded Data Parallelism). 
    > Recall why the optimizer, gradients and model weights have the mentioned memory costs. Optimizer states are stored in FP32 (factor of 4) and the rest are in FP16 (factor of 2).
    
    Due to the communication and memory costs, everyone prefers Stage 2 over Stage 1 today. 
    
    1. **ZeRO Stage 2** - The operation `all-reduce` is `reduce-scatter` followed by `all-gather`. Using this idea, ZeRO stage 2 does the following.
        ![](/assets/img/2025-01-06-data-systems-for-ml/17405399912790.jpg)
        We obtained the same results but with much larger memory savings.
    2. **ZeRO Stage 3** - Extending this idea, we partition the model across the ranks.
        ![](/assets/img/2025-01-06-data-systems-for-ml/17405402117787.jpg)
        The trade-off between Stage 2 and Stage 3 is memory vs communication cost. 
1. **Mesh Tensorflow** - The idea is to map tensor dimensions to mesh dimensions for parallelism. A mesh is essentially a matrix of devices. 
2. **GSPMD** - Successor for Mesh Tensorflow (a compiler based approach) but generalizes it. The users do not have to partition every tensor, the compiler takes care of automatic partitioning across the code. That is, the annotations are propagated to the whole graph.
3. There are more works like Tofu and FlexFlow which we do not cover here. 

Now that we have methods for both intra-op and inter-op, how do we combine the both of them? Such kind of a hybrid approach is compulsory to scale the parallelism to clusters that have 1000s of GPUs. 

Remember that these can be combined with the approaches we described before for the best outcome.
1. System-level memory optimizations - Gradient checkpointing or memory swapping
2. ML-level optimizations - Quantization, Sparsification and Low-rank approximation. 

## Auto-parallelization
The motivation to develop auto-parallelization is very similar to why we designed ML compilers for operator optimization. There are many parallelism models on different model architectures. It is difficult to keep track of which method is the best for a given model and the hardware cluster configuration.

Again, the search space is huge because there are 100-10k operators in a model, 80-200 operator types and number of GPUs in a cluster vary from 10s to 1000s! There have been a multitude of methods for this problem - 

![](/assets/img/2025-01-06-data-systems-for-ml/17405409086956.jpg)

-**Search-based methods** simply search the best configuration for a given model and cluster. Most companies rely on this technique. It is similar to a brute-force approach. 
- **Learning-based methods** - Uses some sort of learned heuristic to navigate the search space. The most popular work is [ColocRL by Mirhoseini et al.](https://arxiv.org/abs/1706.04972) That searches the space of inter-op strategies using an ML model to sample candidates and use a policy gradient algorithm to improve the model based on the execution cost. The method was able to obtain strategies that are 20-30% better than an expert-designed assignment. However, the method is not very efficient since it consumes a lot of resources, and RL is not very efficient as well. 
- **Optimization-based methods** - Search methods but with better optimizations. One good method in this category is [Alpa by Zheng et al.](https://arxiv.org/abs/2201.12023). It tries to optimize considering both inter and intra-op parallelism considering the fast and slow connections in the GPU cluster. Intra-op parallelism strategies require communication, so splitting them across ranks with NVLink is a good heuristic. Inter-op can be done across slower communication methods. The method performs the search in a hierarchical manner - inter-op followed by intra-op.

    ![](/assets/img/2025-01-06-data-systems-for-ml/17405414966691.jpg)

    - The inter-op pass, the graph is partitioned into multiple stages, and each stage is assigned different ranks. The states in the dynamic programming algorithm are formed by these choices - partitioning and the sub-mesh assignment. 
        This step also considers the pipeline execution latency, due to different running times of each stages. The running times are given by the intra-op passes, and we have that information. Since dynamic partitioning can introduce imbalances in the pipeline increasing the bubbles, we use dynamic programming with the execution times of each stage to correctly optimize and reach the best strategy. Essentially, the inter-op pass is like the outer loop and we have the information from the inner-loops (intra-op passes) for optimization.
    - The intra-op pass parallelizes the operators across sub-meshes and minimizes the communication. For example, consider the following - 
        ![](/assets/img/2025-01-06-data-systems-for-ml/17407105767310.jpg)
    ![](/assets/img/2025-01-06-data-systems-for-ml/17407106048963.jpg)
> What about the split cost?

Essentially, within each stages, we try to color every node in the stage so that the execution latency of the stage on the assigned mesh is minimized such that the peak memory usage does not cross the memory budget. Note that this problem formulation has node costs (splitting the operator) and edge costs (layout conversions). 
    
With this hierarchical optimization, Alpa is able to 
- Match specialized manual systems with GPT
- Outperform the manual baseline for Mixture of Experts by up to 8x
- Generalize to models without manual plans

However, the paper is not popular because the industry has stuck to one type of model (transformers) and the manual expert design for these has been good enough. 

In summary, we have

![](/assets/img/2025-01-06-data-systems-for-ml/17407109626252.jpg)

How to choose parallelism then? Transformers have been heavily optimized manually, follow Hao’s ultimate guide below. Otherwise, use the automatic compilers. 

We consider the following factors - #GPUs, Model size, JCT (Job completion time), communication bandwidth, etc.

![](/assets/img/2025-01-06-data-systems-for-ml/17407110650089.jpg)
> What is 3D parallelism?

# Connecting dots…
We have studied all the concepts. Let us know see how they are used in practice. 

## A bit of refresher
We’ll list out all aspects of an LLM that are relevant for implementation - 

LLMs do next-token prediction. They basically do Maximum Likelihood Estimation based on the tokens given so far (with the prompt or generated). Transformers at their core are seq2seq NNs built on top of attention mechanism.

The attention score computes how relevant the position i’s token is to a given output token. With language, we choose the same tokens for input and output (hence, self-attention). 

$$
SelfAttention(Q, K, V) = \text{softmax} \left(\frac{QK^T}{d^{1/2}\right) V
$$

Conceptually, we perform this calculation in two steps - 
1. Pre-softmax attention score $$s_{ti} = \frac{1}{\sqrt{d}} q_tk_i^T
2. Weighted average via softmax - $$h = \sum_i \text{softmax}(s)_i v_i

We also have multiple attention heads in models to calculate different features. We get $$Q, K, V$$ using embedding layers $$W_Q, W_K, W_V$$. 

During training, we also mask the attention so that a *causal relation* is maintained. Only attend to previous tokens. In naïve implementation, we make the entries of the attention matrix corresponding to the mask negative infinity. Can we be more smart about them and skip these calculations? Turns out masking later is more efficient due to GPUs behavior we’ve seen before - oversubscribe and don’t disturb the layout!

Given all this, we summarize the following 
1. Self-attention is slow
2. Layer-norm, residual - fast
3. MLPs - slow
4. Non-linear activations - fast
5. Word and position embeddings - fast
6. Loss function - fast

So from a computing perspective, how do we make self-attention fast?

![](/assets/img/2025-01-06-data-systems-for-ml/17407127855541.jpg)

GPT-3 has 96 of these! So we definitely need to make it fast. 

These were the core parts of an LLM. Notably, only the fast operations have been changing a lot, and the core attention and MLP mechanisms have remained. 

| Feature               | Vaswani et al. | LLaMA   |
|-----------------------|---------------|---------|
| Norm Position        | Post          | Pre     |
| Norm Type           | LayerNorm     | RMSNorm |
| Non-linearity       | ReLU          | SiLU    |
| Positional Encoding | Sinusoidal    | RoPE    |

## Training LLMs
As we have noted, the three main components of implementation are compute, memory and communication. We must know
1. The number of parameters of an LLM - it tells us how much memory and communication bandwidth is needed
2. The FLOPs needed to train the LLM - it tells us how much compute is needed
3. The memory needed to train an LLM - again, tells us about memory and communication considering activations and gradients

Let us delve into each of these questions

### Calculating number of parameters
![](/assets/img/2025-01-06-data-systems-for-ml/17407132381084.jpg)

1. The embedding layer has weight matrices of size $$
2. The SwiGLU layer has the following equation

    $$
        \text{SwiGLU}(x) = \text{Swish{(xW_1 + b_1) \cdot (xW_2 + b_1)
    $$
    
    So in comparison with vanilla feedforward, we have one extra set of parameters. 
    
2. The linear transformation in the end essentially converts embeddings to token space and shares the weights with the embedding layer (`tie_embd` in HuggingFace)
 
 ![](/assets/img/2025-01-06-data-systems-for-ml/17407134503575.jpg)

Notice that the major bottleneck comes from the feedforward layers. It has $$12h^2$$ parameters in LLaMa! The second bottleneck is from self-attention which has $$4h^2$$ parameters. 

### Calculating FLOPs
As we’ve noted multiple times, `matmul` is the most expensive operation in terms of FLOPs. We’ve seen that the total number of FLOPs is approximately $$2mnh$$ for multiplying matrices of sizes $$(m \times n)$$ and $$(n \times h)$$. 

| Component                | Output Shape       | FLOPs                      |
|--------------------------|-------------------|----------------------------|
| **Input:**               |                   |                            |
| X                        | (b, s, h)         | 0                          |
| **Self Attention:**       |                   |                            |
| XW_Q, XW_K, XW_V         | (b, s, h)         | 3 * 2bsh²                  |
| RoPE                     | (b, n, s, d)      | 3bsnd                      |
| P = Softmax(QKᵀ/√d)      | (b, n, s, s)      | 2bs²nd + 3bs²n             |
| PV                       | (b, n, s, d)      | 2bs²nd                     |
| AW_O                     | (b, s, h)         | 2bsh²                      |
| **Residual Connection:** | (b, s, h)         | bsh                        |
| **Output from Self Attn:** |                   |                            |
| X                        | (b, s, h)         | 0                          |
| **Feed-Forward SwiGLU:**  |                   |                            |
| XW_gate / XW_up          | (b, s, i)         | 2 * 2bshi                  |
| Swish Activation         | (b, s, i)         | 4bsi                       |
| Element-wise *           | (b, s, i)         | bsi                        |
| XW_down                  | (b, s, h)         | 2bshi                      |
| **RMS Norm:**            | (b, s, h)         | 4bsh + 2bs                 |

where
- **b**: Batch size  
- **s**: Sequence length  
- **n**: Number of attention heads  
- **d**: Hidden state dimension of one head  
- **h**: Hidden state dimension  
- **i**: SwiGLU projection dimension  

 Note that attention is a very expensive operation because we have $$s^2$$ - it does not allow us to scale to large number of tokens! 
 
The final equation we get is 

$$
\text{Total FLOPs} = \#\text{num layers} *(6bsh^2 + 4bs^2h + 3bs^2 n + 2bsh^2 + \#\text{num layers} (6 bshi) + 2bshv
$$

For Llama 7B, this comes out to be 63 TFLOPs! The MLP is $$\approx 55\%$$ of the total FLOPs and attention is $$\approx 41 \%$$! 

![](/assets/img/2025-01-06-data-systems-for-ml/17407142596268.jpg)


To reduce the bottleneck, one might try to decrease $$h$$ (the hidden dimension) to scale up the model. The other way is to reduce the sequence length.

Note the factor of 3 above that is because of training (the model parameters and optimizer parameters).