<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Data Systems for Machine Learning | Sudhansh Peddabomma</title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="Prompting ChatGPT is not enough. To build large-scale AI systems, it is imperative to understand how to design the proper systems to optimize all the computations. The following blog is a deep-dive into system/data design for Machine Learning frameworks."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/data-systems-for-ml/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> <script src="/assets/js/chat.js?e73db4280bae3cbae4d78219277155b9"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Sudhansh Peddabomma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Articles</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Data Systems for Machine Learning</h1> <p class="post-meta">January 6, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/notes"> <i class="fa-solid fa-tag fa-sm"></i> Notes</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>Machine Learning systems have played a pivotal role in the rapid adaptation of Ai in the world today. This domain is essential for solving future problems and also making the current architectures more efficient. This is crucial considering that companies are reactivating nuclear power plants to power AI in the real-world.</p> <p>Along with the progress in AI from small neural networks to large language models, there has been a development in the size of datasets as well. Big data arrived, and AI today relies on these internet-scale datasets. After all, doesn’t ChatGPT just do pattern-matching in the internet?</p> <p>Moreover, the compute capabilities have been scaling exponentially. Just last year (2024), NVIDIA released a new super-chip architecture <a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing" rel="external nofollow noopener" target="_blank">Blackwell</a> that has 97 billion transistors that can reach up to 1.4 exa-flops! The largest super-computer was barely able to reach 1 exa-flop. All this power in the palm of your hand…</p> <p>Richard Sutton once said, search and learning can scale unparalleled with growing computation power.</p> <p>Consider the year 2012 — AlexNet made waves showing SOTA capabilities with images. They use Stochastic Gradient Descent, dropout, convolution networks and initialization techniques. Without ML systems (CUDA, etc), the code would have been 44,000 lines with days of training! With these systems (Jax, PyTorch, TensorFlow) in place, you can achieve the same result in 100 lines within hours of training.</p> <h3 id="in-practice">In Practice</h3> <p>In industry, problems are typically of the form - improve the self-driving car’s pedestrian detection to be X-percent accurate at Y-ms latency budget. For an ML engineer is, the general approach is to design a better model with better learning efficiency followed by hyper-parameter running, pruning, distillation. An ML systems engineer would take the best model by ML researchers, specialize the implementation to target the H/W platform to reduce latency. <em>Streamlining the entire process from development to deployment</em>.</p> <h2 id="overview">Overview</h2> <p>From ad-hoc methods having diverse models and optimization algorithms with various data pre-processing techniques - we have arrived at an optimal algorithm that is <em>iterative and convergent</em>. As our models have become more and more specialized, the computation resources scaled exponentially.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17363053041704.jpg" alt=""></p> <p>Through the course of this article, we will cover deep learning basics, computational graphs, Autodiff, ML frameworks, GPUs, CUDA and collective communication.</p> <p>There are more related topics to the ones discussed here</p> <ul> <li>ML for systems</li> <li>ML Hardware design</li> </ul> <p>Unfortunately, the textbook for this content is just miscellaneous research papers.</p> <h1 id="background">Background</h1> <h2 id="dl-computation">DL Computation</h2> <p>The idea is to concatenate composable layers</p> \[\theta^{(t + 1)} = f(\theta^{(t)}, \nabla_L(\theta^{(t)}, D^{(t)})\] <p>A <strong>model</strong> is a parameterized function that describes how we map inputs to predictions. The parameters are optimized using optimization methods like <strong>SGD</strong>, Newton methods, etc. A <strong>loss function</strong> guides the model to give feedback on how well the model is performing.</p> <p>Having these basic definitions, we will build abstractions to map all the models being used today. It is not possible to build systems to support all models. A quick refresher of important models</p> <ul> <li> <strong>CNNs</strong> - Learnable filters to convolute across images to learn spatial features. The top 3 breakthrough architectures were - AlexNet, ResNet, U-Net. What are the important components in CNNs? <ul> <li>Convolution (1D, 2D, 3D)</li> <li>Matmul</li> <li>Softmax</li> <li>Element-wise operations - ReLU, add, sub, pooling, normalization, etc.</li> </ul> </li> <li> <strong>Recurrent Neural Networks</strong> - Many problems in nature are many-to-many. RNNs maintain an internal state that is updated as a sequence is processed. Arbitrary inputs and outputs can be generated, and any neural network can be used in the RNN architecture. The top 3 breakthrough architectures were - Bidirectional RNNs, LSTMs, GRU. What are the important components in RNNs? <ul> <li>Matmul</li> <li>Element-wise non-linear - ReLU, Sigmoid, Tanh</li> <li>Underlying MLP RNNs have a problem of forgetting (\(0.9*0.9*… \approx 0\)). Additionally, they lack <strong>parallelizability</strong> - both forward and backward passes have \(O(sequence length)\).</li> </ul> </li> <li> <strong>Transformers</strong> (Attention + MLP) - Treat representations of each element in the sequences as queries to access and incorporate information from a set of values. Transformers have an encoder part (BERT most famous) and a decoder part (GPT most famous). Along with these, DiT is one of the top 3 models. What are the important components in Transformers? <ul> <li>Attention - Matmul, softmax, Normalization</li> <li>MLP</li> <li>Layernorm, GeLU, etc.</li> </ul> </li> <li> <strong>Mixture of Experts</strong> - Voting from many experts is better than one expert. Latest LLMs are mostly MoEs - Grok, Mixtral, Deepseek-v3. A router (Matmul, softmax) is the novel component in MoE - it makes system design difficult.</li> </ul> <h2 id="machine-learning-systems">Machine Learning Systems</h2> <p>As mentioned before, the three pillars for the systems are data, model and compute. The foal is to express as manny as models as possible using one set of programming interface by connecting math primitives.</p> <h3 id="computational-dataflow-graph">Computational Dataflow Graph</h3> <p>A representation to show data flow in programs. A <strong>node</strong> represents the computation (operator) and an <strong>edge</strong> represents the data dependency (data flowing direction). A node can also represent the input/output tensor of the operator.</p> <h3 id="example-deep-learning-with-tensorflow-v1">Example: Deep learning with TensorFlow v1</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tinyflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span> <span class="c1"># Forward declaration 
</span><span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span><span class="n">tf</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">reduction_indices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># Loss function declaration 
</span><span class="n">W_grad</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">,</span> <span class="p">[</span><span class="n">W</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Automatic differentiation
</span><span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">assign</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">W_grad</span><span class="p">)</span> <span class="c1"># SGD update rule 
</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">})</span> <span class="c1"># Real-execution happens here
</span></code></pre></div></div> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17364786998993.jpg" alt=""></p> <p>This DAG representation opens up all possibilities of optimizations. However, creating such a graph doesn’t allow flexibility - once a graph is defined, it cannot be changed based on the input.</p> <h3 id="example-pytorch">Example: PyTorch</h3> <p>PyTorch also uses computational graphs, but it creates it on the fly. Previously, we had defined the graph and then executed it. Symbolic declaration vs imperative programming. Define-then-run vs Define-and-run. C++ vs Python.</p> <p>What are the pros and cons?</p> <table> <thead> <tr> <th> </th> <th>Good</th> <th>Bad</th> </tr> </thead> <tbody> <tr> <td>Symbolic</td> <td>Easy to optimize, much more efficient (can be 10x faster)</td> <td>The way of programming can be counter-intuitive, hard to debug and less flexible</td> </tr> <tr> <td>Imperative</td> <td>More flexible, easy to program and debug</td> <td>Less efficient and more difficult to optimize</td> </tr> </tbody> </table> <p>How does TensorFlow work in Python then? Tensorflow has Python as the interface language.</p> <p>Apart from these two famous frameworks, there were more like Caffe, DyNet, mxnet (has ability to switch between both), etc. Recently, Jax (derived from Tensorflow) has been getting more popular.</p> <h3 id="just-in-time-jit-compilation">Just-in-time (JIT) compilation</h3> <p>Ideally, we want define-and-run during development and define-then-run during deployment. However do we combine both? PyTorch introduced a deploy mode through a decorator <code class="language-plaintext highlighter-rouge">torch.compile()</code>. So is there an issue with JIT? It creates only static graphs, and cannot work with conditionals or loops in the code.</p> <h3 id="static-vs-dynamic-models">Static vs Dynamic models</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17364797895738.jpg" alt=""> Static graphs are defined and optimized only once. The execution follows a defined computation. On the other hand, dynamic graphs depend on the input. It is difficult to express complex flow-control logic and debug. The implementation is also difficult.</p> <p>As seen above, LSTMs are trying to replace the dynamics in the natural language problem.</p> <p><strong>How to handle dynamics?</strong></p> <ul> <li>Just do Define-and-run and forget about JIT - most popular unforunately :(</li> <li>Introduce Control Flow Ops - <ul> <li>Example: Switch and Merge. This can be added a computational primitive in the graph and introduce dynamics in the graph.</li> <li>These ideas are natural across all programming languages - conditionals and loops. However, the problem with this approach is that graphs becomes complex, and more importantly, how does we do back propagation? What is the gradient of “switch”? TensorFlow team has been working on this.</li> </ul> </li> <li>Piecewise compilation and guards - This approach is better adopted than control flow. <ul> <li>Case 1: A graph accepting input shapes of \([x, c1, c2]\) where \(x\) is variable. The solution is to compile for different values of \(x\) (powers of 2).</li> </ul> </li> </ul> <p>So far, we have seen representations that express the forward computations using primitives. But, how do we represent backward computations?</p> <h1 id="autodiff-ad">Autodiff (AD)</h1> <p>Derivative can be taken using the first order principles. However, this approach can be slow since we have to evaluate the function twice \(f(\theta + \epsilon) , f(\theta)\) and it is also error prone \(\theta(\epsilon^2)\).</p> <p>To optimize the derivative calculation, we pre store the gradients of primitives and map the derivative chain rules in the computational graph. There are two ways of doing this as well</p> <ol> <li>Calculating the derivative from left (inside) to right (outside) in a network - from inputs to outputs</li> <li>Calculating it from right to left - from outputs to inputs</li> </ol> <p>Both are valid approaches and we will discuss them in detail.</p> <h2 id="forward-mode-autodiff">Forward Mode Autodiff</h2> <p>We start from the input nodes, and derive the gradients all the way to the output nodes. <strong>Cons</strong> - - For \(f: R^n \to R^k\), we need \(n\) forward passes to get the gradients with respect to each input. - However, it is usually the case that \(k = 1\) (loss) and \(n\) is very large.</p> <blockquote> <p>If this is confusing, think of it this way - we want the gradient of output with respect to all parameters to update weights. However, forward mode calculates the gradient of inputs with respect to all parameters.</p> </blockquote> <h2 id="reverse-mode-autodiff">Reverse Mode Autodiff</h2> <p>We define the quantity <em>adjoint</em> \(\bar v_i = \frac{\partial y}{\partial v_i}\). We then compute each \(\bar v_i\) in the reverse topological order of the graph. This way, we can simply do one backward pass to get the necessary gradients.</p> <p>In some scientific scenarios, we can have \(k &gt;&gt; n\) where the forward mode can be more efficient.</p> <blockquote> <p>What are the size bounds of the backward graph as compared to the neural network?</p> </blockquote> <p>We construct backward graphs in a symbolic way to reuse it multiple times.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369099776396.jpg" alt=""></p> <h2 id="backpropagation-vs-reverse-mode-ad">Backpropagation vs. Reverse-mode AD</h2> <p>In old frameworks like Caffe/cuda-convnet, the backward computations were done through the forward graph itself. Newer frameworks like Tensorflow and PyTorch construct the backward graph explicitly. The reasons to do so are -</p> <ol> <li>Explicit graphs allow backward computation with any input values. They have flexibility to even calculate gradient of gradients.</li> <li>Having an explicit backward graph can help optimization!</li> <li>Gradient update rules can be efficiently implemented.</li> </ol> <h2 id="gradient-update-rules">Gradient update rules</h2> <p>Typically done via gradient descent, the weights are updated with the gradients with the following simplified rule</p> \[f(\theta, \nabla_l) = \theta - \eta \nabla_L\] <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369103443367.jpg" alt=""></p> <h1 id="architecture-overview-of-ml-systems">Architecture Overview of ML systems</h1> <p>The aim is to make the systems fast, scalable, memory-efficient, run on diverse hardware, energy efficient and easy to program/debug/deploy. Phew.</p> <p>We have discussed dataflow and Autodiff graphs. However, there are numerous things that can be added to these - graph optimization, parallelization, runtime memory, operator optimizations and compilation.</p> <h2 id="graph-optimization">Graph Optimization</h2> <p>The goal is to rewrite the original graph \(G\) as \(G’\) that is faster.</p> <p>Consider the following motivating example - Typically, convolution is followed by batch normalization. Instead of performing batch normalization, just update the weights in convolution to do everything in one step!</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369110087495.jpg" alt=""></p> <p>Note that some steps can become slower based on the hardware, but you get the general idea.</p> <p>Similarly, in attention calculations, the code is typically written with a concatenated vector of queries, keys and values. This version is optimal - it can be understood with <em>Arithmetic Intensity</em> which the ratio of #operations and #bytes. For example, an addition operation has intensity of \(1/3\) (2 loads and one store). However, fusing multiple arithmetic operations reduces the loads and stores by bringing all variables into memory once, improving the arithmetic intensity.</p> <h3 id="so-how-do-we-optimize-graphs">So how do we optimize graphs?</h3> <p>We write rules or templates for opportunities to simplify graphs. There is also implementation of <em>auto-discovering</em> optimizations in the latest libraries, we shall study these.</p> <h2 id="parallelization">Parallelization</h2> <p>The goal is to parallelize the graph computation over multiple devices. Note that devices can be connected with fast (memory communication NVLink) and slow connections (across GPUs), with up to 10x performance difference. Ideally, we do not want to describe partitioning rules for every new model that comes up. Based on these communication patterns, distributing the tasks is not an easy problem. So, we shall discuss how we partition the computational graph on a device cluster.</p> <h2 id="runtime-and-scheduling">Runtime and Scheduling</h2> <p>How do we schedule the compute, communication and memory in a way that execution is as fast as possible, communication is overlapped with compute and is subject to memory constraints?</p> <h2 id="operator-implementations">Operator Implementations</h2> <p>The goal is this layer is to get the fastest possible implementation of <code class="language-plaintext highlighter-rouge">matmul</code>s, for different hardware, different precision and different shapes.</p> <p>NVIDIA releases a GPU every 2 years, and they have rewrite all operations every time! Notably, previously, models were trained using 32-bit floating points, but now researchers are emphasizing on lower and lower precisions.</p> <p>Now, we shall delve into each of these architectures.</p> <h1 id="operator-optimization-and-compilation">Operator Optimization and Compilation</h1> <p>The goal is maximize arithmetic intensity. In general there are three ways to speed up operators</p> <h3 id="vectorization">Vectorization</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369120762344.jpg" alt=""></p> <p>The right version is faster because of the hardware - cache sizes, etc. Tensorflow and PyTorch have this built-in.</p> <h3 id="refactoring-data-layout">Refactoring data layout</h3> <p>This is again related to how data is stored in memory. For example, C++ stores matrices in row-major order. Accessing columns of a matrix can be 10x slower! Remember this while writing code to lower cache misses and reduce pointer movements.</p> <p>ML systems don’t store tensors in row or column major but in a new format called <strong>strides format</strong> - <code class="language-plaintext highlighter-rouge">A[i, j, …] = A.data[offset + i*A.strides[0] + j*A.strides[1] + …</code>. It is a generalization of row and column major storage, and it offers more flexibility - so based on the batch-sizes or other parameters in a neural network.</p> <p>Strides can separate the underlying storage and the view of the tensor. Consider the following operations</p> <ol> <li> <code class="language-plaintext highlighter-rouge">slice</code> - simply changing the offsets and shape will output the slice without any copying involved.</li> <li> <code class="language-plaintext highlighter-rouge">transpose</code> - modifying strides will transpose the tensor without any copying! For example, consider the following example <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>     <span class="n">M</span><span class="p">.</span><span class="nf">strides</span><span class="p">()</span> <span class="c1"># (24, 12, 4, 1)
</span>     <span class="n">M</span><span class="p">.</span><span class="nf">permute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
     <span class="n">M</span><span class="p">.</span><span class="n">t</span><span class="p">.</span><span class="nf">strides</span><span class="p">()</span> <span class="c1"># (12, 4, 1, 24)
</span></code></pre></div> </div> </li> <li> <code class="language-plaintext highlighter-rouge">broadcast</code> - Suppose we have to extend a tensor’s data across a dimension for performing operations with another tensor, then by simply adding <code class="language-plaintext highlighter-rouge">0</code> stride in the appropriate dimensions would be enough! Again, no copying</li> </ol> <p>Many more operations can be done without copying the data and simply modifying the strides. For example, consider the following example -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17370816868291.jpg" alt=""></p> <p>However, strides also has an issue - Memory access may become non-contiguous, and many vectorized ops require continuous storage.</p> <h3 id="summary">Summary</h3> <p>To make operators efficient, we have seen the following tactics -</p> <ol> <li>Vectorization - leverage platform-specific vectorized functions that reduce seek time</li> <li>Data layout - strides format that allow zero-copies enabling fast array-manipulations</li> <li>Parallelization on CPUs</li> </ol> <p>These were techniques for general operations. However, we can optimize certain operators with their special properties.</p> <h3 id="matmul-optimization">Matmul optimization</h3> <p>The brute-force approach takes \(\mathcal O(n^3)\). The best approach humans know is \(\mathcal O(n^{2.371552})\)!</p> <p>How to improve the speed in practice then? Recall that we are trying to increase AI = #ops/#bytes.</p> <blockquote> <p><strong>Memory Hierarchy</strong> If everything ran on registers, things would be super-fast. But, that is expensive. Remember that L1-Cache has 0.5ns latency, L2-Cache has 7ns and DRAM has 200ns (400x slower!)</p> </blockquote> <p>Let us analyze the AI of <code class="language-plaintext highlighter-rouge">matmul</code> considering the different layers of memory</p> <ol> <li>We can directly move data to registers in every iteration in inner loop</li> </ol> <h2 id="gpus-and-accelerators">GPUs and accelerators</h2> <p>Recall that parallelizing operations across threads is super useful! CPUs have some level of parallelism through SIMD operations (vectorization) but they are limited. Building on the same idea, GPUs were born.</p> <p>When we started out, the ALU units were limited by the physical space on the chips. As technology improved, we moved from 70nm process all the way 3nm process! That is, we can fit up to 20x more cores in the same area! The majority of the area on CPUs is consumed by Control and Cache, and Jensen thought, ditch those and put cores.</p> <p>Graphical Processing Unit (GPU) are tailored for matrix or tensor operations. The basic idea is to use tons of ALUs (weak but specialized) with massive parallelism (SIMD on steroids).</p> <p>There are other hardware accelerators like Tensor Processing Unit (TPU) or Application specific integrated circuit (ASIC), etc. The common theme across all these is the same - there are specialized cores. What are specialized cores? They can only compute certain computations. Specialized cores can be super powerful - <img src="/assets/img/2025-01-06-data-systems-for-ml/17370849151083.jpg" alt=""></p> <p>Companies also tried reducing precision and maintain the same performance. Additionally, they also tune the distribution of different components for specific workloads.</p> <blockquote> <p>Why does quantization work in ML systems?</p> </blockquote> <h2 id="recap">Recap</h2> <p>Consider the following question - What is the arithmetic intensity of multiplying two matrices?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Load A
Load B
C = matmul(A, B)
</code></pre></div></div> <p>If the size of the matrix \(A\) is \((m, n)\) and \(B\) is \((n, p)\) then the Flops is \(2mnp\).</p> <p><code class="language-plaintext highlighter-rouge">matmul</code> is an important operation. <check></check></p> <p>Now, consider the following operations</p> <ul> <li><code class="language-plaintext highlighter-rouge">broadcast_to</code></li> <li><code class="language-plaintext highlighter-rouge">slice</code></li> <li><code class="language-plaintext highlighter-rouge">reshape</code></li> <li>Permute dimensions</li> <li><code class="language-plaintext highlighter-rouge">transpose</code></li> <li><code class="language-plaintext highlighter-rouge">contiguous</code></li> <li>indexing like <code class="language-plaintext highlighter-rouge">t[:, 1:5]</code> </li> </ul> <p>Just to recap, the strides of a tensor of shape <code class="language-plaintext highlighter-rouge">[2, 9, 1]</code> stored in row major order are <code class="language-plaintext highlighter-rouge">[9, 1, 1]</code></p> <p>Consider the cache tiling operation -</p> <ul> <li>It increases the memory allocated on Cache and memory transfers between cache and register</li> <li>It reuses the memory movement between Dram and Cache</li> <li>The arithmetic intensity <em>decreases</em> since there is more load and store</li> </ul> <h1 id="gpu-and-cuda">GPU and CUDA</h1> <p>We have seen that specialized cores offer much better performance over traditional CPUs. Consider the following basic architecture of a GPU</p> <p>Let us see the basic terminology for understanding the architecture -</p> <ul> <li> <strong>Threads</strong> - Smallest units to process a chunk of data.</li> <li> <strong>Blocks</strong> - A group of threads that share memory. Each block has many threads mapped to a <em>streaming multiprocessor</em> (SM/SMP).</li> <li> <strong>Grid</strong> - A collection of blocks that execute the same kernel.</li> <li> <strong>Kernel</strong> - CUDA program executed by many CUDA cores in parallel.</li> </ul> <p>A GPU can be made more powerful by</p> <ul> <li>Adding SMs</li> <li>Adding more cores per SM</li> <li>Making the cores more powerful - at a point of <em>diminishing rewards</em>.</li> </ul> <p>NVIDIA, the largest GPU company, has released P100, V100, A100, H100 and B100 (Blackwell) for ML development. K80, P4, T4 and L4 were a lower tier of GPUs. Let us analyze how the compute has changed across these versions</p> <ol> <li>V100 (2019 -) - 80SMs, 2048 threads/SM - $3/hour</li> <li>A100 (2020 -) - 108SMs, 2048 threads/SM - $4/hour</li> <li>H100 (2022 -) - 144SMs, 2048 threads/SM - $12/hour</li> <li>B100 and B200 (2025 -)-</li> </ol> <p>The numbers are not doubling, then how has the performance doubled? They decreased the precisions.. :(</p> <h2 id="cuda">CUDA</h2> <p><strong>What is CUDA?</strong> It is a C-like language to program GPUs, first introduced in 2007 with NVIDIA Tesla architecture. It is designed after the grid/block/thread concepts.</p> <p>CUDA programs contain a hierarchy of threads. Consider the following host code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">const</span> <span class="kt">int</span> <span class="n">Nx</span> <span class="o">=</span> <span class="mi">12</span><span class="p">;</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">Ny</span> <span class="o">=</span> <span class="mi">6</span><span class="p">;</span>

<span class="kt">dim3</span> <span class="nf">threadsPerBlock</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// 12</span>
<span class="kt">dim3</span> <span class="nf">numBlocks</span><span class="p">(</span><span class="n">Ns</span><span class="o">/</span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">x</span> <span class="p">,</span> <span class="n">Ny</span><span class="o">/</span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">y</span> <span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// (3, 2, 1) = 6</span>

<span class="c1">// the following call triggers execution of 72 CUDA threads</span>
<span class="n">matrixAddDoubleB</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">);</span>
</code></pre></div></div> <p>The GPUs are associated with constants such as</p> <ul> <li> <code class="language-plaintext highlighter-rouge">GridDim</code> - dimensions of the grid</li> <li> <code class="language-plaintext highlighter-rouge">blocking</code> - the block inter within the grid</li> <li> <code class="language-plaintext highlighter-rouge">blockDim</code> - the dimensions of a block</li> <li> <code class="language-plaintext highlighter-rouge">threadIdx</code> - the thread index within a block With these in mind, the CUDA kernel for the above code is designed as</li> </ul> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__device__</span> <span class="kt">float</span> <span class="nf">doubleValue</span><span class="p">(</span><span class="kt">float</span> <span class="n">x</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// kernel definition </span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">matrixAddDoubleB</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">])</span>
    <span class="p">{</span>
        <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="n">C</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">doubleValue</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
</code></pre></div></div> <p>The host code launched a grid of CUDA blocks, which then call the <code class="language-plaintext highlighter-rouge">matrixAdd</code> kernel. The function definition starts with <code class="language-plaintext highlighter-rouge">__global__</code> which denotes a CUDA kernel function that runs of the GPU. Each thread indexes its data using <code class="language-plaintext highlighter-rouge">blockIdx</code>, <code class="language-plaintext highlighter-rouge">blockDim</code>, <code class="language-plaintext highlighter-rouge">threadIdx</code> and execute the compute. It is the user’s responsibility to ensure that the job is correctly partitioned and the memory is handled correctly.</p> <p>The host code has a serial execution. However, the device code has SIMD parallel execution on the GPUs. When the kernel is launched, the CPU program <em>continues executing</em> without <em>halting</em> while the device code runs on the GPU. Due to this design, it is important that the device code does not have any return values - causes erroneous behavior. To get results from the GPU, <code class="language-plaintext highlighter-rouge">CUDA.synchronize</code> is used (an example will be shown later).</p> <p>It is the developers responsibility to map the data to blocks and threads. The blockDim, shapes etc should be statically declared. This is the reason why compilers like <code class="language-plaintext highlighter-rouge">torch.compile</code> requires static shapes. The CUDA interface provides a CPU/GPU code separation to the users.</p> <p>The SIMD implementation has a constraint for the control flow execution - it requires all ALUs/cores to process in the same pace. In a control flow, not all ALUs may do useful work and it can lead to up to 8 times lower peak performance.</p> <h3 id="coherent-and-divergent-execution">Coherent and Divergent execution</h3> <p>A coherent execution applied the same instructions to all data. Divergent executions do the opposite and they need to be minimized in CUDA programs. This distinction is important to note - even the latest models like the LLMs have this behavior. Concepts such as attention masking and sliding window attention are examples of divergent behavior and they need to be specially implemented to extract the most compute from the GPU.</p> <h2 id="cuda-memory-model">CUDA Memory model</h2> <p>CUDA device (SIMD execution on GPU) has its own memory called the <em>HBM</em>.</p> <p>Unlike host (CPU) memory that is stored as pages in the RAM, GPU memory does not use pages but has memory pools (bulk data) that are accessed all at once.</p> <p>Memory can be allocated <code class="language-plaintext highlighter-rouge">cudaMalloc</code> and populated with <code class="language-plaintext highlighter-rouge">cudaMemcpy</code> like usual. CUDA has a concept called <strong>pinned memory</strong> that is part of the host memory which is optimized for data transfer between CPU/GPU. Ig is not pagable by the OS and is locked, and only certain APIs can access it.</p> <p>Every thread has its own private memory space, and every block has a shared memory that all its threads can access. The HBM is the global device memory in the GPU that can be accessed by all threads. The memory complexity is to balance between speed and shared memory parallelism.</p> <p>For example, consider the program for window averaging -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mf">3.0</span>
</code></pre></div></div> <p>How can this be parallelized? Since every 3-element tuple reduction is independent, each reduction can be mapped to a CUDA core. So, each thread can compute the result for one element in the output array.</p> <p>The host code -</p> <pre><code class="language-C">int N = 1024*1024;
cudaMalloc(&amp;devInput, sizeof(float)*(N+2)); // To account for edge conditions
cudaMalloc(&amp;devOutput, sizeof(float)*N);

convolve&lt;&lt;&lt;N/THREADS_PER_BLK, THREADS_PER_BLK&gt;&gt;&gt;(N, devInput, devOutput); 
</code></pre> <p>The device code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="cp">#define THREADS_PER_BLK = 128
</span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolve</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
        <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">//thread-local variable</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span>
        <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span><span class="mf">3.</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div> <p>This program can be optimized - each element is read thrice!<br> Notice that the number of blocks assigned is much more than what a typical GPU has. This is a general practice in CUDA programming where the blocks are <em>oversubscribed</em>.</p> <p>How to optimize? The memory hierarchy can be utilized -</p> <p>The new device code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="cp">#define THREADS_PER_BLK = 128
</span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolve</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
        <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">//thread-local variable</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span>
        <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span><span class="mf">3.</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div> <p>We introduced a synchronization primitive here. <code class="language-plaintext highlighter-rouge">_syncthreads()</code> waits for all threads in a block to arrive at this point. Another primitive <code class="language-plaintext highlighter-rouge">cudasynchronize()</code> that syncs between host and the device.</p> <h2 id="compilation">Compilation</h2> <p>A CUDA program also needs to be converted to low-level instructions to be executed. A compiled CUDA device binary includes -</p> <ul> <li>Program text (instructions)</li> <li>Information about required resources - 128 threads per block, 8 types of local data per thread and 130 floats (520 bytes) of shared space per thread block.</li> </ul> <p>The issue is that different GPUs have different SMs. If the user asks for a static (large) number of blocks, how to handle this? The first solution is that GPUs have varying (limited) number of blocks.</p> <p>Furthermore, CUDA schedules the threadblocks to many cores using a dynamic scheduling policy that respects the resource requirements. It assumes that the thread blocks can be executed in any order. The blocks are assigned based on the available resources and the remaining ones are <em>queued</em>.</p> <h2 id="understanding-a-gpu">Understanding a GPU</h2> <p>Consider a NVIDIA GTX 980 (2014) that has the following specs -</p> <ul> <li>96KB of shared memory</li> <li>16 SMs</li> <li>2048 threads/SM</li> <li>128 CUDA cores/SM Note that the number of CUDA cores is not equal to the number of CUDA threads.</li> </ul> <p>As the GPUs became better, NVIDIA tried to increase the shared memory per SMM. This is similar to the SRAM which is very important for LLM inference.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/NumAn/">Numerical Analysis Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ipl/">IPL Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/dbms/">DiBS Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ai-agents/">AI Agents</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/automata/">Automata Notes</a> </li> </div> <script>document.querySelectorAll("#table-of-contents a").forEach(function(e){e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substring(1);document.querySelectorAll(".content-section").forEach(function(e){e.classList.add("hidden")});var n=document.getElementById(t);n&&n.classList.remove("hidden")})});</script> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Sudhansh Peddabomma. Last updated: January 22, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0K9MLG0V24");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <div class="chat-toggle-container"> <button id="chat-toggle-btn" class="chat-toggle-btn"> <i class="fas fa-comments"></i> </button> </div> <div id="chat-window" class="chat-window"> <div class="chat-header"> <h5 class="mb-0">Talk to my AI</h5> <button id="close-chat" class="btn-close"> <i class="fas fa-times"></i> </button> </div> <div id="chat-messages" class="chat-messages"></div> <div class="chat-input-container"> <input type="text" id="chat-input" class="form-control" placeholder="Type a message..."> <button id="send-btn" class="btn btn-primary"> <i class="fas fa-paper-plane"></i> </button> </div> </div> </body> </html>