# Lecture 2

## Finite digit arithmetic

When we do arithmetic, we allow for infinitely many digits. However, in the computational world, each representable number has only a fixed and finite number of digits. In most cases, the machine arithmetic is satisfactory, but at times problems arise because of this discrepancy. The error that is produced due to this issue is called the **round-off** error. 

A 64-bit representation is used for a real number. The first bit is a **sign indicator**, denoted by *s*. This is followed by an 11-bit exponent, *c*, called the **characteristic**, and a 52-bit binary fraction, *f*, called the **mantissa**. To ensure that numbers with small magnitude are equally representable, 1023 is subtracted from the characteristic, so the range of the exponent is actually from -1023 to 1024.

Thus, the system gives a floating-point number of the form
$$
(-1)^s\cdot2^{c - 1023}\cdot(1 + f)
$$
Since 52 binary digits correspond to between 16 and 17 decimal digits, we can assume that a number represented in this system is accurate till the 16th decimal.

> Can't a single number be represented in many ways through this representation? Think about 1 for instance.

The smallest positive number that can be represented is with $$s = 0, c = 1, f = 0$$, so it is
$$
2^{-1022}\cdot(1 + 0) \approx 0.22251 \times 10^{-307}
$$
Numbers occurring in calculations that have a magnitude less than this number result in **underflow** and are generally set to 0.

The largest positive number is
$$
2^{1023}\cdot (2 - 2^{-52})
$$
Numbers above this would result in an **overflow**.

## Floating-point representation

We will use numbers o

