# [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)
Presented by [Joycelyn Yiu](https://www.linkedin.com/in/joycelyn-yiu)

What are distribution representations of words? Since symbols have no internal structure, researchers tries to represent words with vectors such that words that are similar to each other are closer in the vector space and vice versa.

The inspiration for this idea comes from 1980’s, to convey semantics of language to machines. These representations can capture patterns and relationships between words like synonyms and grammar rules. How do we capture these patterns in practice?

### Skip-Gram model
The skip gram model is a method that teaches a computer to understand the meaningful of words by reading large pieces of text. It basically learns the word relationships by predicting the surrounding words for a given word.

In essence, from the training text, we compute the probability of *context* words occurring after a *center* word. How do you learn a distribution over words? 

The authors of the paper replaced softmax with a simpler one called **Noise Contrastive Estimation NCE)** that improved training and quality of representations.

This model typically does not work well for rare words. 

In addition to the vanilla model, they added extension to represent phrases. Let us delve into each of these contributions.

## Method
### Hierarchical softmax
The authors introduced hierarchical softmax that brought down the number of evaluations to $$\log_2(W)$$ nodes rather than the typical $$W$$ output nodes (where $$w$$ is the number of words in the vocabulary).

> Is this used in LLMs? LLMs have become ubiquitous because of their parallelizability, and this takes it away to some extent.

### Negative Sampling
Introduced by Guzman and Hyvarinen, NCE tries to differentiate data from noise using logistic regression.

### Subsampling of Frequent Words
Common words like “the”, “a”, etc. occur frequently in text and do not provide a lot of information as compared to rare words. To counter this imbalance, the authors introduced a frequency based discard probability for the words to subsample the words. This improves the training speed and the accuracy of rare words.

### Learning phrases 
The simple addition the authors did was to create new tokens for phrases like “The New York Times” - increasing the vocabulary size potentially making it unscalable. However, iteratively training the model considering longer phrases seemed to obtain a decent performance according to the authors. 

These techniques can be added to any underlying neural network model.

## Experiments

The authors noticed that the representations posses a linear structure that allowed vector arithmetic. This potentially is a consequence of the training objective - the word vectors are in a linear relationship with the inputs to softmax non-linearity. 

# [Attention is all you need](https://arxiv.org/abs/1706.03762)
Presented by [Hansen Jin Lillemark](https://www.linkedin.com/in/hansenlillemark)

What is intelligence? We represented words with high-dimensional vectors, and we seemed to have cracked natural language? How does the brain understand and represent concepts?

Language is one of the easiest avenues to test our algorithms of artificial intelligence. We have stored a lot of data in the past few decades, and this set up a good foundation to train models here.

Transformers, inspired from some brain mechanisms, have become the go-to mechanism for most of the ML applications. It starts off with the language modeling task where the task is to predict the next word $$x_t$$ give the previous words in the sentence. This very general task was first introduced by Shannon to define information and entropy.

Domains like mathematics have deterministic distributions whereas reasoning usually has a flatter distribution. Another issue with language is that it keeps changing with time, and the models need to be dynamic enough to maintain this. 

## Method
Continuing from the Word2Vec paper, we represent words with vector embeddings. 

> The embedding size is essentially like PCA - decomposing meanings into a low-dimensional space. Although there are over hundred thousand words, we are representing them in 500 dimensions?

> Can we create different similarity kernels to create different meanings? This effect is achieved through multi-head attention. 

> Different language to language translation is possible with LLMs. Is there an inherent universal language? 

> LLMs are able to reason well in some scenarios. Try asking it a novel puzzle and see how it does. However, it struggles with math. Is there something we are missing?

> Decoder inference is still sequential… Training seems more efficient but inference… 


 

