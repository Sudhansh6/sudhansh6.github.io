Consider the example of softmax calculation. This function would be slow if implemented using primitives. PyTorch implements an end-to-end kernel for softmax to increase its performance. With triton, we can construct such an end-to-end operation in a simpler manner while achieving slightly higher performance.

```python
import triton.language as tl
import triton
@triton.jit
def _softmax(z_ptr, x_ptr, stride, N, BLOCK: tl.constexpr):
    # Each program instance normalizes a row
    row = tl.program_id(0)
    cols = tl.arange(0, BLOCK)
    # Load a row of row-major X to SRAM
    x_ptrs = x_ptr + row*stride + cols
    x = tl.load(x_ptrs, mask = cols < N, other =    float(‘-inf’))
    # Normalization in SRAM, in FP32    
    x = x.to(tl.float32)
    x = x - tl.max(x, axis=0) # This is to avoid vert large and small values
    num = tl.exp(x)
    den = tl.sum(num, axis=0)
    z = num / den; 
    # Write-back to HBM
    tl.store(z_ptr + row*stride + cols, z, mask = cols < N
```

Note that Triton achieves good performance with low time investment. However, since it is not as flexible as CUDA, achieving very high-performance is not possible with Triton. 

## Recap

A *kernel* in the GPU is a function that is executed simultaneously by tens of thousands of threads on GPU cores. The shared memory during the GPU execution can be used as a *cache* that is used by more than one thread, avoiding multiple accesses to the global memory. *Over-subscribing* the GPU ensures that there are more blocks than SMPs present on the device, helping to hide (tail) latencies by ensuring high occupancy of the GPU.

<iNsert a point about GPU memory)

Operations such as ReLU, batch normalization and max pooling are not arithmetically dense operations. So typically, operations such as linear layers (and layer normalization with large batches) are limited by arithmetic operations. To calculate which linear layer would have more operations, consider the FLOPs calculation for GEMM. 

# (3) Graph Optimization

Our goal is to rewrite $$G$$ as $$G’$$ such that $$G’$$ runs faster than $$G$$ while outputting equivalent results. The straightforward solution is to use a template, wherein human experts write (sub-)graph transformation templates and an algorithm replaces these in the data flow graphs for reduction. 

### Graph Optimization Templates: Fusion
Fusing operators reduces I/O and kernel launching (CPU to GPU overhead, all the operations that the SM has to run). The disadvantages of this method is that creating various fused operations is difficult making the codebase unmanageable (e.g., TensorFlow). 

This also includes **folding constants** in a graph to replace expressions such as `(X + 3) + 4)` with `(X + 7)`.

### CUDA graph
NVIDIA allows users to capture the graph at the CUDA level. 

![](/assets/img/2025-01-06-data-systems-for-ml/17381192844173.jpg)

> Is this define-then-run?

### Standard compiler techniques

- Common subexpression elimination (CSE). 
    The high-level idea is replacing expressions such as `a = b; b = c` with `a = c`
- Dead Code elimination (DCE).
    After the CSE hit, we eliminate the dead-code with unused variables.
    
These both are run iteratively to reach an optimal code. These operations are every useful to eliminate parts of graph based on, say default arguments -

![](/assets/img/2025-01-06-data-systems-for-ml/17381197256019.jpg)

## How to ensure performance gain?

When we greedily apply graph optimizations, we may miss some options that initially decrease the performance but massively increase it later. Furthermore, the same optimizations could lead to an improvement in one hardware and reduction in other. Due to the existence of hundreds of operators (200-300), thousands of graph architectures and tens of hardware backends, it is infeasible to manually design graph optimizations for all cases.

There are other issues with template based optimizations
1. Robustness - Heuristics are not generalizable across architectures and hardware
2. Scalability - New operators and graph structures require newer rules
3. Performance - Misses subtle optimizations specific to DNNs/hardware. 

What’s the solution?

## Automate Graph Transformation

The main idea is to replace manually-designed graph optimizations with automated generation and verification of graph substitutions for tensor algebra. Basically, generate all possible substitutions and verify if they generate the same output. 

We start by enumerating all possible graphs up to a fixed size using available operators.

![](/assets/img/2025-01-06-data-systems-for-ml/17381201066754.jpg)
There are up to 66M graphs with 4 operators!

Then, with a graph substitution generator, we compute the output with random input tensors. For 4 operators, we can still generate up to 28744 substitutions! 

These are further pruned based on *variable renaming* and *common subgraphs*. 

![](/assets/img/2025-01-06-data-systems-for-ml/17381203016208.jpg)

These substitutions are formally verified to ensure that they are equivalent mathematically for all inputs. This verification is done by using the properties of operators. For example, convolution with concatenated kernels is same as concatenation of convolutions of the same kernels. 

So this *automated theorem prover* can be used to generate valid substitutions scaling up. It takes up to 5 minutes to verify 750 substitutions and there are about 45 rules for the operators which takes about 10 minutes. Adding a new operator is easy - just provide its specifications!

### Incorporating substitutions
How do we apply verified substitutions to obtain an optimized graph? The cost is based on the sum of individual operator’s cost and the cost on the target hardware. We greedily apply the substitutions to improve the performance. 

This approach can be further improved to train a model to learn which kind of substitutions optimize the graph. This was successfully implemented by TASO and it showed good results for real-life models -

![](/assets/img/2025-01-06-data-systems-for-ml/17381207164574.jpg)

## Summary

In summary for graph optimization,
1. We first construct a search space
2. Enumerate all possibilities for substitutions
3. Prune the candidates, and select the top ones based on profile/cost model
4. Apply the transformations to iteratively improve the performance.

What could go wrong with this? The search may be slow and the evaluation of various graphs can be expensive. 

Sometimes the substitutions may only be partially equivalent, but can be orders of magnitude faster. In such cases, we can trade off accuracy for performance. E.g., Convolution vs Diluted convolution.

Consider the following example. Suppose we have to use the same kernel to perform convolution on two different tensors. Then, we could concatenate these tensors, apply the convolution, and then apply a correction to achieve the correct result. These transformations use partial equivalent transformations yielding some speed up. These are not explorable in the previous case with fully equivalent operators. 

## Partially Equivalent Transformations

Like the previous example, we *mutate* the programs and correct them to get an optimized graph. 

The steps to do this automatically, we do something similar to before
1. Enumerate all possible programs up to a fixed size using available operators
2. Only consider transformations with equal shapes (in contrast with equal results as compared to before)

With this, all the crux of the algorithm comes to the correction of the mutant programs - how do we detect which part is not equivalent and how to correct it?

By enumeration - For each possible input and position, check if the values match. For complete correctness, this search would be $$m \times n$$ for $$m$$ possible inputs and $$n$$ output shape. We reduce the effort by reducing $$m, n$$ 

- Reducing $$n$$ - Since neural networks are mostly multi-linear, we can make such assumptions. 

    Theorem: For two multi-linear functions $$f$$ and $$g$$, if $$f = g$$ for $$O(1)$$ positions in a region, then $$f = g$$ for all positions in the region. 

    As a consequence, the search reduces from $$\mathcal O(mn)$$ to $$\mathcal O(mr)$$
    
- Reducing $$m$$ - 
    Theorem - If $$\exists l, f(l)[p] \neq g(l)[p]$$, then the probability that $$f$$ and $$g$$ give identical results on $$t$$ random inputs is $$2^{-31t}$$.
    
    Using this, we can run $$t$$ random tests with random inputs, and if all $$t$$ pass then it is very likely that $$f$$ and $$g$$ are equivalent.
    
The search space reduces to $\mathcal O(tr)$$. How does this relate to correct?
 
# ML Compiler Retrospective

This field started in 2013 with Halide. It was a compiler for rendering, but since the workflow is very similar to neural networks, the later compilers draw motivation from here. 

Then came XLA in 2016-17, that has good performance but had very difficult to understand code. Companies tried other operations such as TensorRT, cuDNN and ONNX for template based graph substitution. CuDNN is still popularly used but no one understands the code since it was written in a very low level language. 

Then came `tvm` in 2018 that we’ve discussed before. In 2019-20, MLIR and Flexflow were introduced - these are layers in the compiler that provided specific optimizations. Then came 2.0 and Torch Dynamo.

However, the community is shifting away from compilers. Why? One part is that many optimizations have been found. The main reason is that we’ve seen a certain class of neural networks architectures that work really well. For example, transformers are all the rage. So instead of focusing on compilers, people can focus on just building fused kernels for the attention mechanisms. That’s how we got flash-attention that no compiler is able to beat. 

