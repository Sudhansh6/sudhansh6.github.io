Continuing the previous calculations

$$
\begin{align*}
\mathbb E^*(V_T \vert \mathcal F_{T - 1}) &= \mathbb E^*(\alpha_T S_T + \beta_T B_T \vert \mathcal F_{T - 1}) \\
&= \alpha_T S_{T - 1}(1 + r) + \beta_T B_{T - q}(1 + r) \\
&= (1 + r) \left[\alpha_T S_{T - 1} + \beta_T B_{T - 1}\right] \\
&= (1 + r) \left[\alpha_{T - 1} S_{T - 1} + \beta_{T - 1} B_{T - 1}\right] &\because \text{ self-financing} \\
&= (1 + r) V_{T - 1}(\phi)
\end{align*}
$$

As a result, we have a recursive property

$$
\begin{align*}
V_{t - 1}(\phi) &= \frac{1}{1 + r} \mathbb E^*(V_t(\phi) \vert \mathcal F_{t - 1}) \\
\alpha_t &= \frac{V_t^u - V_t^d}{u - d} \frac{1}{S_{t - 1}} \\
\beta_t &= \frac{uV^d_t - dV^u_t}{u - d} \frac{1}{B_{t - 1}} 
\end{align*}
$$

for $$t = T, \dots, 1$$

The above relation can also be alternately derived using the **Tower law** to relate the value with $$V_t(\phi)$$ for a general $$t$$.

> Tower Law: If  $$X$$ is a random variable whose expected value  $$E ‚Å°( X )$$ is defined, and  $$Y$$ is any random variable on the same probability space, then $$E(X) = E (E (X \vert Y ) )$$, i.e., the expected value of the conditional expected value of $$X$$ given $$Y$$ is the same as the expected value of  $$X$$.

Then, we get
$$
\begin{align*}
V_t(\phi) &= \frac{1}{1 + r} \mathbb E^*(V_{t + 1} (\phi) \vert \mathcal F_t) \\
&= \frac{1}{1 + r} \mathbb E^*\left(\frac{1}{1 + r} \mathbb E^*(V_{t + 2}(\phi) \vert \mathcal F_{t + 1} ) \vert \mathcal F_t\right) \\
&= \frac{1}{(1 + r)^2} \mathbb  E^*\left(\mathbb E^*\left(V_{t + 1} \vert \mathcal F_{t + 1}\right \vert \mathcal F_{t} \right) \\
&= \frac{1}{(1 + r)^2} \mathbb E^*(V_{t + 2} \vert \mathcal F_t)
\end{align*}
$$

Consequently,

$$
\begin{align*}
V_T(\phi) &= \frac{1}{(1 + r)^{T - t}} \mathbb E^*(V_T (\phi) \vert \mathcal F_t) \\
&= \frac{1}{(1 + r)^{T - t}} \mathbb E^*(X \vert \mathcal F_t) \\
V_0 (\phi) &= \frac{1}{(1 + r)^T} \mathbb E^*(X)
\end{align*}
$$

### Conditional Expectation of Martingales

Given $$(\Omega, \mathcal D, P)$$ with $$g \subset \mathcal F$$, $$X \in L'(\mathcal F)$$ then $$Y = \mathbb E(X \vert g)$$ is the random variable such that 

1. $$Y \in L'(g)$$

2. $$\mathbb E(YZ) = \mathbb E(XZ)$$ for all $$Z$$ that are $$g$$-measurable.

where $$L'$$ represents integrability. The above property is the general statement for the law of general statician.

*Special case.* When $$\Omega$$ is finite ($$\#(\Omega) < \infty$$), then $$\mathcal F = \{ \text{all subsets of } \Omega\}$$ and $$\mathcal G = \sigma(G_1, \dots, G_n)$$ form a partition ($$G_i \cap G_j = \phi, \cup_{j = i}^n G_j = \Omega$$) of $$\Omega$$ with $$P(G_k) > 0$$.  

A random variable $$Z$$ is $$g$$-measurable iff $$Z(w) = \sum_{k = 1}^n \mathbb 1_{G_k} (w) \cdot c_k \equiv Z(w) = c_k \text{ if } w \in G_k$$ where $$c_k$$ is the cardinality of $$G_k$$. 

Then,

$$
\mathbb E[X \vert \mathcal G](w) = \sum_{k = 1}^n \left(\mathbb 1_{G_k}(w) \cdot E[X \vert G_k]\right)
$$

From Bayes' rule

$$
\mathbb E(X \vert G_k) = \mathbb E(X . \mathbb 1_{G_k}) /P(G_k) = \mathbb E(X; G_k)/P(G_k)
$$

Using the above results, we get

1. $$X \to \mathbb E(X \vert Y)$$ is linear & positive ($$ X \geq 0 \implies \mathbb E(X \vert g) \geq 0$$)

2. $$\mathbb E(\mathbb E (X \vert g)) = \mathbb E(X)$$

3. $$\mathbb E(XZ \vert g) \equiv E(X \vert y) Z$$

4. If $$\mathbb E \in g$$, then $$E(X \vert g) = X$$ and $$X \perp\!\!\!\perp g$$, then $$\mathbb E(X \vert Y) = \mathbb E(X)$$. 
