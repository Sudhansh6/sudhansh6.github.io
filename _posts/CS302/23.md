# Lecture 23

> `08-04-22`

## Instruction Selection

We need to generate an assembly code from the “register code” we generated after register allocation. Floating point comparison takes 2 arguments whereas integer comparison takes 3 arguments.

### Integrated Instruction Selection and Register Allocation Algorithms

- **Sethi-Ullman Algorithm** - Used in simple machine models, and is optimal in terms of the number of instructions with the minimum number of registers and minimum number of stores. It is also linear in the size of the expression tree.
- **Aho-Johnson Algorithm** - This algorithm is applicable for a very general machine model, and is optimal in terms of the cost of execution. It is also linear in the size of the expression tree (exponential in the arity of instruction which is bounded by a small constant). The main motivation behind this idea is that a sequence of 4 instructions can be more efficient than 2 instructions.

### Sethi-Ullman Algorithm

WE have a finite set of registers $$r_0, \dots, r_k$$ and countable memory locations. We will be using simple machine instructions like loads, store, and computation instructions ($$r_1 \; op \; k$$, $$k$$ can be a register or a memory location). The input to this algorithm is the expression tree (essentially the AST) without

- control flow (no ternary expressions)
- assignments to source variables inside an expression (so no side effects). Assignments are outside the expression.
- no function calls
- no sharing of values (trees, not DAGs). For example, we don’t have expressions like $$b \times c + d - b \times c$$. Basically, we shouldn’t use *common subexpression elimination*.

The key idea behind this algorithm is that the order of evaluation matters. Sometimes, the result is independent of the order of evaluation of some subtrees in the tree. For example, in $$b \times c + a /d $$, the order of evaluation of $$b \times c$$ and $$a/d$$ does not matter.

Therefore, we choose the order of evaluation that minimises the number of registers so that we don’t store an intermediate result in the memory.

In the algorithm, we traverse the expression tree bottom up and label each node with the minimum number of registers needed to evaluate the subexpression rooted at the node. Then, we traverse the tree top down and generate the code. Suppose we have

```
				op
			/    \
		l1      l2
```

Assume $$l_1 < l_2$$. If we evaluate the left subtree first, we need $$l_1$$ registers to evaluate it, 1 register to hold its result and $$l_2$$ registers to evaluate the right subtree. Therefore, the total registers used would be $$\max(l_1, l_2 + 1)$$ = $$l_2 + 1$$. If we follow the other order, we get the number of registers as $$\max(l_2, l_1 + 1) = l_2$$. Therefore, we **evaluate the subtree with larger requirements first**. So, we have the following recursion


$$
label(n) = \begin{cases}
1 & n \text{ is a leaf and must be in a register n or it is a left child}  \\
0 & n \text{ is a leaf and can be in memory or it is a right child} \\
max(label(n_1), label(n_2)) & n \text{ has two child nodes with unequal labels} \\
label(n_1) + 1 & n \text{ has two children with equal labels} \\
\end{cases}
$$


If we generalise to instructions and trees of higher arity, then for node $$n$$ with $$k$$ children we get


$$
label(n) = \max(l_j + j - 1), 1 \leq j \leq k
$$


Note that the above algorithm generates a store free program. Why aren’t we interleaving the codes of two subtrees? There is a notion of contiguity. Also, we are assuming that memory does not add any additional overhead in terms of CPU cycles.

How do we generate the code for the tree? `rstack` is a stack of registers, and `gencode(n)` generates the code such that the result of the subtree rooted at $$n$$ is contained in `top(rstack)`. `tstack` is a stack of temporaries used when the algorithm runs out of registers. `swap(rstack)` swaps the two top registers in `rstack`. Finally, the procedure `emit` emits a single statement of the generated code.

Then, we have

| Cases                                               | `gencode(n)`                                                 |
| --------------------------------------------------- | ------------------------------------------------------------ |
| $$n$$ is a left leaf                                | `emit(top(rstack) = name)`. This invariant that the top of the `rstack` is the result is maintained. |
| The right child of $$n$$ is a leaf                  | `emit(top(rstack) = top(rstack) op r.name)`                  |
| $$l_1 \geq l_2$$ for children $$l_1, l_2$$ of $$n$$ | `gencode(n_1)`<br />`R = pop(rstack)`<br />`gencode(n_2)`<br />`emit(R = R op top(rstack))`<br />`push(R, rstack)` |
| $$l_1 < l_2$$                                       | `swap(rstack)`<br />`gencode(n_2)`<br />`R = pop(rstack)`<br />`gencode(n_1)`<br />`emit(top(rstack) = top(rstack) op R`<br />`push(R, rstack)`<br />`swap(rstack)` |
| Both children need more registers than available    | `gencode(n_2)`<br />`T = pop(tstack)`<br />`emit(T = top(rstack)`<br />`gencode(n_1)`<br />`emit(top(rstack) = top(rstack) op T)`<br />`push(R, rstack)`<br /> |

In the above, `R` can be seen as a local static variable of the procedure. In the last case, we evaluated the right child first because only the right child can be a memory argument.