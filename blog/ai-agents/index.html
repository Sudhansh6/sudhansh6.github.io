<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> AI Agents | Sudhansh Peddabomma </title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="Everyone is talking about agents. But what is an agent? Is it just a buzzword being thrown around? This article talks deeply about this issue along with the technical ideas associated."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?d0883cd261a1387cbdd04fa5cb9cc690" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/ai-agents/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/chat.js?e73db4280bae3cbae4d78219277155b9"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Sudhansh Peddabomma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Articles </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">AI Agents</h1> <p class="post-meta"> Created in January 06, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/notes"> <i class="fa-solid fa-tag fa-sm"></i> Notes</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>The content on this article is based on the <a href="https://github.com/pearls-lab/ai-agents-course" rel="external nofollow noopener" target="_blank">course</a> by <a href="https://prithvirajva.com" rel="external nofollow noopener" target="_blank">Prof. Prithviraj</a> at UC San Diego. This <a href="https://www.kaggle.com/whitepaper-agents" rel="external nofollow noopener" target="_blank">whitepaper</a> about AI Agents by Google is also a good read.</p> <h2 id="what-is-an-agent">What is an agent?</h2> <p>Agent is an entity with <em>agency</em>. <a href="https://minecraft.wiki/w/Agent" rel="external nofollow noopener" target="_blank">A Minecraft agent?</a>. Agents see applications within the workspaces in the form of workflow automations, household or commercial robotics, software development and personal assistants. Generally, the theme is that <em>agents</em> take actions.</p> <p>Historically, the use of agents started in the early 1900s in the field of control theory. They were used for dynamic control of flight systems, and in 1940s it expanded to flight guidance, etc. By the 1950s the concepts of MDPs and dynamic programming were being expanded to many use cases. Surprisingly, one of the first natural language chatbots, Eliza, was created as a psychotherapist simulator in the 1960s! Finally, reinforcement learning became a field of study in the 1990s for sequential decision making.</p> <h2 id="sequential-decision-making">Sequential Decision Making</h2> <p>These tasks are different from other ML problems like classification. A model that has an accuracy of 99% at each step, has a cumulative accuracy of ~30% after 120 steps!</p> <p>These problems are formalized as a Markov Decision Process - an <strong>agent</strong> performs <strong>actions</strong> in an <strong>environment</strong>, and in turn receives <strong>rewards</strong> as feedback. These configurations are distinguished as <strong>states</strong>, and the whole process can be seen as sequential decision making.</p> <p>The core components of an agent, often agreed on, are</p> <ul> <li> <strong>Grounding</strong> - Language is anchored to <em>concepts</em> in the world. Language can be grounded to different forms of information systems - images, actions and cultural norms. <ul> <li>Agency (ability to act) - At each state, an agent needs to have multiple choices to act. <em>If an agent has to select what tools to use but there’s always only one tool, is that agency?</em> The action space has to be well-defined to look for agency. Although there is a single tool call, different parameters for the tool call can probably be considered as different actions. Actions can be defined as something the agent does and changes the environment. The distinction between an agent and environment is not very clear in many cases. Although, our approximations mostly serve us well.</li> <li>Planning (Long horizon)</li> <li>Memory - <ul> <li>Short-term - What is the relevant information around the agent that it needs to use to act now</li> <li>Long term - What information has the agent already gathered that it can retrieve to take an action</li> </ul> </li> <li>Learning (from feedback) - Doesn’t necessarily always mean <em>backpropagation</em>.</li> </ul> </li> <li> <strong>Additional</strong> - <ul> <li>Embodiment (physically acting in the real-world). <em>Embodied hypothesis</em> - embodiment is necessary for AGI.</li> <li>Communication - Can the agent communicate its intentions to other agents. Very necessary pre-requisite for multi-agent scenarios.</li> <li>World Modeling - Given the state of the world and an actions, predict the next state of the world. Is <a href="https://deepmind.google/technologies/veo/veo-2/" rel="external nofollow noopener" target="_blank">Veo</a>/<a href="https://sora.com" rel="external nofollow noopener" target="_blank">Sora</a> a world model? It is an attempt for world model since they have no verifiability. <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/" rel="external nofollow noopener" target="_blank">Genie</a> is another such attempt. So is <a href="https://genesis-embodied-ai.github.io" rel="external nofollow noopener" target="_blank">Genesis</a> - this is much better if it works.</li> <li>Multi-modality - The clean text on the internet is only a few terabytes, and our models have consumed it (took use 2 decades though). YouTube has 4.3 Petabytes of new videos a day. CERN generates 1 Petabyte a day (modalities outside vision and language). Some people believe this form of scaling is the way to go. There are more distinctions -</li> </ul> </li> </ul> <table> <thead> <tr> <th>Model</th> <th>AI System</th> <th>Agent</th> </tr> </thead> <tbody> <tr> <td>GPT-4</td> <td>ChatGPT</td> <td>ChatGPT computer use</td> </tr> <tr> <td>Forward passes of a neural net</td> <td>Mixing models together</td> <td>Has agency</td> </tr> </tbody> </table> <p>It is important to remember that not every use case needs an agent and most use cases just need models or AI systems. <em>Occam’s razor</em>.</p> <h1 id="simulated-environments-and-reality">Simulated Environments and Reality</h1> <p>Why do we need simulations? Most tasks have many ways of completing them. There is no notion of <em>global</em> optimal solutions ahead of time but usually known once the task is complete.</p> <p>The agent needs to explore to find many solutions to compare and see what is the most efficient. However, exploration in the read world is expensive - wear and tear of robots, excessive compute, danger to humans, etc.</p> <p>Simulations offer an easy solution to these problems. Assign a set of rules, and let a world emerge. One of the early examples of this is <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life" rel="external nofollow noopener" target="_blank">Conway’s Game of Life</a> which theorized that complicated behaviors can emerge by just a few rules.</p> <p>From an MDP perspective, a simulation contains \(&lt;S, A, T&gt;\) where</p> <ul> <li>\(S\) is the set of all states. It consists of propositions that are true/false. Example: You are in a house, door is open, knife in drawer</li> <li>\(A\) is the set of all actions. Example: Take knife from drawer, walk through door</li> <li>\(T\) is the transition matrix - (You are in the house, you walk out of the door) -&gt; You are outside the house.</li> </ul> <p>A simulation need not have an explicit reward.</p> <h2 id="sim2real-transfer">Sim2Real Transfer</h2> <p>The ability of an agent trained in simulation transfer to reality is dependent on how good the model extrapolates out of distribution. With the current stage of agents, the simulation is made as close to reality as possible to reduce the Sim2Real gap.</p> <p>How do we measure closeness to reality? The tasks in the real world have different types of complexities -</p> <ol> <li>Cognitive complexity - Problems that requires long chains of <em>reasoning</em> - puzzles, math problems or moral dilemmas</li> <li>Perceptive complexity - Requires high levels of vision and/or precise motor skills - bird watching, threading a needle, Where’s Waldo</li> </ol> <p>Examples of simulations -</p> <ol> <li>Grid world - low cognitive and almost zero perceptive. However, this idea can arbitrarily scale to test algorithms for their generalization potential in controllable settings.</li> <li>Atari - low perceptive, medium cognitive. Atari games became very popular in 2013, when Deepmind released their <a href="https://arxiv.org/pdf/1312.5602" rel="external nofollow noopener" target="_blank">Deep Q-Net</a> paper that achieved human level skills on these games.</li> <li> <a href="https://en.wikipedia.org/wiki/Zork" rel="external nofollow noopener" target="_blank">Zork</a>, <a href="https://www.nethack.org" rel="external nofollow noopener" target="_blank">NetHack</a> - low perceptive, high cognitive. These are configurations or worlds that you purely interact with text. The worlds are actually so complex that there is no agent that is able to finish the challenge!</li> <li> <a href="https://cs.stanford.edu/people/jcjohns/clevr/" rel="external nofollow noopener" target="_blank">Clevr simulation</a> - medium perceptive, low cognitive - This simulation generates images procedurally with certain set of objects and has reasoning questions for each image.</li> <li> <a href="https://ai2thor.allenai.org" rel="external nofollow noopener" target="_blank">AI2 THOR</a> - medium perceptive, medium cognitive. Worlds with ego-centric views for robotics manipulation and navigation simulations</li> <li> <a href="https://arxiv.org/pdf/2407.18901" rel="external nofollow noopener" target="_blank">AppWorld</a> - medium perceptive, medium cognitive. A bunch of different apps that you would generally use in daily life. The agents can access apps, and the simulation also has human simulators. This simulation is one that is closest to reality in the discussed so far!</li> <li> <a href="https://www.minecraft.net/en-us" rel="external nofollow noopener" target="_blank">Minecraft</a> - medium perceptive, high cognitive. A voxel based open-world game that lets players take actions similar to early-age humans.</li> <li> <a href="https://mujoco.org" rel="external nofollow noopener" target="_blank">Mujoco</a> - high perceptive, low cognitive. It is a free and open source physics engine to aid the development of robotics.</li> <li> <a href="https://ai.meta.com/research/publications/habitat-a-platform-for-embodied-ai-research/" rel="external nofollow noopener" target="_blank">Habitat</a> - high perceptive, medium cognitive. A platform for research in embodied AI that contains indoor-world ego-centric views similar to AI2 THOR, but with much better graphics. They have recently added sound in the environment too!</li> <li> <p>High perceptive, high cognitive - Real world, and whoever gets this simulation right, wins the race to AGI. It requires people to sit down and enumerate all kinds of rules. Game Engines like Unreal and Unity are incredibly complex, and are the closest we’ve gotten.</p> <p>Some researchers try to “learn” the simulations from real-world demonstrations.</p> </li> </ol> <p>In each of these simulators, think of the complexity and reward sparsity in the environment. It is easy to build a simulator that gives rewards at a goal state than the one that gives a reward for each action. There are some open-lines of research in this domain -</p> <ol> <li>Which dimensions of complexity transfer more easily? Curriculum learning</li> <li>Can you train on lower complexity and switch to a higher complexity?</li> <li>Can we learn the world model holy grail?</li> </ol> <h2 id="how-to-make-simulations">How to make simulations?</h2> <p>As we’ve seen, simulations can range from games to real-world replications with physics involved. Most simulations are not designed keeping AI in mind. However, with the current state of AI, this is an important factor to keep in mind.</p> <p>Classical environments like in Zork/AI2 Thor/Mujoco have something known as <strong>PDDLs</strong>. Some simulations are built through AI, like <em>AI Dungeon</em> that spins up worlds for role-play games.</p> <h3 id="planning-domain-definition-language-pddl">Planning Domain Definition Language (PDDL)</h3> <p>Standard encoding for classic planning tasks. Many specific languages for creating simulations have similarities with PDDL.</p> <p>A PDDL Task consists of the following</p> <ul> <li>Objects - things in the world that interest us</li> <li>Predicates - Properties of objects that we are interested in, can be true or false</li> <li>Initial state - The state of the world that we start in</li> <li>Goal specification - Things that we want to be true</li> <li>Actions/Operators - Ways of changing the state of the world.</li> </ul> <p>These are split across two files - domain and problem <code class="language-plaintext highlighter-rouge">.pddl</code> files.</p> <p>Classic symbolic planners read PDDLs and give possible solutions. Checkout the <a href="https://planning.wiki/ref/planners/atoz" rel="external nofollow noopener" target="_blank">Planning.wiki</a>. In many cases these planners are used over reinforcement learning due to lack of algorithmic guarantees.</p> <p>There were other attempts</p> <h1 id="search-for-planning-in-simulations">Search for Planning in Simulations</h1> <h1 id="reinforcement-learning-abridged">Reinforcement Learning (Abridged)</h1> <p>We have been building chronologically, and next in line is basic RL.</p> <h2 id="terminology">Terminology</h2> <ul> <li> <p>A <strong>policy</strong> is a function that defines an agent’s behavior in the environment. Finding the optimal policy is known as the <em>control</em> problem.</p> <p>Formally, a policy is a distribution over actions for a given state.</p> \[\pi(a \vert s) = P(A_t = a \vert S_t = s)\] <p>Due to the Markov property, the policy depends only on the current state and not history. In cases where the history is needed, the state is modified to embed the history to evade this dependence.</p> <p>For a given MDP, an optimal policy always exists that achieves the maximum expected reward at every state. (This is proved using the compactness properties of the state-action space using the Banach’s fixed point theorem - refer to <a href="/blog/cse291g">these notes</a> for more details).</p> </li> <li> <p>The <strong>value function</strong> determines how good each state or actions is. Finding the optimal value functions is known as the <em>prediction</em> problem.</p> <p>There are two functions that capture the value of the current state/action of the agent</p> <ol> <li>\(V_\pi(s) = E_\pi[R_{t + 1} + \gamma V_\pi(S_{t + 1} \vert S_t = s]\) - The expected reward obtained by a policy \(\pi\) starting from a given state \(s\).</li> <li>\(Q_\pi(s, a) = E_\pi[R_{t + 1} + \gamma Q_\pi(S_{t + 1}, A_{t + 1}) \vert S_t = s, A_t = a]\) - The expected reward for a given state \(s\) upon taking a certain action \(a\). These two functions are closely related to each other, and knowing one determines the other. In general, these functions (matrices, in discrete spaces) do not have a closed form solution.</li> </ol> </li> <li> <p>A <strong>model</strong> is the agent’s representation of the environment</p> </li> </ul> <p>RL algorithms are classified under various categories</p> <ul> <li>Model free and Model-based</li> <li>Off-policy and on-policy</li> </ul> <h1 id="its-all-dynamic-programming">It’s all Dynamic Programming?</h1> <p>The core theory of RL, the properties of the Bellman equation (refer to these notes for more details) and the recursive nature of the value functions, ties to dynamic programming. This insight helps us design algorithms to solve the problems in RL (Prediction and Control).</p> <p>We ideally want the solution to the control problem since we want to define the optimal behavior of an agent in the environment. To do so, the prediction problem is a pre-cursor that needs to be solved.</p> <h2 id="policy-evaluation">Policy Evaluation</h2> <p>The prediction problem involves calculating the rewards obtained by a given policy \(\pi\). The expectation can be written as</p> \[\begin{align*} V_\pi(s) &amp;= \sum_{a \in A} \pi(a \vert s) Q_\pi(s, a) \\ &amp;= \sum_{a \in A} \pi(a \vert s) (R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’)V_\pi(s’)) \end{align*}\] <p>where \(T\) is the state-transition function of the MDP.</p> <p>Ideally, we want to find the optimal policy that reaches the maximum value at every state.</p> \[\begin{align*} V*(s) &amp;= \max_\pi V_\pi(s) \\ Q*(s, a) &amp;= \max_\pi Q_\pi(s, a) \\ \pi*(s) &amp;= \arg\max_\pi Q_\pi(s, a) \end{align*}\] <p>These can be determined (iteratively) from the Bellman’s Optimality equations -</p> \[\begin{align*} Q*(s, a) &amp;= R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’) V*(s’) &amp;= R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’) \max_{a’} Q*(s’, a’) \end{align*}\] <blockquote> <p>Note the subtlety here. Although Bellman’s optimality equations aren’t seemingly much different from the Bellman’s equations, there is a very strong claim the optimality equations make - they claim that the existence a policy that gets the maximum possible value at every state. The existence is not a trivial claim and it is the proof I referred to in the terminology. Furthermore, it turns out that a deterministic policy is just as good as a stochastic one.</p> </blockquote> <p>So how do we find these optimal values?</p> <h2 id="policy-iteration">Policy Iteration</h2> <p>Given a policy \(\pi\), we iteratively update its actions at each state to improve its value. Remember that we can <em>evaluate a policy</em> to get its value function.</p> <p>At each state, if there is an action \(a\) such that \(Q_\pi(s, a) &gt; Q_\pi(s, \pi(s))\), then the policy is <em>strictly improved</em> by updating \(\pi(s) \gets a\). In each iteration, we update the actions this way across all the states, and repeat this until the policy does not change.</p> <p>How many iterations should we repeat this for? Because the number of policies is finite (bounded by \(O(\vert A \vert^{\vert S\vert})\), we are guaranteed to reach the optimum. Each iteration costs \(O(\vert S\vert^2 \vert A\vert + \vert S\vert^3)\). Although these numbers seem big, in practice, this algorithm typically takes only a few iterations.</p> <h2 id="value-iteration">Value Iteration</h2> <p>It is similar to the policy iteration algorithm, but focuses on recursively improving the value function instead.</p> <p>We start out with a random value function, and at each state, we choose the action that gives the maximum value (with the currently set values across the states). Once the values are updated across all the states, the process is repeated until the improvement is below a threshold. At the end of the iterations, we can extract the policy from the value function deterministically (the algorithm itself is a hint to this).</p> <p>Although this seems very similar to the policy iteration algorithm, there are some key differences. We do need to reach the optimal value function to get the optimal policy - if it is close enough, we can get the optimal policy. Also, the iterations <em>asymptotically reach</em> the optimal policy and there is no upper bound to this.</p> <h2 id="limitations">Limitations</h2> <p>Although dynamic programming approaches have theoretical guarantees, they are not widely used in practice. Why?</p> <p>The curse of dimensionality. These algorithms are have very limited applicability in practice. Many environments have a very large set of states and actions. In some cases, these could be continuous as well. The iteration algorithms are computationally infeasible in such cases.</p> <h1 id="model-free-rl">Model-free RL</h1> <p>Since we cannot look at every state action combination, we resort to approximations. We explore the world (say, with Monte Carlo sampling) and build experiences to heuristically guide the policy.</p> <p>The goal is to optimize the value of an unknown MDP through experience based learning. Many real world problems are better suited to be solved by RL techniques over dynamic programming based approaches (the iterative algorithms).</p> <h2 id="monte-carlo-control">Monte Carlo Control</h2> <p>It suggests greedy policy improvements over \(V(s)\) requires a model of the MDP. However, improvement over \(Q(s, a)\) is a model-free method! (This was the importance of defining both \(V_\pi(s)\) and \(Q_\pi(s, a)\)).</p> <p>The \(Q\) function can be learned from experiences. These concepts are the foundation concepts for deep RL!</p> <p>This approach can be thought of as a hybrid approach between policy and value iteration. In these exploration/sampling based techniques, it is important to gather data about the model through exploration and not be greedy. This forms the basis of <strong>epsilon-greedy</strong> algorithms.</p> <h2 id="epsilon-greedy-exploration">\(\epsilon\)-greedy exploration</h2> <p>At each state, with a certain probability we choose to exploit (greedily take the action based on the optimal policy we developed so far) or explore (take a random action to sample more outcomes)</p> \[\pi(a \vert s) = \begin{cases} \epsilon/m + 1 - \epsilon &amp; a* = \arg\max_{a \in A} Q(s, a) \\ \epsilon/m \end{cases}\] <p>This class of algorithms also has some theory but it is limited. This core trade-off between exploration/exploitation is still a core element in the modern RL algorithms.</p> <h2 id="temporal-difference">Temporal Difference</h2> <p>In Monte Carlo methods, we update the value function from a complete episode, and so we use the actual accurate discounted return of the episode. However, with TD learning, we update the value function from a step, and we find an estimated return called <strong>TD target</strong> - a bootstrapping method similar to DP.</p> \[\begin{align*} \text{Monte Carlo }&amp;: V(S_t) \gets V(S_t) + \alpha[G_t - V(S_t)] \\ \text{TD Learning }&amp;: V(S_t) \gets V(S_t) + \alpha[R_{t + 1} + \gamma V(S_{t + 1}) - V(S_t)] \end{align*}\] <p>The high-level view of MCTS is <img src="/assets/img/AIAgents/17388852350762.jpg" alt=""></p> <h2 id="alphago-a-case-study">AlphaGo: A case study</h2> <p>The game has a large number of states. The rewards we use are \(\plusminus 1\) based on the player won. We define policies for both the players and <em>train</em> the policies with self-play.</p> <p>Making use of the symmetry of the game, we can use the episodes of the opponent player seen before to train the policy.</p> <p>AlphaGo used these exact MC methods with neural networks (CNNs, which is super useful for Go) to learn the probabilities and outcome rewards. It was trained with a lot of human games to train initial value networks. The developers also hand-crafted features to represent knowledge in the game.</p> <p>AlphaZero, an extension of this, relaxed the constraint of requiring a lot of human data and scaled.</p> <p>All these algorithms have been model-free. That is, we cannot estimate the consequences of our actions in the environment, and are simply learning based off our experiences. We are not learning anything about the dynamics of the environment.</p> <p>On the flip side, if we know the model of the world, can we do better? So given a <em>world model</em>, how do we use it?</p> <h1 id="model-based-rl">Model-based RL</h1> <p>We learn the model of the world from experiences, and then plan a value function (and/or policy) from the model. What is a model? A representation of an MDP \((S,A,T,R, \gamma)\), and we try to approximate \(T, R\).</p> <p><em>Assumption.</em> A key assumption that developers make is that the state transitions and rewards are conditionally independent.</p> <p>We have the experience \(S_1, A_1, R_2, \dots, S_T\), and we just train a model in a supervised problem setting \(S_i, A_i \to R_{i + 1}, S_{i + 1}\). Learning \(R\) is a regression problem and learning \(P\) is</p> <p>How do we use the learned model? Since the learned model has errors and uncertainty, training a policy would take a long time. It is like we are learning the rules of Go whereas previously we knew the rules, and were just trying to win.</p> <p>The advantages of model-based RL is the it can use all the (self, un) supervised learning tricks to learn from large scale data and can reason about uncertainty. The disadvantage is that we need to first build a model, and then estimate a value from the estimated model - introduced two sources of error.</p> <h2 id="introduction-to-transformers-and-language">Introduction to Transformers and Language</h2> <p>A function approximator (neural networks) needs to be good for the task. For example, CNNs were great for Atari and Go but they did not work well for language.</p> <p>What are the right inductive biases for language then? The <em>attention mechanism</em> was again borrowed from some cognitive functions of the brain. An attention matrix tries to find the <em>alignment</em> of elements within a sequence. Before the scaled dot product attention, there were multiple variants of the formulation -</p> <p><img src="/assets/img/AIAgents/17393174376471.jpg" alt=""> Self-attention was first proposed by Cheng et al. in 2016 (originally called intra-attention) that brought in a sense of understanding in models - finding relations between elements of the same sequence.</p> <p>In 2015, there was a concept of global vs local attention introduced in the form of windowed-attention. This concept is being used widely in vision and language based systems.</p> <p>Let us discuss the intuition for <em>scaled dot product attention</em> -</p> <ol> <li>Query: What are the things I am looking for?</li> <li>Key: What are the things I have?</li> <li>Value: What are the things I will communicate?</li> </ol> <p>So essentially, the queries attend to the keys to find the aligned ones, and the corresponding values are returned. The multi-head part of the architecture is simply the multiple ways of learning these alignments between the query and key sequences. The alternate way of thinking about it is, using an ensemble representation to provide robustness in our predictions. Furthermore, to add a notion of position in the sequence, a positional encoding is added to the sequence elements.</p> <p>To apply all these mechanisms to language, we need a way to represent language as sequences - this step is known as <em>tokenization</em>. Word-level is too discrete (causes issues for questions like “how many r’s are in ‘strawberry’?”) and character-level is too redundant (often causes issues for questions like “Is 9.9 greater than 9.11?”. The current standard is a sub-word (Tiktoken Byte Pair Encoding BPE) that is learned from a representative subset of data.</p> <p>The other ideas are to use <em>heuristics</em> for numerical values like new tokenizers and right-to-left processing, etc. The amount of research done in tokenization is underwhelming as compared to the rest of the transformer stack. People have also come up with <a href="https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/" rel="external nofollow noopener" target="_blank">byte-level transformers</a> and <a href="https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/" rel="external nofollow noopener" target="_blank">LCMs</a>. Since bytes form too long sequences, people tried hierarchical representations that kind of work. However, there are many training issues with this and byte-encoding issues (PNG and Unicode are weird encodings). They were only able to train a 7B model with the byte-level learning.</p> <p>Language modeling is another long-standing problem that is a fundamental cornerstone to make these networks learn. The two popular mechanisms used are</p> <ul> <li>Infill (like in BERT): The 47th US President is [??] Trump</li> <li>Next Token prediction (like in GPT): The sun rises in the ??</li> </ul> <p>RNNs had an encoder-decoder architecture wherein the initial sequence is <em>encoded</em> into a vector (latent space) and it is then <em>decoded</em> into another sequence. This terminology has been carried through and is still used in transformers. The original paper started out as an encoder-decoder architecture, but these models are not used much anymore.</p> <ol> <li>Encoder only models (auto encoding models) - They are used for Sentence classification, NER, extractive question-answering and masked language modeling. Examples are BERT, RoBERTa, distilBERT.</li> <li>Decoder only models (auto regressive models) - They are used for text generation and causal language modeling. Examples are GPT-2, GPT Nero, GPT-3</li> </ol> <p>Check <a href="https://github.com/karpathy/nanoGPT" rel="external nofollow noopener" target="_blank">this repository by Andrej Karpathy</a> for a clean implementation of GPT.</p> <h2 id="how-does-all-this-relate-to-rl">How does all this relate to RL?</h2> <p>We now have a neural net architecture that works well on language. How about using this for MDPs with language-based state-action spaces.</p> <h2 id="paper-discussion-2xdqn-network">[Paper Discussion] 2xDQN network</h2> <p>As we have seen the Q-learning update</p> \[Q(S_t, a_t) \gets Q(s_t, a_t) + \alpha[r(s_t, a_t) + \gamma \max_{a’} Q(s’, a’) - Q(s, a)]\] <p>The problems with Q-learning are</p> <ol> <li>Curse of dimensionality - High dimensions and continuous state spaces do not work</li> <li>Slow convergence</li> <li>Inefficient generalization</li> <li>Exploration-exploitation trade-off</li> </ol> <p>This led to <strong>Deep Q-learning</strong>. The idea is to replace Q0tables with Neural networks to approximate Q-functions.</p> <ul> <li> <em>*Experience replay</em> stores the agent’s past experiences in a replay buffer. The algorithm randomly samples episodes for training, to break the correlation between consecutive experiences.</li> <li> <strong>Target Networks</strong> - Uses a separate, slower updating networking to compute target Q-values. Reduces instability caused by changing Q-values too frequently during learning.</li> </ul> <p>The problems with this are</p> <ul> <li>Overestimation of Q-values - due to the mathematical formulation the Q-values are over-estimated (has been proved mathematically)</li> <li>Sample inefficient - uses large amount of e</li> <li>Catastrophic forgetting - may forget previously learned experiences when trained on new experiences</li> </ul> <p>This led to <strong>Double Deep Q-learning</strong>. They decouple the action evaluation and action selection networks. The online network is used for action selection whereas the action value function evaluates the action. Previously, to stabilize the learning in Q-learning, the updates to action selection network were less frequent leading to choosing sub-optimal actions in some cases.</p> <p>The problems with this approach are</p> <ol> <li>Might not completely eliminate the overestimation bias - Since action selection and value estimation are not completely decoupled.</li> <li>Leads to slower convergence in some environments - where overestimation is helpful (high exploration settings)</li> <li>Sensitive to hyper parameters</li> <li>Still susceptible to some core DQN issues - Sparse rewards, discrete action space, sample inefficient.</li> </ol> <p>After this, newer methods were proposed such as <em>Rainbow Deep Q-learning</em> - Combination of previous improvements - Prioritized experience replay, uses a dueling network (two explicit specialized networks) for evaluation and selection, Stabilized learning via distributional RL, and Multi-step updates.</p> <h1 id="rl-agents-for-llms">RL Agents for LLMs</h1> <p>The typical LLM training pipeline is as follows</p> <ol> <li>SfM fine-tuning - Depends heavily on human expert demo data, and requires a lot of effort. This step can be compared to behavior cloning in RL</li> <li>Some pipelines also talk about pre-training that can be thought of as weight initialization. This step is not used so much anymore.</li> <li> <p>After this stage, <em>learning based on feedback</em>, has become a standard step. It comes in many forms, RLHF, RLAIF, etc. The primary approach, Reinforcement Learning with Human Feedback essentially rates different texts generated by the AI, and uses this as a reward model and improves the model considering it as a policy.</p> <p>This step is cheaper than the other steps, so companies are pushing towards improving this. However, in practice it does not seem to work without pre-training.</p> <p>This step also involves training a human-proxy reward function. In whole, this is known as post-training development.</p> </li> </ol> <h2 id="rlhf">RLHF</h2> <p>As we mentioned, the goal is to collect preference feedback and train a reward model. It is a kind of a personalized learning to correct the text generated by the model. One thing to note is that the rewards are given towards the end and there are no intermediate rewards.</p> <p>For the policy training itself, we have studied value and policy based algorithms. If the value of the MDP is estimated, the policy can be determined implicitly (argmax or epsilon-greedy). However, values and rewards of partial sentences are more difficult to estimate.</p> <p>Due to these limitations, researchers have gravitated towards policy-based RL. It has better convergence properties and is effective in high-dimensional or continuous actions spaces. However, they can converge to a local optimal rather than a global optima.</p> <p>“”” I was super sleepy after this, please update after watching the recording “””</p> <p>Now, for each step, we make a decision within a one-step MDP</p> <ul> <li>Expectation equation <ul> <li>SFT step is important</li> </ul> </li> </ul> <p><strong>Policy Gradient Theorem</strong> - For any differential blue policy $$\pi_\theta (s, a)</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/brains-and-ai/">Brains and AI</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/machine-learning-systems/">Machine Learning Systems</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/rl-theory/">Reinforcement Learning Theory</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/data-systems-for-ml/">Data Systems for Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Statistical-NLP/">Statistical Natural Language Processing</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Sudhansh Peddabomma. Last updated: March 09, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script src="/assets/js/tooltips-setup.js?53023e960fbc64cccb90d32e9363de2b"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-0K9MLG0V24');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> <div id="chat-window" class="chat-window"> <div class="chat-header"> <h5 class="mb-0">Talk to my AI</h5> <button id="close-chat" class="btn-close"> <i class="fas fa-times"></i> </button> </div> <div id="chat-messages" class="chat-messages"></div> <div class="chat-input-container"> <input type="text" id="chat-input" class="form-control" placeholder="Type a message..."> <button id="send-btn" class="btn btn-primary"> <i class="fas fa-paper-plane"></i> </button> </div> </div> </body> </html>