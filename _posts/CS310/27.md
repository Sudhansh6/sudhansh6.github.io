# Lecture 27

> `17-03-22`

We will now see the limitations of primitive-recursion. 

## Limitations of primitive recursion

We have seen composition as $$h \circ (g_1, \dots, d_n)$$. This concept is equivalent to passing parameters to functions in programming. To refresh the shorthand notion of primitive recursive functions, we define if-then-else as $$if\_then\_else(x, y, z)$$ as $$y$$ when $$x > 0$$ and $$z$$ otherwise. Therefore, $$if\_then\_else(x, y, z) = \rho(P_2^2, P_3^4)$$. Using this, we define 


$$
\begin{align}
quotient(x, y) &\triangleq \rho
(
	ite \circ
		(P_1^1, 0, \infty), \\ & 	
	ite \circ 
		(P_3^3, 
			ite \circ 
				(  \\ &monus 	\circ 
					(mult \circ 
							(P_3^3, S\circ P_2^3)
					, \\ &S \circ P_1^3)
				, P_2^3 , S \circ P_2^3), 		\infty)
)
\end{align}
$$


### Ackermann function

Many mathematicians tried to show that primitive recursion is not enough to represent all functions. In this pursuit, Ackermann came up with the following function


$$
\begin{align}
A(0, x) &= x + 1\\
A(y + 1, 0) &= A(y, 1) \\
A(y + 1, x + 1) &= A(y, A(y + 1, x)) \\
\end{align}
$$


Similarly, there is a ‘91-function’ that Mc Carthy came up with. It is given as


$$
M_c (x)
\begin{cases}
	x - 10 & x> 100 \\
	M_c(M_c(x + 11)) & \text{otherwise}
\end{cases}
$$


The above function always returns $$91$$ for $$x < 100$$. However, it requires a lot of recursive calls for evaluating its value.

### All primitive functions are total

The converse of the above statement is false. For example, the Ackermann function is not primitive. The idea is to show that any $$f$$ defined using primitive recursion grows slower than $$A(n_f, y)$$ for some $$n_f$$. Using *Godel numbering* we can count all the possible primitive recursive functions.

## Partial Recursive functions

We use the idea of **minimisation** to define partial functions and also increase the expressive power of our definitions. We have the following definition


$$
\mu(f)(x_1, \dots, x_k) \triangleq \begin{cases}
	z & \forall z_1 \leq z \; f(z_1, x_1, \dots, x_k) > 0 \land f(z, x_1, \dots, z_k) = 0\\
		
\end{cases}
$$


Notice that the first case in the above definition behaves like a while loop, as it gives the smallest value of $$z$$ that renders the function zero. The ‘partiality’ in the function definition comes from the fact that $$f$$ may never be zero. 

<u>The Church-Turing thesis states that all definable functions that can be defined using primitive recursion, minimisation, and substitution can be computed by a Turing machine.</u> There is no proof for this yet. The set of these functions is the set of ‘all effectively computable functions’. However, there are undecidable functions that are not computable by a TM. 

## Equational Logic

This paradigm has the same expressive power as primitive recursion but is easier to express. To start off, we consider the function $$leq$$. We will define $$N$$ as $$\{0, S(0), S(S(0)), \dots\}$$. We write the rules as


$$
\begin{align}
leq(0, x) &= S(0) \\
leq(S(x), 0) &= 0 \\
leq(S(x), S(y)) &= leq(x, y) 
\end{align}
$$


This way of writing rules is known as **Pattern matching**. Similarly, $$gcd$$ is given by


$$
\begin{align}
gcd(0, x) &= x \\
gcd(add(x, y), x) &= gcd(y, x) \\
gcd(y, add(x, y)) &= gcd(y, x) \\
\end{align}
$$


In the above definition, the pattern matching is *semantic* and not *syntactic*. The *syntactic* pattern matching definition uses $$if\_then\_else$$. However, note that $$if\_then\_else$$ is always assumed to be calculated in a **lazy manner**. That is, the condition is evaluated first and the corresponding branch is then evaluated. If the expressions in the two branches are evaluated along with the condition, then there is a chance that the computation never ends. 

Also, the factorial function $$fact$$ is given by


$$
\begin{align}
fact(0) &= S(0) \\
fact(S(x)) &= mult(S(x), fact(x))
\end{align}
$$


Another way of writing the factorial function using **tail recursion** is


$$
\begin{align}
f(x) &= f_a(1, x) \\
f(y, 1) &= y \\
f_a(y, S(x)) &= f_a(y\times S(x), x)
\end{align}
$$


Notice that the first definition of $$fact$$ is very inefficient compared to the latter. The second definition does inline multiplication with the arguments carrying the answer. However, the values are pushed on to the stack in the case of the first definition. These concepts of optimisation come to great use in building compilers.