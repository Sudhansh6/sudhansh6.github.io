<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reinforcement Learning Theory | Sudhansh Peddabomma </title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="A deep dive of the basic RL theory and how we used them in modern ML systems."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/rl-theory/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/chat.js?e73db4280bae3cbae4d78219277155b9"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Sudhansh Peddabomma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Articles </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reinforcement Learning Theory</h1> <p class="post-meta"> Created in January 06, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/notes"> <i class="fa-solid fa-tag fa-sm"></i> Notes</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>Through this article, we aim to understand the theory of reinforcement learning in the context of Large Language Models.</p> <p>Reinforcement learning addresses the domain of sequential decision problems, wherein an <em>agent</em> takes <em>actions</em> in an <em>environment</em>. These configurations are generally represented using <em>Markov Decision Processes</em> (MDP). An MDP is characterized by -</p> <ul> <li>Time - T - Discrete, infinite</li> <li>States - S - Discrete</li> <li>Actions - A - Discrete</li> <li> <p>Transitions - \(\tau : S \times A \to \Delta(S)\) - a probability distributions of states</p> \[P(s’ \vert s, a) = P_R[ \tau (s, a) = s’]\] <p>In more complicated setups, the transition function could be a function of the time \(T\) (stationarity and non-stationarity). In finite-time horizons, the time can be embedded inside the state itself, converting non-stationarity scenarios to stationarity ones.</p> </li> <li> <p>Reward - \(R: S \times A \times S \to \mathbb R: [-M, M]\). There is no need to make this non-deterministic since that is already generalized by \(\tau\). The rewards are usually bounded. The expected reward for an action is given by</p> \[R’(s, a) = \sum_{s’ \in S} R(s’, a, s) p(s’ \vert s, a)\] </li> <li>Initial state \(S_0\) - Can be a single state or a distribution over states.</li> <li>Discount Factor \(\gamma\) - A factor \(&lt;1\) to bound the total expected reward from the future. This will be better understandable from the text later.</li> </ul> <p>A <strong>policy</strong> \(\pi\) is a probability distributions over actions based on the current state. Consider a distribution \(\beta: S \to \delta(A)\), then a policy formally is</p> \[\pi(s, a) = P_R[\beta(s) = a]\] <p>A <strong>trajectory</strong> describes the sequence of states, actions of an agent in a <em>run</em> - \(s_0, a_0, r_0, s_1, a_1, r_1, \dots\).</p> <p>A policy is associated with a <strong>value function</strong> \(V_{\pi, t} (s) = \sum_{i = t}^\infty \gamma^{i - t} r_i\) Note that the dependence of the value of function on time is redundant in stationarity situations.</p> <p>The goal is to maximize this value function over all possible policies in an environment</p> \[\pi^* = \arg \max_{\pi} V_\pi (S_0)\] <p>This is known as the <em>Markov Decision Problem</em>. In a strict setting, the supremum may not exist (be a valid policy).</p> <p>How is Reinforcement Learning different from Supervised learning? The key differences are</p> <ul> <li>There is no sequentiality</li> <li>Every decision in SL has an associated reward, and there is no stochasticity</li> </ul> <p>Policies are optimized in multiple methods</p> <ul> <li>There are hill-climb methods - policy iteration, value iteration</li> <li>Learning based methods - think of a policy as a matrix that has to optimized to satisfy certain constraints.</li> </ul> <h2 id="bellman-equations">Bellman Equations</h2> <p>These are a recursive formulations of policies and value functions.</p> \[V_\pi(s) =3.142 s \pi(s, a)[ p(s’ \vert s, a) [R(s’, a, s) + \gamma V_\pi (s’)]\] <h2 id="the-markov-assumption">The Markov Assumption</h2> <p>Many processes in nature are <em>Markov</em> in nature - the action at the current state only depends on the current state, and not the history of states.</p> <p>A general policy is a function from trajectories \(H_t\) to a distribution over actions. However, such general representation is difficult to compute. Additionally, due to the Markov assumption, a stationary policy \(\pi: S \to \delta(A)\) is general enough for optimality. On surface, this may seem like we are limiting the power of an agent, but we will see that this is not the case. In fact, even a deterministic policy will do.</p> <p>How about reward? Are we limiting the reward function with a Markovian assumption? Since we consider expected rewards - the expectation value can be embedded within the state-based rewards as well. So in fact, both representations are the same.</p> <p>With a general policy, the probability of a trajectory is given by</p> \[P[S_0 = s_0, A_0 = a_0, \dots, S_t = s_t] = \mu(s_0) \pi(a_0 \vert s_0) P(s’ \vert s_0, a_0) \pi(A_1 \vert S_0, a_0, s_1) \dots\] <p>Continuing with the Bellman equations, our goal was to find a policy that maximizes the value function</p> \[\begin{align*} V_\pi(s) &amp;= \mathbb E_{P, \pi} \sum_{t = 0}^\infty \gamma^t [R(S_t, A_t) \vert S_0 = s] \max_{\pi} V_\pi(s) \\ &amp;= V^*(s) \\ \end{align*}\] <p>Firstly, does there exist a policy such that \(V_\pi(s) = V^*(s)\) for all \(s\)? An optimal policy for a particular state is guaranteed because of compactness, but is there such a policy for all states?</p> <p>To aid our analysis, we also define a <strong>Q-function</strong> \(Q: S \times A \to R\), that describes the expected reward \(Q(s, a)\) of taking an action \(a\) at state \(s\). This function is very similar to the value function but has an additional condition on the action as well. Why have both these functions? Convenience. Analogously, we also define \(Q^*(s, a) = \max_\pi Q_\pi (a, a)\).</p> <p><strong>Theorem (Existence theorem).</strong> There exists a stationary and deterministic policy that has the optimal value for all states.</p> <p>Therefore, Bellman equations are possible due to assumptions of Markovian nature of the policy and state transition functions. We have the following equations</p> \[\begin{align*} V_\pi(s) &amp;= R(s, \pi(s)) + \gamma \sum_{s’} P(s’ \vert a, \pi(s)) V_\pi (s’) \\ Q_\pi(s, a) &amp;= R(s, a) + \gamma \sum_{s’} P(s’ \vert s, a) \sum_{a’} \pi(a’ \vert s’) Q_\pi(s’, a’) \\ V_\pi(s) &amp;= \sum_a \pi(s) Q_\pi(s, a) \\ Q_\pi(s, a) &amp;= R(s, a) + \gamma \sum_{s’} P(s’ \vert s, a) V_\pi(s’) \end{align*}\] <h2 id="summary">Summary</h2> <p>The equation we’ve been working with for value iteration is</p> \[Q_{t + 1}(s, a) = L(Q_t) = R(s, a) + \mathbb E_{s’ \sim P(. \vert s, a)} \max_{a’} Q_{t}(s’, a’)\] <p>The key step here is to extract a policy from the current \(Q\) function. We noted that the optimal function \(Q^*\) satisfies the optimality equation.</p> <p>Aside from the optimality equation, we have the expectation equation that every \(Q\) function satisfies</p> \[Q_\pi(s, a) = R(s, a) + \mathbb E_{s’} \mathbb E_{a’ \sim \pi(\cdot \vert s’)} Q(s’, a’)\] <p>The advantage of looking at the optimality equation as an operation \(Q_{t + 1} = L(Q_{t})\) is that we can apply the contraction concepts to arrive at \(Q^*\) with Banach’s fixed point theorem. This way, we prove that there is a unique optimal \(Q\) function.</p> <p>Now we show that, after enough number of iterations, we can also get the value function \(V\) arbitrarily close the optimal value. All these subtleties together show that the value iteration algorithm works!</p> <p>How do obtain these bounds based on iterations? We need to find an upper bound for \(\| Q_{t} - Q^* \|_\infty\). We can show that this value is \(leq \|Q^* - Q_0 \|_\infty\). Assuming we start with a \(Q\) with all zeroes, the maximum value of \(Q^*\) is simply \(R_{\max}/(1 - \gamma) = (\max_{s, a} \vert R(s, a)\vert)/(1 - \gamma)\).</p> <p><em>Lemma.</em> \(\|V_m(s) - V^*(s)\| \leq \frac{2}{1 - \gamma}\| Q_m - Q^*\|_\infty\).</p> <p><em>Proof.</em></p> \[\begin{align*} V^*(s) - V_m(s) &amp;= Q^*(s, \pi^*(s)) - Q_m(s, a = \pi_m(s)) \\ &amp;= Q^*(s, \pi^*(s)) - Q^*(s, a) + Q^*(s, a) - Q_m(s, a) \\ &amp;= Q^*(s, \pi^*(s)) - Q^*(s, a) + \gamma \mathbb E_{s’} (V^*(s’) - V_m(s’)) &amp;\leq Q^*(s, \pi^*(s)) - Q^*(s, a) + \gamma \|V^* - V_m\|_\infty \\ &amp;\leq (Q^*(s, \pi^*(s)) - Q_m(s, \pi^*(s))) + (Q_m(s, \pi^*(s)) - Q^*(s, a)) + \gamma \|V^* - V_m \|_\infty \\ &amp;\leq (Q^*(s, \pi^*(s)) - Q_m(s, \pi^*(s))) + (Q_m(s, a) - Q^*(s, a)) + \gamma \|V^* - V_m \|_\infty \\ &amp;\leq 2\|Q^* - Q_m\|_\infty + \gamma \|V^* - V_m\|_\infty \\ \|V^* - V_m \|_\infty \leq \frac{2}{1 - \gamma} \|Q^* - Q_m \|_\infty \end{align*}\] <h1 id="policy-iteration">Policy Iteration</h1> <p>Instead of modifying the policy based on the current value, why not do it the other way round? Iterate over the policy, get its value and improve it again? There is a subtle different as compared to the previous algorithm, and it turns out that this method is much more efficient!</p> \[\pi_0 \underset{Q_0}{\longrightarrow} \to \pi_1 \to \cdots \to \pi_k \underset{Q_k}{\longrightarrow} \pi_{k + 1}\] <p>Policy iteration takes \(\mathcal O(\vert S\vert^3 + \vert S \vert^2 \vert A\vert)\) whereas value iteration is \(\mathcal O(\vert S \vert^2 \vert A\vert)\).</p> <h1 id="model-free-methods">Model-free methods</h1> <p>Policy iteration and Value iteration are closely related to each other. For both the algorithms, we need to evaluate a policy to find the corresponding value function. However, in many cases, we do not know the exact transition and reward functions. In other cases, the environment can have a large number of states, making it impossible to model it.</p> <p>For such situations, we rely on <strong>Monte-Carlo methods</strong>. Any method that solves a problem by generating suitable random numbers and observing that a fraction of numbers obey some property or properties, can be classified as a Monte Carlo method. The key ideas here are using a <em>sampling technique</em> for a heuristic <em>estimator</em>. These methods do not make use of the Markov assumption much, making them much more generalizable.</p> <p>The idea is to learn directly from episodes of experience without a prior knowledge of MDP transitions (rewards). Since the idea relies on episodes, one caveat is that it can only be applied to <em>episodic MDPs</em> - episodes have to terminate.</p> <h2 id="prediction-problem">Prediction Problem</h2> <p>Let us consider the first problem - estimating \(V_\pi(s)\) for a state. Instead of updating after every action, we update after each episode by taking the mean reward across all sample trajectories sampled from this stage. The <em>first-visit</em> algorithm is given by</p> \[\tilde V_\pi(s) = \frac{1}{m} \sum G_i^s\] <p>Where \(G_i^s\) is the total reward after \(s\) first appears in the episode. Even though the number of states is large, it’s nonetheless finite. Using this fact, we can show theoretically that the value above can be bounded. The convergence time is also associated with the underlying transition probabilities (rare states require more episodes to appear in the trajectory).</p> <p>There are more questions to answer. For example, is this a biased or unbiased estimator?</p> \[\begin{align*} \mathbb E(\tilde V_\pi(s)) &amp;= \frac{1}{m} \sum_i \mathbb E(G_i^s) \\ &amp;= V_\pi(s) \quad \because \text{ Markovian assumption} \end{align*}\] <p>How about doing a second-visit algorithm or every-visit algorithm? They are valid approaches too, the theoretical analysis slightly varies. The estimators may not be unbiased but they use the data more efficiently. That is useful in cases where sampling is expensive. The <em>every-visit algorithm</em> typically has a higher bias (due to dependencies in an episode for the occurrences) but lower variance and higher efficiency.</p> <p>These differences are important to understand. For example, in the latest <a href="https://openai.com/index/openai-o3-mini/" rel="external nofollow noopener" target="_blank">o3-mini</a> model, they observed that the every-visit variant of an RL algorithm obtained much better performance than the first-visit variant.</p> <p>In contrast, consider estimating the \(Q_\pi(s, a)\) function. A policy might choose a certain action for a given state. However, to get all the Q-values, the policy must account for exploring all actions at different states to get a good estimate. The exploration probability is captured by the <strong>epsilon-greedy class of algorithms</strong>.</p> <h2 id="temporal-difference-tdlambda-algorithms">Temporal Difference TD(\(\lambda\)) Algorithms</h2> <p>The idea of this class of algorithms is to improve the policy as we keep exploring the environment more.</p> <p>The first variant of these algorithms is TD(0) - We sample state transition from the trajectory (one from each) and update the existing value function based on the action and reward obtained. Formally, given \(V_t(.)\) and a sample from the trajectory \((s_t, a_t, r_t, s_{t + 1})\), how do we obtain \(V_{t + 1}(s_t)\)?</p> <p>From the Bellman’s equations, we have</p> \[V_\pi(s) = \mathbb E_{a \sim \pi(s)} [R(s, a) + \gamma \mathbb E_{s’ \in P(s, a)} V_\pi(s’)]\] <p>So, can we do</p> \[V_{t + 1}(s_t) \gets r_t + \gamma V_t(s_{t + 1})\] <p>This equation omits our previous estimate \(V_t(s_t)\). How do we use it? We can do a sort of an averaging or gradient descent</p> \[\begin{align*} V_{t + 1}(s_t) &amp;\gets V_t(s_t) + \alpha_{s_t} \delta_t \\ &amp;\delta_t = r_t + \gamma V_t(s_{t + 1}) - V_t(s_t) \end{align*}\] <p>In essence, we are averaging over the previous visits in the trajectory but with slightly different update rules.</p> <p>How do we choose the \(\alpha\)’s? For convergence purposes, we require \(\sum_{t = 0}^{\infty} \alpha(t) \to \infty\) and \(\sum_{t = 0}^\infty \alpha^2(t) &lt; \infty\). These parameters are important for controlling the bias and variance of the estimators as we will discuss later. Possible learning schedules include \(\alpha(t) = \frac{1}{t^{0.5 + \epsilon}}\) with \(0 &lt; \epsilon \leq 0.5\) and $$\alpha(t) = \frac{1}{\sqrt{t}\log t}.</p> <p>With these learning schedules, you can prove the convergence thinking of the process as a contraction. It converges to the Bellman’s equation for some new value \(\gamma\).</p> <p>To compare various statistical approaches (Monte Carlo methods), we need to compare the bias, variance, convergence and sampling complexity into account. To improve on some of these criteria, we have the general TD(\(\lambda\)) algorithms.</p> <p>Instead of considering a single transition, the general algorithm considers more transitions to update the value function.</p> \[V^k_t = r_t + \gamma r_{t + 1} + \dots + \gamma^{k} r_{t + k} +\gamma^{k + 1} V_t{s_t + k}\] <p>What is the advantage of this approach? We slowed the updates, which seemingly increases the bias but may decrease the variance. Another way to understand TD(\(\lambda\)) is to think of it as a combination of Temporal Difference and Monte Carlo learning. It is an average of \(k\)-step returns. Kind of <em>truncated Monte Carlo method</em>.</p> <p>What’s more? We can generalize the above mention equation a bit more. For different states, we can consider different \(k\)’s, and take the average of them.</p> \[V_t^\lambda(s_t) = (1 - \lambda)\sum_{i = 0}^\infty \lambda^i V_t^k(s_t)\] <p>where the update algorithm is</p> \[V_{t + 1} \gets V_t(s_t) + \alpha_t (V_t^{\lambda} (s_t) - V_t(s_t))\] <p>It reduces the variance because we are considering a a geometric weighted mean. Since longer windows \(k\) have higher variance, we reduce their weight in the average with \(lambda\).</p> <p>In practice, when we approach a new state, we take our current estimate of \(V_t\), update all our previous calculations to recompute the new value function. The convergence proofs for these algorithms is not mathematically rigorous. It is a good area of research to find better proofs or better yet, more efficient algorithms.</p> <p>How do we execute these algorithms in practice? Theoretically, setting the upper limit of \(j\) to \(\infty\) is the best estimate. However, \(\lambda\) acts like a discount factor, and if the rewards are sparse (towards the end of the episode), then the convergence would take a long time - <em>episodic</em> algorithms.</p> <p>In essence, this version of the algorithm requires many forward executions of the simulation. In many cases, since this is infeasible in practice, developers have started using a <em>backwards version</em> of the algorithm.</p> <p>We define the eligibility trace of a state \(e_t(s) = \gamma \lambda e_{t - 1}(s) + 1\). The update equation becomes</p> \[V_{t + 1}(s) \to V_t(s) + \alpha \delta_t e_t(s)\] <p>where \(\delta_T = R_t + \gamma V_{t + 1} (S_{t + 1}) - V_{t - 1}(S_t)\).</p> <p>These equations are almost same as the previous algorithm but with better practice implementation. (When you expand the formulae, and interchange the summations, this is what you get).</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/brains-and-ai/">Brains and AI</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/machine-learning-systems/">Machine Learning Systems</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ai-agents/">AI Agents</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/data-systems-for-ml/">Data Systems for Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Statistical-NLP/">Statistical Natural Language Processing</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Sudhansh Peddabomma. Last updated: February 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script src="/assets/js/tooltips-setup.js?53023e960fbc64cccb90d32e9363de2b"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-0K9MLG0V24');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> <div id="chat-window" class="chat-window"> <div class="chat-header"> <h5 class="mb-0">Talk to my AI</h5> <button id="close-chat" class="btn-close"> <i class="fas fa-times"></i> </button> </div> <div id="chat-messages" class="chat-messages"></div> <div class="chat-input-container"> <input type="text" id="chat-input" class="form-control" placeholder="Type a message..."> <button id="send-btn" class="btn btn-primary"> <i class="fas fa-paper-plane"></i> </button> </div> </div> </body> </html>