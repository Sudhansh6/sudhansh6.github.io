## Recap

Consider the following question - What is the arithmetic intensity of multiplying two matrices?

```
Load A
Load B
C = matmul(A, B)
```

Given $$A \in \mathbb{R}^{mxn}, B \in \mathbb{R}^{nxp}, C \in \mathbb{R}^{mxp}$$, the number of I/O operations is $$mn + np + mp$$, and the number of compute operations is $$2mnp$$ since there are approximately $$mnp$$ addition and multiplication operations. The arithmetic intensity is then $$\frac{\text{\#compute operations}}{\text{\#I/O operations}} = \frac{2mnp}{mn + np + mp}$$. Setting $$m=n=p=2$$, results in $$\frac{2x2x2x2}{2x2 + 2x2 + 2x2} = \frac{4}{3}$$. 

\textit{Note.} The addition operation discussed in the previous lecture also has the same I/O operations. However, \texttt{matmul} is a denser operation that results in a higher arithmetic intensity. In practice, it takes the same time to execute matrix addition and multiplication on GPUs, which is why they are so powerful.

`matmul` is an important operation. <Check>

Now, consider the following operations

- `broadcast_to`
- `slice`
- `reshape`
- Permute dimensions
- `transpose`
- indexing like `t[:, 1:5]` 

All these operations are optimized due to strided access in tensors. On the other hand `contiguous()` cannot take advantage of this.

Just to recap, the strides of a tensor of shape `[2, 9, 1]` stored in row major order are `[9, 1, 1]`

Consider the cache tiling operation - 

- It increases the memory allocated on Cache and memory transfers between cache and register
- It reuses the memory movement between Dram and Cache
- The arithmetic intensity _decreases_ since there is more load and store

# GPU and CUDA

We have seen that specialized cores offer much better performance over traditional CPUs. Consider the following basic architecture of a GPU

Let us see the basic terminology for understanding the architecture -

- **Threads** - Smallest units to process a chunk of data.
- **Blocks** - A group of threads that share memory. Each block has many threads mapped to a *streaming multiprocessor* (SM/SMP).
- **Grid** - A collection of blocks that execute the same kernel.
- **Kernel** - CUDA program executed by many CUDA cores in parallel. 

A GPU can be made more powerful by

- Adding SMs
- Adding more cores per SM
- Making the cores more powerful - at a point of *diminishing rewards*.

NVIDIA, the largest GPU company, has released P100, V100, A100, H100 and B100 (Blackwell) for ML development. K80, P4, T4 and L4 were a lower tier of GPUs. Let us analyze how the compute has changed across these versions

1. V100 (2019 -) - 80SMs, 2048 threads/SM - $3/hour
2. A100 (2020 -) - 108SMs, 2048 threads/SM - $4/hour
3. H100 (2022 -) - 144SMs, 2048 threads/SM - $12/hour
4. B100 and B200 (2025 -)- 

The numbers are not doubling, then how has the performance doubled? They decreased the precisions.. :( 

## CUDA

**What is CUDA?** It is a C-like language to program GPUs, first introduced in 2007 with NVIDIA Tesla architecture. It is designed after the grid/block/thread concepts.

CUDA programs contain a hierarchy of threads. Consider the following host code -

```cuda
const int Nx = 12;
const int Ny = 6;

dim3 threadsPerBlock(4, 3, 1); // 12
dim3 numBlocks(Ns/threadsPerBlock.x , Ny/threadsPerBlock.y , 1); // (3, 2, 1) = 6

// the following call triggers execution of 72 CUDA threads
matrixAddDoubleB<<<numBlocks, threadsPerBlock>>>(A, B, C);
```

The GPUs are associated with constants such as 

- `GridDim` - dimensions of the grid
- `blocking` - the block inter within the grid
- `blockDim` - the dimensions of a block
- `threadIdx` - the thread index within a block
  With these in mind, the CUDA kernel for the above code is designed as

```cuda
    __device__ float doubleValue(float x)
    {
        return 2*x;
    }

    // kernel definition 
    __global__ void matrixAddDoubleB(float A[Ny][Nx], float B[Ny][Nx], float C[Ny][Nx])
    {
        int i = blockIdx.x * blockDim.x + threadIdx.x;
        int j = blockIdx.y * blockDim.y + threadIdx.y;
        C[j][i] = A[j][i] + doubleValue(B[j][i]);
    }
```

The host code launched a grid of CUDA blocks, which then call the `matrixAdd` kernel. The function definition starts with `__global__` which denotes a CUDA kernel function that runs of the GPU. Each thread indexes its data using `blockIdx`, `blockDim`, `threadIdx` and execute the compute. It is the userâ€™s responsibility to ensure that the job is correctly partitioned and the memory is handled correctly.

The host code has a serial execution. However, the device code has SIMD parallel execution on the GPUs. When the kernel is launched, the CPU program *continues executing* without *halting* while the device code runs on the GPU. Due to this design, it is important that the device code does not have any return values - causes erroneous behavior. To get results from the GPU, `CUDA.synchronize` is used (an example will be shown later).

It is the developers responsibility to map the data to blocks and threads. The blockDim, shapes etc should be statically declared. This is the reason why compilers like `torch.compile` requires static shapes. The CUDA interface provides a CPU/GPU code separation to the users.

The SIMD implementation has a constraint for the control flow execution - it requires all ALUs/cores to process in the same pace. In a control flow, not all ALUs may do useful work and it can lead to up to 8 times lower peak performance. 

### Coherent and Divergent execution

A coherent execution applied the same instructions to all data. 
Divergent executions do the opposite and they need to be minimized in CUDA programs. This distinction is important to note - even the latest models like the LLMs have this behavior. Concepts such as attention masking and sliding window attention are examples of divergent behavior and they need to be specially implemented to extract the most compute from the GPU.

## CUDA Memory model

CUDA device (SIMD execution on GPU) has its own memory called the *HBM*.

Unlike host (CPU) memory that is stored as pages in the RAM, GPU memory does not use pages but has memory pools (bulk data) that are accessed all at once. 

Memory can be allocated `cudaMalloc` and populated with `cudaMemcpy` like usual. CUDA has a concept called **pinned memory** that is part of the host memory which is optimized for data transfer between CPU/GPU. Ig is not pagable by the OS and is locked, and only certain APIs can access it.

Every thread has its own private memory space, and every block has a shared memory that all its threads can access. The HBM is the global device memory in the GPU that can be accessed by all threads. The memory complexity is to balance between speed and shared memory parallelism. 

For example, consider the program for window averaging -

```python
    for i in range(len(input) - 2):
        output[i] = (input[i] + input[i + 1] + input[i + 1])/3.0
```

How can this be parallelized? Since every 3-element tuple reduction is independent, each reduction can be mapped to a CUDA core. So, each thread can compute the result for one element in the output array.

The host code -

```C
int N = 1024*1024;
cudaMalloc(&devInput, sizeof(float)*(N+2)); // To account for edge conditions
cudaMalloc(&devOutput, sizeof(float)*N);

convolve<<<N/THREADS_PER_BLK, THREADS_PER_BLK>>>(N, devInput, devOutput); 
```

The device code - 

```cuda
    #define THREADS_PER_BLK = 128

    __global__ void convolve(int N, float* input, float* output) {
        int index = blockIdx.x *blockDim.x + threadIdx.x; 
        float result = 0.0f; //thread-local variable
        result = input[index] + input[index + 1] + input[index + 2];
        output[index] = result /3.f;
    }
```

This program can be optimized - each element is read thrice!  
Notice that the number of blocks assigned is much more than what a typical GPU has. This is a general practice in CUDA programming where the blocks are *oversubscribed*.

How to optimize? The memory hierarchy can be utilized -

The new device code - 

```cuda
   #define THREADS_PER_BLK = 128

    __global__ void convolve(int N, float* input, float* output) {
        int index = blockIdx.x *blockDim.x + threadIdx.x; 
        
        __shared__ float support[THREADS_PER_BLK];
        support[threadIdx.x] = input[index];
        if(threadIdx.x < 2){
            support[THREADS_PER_BLK + threadIdx.x] = input[index + THREADS_PER_BLK];
        }

        __syncthreads();

        float result = 0.0f; //thread-local variable
        for(int i=0; i<3; i++)
            result += support[threadIdx.x + i];
            
        output[index] = result /3.f;
    }
```

We introduced a synchronization primitive here. `_syncthreads()` waits for all threads in a block to arrive at this point. Another primitive `cudasynchronize()` that syncs between host and the device. 

## Compilation

A CUDA program also needs to be converted to low-level instructions to be executed. A compiled CUDA device binary includes - 

- Program text (instructions)
- Information about required resources - 128 threads per block, 8 types of local data per thread and 130 floats (520 bytes) of shared space per thread block. 

The issue is that different GPUs have different SMs. If the user asks for a static (large) number of blocks, how to handle this? The first solution is that GPUs have varying (limited) number of blocks. 

Furthermore, CUDA schedules the threadblocks to many cores using a dynamic scheduling policy that respects the resource requirements. It assumes that the thread blocks can be executed in any order. The blocks are assigned based on the available resources and the remaining ones are *queued*. 

## Understanding a GPU

Consider a NVIDIA GTX 980 (2014) that has the following specs -

- 96KB of shared memory
- 16 SMs
- 2048 threads/SM
- 128 CUDA cores/SM
  Note that the number of CUDA cores is not equal to the number of CUDA threads.

As the GPUs became better, NVIDIA tried to increase the shared memory per SMM. This is similar to the SRAM which is very important for LLM inference. 
