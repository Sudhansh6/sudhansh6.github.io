# Language Models

Recently, RL techniques have been applied extensively to language models to improve their performance. What are the ideas behind it?

A language model (LLM) can be thought of as an agent in an environment. We (humans) give it feedback, and thereby we constitute the environment. The state of the LLM is the tokens generated so far, and the actions essentially are the set of tokens it can generate in the next instance. The probabilities associated with each token can be considered as the LLM policy (the language model itself encodes the policy). So, we have given an MDP formulation for the LLM. Can we use RL?

The problem with RL is that it does not scale well to high dimensions. With LLMs, the number of tokens is very large and rewards are super-sparse. We only give feedback to the LLM at the end of a generated sentence rather than for each token. That makes the exploration of this space to find the optimal policy (language model) very intractable. Due to this, we have resorted to SFT (supervised fine-tuning) techniques to encode an initial policy. By training an LLM with internet text data, we are constraining the search domain implicitly by developing a really good initial policy. This policy (language model) can then be used with RL techniques to improve even further. 

The current state of research is at this intersection - How do we improve the credit assignment to make it more fine-grained (token-wise)? Obtaining human feedback continuously is not feasible. Human feedback is subjective, inconsistent with time and sometimes without any rational basis. So some works try to train a reward model learning from human feedback, and that can be used with the LLM for RL. Thereâ€™s also the problem of catastrophic forgetting in smaller LLMs. 

## Overview of LLM training
There are three major steps involved in building models like Instruct-GPT.

### Pre-training
The model with randomized weights is trained on a large corpus of data in a self-supervised manner. This step essentially involves extracting large amounts of text, and training models like GPT for next-token prediction. Note that it does not require any labels. Intuitively, we are teaching the model human languages and making it learn how to generate meaningful sentences. The datasets have of the order trillions of tokens. 

### Super-vised Fine-tuning
In this step, we train the model with *prompts* and *answers*. We teach the model with specific set of questions and the corresponding answers. Again, there are many ways in which this step is done, some developers only train only a few layers of the model or use some other fine-tuning techniques like LoRA or Adapters. The datasets are much smaller but still consist millions of tokens. 

### Reinforcement-learning with Human Feedback (RLHF)
The most important aspect as we have discussed previously, is defining the reward. Many works are based on cognitive and neuroscience techniques. One popular observation is that humans are good at giving preferences over assigning numerical scores to choices. So, with this motivation, we ask the LLM to generate multiple responses to a given prompt. The human can then give a preference or a ranking order as a feedback to the model. These preferences can be related to scores for the reward model using the Bradley-Terry and Placket-Luce models. 