# AlphaZero

Prior to 2014, researchers came up with specific rules to master board games. For example, they created very sophisticated Chess engines (e.g., [Stockfish](https://stockfishchess.org)) and developed it iteratively. The issue with such games is that the state-space is very large (a computationally infeasible number). Training reinforcement learning algorithms (most suitable since these are sequential series of steps) is difficult since the rewards are usually given at the end of the plays. A single reward for a whole trajectory with states and actions is too sparse to train any useful policy. 

> Hierarchical action space with rewards

Addressing this, [AlphaZero by Deepmind](https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/) was proposed in 2014 leveraging the advancements in neural networks. Given a state as an input, the neural network returns the optimal action and expected value for that state. 

We take the examples of trajectories from the game $$\{s_i, \pi(s_i), V(s_i)\}$$ and minimize the following loss function
$$
    \mathcal L = (V_\theta(s_i) - V(s))^2 - \pi(s_i)^T \log \pi_\theta (s_i) + c \|\theta\|^2
$$

> Note that cross-entropy loss is the same as KL Divergence - we only consider the trainable part for the purposes of back-propagation.

It is easy to see why this may work. Along with this, they introduced *self-play* to iteratively and autonomously improve the policy. For this to work, the agent has to explore the state-action space while exploiting the policy. The trade-off is implemented through a KL-UCB based action choice

$$
U(s, a) = Q(s, a) + c \pi_\theta (s, a) \frac{\sqrt{\sum_b N(s, b)}}{N(s, a) + 1}
$$

where $$N(s, a)$$ represents the number of times we have chosen $$a$$ in state $$s$$.
