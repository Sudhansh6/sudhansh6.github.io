<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Advanced Computer Vision | Sudhansh Peddabomma</title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/Advanced-Computer-Vision/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Sudhansh Peddabomma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Articles</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Advanced Computer Vision</h1> <p class="post-meta">April 6, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/category/notes"> <i class="fa-solid fa-tag fa-sm"></i> Notes</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>Vision is a fundamental interface to the world, and it has become a crucial component in the development of intelligent systems. The field has deep and attractive scientific problems, which have been advancing at a rapid pace in the past few years.<br> In the early days of research, the focus in vision was on engineering “good” features coupled with a optimisation algorithm or a shallow neural network. As the processors became more powerful, the emphasis shifted to end-to-end approaches with inclusion of self-supervision and multi-modal learning paradigms.</p> <p>It is often helpful to breakdown the perception tasks into known-algorithms. For example, in autonomous driving, the tasks include SLAM (visual, Structure from Motion), path planning (lane detection, obstacle detection, 3D localization), Semantic segmentation etc. Similarly, the tasks in augmented reality devices are gaze tracking, material and lighting estimation, head pose estimation, depth estimation, etc.</p> <p>Deep learning has opened new areas of research in vision. Features such as generation of high-quality content, end-to-end training, data-driven priors and highly parallelizable architectures have proven advantageous for many problems in computer vision. However, it is also important to note the limitations of these techniques -</p> <ul> <li>Large scale labeled data is not always available</li> <li>Lack of generalization to unseen domains</li> <li>Good at narrow “classification”, not at broad “reasoning”</li> <li>Lack of interpretability</li> <li>Lack of reliability, security or privacy guarantee</li> </ul> <p>To counter these problems, we typically couple our algorithms with self-supervision, physical modelling, multi-modal learning and <em>foundation models</em>. In the recent years, these techniques have been applied to various problems, and the following are arguably the biggest advances in Computer Vision -</p> <ul> <li>Vision Transformers</li> <li>Vision-Language Models</li> <li>Diffusion Models</li> <li>Neural Rendering</li> </ul> <p>These techniques show promise to solve keystone problems in augmented reality, interactive robotics, and autonomous driving. The course will cover the following these topics, along with other fundamentals required.</p> <blockquote> <p>Demonstrate code in presentation</p> </blockquote> <h3 id="neural-architectures">Neural Architectures</h3> <p>The motivation for an artificial neuron (perceptron), comes from a biological neuron where the output is linear combination of the inputs combined with a non-linear activation function. From here, we develop multi-layer networks which are again motivated from the Hubel and Weisel’s architecture in biological cells.</p> <h3 id="neural-networks">Neural Networks</h3> <p>The simplest neural network is a perceptron represented by \(\sigma (x) = \text{sign}(\sum_i w_i x_i + b)\) where the optimal weight values are obtained using an unconstrained optimization problem. These concepts can be extended for “Linear Regression” and “Logistic Regression” tasks.</p> <p>Non-linearity in neural networks is introduced through <strong>Activation functions</strong> such as</p> <ul> <li>Sigmoid - Have vanishing gradient issues</li> <li>tanh - Centered version of sigmoid</li> <li>ReLU - Simplest non-linear activation, with easy gradient calculation.</li> <li>ELU - Added to prevent passive neurons.</li> </ul> <p>At the <strong>output layer</strong>, we apply a final non-linear function is applied to calculate the <strong>loss</strong> in the predicted output. Typically for <strong>classification problems</strong>, <em>Softmax</em> function is used to map the network outputs to probabilities. One-hot representations are not differentiable, and are hence not used for this task. In image synthesis problems, the output layer usually has \(255*sigmoid(z)\).</p> <p><strong>Theorem (Universal function approximators)</strong>: A two-layer network with a sufficient number of neurons can approximate any continous function to any desired accuracy.</p> <p><strong>Width or Depth?</strong> A wider network needs more and more neurons to represent arbitrary function with high enough precision. A deeper network on the contrary, require few parameters needed to achieve a similar approximation power. However, “overly deep” plain nets do not perform well. This is due to the vanishing gradient problem, wherein we are not able to train deep networks with the typical optimization algorithms.</p> <h3 id="convolution-networks">Convolution Networks</h3> <p>The neural network architecture is modified for images using “learnable kernels” in convolutional neural networks. Each convolution layer consists of a set of kernels that produce feature maps from the input. These feature maps capture the <em>spatial</em> and <em>local</em> relationships in the input which is crucial for images.</p> <p>The <em>induction bias</em> in images is that neighbouring variables are locally correlated. An image need not be 2D, it can consist of multiple channels (RGB, hyperspectral, etc.), and convolutional layers work <em>across all</em> these channels to produce feature maps.</p> <p>In a classical neural network, each pixel in the input image would be connected to every neuron in the network layer leading to <em>many</em> parameters for a single image. Using kernels, we use <em>shared weights</em> across all pixel locations, and this greatly reduces the number of learnable parameters without losing much information.</p> <p>Convolution layers are generally accompanies with <strong>Pooling layers</strong> which do not have any learnable parameters, and are used to reduce the size of the output. These layers are invariant to small (spatial)transformations in the input and help observe a larger <em>receptive field</em> in the next layer. The latter property is important to observe hidden layers in the feature maps.</p> <p><strong>Receptive Field</strong> - It is the area in the input iamge “seen” by a unit in a CNN. Inits with deeper layers will have wider receptive fields whereas wider receptive fields allow more global reasoning across entire image. This way, the pooling leads to more rapid widening of receptive fields. We need \(\mathcal O(n/k)\) layers with \((k \times k)\) convolutional filters to have a receptive field of \(n\) in the input. <em>Dilation layers</em> are used to achieve the same receptive field with \(\mathcal O(\log n)\) layers.</p> <p>However, in practice, the empirical receptive fields in the deeper networks is lower than the theoretical value due to sparse weights.</p> <p>Convolution networks are augmented with <em>dense</em> layers to get the output, to learn from the feature maps.</p> <p>The vanishing gradient problem in deeper networks has been solved using <strong>skip connections</strong> wherein the features from the earlier layers are concatenated with the deeper ones to allow passage of information. This way, we provide the network with the original input allowing it to learn the smaller fluctuations in the input (rather than focusing on learning the input itself).</p> <p>In summary, the key operations in convolutional layers are</p> <p><code class="language-plaintext highlighter-rouge">Input image -&gt; Convlution -&gt; Non-linearity -&gt; Spatial Pooling -&gt; Feature Maps</code></p> <p>CNNs have the above set of operations repeated many times. CNNs have been successful due to the following reasons</p> <ul> <li>Good Abstractions - Hierarchical and expressive feature representations. Conventional image processing algorithms relied on a pyramidal representation of features, and this methodology has also paved its way in CNNs.</li> <li>Good inductive biases - Remarkable in transferring knowledge across tasks. That is, pretrained networks can be easily augmented with other general tasks.</li> <li>Ease of implementation - Can be trained end-to-end, rather than hand-crafted for each task, and they can easily be implemented on parallel architectures.</li> </ul> <p>The key ideas -</p> <ul> <li>Convolutional layers leverage the local connectivity and weight sharing to reduce the number of learnable parameters.</li> <li>Pooling layers allow larger receptive fields letting us capture global features.</li> <li>Smaller kernels limit the number of parameters without compromising the performance much. This design decision comes from preferring deeper networks over wider networks. For example, \((1 \times 1)\) kernels are reduce the dimension in the channels dimension.</li> <li>Skip connections allow easier optimization with greater depth.</li> </ul> <blockquote> <p>Why are (1, 1) kernels useful? Use fewer channels instead?</p> </blockquote> <h1 id="transformers">Transformers</h1> <p>Transformers have shown better results in almost every task that CNNs have shone previously in. CNNs require significant depth or larger kernels to share information between non-local spatial locations (recall receptive fields).</p> <p>Many tasks, such as question-answering, require <em>long-range</em> reasoning and transformers are very good at this. For example, placing objects in augmented reality requires reasoning about light-sources, surface estimation, occlusion/shadow detection, etc. This is the primary intuition behind <strong>attention mechanism</strong> which is representative of foveated vision in humans.</p> <p><img src="../../assets/img/Computer%20Vision/2024-04-11-12-10-13-image.png" alt=""></p> <p><strong>Tokens</strong> - A data type than can be understood as a set of neurons obtained from vectorizing patches of an image. Typically need not be vectors, but they can be any structured froup that alows a set of differentiable operations. Note that these tokens in hidden layers might not correspond to pixels or interpretable attributes.</p> <p>The following captures a very good intuition for transformers.</p> <p><em>A transformers acts on tokens similarly as neural network acts on neurons. That is, combining tokens is same as for neurons, except tokens are vectors \(t_{out }= \sum_i w_i t_i\). In neural networks, linear layers are represented by \(x_{out} = W x_{in}\) and \(W\) is data-free, whereas in transformers, \(T_{out} = AT_{in}\), \(A\) depends on the data (attention). Again, non-linearity in neural networks is implemented via functions like ReLU whereas transformers use dense layers for non-linearity (applied token wise).</em></p> <p>The attention layer is a spsecial kind of linear transformation of tokens, wherein the attention function \(A = f(.)\) tells how much importance to pay to each token depending on the input query and other signals. <em>Attention-maps</em> help us visualize the global dependencies in the information. The required information is embedded in some dimension of the token representation. For example, the first dimension can count the number of horses in an iamge, and the bottom 3 dimensions can encode the color of the horse on the right. Attention has this flexibility to different allocations address different parts of a query. They can “attend” to only certain patches <em>which are important to the query</em>. This kind of functionality is difficult with CNNs.</p> <blockquote> <p>Apply embedding and neural network (before CNNs and Transofrmers)? Same number of parameters? Essentially similar thing? Associated higher weight to more related embedding.</p> </blockquote> <h3 id="query-key-value-attention">Query-Key-Value Attention</h3> <p>The mechanisms described previously are implemented by projecting tokens into queries, keys and values. Each of these are a vector of dimensions $p$, where $p &lt; d$. The <strong>query</strong> vector for the question is used to weigh the <strong>key</strong> vector for each token to obtain the <strong>values</strong>. This is done via computing the similarity between query and each key, and then the <em>attention</em> is given by the extent of similarities, normalized with softmax. The output token is obtained by summing all value vectors with weights assigned from the previous calculations. Roughly, the process looks like -</p> \[\begin{align*} \left. \begin{align*} q = W_q t \\ K_i = W_k t_i \end{align*} \right\} \implies s_i = q^T k_i \\ a_i = softmax(s_i) \\ t_{out} = \sum a_i v_i \end{align*}\] <p>The purpose of “values” is to ‘project’ back the similarities between queries and keys to the ‘token space’.</p> <p>Also note that, if the patch-size is too large, then we might lose the information within the patches. This is a tradeoff, and the decision is made based on the task at hand. For example, classification may work with large patches but tasks such as segmentation may not.</p> <h4 id="self-attention">Self-Attention</h4> <p>How do we learn implicit representations that can work with general queries? We compute <em>self-attention</em> using the image tokens as the queries - information derived from the image itself. How does this process help us? For example, if we are performing instance segmentation of an image with horses. Then, the concept of ‘horse’ is learned by attending more to tother horse tokens and less to background. Similarly, ‘a particular instance’ is learned by attending less to the tokens from other horses. When we do this for all pairs across \(N\) tokens gives us an \(N \times N\) attention matrix.</p> <p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png" title="" alt="" width="307"></p> <p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" title="" alt="" width="393"></p> <p>Typically, this matrix is computed in the <strong>encoder</strong> part of a transformer. This matrix is then used in the decoder with an input query to obtain the required results.</p> <h3 id="encoders">Encoders</h3> <p>An encoder of a transformer typically consists of many identical blocks connected serially with one another. Each such encoder block, computes a self-attention matrix and passes it through a feed-forward neural network, and they need to be applied across all the tokens in the input. Since the embeddings of tokens are independent of one another, each level of encoder blocks can be applied <em>in parallel</em> to all the tokens (patches in vision transformers) to obtain the embeddings. Such parallel computations were not possible in previous models like RNNs, which heavily relied on sequential dependencies.</p> <p>The original transformers paper normalizes the similarities with \(\sqrt{N}\) where \(N\) is the embedding dimension. This allows the gradients to stabilise and gives much better performance. This a simplified view of the mechanisms used in transformers.</p> <p>In addition to these, transformers also use <em>positional encoding</em> to encode the position of the tokens in the input sequence. In the context of images, is encodes where each patch occurs in the image.</p> <p><img src="C:\Users\ITSloaner\AppData\Roaming\marktext\images\2024-04-10-18-07-17-image.png" alt=""></p> <p>Positional encoding is usually done via sinusoidal functions. Other “learning-based” representations have been explored but they don’t have much different effect. This encoding structure allows extrapolation to sequnce lengths not seen in training.</p> <p>They also have <em>multi-head attention</em> which is equivalent to multiple channels in CNNs. That is, it allows patches to output more than one type of information.</p> <p><img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt=""></p> <p>This <a href="https://jalammar.github.io/illustrated-transformer/" rel="external nofollow noopener" target="_blank">blog</a> explains these mechanisms in-depth for interested readers. In summary, the functions of the encoder is visualized as</p> <p><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png" alt=""></p> <h3 id="decoders">Decoders</h3> <p>A decoder block is similar to an encoder block, with auto-regressive . The attention values learned in Encoders are used as ‘keys’ in the decoder attention block. This is called as <em>cross attention</em>.</p> <h2 id="vision-transformers">Vision-Transformers</h2> <p>Vision transformers build on the same ideas used in a transformer. Typically, they use only the encoder part of the architecture where patches are extracted from images and are flattened to form the tokens for the attention mechanism. They are usually projected using a linear layer or a CNN before performing the calculations. Apart from this, vision transformers also include an additional <em>class token</em> in the embeddings to help learn the global information across the image.</p> <p>CNNs typically have better inductive bias whereas transformers excel in shorter learning paths for long-range reasoning. However, the cost of self-attention is quadratic in the image size. Also note that, if the patch-size is too large, then we might lose the information within the patches. This is a tradeoff, and the decision is made based on the task at hand. For example, classification may work with large patches but tasks such as segmentation may not.</p> <h3 id="swin-transformers-and-dense-prediction-transformers">Swin Transformers and Dense Prediction Transformers</h3> <p>The vanilla vision transformer is restricted to classification tasks and is not optimal for other tasks like detection and segmentation. Also, as we have noted before, the quadratic complexity limit the number of patches in the image. Some image processing pipelines extract features across different scales of an image whereas the vanilla transformer is restricted to the uniform (coarse) scale.</p> <p>To address these limitations, <strong>swin transformers</strong> bring two key ideas from CNNs</p> <ul> <li> <p>Multi-scale feature maps - Feature maps from one resolution are downsampled to match the size in the next block.</p> <p><img src="../../assets/imgs/2024-04-12-17-27-50-image.png" title="" alt="" data-align="left"></p> </li> <li> <p>Local connectivity -</p> <ul> <li> <p>Windowed self-attention - Limit the computations by considering a local window (this was actually left as future work in the original attention paper).</p> </li> <li> <p>Shifted window self-attention - Allows windowed self-attention to learn long-range features using “shifts”. Essentially, we move the patches around to bring farther patches close together.</p> </li> </ul> </li> </ul> <p>However, these modifications are not enough for tasks like segmentation. Instead, we use something called as a <em>dense prediction transformer</em> (<strong>DPTs</strong>) where we couple the transformer encoder with a convolutional decoder to upsample and predict the required output.</p> <blockquote> <p>CNNs are shift-invariant whereas ViTs are permutation invariant. Why?</p> </blockquote> <p><img src="../../assets/imgs/2024-04-12-17-36-33-image.png" alt=""></p> <p>At each scale level in the above picture, we <em>reassemble</em> the tokens by concatenating and convolving with appropriate kernels to recover image-like representations in the decoder layers.</p> <h3 id="multimodality">Multimodality</h3> <p>Transformers allowed for easy multi-modal representations by tokenizing data from each modality with its own variant of <em>patches</em>. Works such as VATT have explored merging audio waveforms, text and images.</p> <h1 id="generative-models">Generative Models</h1> <h2 id="discriminative-and-generative-models">Discriminative and Generative Models</h2> <p>Disciminative models (classifiers) learn a many-to-one function \(f\) to learn labels for a given input. A generative model \(g\) maps these labels to the input space, and this function is one-to-many since one label can map to multiple inputs. It is difficult to model a stochastic function. Therefore, a generator model is coupled with a <em>noise vector</em> to construct a deterministic \(g\) with stochastic input \(z\). This variable \(z\) is called a <strong>latent variable</strong> since it is not observed in training data composed of \(\{x, y\}\) pairs.</p> \[\begin{align*} \text{Discriminative Models learn } P(Y \vert X) \\ \text{Generative Models learn } P(X, Y) \\ \text{Conditional Generator learn } P(X \vert Y) \\ \end{align*}\] <p>Let us focus on image generative models. Then, each dimension of the latent variable can encode the various characteristics of the image essentially allowing us to generate a wide variety of images. The labels \(y\) for images need not be classification labels, but textual descriptions can also be used for supervision.</p> <p>To understand how these models work, consider the case of uncoditional generative models. The goal is, given the real data \(x\), to generate synthetic data \(\hat x\) that <em>looks like</em> the real data. How do we quantify ‘looks like’?</p> <ul> <li> <p>We can try and match some marginal statistics - mean, variance, edge statistics, etc of the real data. Such measures are very useful in techniques for texture synthesis. For example, <a href="https://www.cns.nyu.edu/labs/heegerlab/content/publications/Heeger-siggraph95.pdf" rel="external nofollow noopener" target="_blank">Heeger-Bergen texture synthesis [1995]</a> uses an iterative process starting from the Gaussian noise and matches intensity histograms across different scales. Such design choices are used in modern techniques like <a href="https://en.wikipedia.org/wiki/StyleGAN" rel="external nofollow noopener" target="_blank">StyleGANs</a> and <a href="https://en.wikipedia.org/wiki/Diffusion_model" rel="external nofollow noopener" target="_blank">Diffusion models</a>.</p> </li> <li> <p>Have a high probability of success under a model fit to real-data (A discriminator).</p> </li> </ul> <p>The key-challenge in these models is novelity in the generation to ensure generalization.</p> <p>Generative models are classified into the following approaches -</p> <ul> <li> <p><strong>Energy-based models</strong> - Learn a scoring function \(s:X \to R\) that scores real-samples with a high score which represents energy/probability. During generation, return samples which have a high score. This paradigm is particularly useful for applications like anomaly detection.</p> </li> <li> <p><strong>Sampling from Noise</strong> - Learn a generative function \(g: Z \to X\) without needing to learn a probability density function. These methods explicitly model the data distribution, and techniques like GANs and Diffusion Models come under this regime.</p> </li> </ul> <h2 id="densityenergy-based-models">Density/Energy based models</h2> <p>Learn a scoring function \(s:X \to R\) that scores real-samples with a high score which represents energy/probability. During generation, return samples which have a high score. This paradigm is particularly useful for applications like anomaly detection.</p> <p>In these methods, we produce \(p_\theta\) fit to the training data by optimizing \(\arg\min_{p_\theta} D(p_{\theta}, p_{data})\). Since we don’t know \(p_{data}\) explicitly (we only have access to samples \(x \sim p_{data}\)), we minimize the <strong>KL-divergence</strong> to reduce the distance between the distributions using the samples.</p> \[\begin{align*} p_\theta^* &amp;= \arg\min_{p_\theta} KL(p_{data} \vert\vert p_\theta)\\ &amp;= \arg\max_{p_\theta} \mathbb E_{x\sim p_{data}}[\log p_\theta] - \mathbb E_{x\sim p_{data}}[\log p_{data}] \\ &amp;= \arg\max_{p_\theta} \mathbb E_{x\sim p_{data}}[\log p_\theta] \end{align*}\] <p>Intuitively, this way of optimization increases the density where the models are observed.</p> <p>The energy-based models have a slight modification wherein the energy function E_\theta is unnormalized unlike the probability density function p_\theta. Indeed, p_\theta can be determined as</p> \[\begin{align*} p_\theta &amp;= \frac{e^{-E_\theta}}{Z(\theta)}, \\ &amp;\text{ where } Z_\theta = \int_x e^{-E_{\theta}(x)} dx \end{align*}\] <p>This formulation is called as <em>Boltzmann or Gibbs</em> distibution. Learning energy function is convenient since normalization is intractable since we cannot see all the samples in the distribution. What is the motivation to use exponential functions?</p> <ol> <li> <p>It arises naturally in physical statistics</p> </li> <li> <p>Many common distributions (Normal, Poisson, …) are modeled using exponential functions.</p> </li> <li> <p>Want to handle large variations in probability</p> </li> </ol> <p>Although we don’t have the probabilities, we can still compare different samples using ratios of their energies. How do we sample in these approaches? Sampling approaches <strong>Markov Chain Monte Carlo</strong> (MCMC) only require relative probabilities for generating data. We can also find data points that maximize the probability.</p> \[\nabla_x \log p_\theta(x) = -\nabla_x E_\theta(x)\] <p>On the other hand, where would be prefer probability formulation over energy? Certain systems like safety in autonomous driving require absolute quantifiable probabilities rather than relative scores.</p> <p>In probability modeling, the probability of space where no samples are observed is automatically pushed down due to normalization. However, in energy based models, we need an explicit negative term to push energy up where no data points are observed. To do so, we set up an iterative optimization where the gradient of the log-likelihood function naturally decomposes into contrastive terms</p> \[\begin{align*} \nabla_\theta \mathbb E_{x \sim p_{data}} [\log p_\theta(x)] &amp;= \frac{1}{N}\nabla_\theta\sum_{i = 1}^N (\underbrace{-E_\theta(x^{(i)})}_{\text{ data samples}} + \underbrace{E_\theta(\hat x^{(i)})}_{\text{model samples}}) \\ &amp;= - \mathbb E_{x \sim p_{data}} [\nabla_\theta \mathbb E_\theta (x)] + \mathbb E_{x \sim p_\theta} [\nabla_\theta \mathbb E_\theta(x)] \end{align*}\] <h4 id="sampling">Sampling</h4> <p>We randomly initialize \(\hat x_0\)$ at \(t = 0\). Then, we repeat the following</p> <ol> <li> <p>Let \(\hat x' = \hat x_t + \eta\)$ where \(\eta\) is some noise</p> </li> <li> <p>If \(\mathbb E_\theta(\hat x') &lt; \mathbb E_\theta(\hat x_t)\), then choose \(\hat x_{t + 1} = \hat x'\).</p> </li> <li> <p>Else choose \(\hat x_{t + 1} = \hat x'\) with probability \(e^{(\mathbb E_\theta(\hat x_t) - \mathbb E_\theta(\hat x'))}\)</p> </li> </ol> <p>In practice, this algorithm takes a long time to converge. A variant of this called <strong>Langevin MCMC</strong> uses the gradient of the distribution to accelerate the sampling procedure</p> <ol> <li> <p>Choose \(q(x)\) an easy to sample prior distribution.</p> </li> <li> <p>Repeat for a fixed number of iterations \(t\)</p> \[\hat x_{t + 1} \sim \hat x_t + \epsilon \nabla_x \log p_\theta(\hat x_t) - \sqrt{2\epsilon} z_t\] <p>where \(z_t \sim \mathcal N(0, I)\). When \(\epsilon \to 0\), and \(t \to \infty\), we have \(\hat x_t \sim p_\theta\)</p> </li> </ol> <h2 id="diffusion-models">Diffusion Models</h2> <p>The inuition for these models builds from our previous approach. It is hard to map pure noise to \(x \sim N(0, I)\) to structured data, but it is very easy to do the opposite. To readers familiar with autoregressive models, where we remove one pixel of information at a time, this is much more generalized.</p> <p>The forward process in these models involves adding noise to the image \(x_0\) over many time steps. At time \(t\), we add noise \(\epsilon_t\) to obtain \(x_{t + 1}\) from \(x_{t}\). The noise addition is modeled with respect to the following equation</p> \[x_t = \sqrt{(1 - \beta_t) x_{t - 1}} + \sqrt{\beta_t}\epsilon_t, \quad \epsilon_t \sim N(0, I)\] <p>If this process is repeated for a large number of times, it can be shown that the final output simulates white noise. Now, in the learning part, we train a predictor to learn denoising from \(x_t\) to \(x_{t - 1}\). That is, given our model \(f_\theta\)</p> \[\hat x_{t - 1} = f_\theta(x_t , t)\] <h3 id="forward-noising">Forward Noising</h3> <p>Given an image \(x_0 \sim q(x)\), we essentially add the following Gaussian noise in \(T\) time steps</p> \[q(x_t \vert x_{t - 1}) = \mathcal N(x_t ; \sqrt{1 - \beta} x_{t - 1}, \beta_t I)\] <p>The term \(\beta_t\) is referred to as the schedule and \(0 &lt; \beta_t &lt; 1\). Typically, we set it to a small value in the beginning and do linear increments with time. The above process is a Markov’s process, resulting in the following property</p> \[q(x_{1:T} \vert x_0) = \prod_{t = 1}^T q(x_t \vert x_{t - 1})\] <p>Instead of a slow step-by-step process, training uses samples from arbitrary time step. To decrease the computations, we use the following properties of Gaussian distributions</p> <ul> <li> <p>Reparameterization trick: \(\mathcal N(\mu, \sigma^2)= \mu + \sigma \epsilon\) where \(\epsilon \sim \mathcal N(0, I)\)</p> </li> <li> <p>Merging Gaussians \(\mathcal N(0, \sigma_1^2 I)\) and \(\mathcal N(0, \sigma_2^2 I)\) is a Gaussian of the form\(\mathcal N(0, (\sigma_1^2 + \sigma_2^2) I)\)</p> </li> </ul> <p>Define \(\alpha_t = 1 - \beta_t\), and \(\bar \alpha_t = \prod_{i = 1}^t \alpha_i\), we can now sample \(x_t\) at an arbitrary time step</p> \[\begin{align*} x_t &amp;= \sqrt{\alpha_t} x_{t - 1} + \sqrt{1 - \alpha_t}\epsilon_{t - 1} \\ &amp;= \sqrt{\alpha_t\alpha_{t - 1}} x_{t - 1} + \sqrt{1 - \alpha_t\alpha_{t - 1}}\epsilon_{t - 1} \\ &amp;\dots \\ &amp;= \sqrt{\bar \alpha_t}x_0 + \sqrt{1 - \bar \alpha_t}\epsilon \end{align*}\] <p>When we schedule \(\beta_t\) such that \(\beta_1 &lt; \beta_2 &lt; \dots, \beta_T\) so that \(\bar \alpha_1 &gt; \dots, &gt; \bar \alpha_T\), such that \(\bar \alpha_T \to 0\), then</p> \[q(x_T \vert x_0) \approx \mathcal N(x_T; 0, I)\] <p>The intuition is that the diffusion kernel is a Gaussian</p> \[q(x_t) = \int q(x_0, x_t) dx_0 = \int q(x_0) q(x_t \vert x_0) dx_0\] <p>There are theoretical bounds showing a relation between the number of steps and overfitting of the model to the distribution. There are no such guarantees for VAEs.</p> <p>The increasing \(\beta_t\) schedule sort of accelerates the diffusion process wherein we simulate white noise with few iterations. However, the stable diffusion paper to generate images chose the cosine learning schedule which gave better results.</p> <h3 id="reverse-denoising">Reverse Denoising</h3> <p>The goal is to start with noise and gradually denoise to generate images. We start with \(x_T \sim \mathcal N(0, I)\) and sample form \(q(x_{t - 1} \vert x_t)\) to denoise. When \(\beta_t\) is small enough, we can show that this quantity is a Gaussian. However, estimating the quantity is difficult since it requires optimization over the whole dataset.</p> <p>Therefore, we learn a model \(p_\theta\) to reverse the process</p> \[P_\theta(x_{t - 1} \vert x_t) = \mathcal N(x_{t - 1}; \mu_\theta(x_t, t), \Sigma\] <p>#</p> <p>Essentially, the training objective is to maximise log-likelihood over the data distribution. A variational lower bound can be derived and is used in practice</p> \[\mathbb E_{q(x_0)}[-\log p_\theta(X_0)] \leq \mathbb E_{q(x_0) q(x_{1:T}\vert x_0)} \left[-\log \frac{p_\theta (x_{0:T})}{q(x_{1:T} \vert x_0)}\right]\] <p>Then, this decomposes into</p> \[\sum_{t &gt; 1} D_{KL} (q(x_{t - 1} \vert x_t, x_0) \vert\vert p_\theta(x_{t - 1} \vert x_t)) + \text{other terms}\] <p>The idea is that $q(x_{t - 1} \vert x_t, x_0)$ is tractable even though $q(x_{t - 1} \vert x_t)$ is not. That is because</p> \[\begin{align*} q(x_{t - 1} \vert x_t, x_0) &amp;= \mathcal N(x_{t - 1}; \tilde \mu_t(x_t, x_0), \tilde \beta_t I) \\ \text{where } \tilde \mu_t(x_t, x_0) &amp;= \frac{\sqrt{\bar \alpha_{t - 1}}}{1 - \bar \alpha_t}x_0 + \frac{\sqrt{1 - \beta_t} ( 1- \bar\alpha_{t - 1})}{1- \bar\alpha_{t - 1}}x_t \\ &amp;=\tilde \beta_t = \frac{1 - \bar \alpha_{t - 1}}{1 - \bar \alpha_t} \beta_t \end{align*}\] <h2 id="sampling-from-noise">Sampling from Noise</h2> <p>Generative Adversarial networks mainly fall in this domain. In these architectures, a generator network tries to fool the discriminator by generating real-looking images. In contrast, a discriminator network tries to distribguish between real and fake images. GANs don’t produce as good images as diffusion models, but the concept of adversarial learning is a crucial concept in many fields. The framework looks like this -</p> <p><img src="../../assets/img/Computer%20Vision/2024-04-24-17-21-56-image.png" alt=""></p> <p>The objective function for a GAN is formulated as a mini-max game - the generator tries to maximize the loss function whereas the discriminator tries to reduce it.</p> \[L = \min_{\theta_g} \max_{\theta_d} [\mathbb E _{x \sim p_{data}} \log D_{\theta_d} (x) + \mathbb E_{z \sim p(z)} \log (1 - D(G_{\theta_g}(z)))]\] <p>The training is done alternately, performing gradient ascent on the generator and descent on the discriminator.</p> <h1 id="object-detection">Object Detection</h1> <p>Simply put, the goal is identify objects in a given image. The evolution of algorithms for object detection is summarized as</p> <ul> <li> <p>HoG + SVM - Sliding window mechanism for feature detection.</p> </li> <li> <p>Deformable Part Models -</p> </li> <li> <p>CNN - Using a sliding window mechanism is computationally expensive. Instead, these methods use region proposals and filter out the ones with satisfying criteria.</p> </li> <li> <p>Transformers</p> </li> <li> <p>Self-supervision</p> </li> <li> <p>Open vocabulary</p> </li> </ul> <h2 id="multi-stage-cnns">Multi-stage CNNs</h2> <h3 id="rcnn">RCNN</h3> <p>The first demonstration that showed classification features from CNNs can be used to detect objects. Essentially, the last layer of the ImageNet classifier network was removed . Each region proposal is warped/cropped to match the CNN input size. The post-processing involves bounding box regression</p> <h3 id="fastrcnn">FastRCNN</h3> <h3 id="fasterrcnn">FasterRCNN</h3> <p>Uses a Region Proposal Network (RPN) after the last convolutional layer. The RPN is trained to produce region proposals directly without any need for external region proposals. After RPN, the RoI Pooling and an upstream classifier are used as regressors similar to FastRCNN.</p> <h4 id="region-proposal-network">Region Proposal Network</h4> <p>The network does the following -</p> <ul> <li> <p>Slide a small window on the feature map</p> </li> <li> <p>A small network to do classification - object or no-object</p> </li> <li> <p>A regressor to give the bounding box</p> </li> </ul> <h4 id="anchors">Anchors</h4> <p>Anchors are a set of reference positions on the feature map</p> <p>3 dimensions with aspect ratio, 3 different scales of boxes,</p> <h4 id="training">Training</h4> <h4 id="non-maximal-supression">Non-Maximal Supression</h4> <p>Since, multiple anchor boxes can map to an object, we iteratively choose the highest scoring box among the predictions and supp ress the predictions that have high IoU with the currently chosen box.</p> <h2 id="transformer-based-architectures">Transformer based Architectures</h2> <h3 id="detr">DETR</h3> <p>Faster R-CNN has many steps, handcrafted architecture and potentially non-differentiable steps. In contrast, DETR was proposed - an end-to-end transformer based architecture for object detection. The motivation was to capture all the human-designed optimization parts of the pipeline into one black-box using a transformer.</p> <p><img src="../../assets/img/Computer%20Vision/2024-04-24-17-53-05-image.png" alt=""></p> <p>Why do we want such an end-to-end architecture? They are more flexible to diverse data, capturing large datasets, finetuning, etc.</p> <p>In this architecture, a CNN backbone like ResNet extracts the features which are passed through the encoder to obtain latent representations. These are then used with a decoder along with object queries in the form of cross-attention to give bounding boxes as the output.</p> <p>After obtaining the bounding boxes, the loss is appropriately calculated by matching the boxes to the correct labels. It is our task to assign a ground truth label to the predictions</p> <blockquote> <p>Hungarian??</p> </blockquote> <p>The decoder takes object queries as input,</p> <p>500 epochs</p> <p>encoder is segmented, decoder is just getting bounding boxes</p> <h3 id="dino">DINO</h3> <p>lagging in scale variant objects.</p> <p>contrastive loss acting for negative samples</p> <p>YOLOX</p> <h2 id="single-shot-detectors">Single Shot-Detectors</h2> <p>The motivation for these networks is to infer detections faster without much tradeoff in accuracy.</p> <p><strong>Anchor boxes</strong></p> <p>no ambiguous IoU</p> <p>Hard negative mining</p> <h3 id="you-only-look-once-yolo">You Only Look Once (YOLO)</h3> <p>Divide into grids</p> <p>anchor boxes in grid and class probability map</p> <h3 id="yolo-x">YOLO-X</h3> <p>No anchored mechanism - hyperparameter tuning, more predictions, less generalization (OOD fails)</p> <p>Detection head decoupling, no anchor boxes, SimOTA</p> <p>predict size and centers rather than top-left</p> <p>mosaicing and mixing</p> <h3 id="pyramid-feature-extractions">Pyramid Feature Extractions</h3> <p>RFCN</p> <h1 id="semantic-segmentation">Semantic Segmentation</h1> <p>The goal in this task is much more fine-tuned wherein we assign class pixel-wise. We want locally accurate boundaries as compared to finding bounding boxes in object detection. The approaches require global reasoning as well.</p> <p>A naive approch would be to classify a pixel by considering the patch around the pixel in the image. This is very expensive computationally and does not capture any global information.</p> <p>Another approach is to consider image resolution convolutions (purely convolutions with stride 1, no pooling) and maining the image resolution to produce the output. This still does not work quite well, and they also consume more memory.</p> <p>Taking classification networks as the motications, some networks try pooling/striding to downsample the features. This architecture corresponds to the encoder part of the model, and the decoder then upsamples the image using transposed convolution, interpolation, unpooling to recover the spatial details in the original size. The convolutions can go deeper without occupying a lot of memory.</p> <p>How do we upsample?</p> <ul> <li> <p>Transposed convolution to upsample. The operation is shown below -</p> <table> <tbody> <tr> <td>![Transpose Convolution for Up-Sampling Images</td> <td>Paperspace Blog](https://blog.paperspace.com/content/images/2020/07/conv.gif)</td> </tr> </tbody> </table> <p>However, the outputs are not very precise. Combine global and local information.</p> </li> <li> <p>U-Net</p> </li> <li> <p>Unpooling followed by convolutions (simpler implementation of transposed convolution)</p> <p>The <strong>max-unpooling</strong> operation does the following -</p> <p>Why is this better? Lesser memory</p> </li> </ul> <p>In all the architectures above, once you downsample the image, you lose a lot of spatial information and the network uses a significant amount of parameters learning the upsampling process to represent the final output. To help with this, we can do the following</p> <ul> <li> <p>Predict at multiple scales and combine the predictions</p> </li> <li> <p><strong>Dilated convolutions</strong> - These are used to increase the receptive field without downsampling the image too much -</p> <p><img src="https://th.bing.com/th/id/R.4992be8ec58775d0f6f963c2ae7129b3?rik=orAxXCOkxWt5dw&amp;pid=ImgRaw&amp;r=0" alt="Animations of Convolution and Deconvolution — Machine Learning Lecture"></p> <p>It’s multi-scale and also captures full-scale resolution. The idea of dilated convolution is very important in other tasks as well - It prevents the loss of spatial information in downsampling tasks. However, there are gridding artifacts and higher memory consumptions with Dilated networks.</p> <p><strong>Degridding solutions</strong></p> <ul> <li> <p>Add convolution layers at end of the network with progressively lower dilation</p> </li> <li> <p>Remove skip connections in new layers, can propogate gridding artefacts because skip connections transfer high-frequency information.</p> </li> </ul> </li> <li> <p>Skip-connections</p> </li> <li> </li> </ul> <h1 id="physiological-grouping-for-perception">Physiological Grouping for Perception</h1> <h2 id="deeplab-v3">DeepLab v3+</h2> <h2 id="segformer">Segformer</h2> <h1 id="instance-segmentation">Instance Segmentation</h1> <h2 id="mask-rcnn">Mask RCNN</h2> <p>Predict eh</p> <h1 id="panoptic-segmentation">Panoptic Segmentation</h1> <p>Panoptic segmentation extends the idea of instance segmentation even further to segment objects beyond a fixed set of classes.</p> <p>#</p> <h1 id="universal-segmentation">Universal Segmentation</h1> <p>Specialized architectures for semantic</p> <h2 id="maskformer">MaskFormer</h2> <p>The architecture contains a strong CNN backbone for the encoder along with a query and pixel decoder. The query decoder essentially takes $n$ inputs to generate $n$ masks. The outputs from these layers are passed through an MLP to get binary masks for different classes.</p> <h2 id="masked2former">Masked2Former</h2> <p>Uses a masked attention layer instead of cross-attention to provide faster convergence</p> <h3 id="limitations">Limitations</h3> <p>Needs to be trained on speicfic datasets, struggles with small objects and large inference time.</p> <p>Hasn’t been extended to video segmentation and panoptic segmentation.</p> <h2 id="mask-dino">Mask DINO</h2> <p>Does both detection and segmentation. A Mask prediction branch along with box prediction in DINO.</p> <h3 id="unified-denoising-training">Unified Denoising Training</h3> <h1 id="foundation-models">Foundation Models</h1> <p>Models that have been trained on simple tasks which have good performance in the pipelines of other tasks for <strong>zero-shot transfer</strong> capabilities. Such models were first used in Natural Language Processing and using these in vision is typicalled hindered by unavailability of labeled data.</p> <h2 id="segment-anything">Segment Anything</h2> <p>A model developed by Meta with zero-shot segmentation capabilities. The task this model tries to solve is to allow for interactive and automatic use with zero-shot generatlization and re-use. The model allows for flexibile prompts with real-time usage and is ambiguity-aware. The model also has a data-engine which is used to generate data through the model.</p> <h3 id="promptable-segmentation">Promptable segmentation</h3> <p>The model generates a valid mask for a given prompt (even if ambiguous) as input. How is the ambiguity resolved? Ambiguous cases arise when the mutiple objects lie on top of each other.</p> <p><img src="../../assets/img/Computer%20Vision/2024-05-03-17-21-39-image.png" alt=""></p> <p>The image encoder is heavy - a pretrained ViT with masked autoencoder. The design choice for the masking involved masking around 75% of the patches. Unlike NLP tasks, vision tasks rely heavily on spatial information and neighbor patches can be reconstructed without much effort. Also, NLP tasks can get away with a simple MLP for decoders but vision taks require strong decoders.</p> <p>The model allows for prompts using points, box or text. The decoder has self-attention on the primpts followed by cross-attention with image and MLP for non-linearity to get masks. The model also estimates the IoU to rank masks.</p> <p>The emphasis of the model was also to make it universal. Since the encoder is heavy, the embeddings are precomputed after which the prompt encoder and mask decoder work quite fast.</p> <h3 id="data-engine">Data Engine</h3> <p>There is no large-scale dataset available for training a segmentation model of this scale. Initially, the model has been trained iteratively on available datasets with several rounds of human correction. Following this, in the semi-automatic stage, the diversity of outputs (increasing ambiguity in overlapping objects) is improved by human refinement. Finally, in the fully automatic stage, prompting is introduced and masks with high IoU are preserved followed by NMS.</p> <h3 id="zero-shot-transfer-capabilities">Zero-shot transfer capabilities</h3> <p>The idea is to fine-tune the model for specific tasks like edge-derection, object proposals and isntance segmentation.</p> <h1 id="future-trajectory-prediction">Future Trajectory Prediction</h1> <p>This problem arises in many applications, particularly in autonomous driving. Given traffic participants and scene elements which are inter-dependent, we need to predict trajectories with scene awareness and multimodality. Humans are able to predict up to 10 seconds in highway scenes and plan their trajectory effectively whereas the SOTA models have a large gap in performance.</p> <p>More concretely, the algorithm takes a video sequence as an input, and needs to predict <strong>probabilistic</strong> trajectories which are diverse, have 3D awareness, capture semantics interactions and focus on <strong>long-term rewards</strong>. There are many components in these solutions including 3D localization module, generative sampling modules, semantic scene modules and scene aware fusion modules with encoders and decoders.</p> <p>#</p> <h3 id="representation-framework">Representation Framework</h3> <p>The past observed trajectories are represented by X = {X_{i, t - l + 1}, \dots, X_{i, t}} usng we which we wish to predict future trajectory Y_i = {Y_{i, t + 1}, \dots, Y_{i, t + \delta}}. The parameter \delta represents how far ahead we want to look in the future.</p> <p>The <em>sample generation module</em> produces future hypotheses \hat Y, and then a ranking module assigns a reward to each hypothesis considering long-term rewards. Following this, the refinement step calculates the displacement \Delta Y_t for the selected trajectory.</p> <p>Focusing on the sample generation module, the goal is to estimate posterior P(Y \vert X, I). Earlier, RNNs and other deterministic function maps from {X, I} to Y have been designed to address this. The problem with training in this approach is that the ground truth has a single future trajectory whereas the sampler predicts a distribution. How do we compare the two?</p> <h2 id="principal-component-analysis">Principal Component Analysis</h2> <p>Principal component analysis (PCA) is a linear dimensionality reduction technique where data is linearly transformed onto a new coordinate system such that the directions (principal components) capturing the largest variation in the data can be easily identified. PCA works well when the data is near a <strong>linear manifold</strong>* in high dimensional spaces.</p> <p>CAn we approximate PCA with a Network? Train a network with a bottleneck hidden layer, and try to make the output the same as the input. Without any activations, the neural network simply performs least-squares minimization which essentially is the PCA algorithm.</p> <p>Adding non-linear activation functions would help the network map non-linear manifolds as well. This motivates for autoencoder networks.</p> <h2 id="autoencoders">Autoencoders</h2> <p>Network with a bottleneck layer that tries to reconstruct the input. The decoder can map latent vectors to images essentially helping us reconstruct images from a low-dimension. However, this is not truly generative yet since it learns a one-to-one mapping. An arbitrary latent vector can be far away from the learned latent space, and sampling new outputs is difficult.</p> <h2 id="--variational-autoencoder">### Variational Autoencoder</h2> <p>This network extends the idea in autoencoders and trains the network to learn a Gaussian distribution (parametrized by $\mu, \sigma$) for the latent space. After learning, say, the normal distribution, sampling is quite easy to generate new samples representative of the trianing dataset using the decoder.</p> <p>To train this network, a distribution loss, like the KL-divergence is added in the latent space. But how do we backpropagate through random sampling? The reparametrization trick!</p> <h3 id="conditional-variational-autoencoder">Conditional Variational Autoencoder</h3> <p>The encoder and decoder networks are now conditioned on the class variable.</p> <h2 id="conditional-diffusion-models">Conditional Diffusion Models</h2> <p>Gradient of log, score function, conditioning at the cost of diversity - train a classifier and a diffusion model together in principle it’s fine. Potential issues - classifier can’t learn from noisy images and gradient of the classifier is not meaningful.</p> <h2 id="classifier-free-guidance">Classifier-Free Guidance</h2> <p>learn unconditional diffusion model $p_\theta(x)$ and use the conditional model $p_\theta(x \vert y)$ for some part of the training. this would not require any external classifier for training.</p> <h2 id="ctg---language-guided-traffic-simulation-via-scene-level-diffusion">CTG++ - Language Guided Traffic Simulation via Scene-Level Diffusion</h2> <p>Testing autonomous agents in the real-world scenarios is expensive. Alternately, researchers aim to build good simulator that are realistic and can be controlled easily.</p> <p>Why not RL-based methods? Designed long-term reward functions is difficult for such scenarios. The goal here is in a generation regime rather than problem solving. Also, the focus here is on controllability rather than realism. For realism, the methods would be more data-driven.</p> <h3 id="trajectory-planning">Trajectory Planning</h3> <p>Recent works have proposed diffusion-models for planning algorithms using classifier-guidance. <em>Diffuser</em> for example, is one such work where the authors generate state action pairs which are guided using a reward function to generate paths in a maze.</p> <h3 id="traffic-simulation-methods">Traffic simulation methods</h3> <p>Early works for this task rrelied on rule-based algorithms which were highly controllable but not very realistic.</p> <h3 id="ctg">CTG</h3> <p>One of the first workd for diffusion models for planning and conditional guidance for control with STL-based loss. Howeber, it modelled each agent independently which caused issues.</p> <p>CTG++ uses <strong>scene-level control</strong></p> <h3 id="problem-formulation">Problem Formulation</h3> <p>The state is given by the locations, speeds and angle of $M$ agents whose past history and local semantic maps are given. We aim to learn a distribution of trajectories given this information.</p> <p>The encoder represents the state action pairs in an embedded space on which temporal and cross attention on a diffusion based loss for training/</p> <h3 id="inference">Inference</h3> <p>The model uses LLMs to generate code for guidance function.</p> <h3 id="limitations-and-future-work">Limitations and future work</h3> <p>CTG++ is able to generate more realistic example, but is still far-off from realism. There are no ablation studies or emperical evaluations with complicated scenarios. Also, the multi-agent modeling can be improved for scalability.</p> <p>In conclusion, trajectory prediction requires generative models which are able to distinugish between feasible and infeasible trajectories. Variational Auto-Encoders have shown good performance for this task, and the current works aim to explore the scope of diffusion models in these tasks.</p> <h2 id="leapfrog---stochastic-trajectory-prediction">LeapFrog - Stochastic Trajectory Prediction</h2> <p>Similar to CTG++, this model aims to simulate real-world traffic scenarios. The authors aim for real-time predictions and better prediction accuracy. Leapfrog initializer skips over several steps of denoising and uses only few denoising steps to refine the distribtuion.</p> <h3 id="system-architecture">System Architecture</h3> <p>Leapfrog uses physics-inspired dynamics to reduce the number of steps required for the denoising process. The leapfrog initializer estimates mean trajectory for backbone of pericition, variance to control the prediction diversity and K samples simultaneously.</p> <h3 id="limitations-1">Limitations</h3> <p>The inference speed improved dramatically and achieves state of the art performance on the datasets. The model’s efficiency is higher for lower dimensional data and requires more work for scaling to higher dimensions.</p> <h1 id="structure-from-motion">Structure from Motion</h1> <p>The problem statement for structure from motion is - given a set of unordered or ordered set of images, estimate the relative positions of the cameras and recover the 3D structure of the world, typically as point clouds. In scenarios like autonomous driving, the images are ordered and the emphasis is on real-time performance.</p> <h2 id="feature-detection">Feature Detection</h2> <p>The first step involves identifying matching features across images to determine the camera movements. In unordered feature matching, the images to be compared are identified using vocabulary based retrieval methods to reduce the complexity from $\mathcal O(n^2)$ to $\log$ complexity.</p> <p>In the canonical coordinate system, the camera axis passes through the origin, and the $k$-axis points away from the camera. The $i$-axis and $j$-axis are parallel to the image plane. The projection of 3D points in the pixel space involves a non-linear operation -</p> \[(x, y, z) \to (-d\frac{x}{z}, -d\frac{y}{z})\] <p>where $d$ is the distance of the camera plane from the origin. For computational reasons, we want to convert these to a linear transformation. This is done via homogenous point representations -</p> \[\underbrace{(\frac{x}{w}, \frac{y}{w})}_\text{Euclidean} \to\underbrace{(x, y, w)}_\text{Homogenous}\] <p>Such representations are useful for ray marching operations. The camera transformation then becomes</p> \[\begin{bmatrix}-d &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; -d &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix} \begin{bmatrix}x \\ y \\ z \\ 1\end{bmatrix} = \begin{bmatrix}-dx \\ -dy \\ z\end{bmatrix}\] <p>which represents $(-d\frac{x}{z}, -d\frac{y}{z})$ in Euclidean space. The matrix above can be decomposed as</p> \[\underbrace{\begin{bmatrix}-d &amp; 0 &amp; 0 \\ 0 &amp; -d &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}}_{K}\underbrace{\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix}}_\text{projection}\] <p>Here, we have considered a simple model for the camera intrinsics matrix $K$ whose general form is</p> \[\begin{bmatrix}-d_x &amp; s &amp; c_x \\ 0 &amp; -d_y &amp; c_y \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\] <p>where $\alpha = \frac{d_x}{d_y}$ is the aspect ratio (1 unless pixels are not square), $s$ is the skew, $c_x, c_y$ represent the translation of the camera origin wrt world origin.</p> <h3 id="coordinate-systems">Coordinate Systems</h3> <p>We need to determine the transformations between the world and camera coordinate systems. Since camera is a rigid body, the transformation is represented by a translation and a rotation. Considering these, the projection matrix becomes</p> \[\Pi = K \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0\end{bmatrix}\begin{bmatrix}R &amp; 0 \\ 0 &amp; 0 \end{bmatrix} \begin{bmatrix} T &amp; - c \\ 0 &amp; 0\end{bmatrix}\] <h2 id="problem-formulation-1">Problem Formulation</h2> <p>Given the projections $\Pi_i X_i \sim P_{ii}$, we aim to minimize a non-linear least squares of the form $G(R, T, X)$ -</p> \[G(X, R, T) = \sum_{i = 1}^m \sum_{j = 1}^n w_{ij} \cdot \left\|P(X_i, R_j, t_j) - \begin{bmatrix}u_{i, j} \\v_{i, j}\end{bmatrix} \right\|^2\] <p>This problem is called as <strong>Bundle Adjustment</strong>. Since this is an non-linear least squares, Levenberg-Marquardt is a popular choice to solve this. In theory, given enough number of points, we should get a unique solution. However, practically, we require a very good initialization to navigate the high-dimensional space. Also, outliers in the feature detection can deteriorate the performance of the model drastically.</p> <h2 id="tricks-for-real-world-performance">Tricks for Real-world performance</h2> <p>Basically, we aim to make the feature detection very robust to remove all the possible outliers. Towards that, we note the following background</p> <h4 id="fundamental-matrix">Fundamental Matrix</h4> <p>Given pixel coordinates $x_1, x_2$ of a point $x$ in two camera views, we define a fundamental matrix $F$ such that $x_1Fx_2 = 0$. The fundamental matrix is unique to the cameras and their relative configuration. It is given by</p> \[\begin{align*} F &amp;= K_2^{-T} E K_1^{-1} \\ E &amp;= [t]_{\times} R \end{align*}\] <p>where $R$ and $[t]_\times$represent the relative base transformations given by</p> \[[t]_X = \begin{bmatrix} 0 &amp; -t_z &amp; t_y \\ t_z &amp; 0 &amp; -t_x \\ -t_y &amp; t_x &amp; 0\end{bmatrix}\] <p>The matrix $E$ is called as the essential matrix which has similar properties as the fundamental matrix and is also independent of the camera parameters.</p> <p>The fundamental matrix is scale-invariant and has rank $2$. Therefore, it has $7$ degrees of freedom implying that we just need $8$ correctly configured points ($x$ and $y$) across the two pixel spaces to compute the fundamental matrix. We pose this problem as solving $Af = 0$ - minimizing $|Af|$ using SVD (to directly operate on the rank). This procedure is popularly known as the $8$-point algorithm. Furthermore, we can also use this to find the camera parameters by using more points.</p> <p>The essential matrix can be derived from $F$ using $RQ$ decomposition to obtain skew-symmetric and orthogonal matrices $[t]_\times$ and $R$. These matrices are what we were looking to determine - the relative configuration of the cameras with respect to each other. Now, triangulation can be used to locate $x$ in the 3D space. However, this may not work since the projection lines may not intersect in 3D space.</p> <p>To filter outliers, we will use the fundamental matrix as a model - it is a geometric property which all points have to abide by. However, the fundamental matrix itself is constructed from the detected features, which can be noisy. We need to design a robust method using heuristics to filter the outliers. We use a population-voting metric to choose the best model.</p> <h3 id="ransac">RANSAC</h3> <p>Suppose we are finding the best-fit line for a given set of points. For every pair of points, we determine the corresponding line and count the number of outliers for that particular line. The best model would then be the on which has the minimum number of outliers. This idea can be extended to any model, and it can be used to calculate the best fundamental matrix. The algorithm is as follows -</p> <ol> <li>Randomly choose $s$ samples - typically $s$ is the minimum sample size to git a model (7 for a Fundamental matrix)</li> <li>Fit a model to these samples</li> <li>Count the number of inliers that approximately fit the models.</li> <li>Repeat $N$ times</li> <li>Choose the model with the largest set of inliers.</li> </ol> <p>How do we choose $N$? It depends on the ratio of outliers to the dataset size, the minimum number of samples $s$ for the model and confidence score. We get \(N = \frac{\log(1 - p)}{\log(1 - (1 - \epsilon)^s)}\)</p> <p>Where $\epsilon$ is the proportion of outliers and $p$ is the confidence score.</p> <h3 id="bundle-adjustment">Bundle Adjustment</h3> <p>Assuming we have a good $R, t$ robust to outliers with the above methods, we can now solve the least-squares problem to determine the scene geometry.</p> <h2 id="minimal-problems-in-computer-vision">Minimal Problems in computer Vision</h2> <insert image=""> ### Three—point absolute pose estimation Assume the intrinsics of the camera are known. Determine the camera pose from given 3D-2D correspondences. Minimal case is 3 points: 6 degrees of freedom, 2 constraints per point $(u, v)$. Given $p_1, p_2, p_3$ in world coordinates, find positions In camera coordinates. Equivalently, determine how to move camera such that corresponding 2D-3D points align. ### Real-time SFM: Steady state Suppose we have point-cloud and poses available at frame $k$, find the pose in frame $k + 1$. #### Option 1 Find correspondences between consecutive frames, estimate the essential matrix and find $R, t$. Keep repeating this as frames are read. 5 point estimation #### Option 2 Estimate correspondences between point cloud and frame - 3 point absolute pose. Challenges - inliers calculations, which points to match. RANSAC maybe easier here. Option 1 is narrow baseline triangulation. ### Drift Problem Absolute scale is undeterminable. Why does scale matter? Noise and RANSAC. External measurements to fix this. ### Loop Closure Retrieval mechanism ## Lighting Unlike material properties and geometry, lighting is an external factor that is very complicated to infer. Lighting in a scene can. Be categorised as - Local illumination - Li - Global illumination ## Shape Normals Explicit representations Implicit representations - SDF # Inverse Rendering ## PhySG Geometry - SDF, Lighting - Environment map, Material (BRDF) Sphere Tracing! Monte Carlo integration for BRDFs - too slow. PhySG - spherical Gaussians # Neural Fields The central question associated with neural fields is "what can be seen?". That is, how can we measure what we see and how to model it for further computations? This question is important to not only the researchers in Computer Graphics but also in a field called Plenoptic Functions. It is basically the bundle of all light rays in a scene - a 5D function parametrized by position ($$x, y, z$$) and viewing direction ($$\theta, \phi$$). Neural Fields aim to approximate this function for a scene using Neural networks. Our core problem is that we have 2D slides and we need to estimate a 5D function. This problem is referred to as Image-based Rendering. It was taken into serious consideration in literature with the advent of the digital Michaelangelo project by Stanford, wherein researchers reconstructed the 3D models of all the sculptures in the Louvre museum. A closely related problem called "view synthesis" aims to generate images from views not present in the training data. In this problem, the 5D function is simplified to a 4D light field (rays passing through 3D space). Images are then captured from multiple view points, and **novel view** synthesis is done via interpolation with the closest rays. The earliest work in virtual reality by Sutherland in 1968, which talks about view synthesis and the ideas introduced in this paper are still relevant till date. The problem also gained interest in the graphics community - Chen et. al in SIGGRAPH 1993 talks about representing 3D models using images. However, researchers viewed view-synthesis as a small part of overall 3D scene understanding. ## Perfect 3D understanding Research then aimed to solve this problem rather than view synthesis. How is it different? In view syntheis, there is no information regarding the geometry of the scene itself. We want to infer the structure of the scene as well. So, how do we estimate the geometry of the scene? ### Image-Depth Warping As we have seen earlier, one of the first approaches for this problem was to perform feature detection and matching to triangulate points in 3D space. This core approach is being used in Depth from Stereo, multi-view stereo, 3D reconstruction, SLAM and so on. However, the output from these techniques is a *point cloud* which is sparse and contains "holes" which are not useful for the "structure of the scene". #### Surface reconstruction Another subfield of graphics/vision tries to estimate the surface from these points clouds. There were methods using Local Gaussians, Local RBF functions, Local Signed Distance functions, etc. The famous approaches for this problem is Poisson surface reconstruction and Neural Signed Distance functions. In a way, Gaussian splatting is a surface reconstruction technique. ### Space Carving The underlying idea is motivatede from how humans create a sculpture - start out with a block, and carve/chip out regions that are not consistent with the camera viewpoints. However, this does not work well if there are not enough view points of the object (imaging back-projection problem in tomography). ### Using Depth data Suppose we have the depth information of each pixel as well - in such scenario, dealing with occlusions and "best-view planning" for 3D reconstruction becomes quite easy. ### The slump Between 2005 and 2014, there wasn't much work done in the area of view synthesis, but many researchers focused on solving sub-problems in 3D scene understanding. There were papers for single-view depth estimation with a variety of diverse approaches. Along with these advances, the camera technologies also improved rapidly. Multi-array cameras were improved to hand-held cameras with much better resolution. The sensors to capture depth also improved - structured light was used in the past which did not work well in real-life scenarios. In contrast, we now have LIDARs to capture depth accurately and instantly. Given these advances, large-scale datasets came into place. KITTI dataset is one such famous dataset still being used as a benchmark for autonomous driving applications today. On the other front, GPUs improved exponentially, and the deep learning architectures grew better - ImageNet, Resnet, and other advances. With all these advances together, the focus on 3D reconstruction was on the rise again. Approaches using deep CNN networks, sparial transformer networks, etc were being applied to these problems. One key idea introduced in spatial transformer networks is **differentiable samplers**, which is an important property approaches aim to have now too. ## 3D understanding with Modern Tools Since monocular reconstruction did not yield good results due to disocclusion problems, works focused on RGB$$\alpha$$ planes wherein the depth was captured by **multi-plane images**. To get the image from a novel view point, **alpha compositing** is used to render the image. At this point, since we have perfect depth and color data from a viewpoint, we should be able to generate new views with ease. However, the novel views that can be generated are limited by single view data. Then, the idea of using multiple images of a scene started entering the literature. &gt; What is the difference between depth map and MPI? Both capture the depth of the scene. &gt; Fast forward into the future, NeRFs started using multiplane images for each pixel rather than a single image. ## Neural Radiance Fields Alpha-composting is used to generate the value of each pixel in the rendering image, along with which, continuous volume rendering is done to represent the scene itself - $$ C_0 = \int C_t \sigma_t \exp\left(-\int \sigma_u du\right) dt $$ which is discretized as $$ C_0 = \sum_i C_i \alpha_i \prod_j (1 - \alpha_j) $$ Here, $$\sigma_t$$ and $$\alpha$$ are related to the transmittance/opacity of the pixel. The idea is that we shoot out a ray corresponding to a pixel in the camera image, sample points on the ray (ray-marching) to estimate their opacities and color. Neural networks are used to calculate the opacity for each spatial point and color value depending on the spatial location and viewing direction. The color depends on viewing direction because of BRDF functions that vary based on the viewing direction (image specular surfaces). Using the positions and viewing directions directly does not yield good results. Why? It is difficult to estimate high-frequency parameters required for images from these low-dimensional values. Neural networks tend to estimate low-frequency functions over high-frequency functions. To solve this, these coordinates are mapped to a high-dimensional space using **positional encoding**. This addition yields much superior results. Since the whole framework is completely differentiable, it is easy to find the optima minimizing the error across multiple camera views. NeRF was a very influential paper, enabling applications in new fields such as text-to-3D and it embodies the perfect combination of the right problem, at the right time, with the right people. # Mip-NeRF 360 The original paper has two major assumptions - - Bounded scenes - Front-facing scenes - camera views are limited to certain directions The outputs from NeRF had a lot of aliasing issues when these assumptions are broken. Instead of assuming rays shooting out from the camera, Mip-Nerf shoots out conical frustrums with multivariate Gaussians to represent 3D volumes in the scenes. This solves the problem of front-facing scenes. To solve the problem of unbounded scenes, Mip-NeRF 360 uses "parameterization" to warp the points outside a certain radius using Kalman Filtering. Essentially, farther points are warped into a non-Euclidean space which is a tradeoff - for applications in SLAM and robotics, the geometry may be precisely needed which is sort of lost in such transformations. To speed up training in unbounded scenes, the authors proposed "distillation", wherein sampling is done in a hierarchical manner to identify regions with higher opacities. These higher opacity regions are then used to estimate the color in the finer-sampled regions. To supervise the training of "density finding network", the idea of distribution matching in multi-resolution histograms is used to formulate a loss function. Given limited number of camers, it is difficult to estimate all the parameters correctly - causing artefacts in the reconstructions. To solve this, the authors use a **distortion regularizer** to clump together points with higher densities. This acts as a prior to resolve some ambiguities in the scene. - ## Instant-NGP ### Graphics Primitives A form of representation for objects in the real-world. For example, an image is a graphics primitive that maps 2D pixels to RGB values. So what is the motivation to have neural fields represent 3D volumes over voxels or point clouds? The latter are explicit representations which take up a lot of space! ### NeRFs We have seen that high-frequency encoding is used to effectively represent high-frequency features. This is a form of non-parametric encoding. Parametric encodings on the other hand are more complicated, and can be used to learn feature vectors. ### Multi-resolution hash-encoding The scene is divided into multiple grids - the corners of each cube are hashed and stored in a hash table. For any point outside this lattice, we simply use a linear combination based on distances - this ensures continuity and differentiability. What exactly is hashed? The high-dimensional vector embedding for each 3D point is stored. Furthermore, the authors implement a low-level CUDA kernel to realise fast matrix multiplications. This implementation greatly reduces the training time and reduces the memory used. It takes days to train NeRFs, and with this the model could be trained in a couple of seconds. ### Method Agnostic! This idea is not only for NeRFs but can be used for other primitives like Gigapixel images, Neural SDFs, etc. ### Weaknesses - Rely on neural networks to resolve hash collisions - cause microstructure artifacts - Hand-crafter hash function - May not be robust to real world noise like flossy surfaces or motion blur. # 3D Generative Models Taking motivation from 2D generation, we use noise-sampling based approaches to build 3D scenes. While our 2D models are able to generate very high quality results, 3D models aren't able to match these outputs. The first limitation is due to the unavailability of data (Objaverse-XL 10M vs LAION 5B). The dataset itself has very simple models, so it is difficult to build highly-detailed models. ## Pretrained 2D Diffusion Models How about [Lifting Pretrained 2D Diffusion Models for 3D Generation](https://arxiv.org/abs/2212.00774)? After the advent of NeRFs, this approach became feasible. The problem of creating a 3D model can be distilled to updated view-dependent 2D images! To this end, people tried using diffusion models for the generative capabilities. An optimal denoiser should understand the image structure. What does this mean? The model needs to understand that there is a low-dimensional image-manifold in a high-dimensional space. The noising process takes the image out of this manifold whereas the denoising process tries to bring it back to this space - A projection function. An important question arises here - How does the denoiser know *which direction* to project on? The models typically project to the **weighted mean** (Gaussian likelihood based) direction of the training samples. This is known as **mean shifting** - used widely in clustering methods. How is this relevant to 3D generation? When we start with white noise, the initial direction would be towards the mean of all training samples. [Elucidating the design space of Diffusion-Based Generative Models](https://openreview.net/pdf?id=k7FuTOWMOc7) examines this property in detail. The mean-shift essentially generates more 'likely' samples. Start out with a 3D blob, add Gaussian noise (to take it to the space where the diffusion model has been trained on) and then denoise it. The noise function used is $$ \partial \theta = \sum_i \mathbb E[w(\sigma) (\hat \epsilon_\phi (x_{c_i} + \sigma \epsilon) - \epsilon) \frac{\partial x_c}{\partial \theta}] $$ Alternately, [DreamFusion: Text-to-3D using 2D Diffusion](https://arxiv.org/abs/2209.14988) tries to optimize using a KL divergence loss function but further derivation shows that it is equivalent to the above loss function. This noise function is called as **Score Distillation Sampling (SDS)**. However, these still don't yield good results. Here, researchers realised that unconditional distribution is much harder than conditional generation. Unconditional generation is still an open problem. For conditional generation, text prompts are able to generate results with higher-fidelity. Authors in DreamFusion added some more tricks wherein they decomposed the scene into geometry, shading, and albedo to improve the results. ### Mode-seeking The score distillation sampling function (mean-shifting) has a mode-seeking behavior. This behavior is not necessarily good - it causes artefacts like saturated colors, lower diversity and fuzzy geometry. To alleviate these issues, there has been a new loss function crafted called **variational score distillation**. ### Improvements - *Speed* - Sampling speed can be improved using Gaussian Splatting - *Resolution* - Resolution can be improved using Coarse-to-Fine refinement ## Alternative Approaches ### Multi-view 2D Generative Models Fine-tune a 2D diffusion or any other generative model to generate multiple views of the same object. We pass in a single view of the object, and the generative model generates multiple views. This approach is explored in the paper [Zero-1-to-3: Zero-shot One Image to 3D Object](https://zero123.cs.columbia.edu). Then, NeRF or Gaussian splatting can be used to create the 3D models. The problem becomes "Images-to-3D" which is a much more easier problem. The recovered geometry may not have very good - the 2D models do not understand the geometry of the scene quite well. ### Multi-view 2.5D Generative Models Along with the RGB images, we could also estimate the normals to estimate the geometry better. This method is implemented in [Wonder3D: Single Image to 3D using Cross-Domain Diffusion](https://arxiv.org/abs/2310.15008), and it obtains better results. --- At this point, the Objaverse-XL dataset was released, and people tried to train image to 3D directly - [LRM: Large Reconstruction Model for Single Image to 3D](https://arxiv.org/abs/2311.04400). However, the issue with this approach is that since the dataset is object-centric and we want a more general model! Capturing such a general dataset is quite difficult. An alternative idea could be to use videos as a dataset. Such an idea is explored in . Also, video generation is an achievable task with the current models - [Sora](https://openai.com/index/sora/). People are still figuring out other ways to solve this general problem, and it is a very lucrative field - hundreds of papers in the past year! Let us see some more papers which tried to address other issues in this problem. # ## Prolific Dreamer As mentioned before, Google first released Dream Fusion for this problem. It used Imagen and Classifier-free guidance, but it did have the limitations mentioned before. To improve on this performance, the authors of Prolific Dreamer modified the SDS Loss to something known as **Variation Distillation Score** - our goal is match the generated distribution with the distribution formed by multi-view images of a scene. This is a difficult task as it is a sparse manifold in a complex high-dimensional space. Do we have a metric to quantitatively analyse these models? **T3 Bench** is a benchmark score that checks the text-3D model alignment and the quality of the result itself. ## ReconFusion ZeroNVS is a modification over Zero-1-2-3 that does not require any pretraining on 3D models to generate models. However, this paper along with other approaches during this time required heavy pretrained models, with high computational requirements and scene specific fine-tuning. Along with these, they also had floater artifacts and inconsistencies in multi-view generation. PixelNerf is one of the state-of-the-art 3D reconstruction papers that does not require dense training data because it relies on underlying pixel structure. The idea is to use this scene representation with latent diffusion models to address the limitations of the previous papers. How do we train NeRF and Diffusion models simultaneously? - We first have a reconstruction loss for the NeRF part wherein we sample a novel view and use some image similarity loss like L2 to optimize the network. - Then, we have a sampling loss for the diffusion model that has LPIPS and a component for multi-view consistency Interestingly, the authors choose DDIM for the diffusion model over stochastic sampling (probably helps with multi-view consistency). Also, they use a trajectory based novel view sampling to further maintain consistency across views. The resultant method is able to reconstruct a consistent 3D model even with inconsistent 2D views! # Vision and Language The idea is to use an LLM agent to use language as a means to solve complex visual workflows. Along with human-curated high-level concepts, these can solve planet-scale problems. Robots can work on more general tasks - physically grounding them on images. The main problem in realizing these models is aligning text and image concepts. ## Pre-training Tasks: Generative GPT uses **causal modeling** whereas BERT uses **masked modeling**. In the latter method, the words from the input data is masked out, and the model must predict the missing words. This allows global supervision allowing the model to look at both past and future data. In context of images, we mask out some patches in the image and the model has to predict the remaining patches. This sort of an approach is displayed in **Segment Anything Model (SAM)**. However, some types of tasks may not support such a paradigm - text generation in a conversation. This is where causal modeling is used - the model is only allowed to look at the past data. How do we do supervision in this case? It is similar to what we do in auto-regressive models for generating images and for generating text for images, it is similar to MLE. </insert> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/NumAn/">Numerical Analysis Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ipl/">IPL Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/philosophy/">Philosophy Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/dbms/">DiBS Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/operating-systems/">Operating System Notes</a> </li> </div> <script>document.querySelectorAll("#table-of-contents a").forEach(function(e){e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substring(1);document.querySelectorAll(".content-section").forEach(function(e){e.classList.add("hidden")});var n=document.getElementById(t);n&&n.classList.remove("hidden")})});</script> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Sudhansh Peddabomma. Last updated: June 06, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0K9MLG0V24");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>