> Calculate model sizes from parameters table.

Notice that the precision of floating point numbers is much higher when the values themselves are small. This is a tradeoff we make based on the applications. Here is a summary of other floating point representations - 

![](/assets/img/2025-01-06-data-systems-for-ml/17387241256605.jpg)

The BF16 system has been observed to provide much better performance over FP16 for neural network training. The representation lowers the precision for a higher range. It could be that the higher range can stabilize the training during exploding or vanishing gradients. During inference, the precision matters more so FP16 might perform better in some cases.

For practice, consider the following examples
- The FP16 bit set `1 10001 1100000000` represents -7.1. Why? The bias in the exponent is always the median value subtracted by 1. Here it is $$2^4 - 1 = 15$$. The exponent is then $$17 - 15 = 2$$, and the fraction is $$0.5 + 0.25 = 0.75$$
- The decimal 2.5 is represented as `0 10000000 0100000`. The bias is $$2^7 - 1 = 127$$


After these representations, newer ones came up to improve the performance in deep-learning. 
![](/assets/img/2025-01-06-data-systems-for-ml/17387248528126.jpg)
The rule of thumb is that we require higher range for training and higher precision for inference. 

As if this were not enough, for even lower compute, NVIDIA has been pushing for INT4 and FP4 to keep up with the Mooreâ€™s law. 

![](/assets/img/2025-01-06-data-systems-for-ml/17387250679255.jpg)

There has been no successful model with these representations, making it a very promising avenue for research.

Alright, with these things in mind, let us come back to quantization. There have been two approaches for quantization
1. **K-Means based quantization** - widely used, almost all mobile devices have it. 
    
    Consider we have a weight matrix that we are trying to quantize. The idea is to cluster close values together into an integer or a lower-precision representation. The number of clusters is chosen based on the chosen requirements - for 2-bit quantization, number of clusters is 4.  
    To do so, we simply apply K-means. The centroids are stored in a code book whereas the matrix is simply stored in the lowest possible representation to further reduce storage. 
    ![](/assets/img/2025-01-06-data-systems-for-ml/17387254097673.jpg)
    How do we perform a backward pass on these matrices? The gradients are accumulated based on the classes and are applied to the centroids to fine-tune them. 
    In practice, quantization starts affecting the performance sharply only after a certain threshold. Therefore, quantization becomes a hyper-parameter tuning problem, and we can achieve significantly lower memory consumption.
    The number of bits used can vary with layers as well! 
    > Try coding a library with variable quantization layers. Shared code books across layers
    
    How is the run-time affected? The computations are still FP arithmetic. There is an added computation cost for weight compression/decompression and code book lookup. K-means has been quite effective with convolution networks.
    
2. **Linear quantization** - The idea is to determine a linear mapping of integers to real numbers. It can be seen as a linear optimization problem.
    
    ![](/assets/img/2025-01-06-data-systems-for-ml/17387262709675.jpg)

    So, we need to determine zero-point and scale parameters. These parameters are often also publicly shared on platforms such as HuggingFace. The parameters can be chosen for the whole model or for each layer based on our performance tradeoff appetite. For many popular models like Llama, the quantization is done tensor-wise.
    
   The parameters can easily be determined as follows
   
   $$
    \begin{align*}
        S &= \frac{r_{max} - r_{min}}{q_{max} - q_{min}} \\
        Z &= \text{round}(q_{min} - \frac{r_{min}}{S})
    \end{align*}
   $$
   
   The bit-width $$n$$ determines $$q_{min} = -2^{n -1}$$ and $$q_{max} = 2^{n - 1} - 1$$. 
   
   
   Suppose we apply linear quantization to `matmul` - 
   
   $$
    \begin{align*}
        Y &= WX \\
        S_Y(q_Y - Z_Y) &= S_W(q_W - Z_W)S_X(q_X - Z_X) \\
        Q_Y = \frac{S_W S_X}{S_Y} \left( q_W q_X - Z_W q_X - \underset{Z_X q_W + Z_W Z_X) + Z_Y}{\text{precomputed for inference}} \right)
    \end{align*}
   $$
   
   Empirically, the factor $$\frac{S_W S_X}{S_Y} $$ is between 0 and 1. Instead of using floating point multiplications, it is represented as fixed point multiplication and bit shift. Also, empirically $$Z_W$$ follows normal distribution and can be approximated as 0. Thus, the heavy lifting operation is $$q_Wq_X$$ which is simply integer multiplication. 
   
   Therefore, we reduced both storage and computation time (integer arithmetic is much faster and cheaper). This can also be used to reduce FP16 to FP8 rather than integers. 
   
In summary, we have
![](/assets/img/2025-01-06-data-systems-for-ml/17387273312564.jpg)
