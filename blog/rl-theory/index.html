<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Reinforcement Learning Theory | Sudhansh Peddabomma</title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="A deep dive of the basic RL theory and how we used them in modern ML systems."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/rl-theory/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> <script src="/assets/js/chat.js?e73db4280bae3cbae4d78219277155b9"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Sudhansh Peddabomma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Articles</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reinforcement Learning Theory</h1> <p class="post-meta">January 6, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/notes"> <i class="fa-solid fa-tag fa-sm"></i> Notes</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>Through this article, we aim to understand the theory of reinforcement learning in the context of Large Language Models.</p> <p>Reinforcement learning addresses the domain of sequential decision problems, wherein an <em>agent</em> takes <em>actions</em> in an <em>environment</em>. These configurations are generally represented using <em>Markov Decision Processes</em> (MDP). An MDP is characterized by -</p> <ul> <li>Time - T - Discrete, infinite</li> <li>States - S - Discrete</li> <li>Actions - A - Discrete</li> <li> <p>Transitions - \(\tau : S \times A \to \Delta(S)\) - a probability distributions of states</p> \[P(s’ \vert s, a) = P_R[ \tau (s, a) = s’]\] <p>In more complicated setups, the transition function could be a function of the time \(T\) (stationarity and non-stationarity). In finite-time horizons, the time can be embedded inside the state itself, converting non-stationarity scenarios to stationarity ones.</p> </li> <li> <p>Reward - \(R: S \times A \times S \to \mathbb R: [-M, M]\). There is no need to make this non-deterministic since that is already generalized by \(\tau\). The rewards are usually bounded. The expected reward for an action is given by</p> \[R’(s, a) = \sum_{s’ \in S} R(s’, a, s) p(s’ \vert s, a)\] </li> <li>Initial state \(S_0\) - Can be a single state or a distribution over states.</li> <li>Discount Factor \(\gamma\) - A factor \(&lt;1\) to bound the total expected reward from the future. This will be better understandable from the text later.</li> </ul> <p>A <strong>policy</strong> \(\pi\) is a probability distributions over actions based on the current state. Consider a distribution \(\beta: S \to \delta(A)\), then a policy formally is</p> \[\pi(s, a) = P_R[\beta(s) = a]\] <p>A <strong>trajectory</strong> describes the sequence of states, actions of an agent in a <em>run</em> - \(s_0, a_0, r_0, s_1, a_1, r_1, \dots\).</p> <p>A policy is associated with a <strong>value function</strong> \(V_{\pi, t} (s) = \sum_{i = t}^\infty \gamma^{i - t} r_i\) Note that the dependence of the value of function on time is redundant in stationarity situations.</p> <p>The goal is to maximize this value function over all possible policies in an environment</p> \[\pi^* = \arg \max_{\pi} V_\pi (S_0)\] <p>This is known as the <em>Markov Decision Problem</em>. In a strict setting, the supremum may not exist (be a valid policy).</p> <p>How is Reinforcement Learning different from Supervised learning? The key differences are</p> <ul> <li>There is no sequentiality</li> <li>Every decision in SL has an associated reward, and there is no stochasticity</li> </ul> <p>Policies are optimized in multiple methods</p> <ul> <li>There are hill-climb methods - policy iteration, value iteration</li> <li>Learning based methods - think of a policy as a matrix that has to optimized to satisfy certain constraints.</li> </ul> <h2 id="bellman-equations">Bellman Equations</h2> <p>These are a recursive formulations of policies and value functions.</p> \[V_\pi(s) =3.142 s \pi(s, a)[ p(s’ \vert s, a) [R(s’, a, s) + \gamma V_\pi (s’)]\] <h2 id="the-markov-assumption">The Markov Assumption</h2> <p>Many processes in nature are <em>Markov</em> in nature - the action at the current state only depends on the current state, and not the history of states.</p> <p>A general policy is a function from trajectories \(H_t\) to a distribution over actions. However, such general representation is difficult to compute. Additionally, due to the Markov assumption, a stationary policy \(\pi: S \to \delta(A)\) is general enough for optimality. On surface, this may seem like we are limiting the power of an agent, but we will see that this is not the case. In fact, even a deterministic policy will do.</p> <p>How about reward? Are we limiting the reward function with a Markovian assumption? Since we consider expected rewards - the expectation value can be embedded within the state-based rewards as well. So in fact, both representations are the same.</p> <p>With a general policy, the probability of a trajectory is given by</p> \[P[S_0 = s_0, A_0 = a_0, \dots, S_t = s_t] = \mu(s_0) \pi(a_0 \vert s_0) P(s’ \vert s_0, a_0) \pi(A_1 \vert S_0, a_0, s_1) \dots\] <p>Continuing with the Bellman equations, our goal was to find a policy that maximizes the value function</p> \[\begin{align*} V_\pi(s) &amp;= \mathbb E_{P, \pi} \sum_{t = 0}^\infty \gamma^t [R(S_t, A_t) \vert S_0 = s] \max_{\pi} V_\pi(s) \\ &amp;= V^*(s) \\ \end{align*}\] <p>Firstly, does there exist a policy such that \(V_\pi(s) = V^*(s)\) for all \(s\)? An optimal policy for a particular state is guaranteed because of compactness, but is there such a policy for all states?</p> <p>To aid our analysis, we also define a <strong>Q-function</strong> \(Q: S \times A \to R\), that describes the expected reward \(Q(s, a)\) of taking an action \(a\) at state \(s\). This function is very similar to the value function but has an additional condition on the action as well. Why have both these functions? Convenience. Analogously, we also define \(Q^*(s, a) = \max_\pi Q_\pi (a, a)\).</p> <p><strong>Theorem (Existence theorem).</strong> There exists a stationary and deterministic policy that has the optimal value for all states.</p> <p>Therefore, Bellman equations are possible due to assumptions of Markovian nature of the policy and state transition functions. We have the following equations</p> \[\begin{align*} V_\pi(s) &amp;= R(s, \pi(s)) + \gamma \sum_{s’} P(s’ \vert a, \pi(s)) V_\pi (s’) \\ Q_\pi(s, a) &amp;= R(s, a) + \gamma \sum_{s’} P(s’ \vert s, a) \sum_{a’} \pi(a’ \vert s’) Q_\pi(s’, a’) \\ V_\pi(s) &amp;= \sum_a \pi(s) Q_\pi(s, a) \\ Q_\pi(s, a) &amp;= R(s, a) + \gamma \sum_{s’} P(s’ \vert s, a) V_\pi(s’) \end{align*}\] <h2 id="summary">Summary</h2> <p>The equation we’ve been working with for value iteration is</p> \[Q_{t + 1}(s, a) = L(Q_t) = R(s, a) + \mathbb E_{s’ \sim P(. \ver s, a)} \max_{a’} Q_{t}(s’, a’)\] <p>The key step here is to extract a policy from the current \(Q\) function. We noted that the optimal function \(Q^*\) satisfies the optimality equation.</p> <p>Aside from the optimality equation, we have the expectation equation that every \(Q\) function satisfies</p> \[Q_\pi(s, a) = R(s, a) + \mathbb E_{s’} \mathbb E_{a’ \sim \pi(\cdot \vert s’)} Q(s’, a’)\] <p>The advantage of looking at the optimality equation as an operation \(Q_{t + 1} = L(Q_{t})\) is that we can apply the contraction concepts to arrive at \(Q^*\) with Banach’s fixed point theorem. This way, we prove that there is a unique optimal \(Q\) function.</p> <p>Now we show that, after enough number of iterations, we can also get the value function \(V\) arbitrarily close the optimal value. All these subtleties together show that the value iteration algorithm works!</p> <p>How do obtain these bounds based on iterations? We need to find an upper bound for \(\| Q_{t} - Q^* \|_\infty\). We can show that this value is \(leq \|Q^* - Q_0 \|_\infty\). Assuming we start with a \(Q\) with all zeroes, the maximum value of \(Q^*\) is simply \(R_{\max}/(1 - \gamma) = (\max_{s, a} \vert R(s, a)\vert)/(1 - \gamma)\).</p> <p><em>Lemma.</em> \(\|V_m(s) - V^*(s)\| \leq \frac{2}{1 - \gamma}\| Q_m - Q^*\|_\infty\).</p> <p><em>Proof.</em> $$ \begin{align<em>} V^</em>(s) - V_m(s) &amp;= Q^<em>(s, \pi^</em>(s)) - Q_m(s, a = \pi_m(s)) <br> &amp;= Q^<em>(s, \pi^</em>(s)) - Q^<em>(s, a) + Q^</em>(s, a) - Q_m(s, a) <br> &amp;= Q^<em>(s, \pi^</em>(s)) - Q^<em>(s, a) + \gamma \mathbb E_{s’} (V^</em>(s’) - V_m(s’)) &amp;\leq Q^<em>(s, \pi^</em>(s)) - Q^<em>(s, a) + \gamma |V^</em> - V_m|<em>\infty <br> &amp;\leq (Q^*(s, \pi^*(s)) - Q_m(s, \pi^*(s))) + (Q_m(s, \pi^*(s)) - Q^*(s, a)) + \gamma |V^* - V_m |</em>\infty <br> &amp;\leq (Q^<em>(s, \pi^</em>(s)) - Q_m(s, \pi^<em>(s))) + (Q_m(s, a) - Q^</em>(s, a)) + \gamma |V^* - V_m |<em>\infty <br> &amp;\leq 2|Q^* - Q_m|</em>\infty + \gamma |V^* - V_m|<em>\infty <br> |V^* - V_m |</em>\infty \leq \frac{2}{1 - \gamma} |Q^* - Q_m |_\infty</p> <p>\end{align*} $$</p> <h1 id="policy-iteration">Policy Iteration</h1> <p>Instead of modifying the policy based on the current value, why not do it the other way round? Iterate over the policy, get its value and improve it again? There is a subtle different as compared to the previous algorithm, and it turns out that this method is much more efficient!</p> \[\pi_0 \underset{\longrightarrow}_{Q_0} \to \pi_1 \to \cdots \to \pi_k \underset{\longrightarrow}_{Q_k} \pi_{k + 1}\] <p>Policy iteration takes \(\mathcal O(\vert S\vert^3 + \vert S \vert^2 \vert A\vert)\) whereas value iteration is \(\mathcal O(\vert S \vert^2 \vert A\vert)\).</p> <h1 id="model-free-methods">Model-free methods</h1> <p>Policy iteration and Value iteration are closely related to each other. For both the algorithms, we need to evaluate a policy to find the corresponding value function. However, in many cases, we do not know the exact transition and reward functions. In other cases, the environment can have a large number of states, making it impossible to model it.</p> <p>For such situations, we rely on <strong>Monte-Carlo methods</strong>. Any method that solves a problem by generating suitable random numbers and observing that a fraction of numbers obey some property or properties, can be classified as a Monte Carlo method. The key ideas here are using a <em>sampling technique</em> for a heuristic <em>estimator</em>. These methods do not make use of the Markov assumption much, making them much more generalizable.</p> <p>The idea is to learn directly from episodes of experience without a prior knowledge of MDP transitions (rewards). Since the idea relies on episodes, one caveat is that it can only be applied to <em>episodic MDPs</em> - episodes have to terminate.</p> <h2 id="prediction-problem">Prediction Problem</h2> <p>Let us consider the first problem - estimating \(V_\pi(s)\) for a state. Instead of updating after every action, we update after each episode by taking the mean reward across all sample trajectories sampled from this stage. The <em>first-visit</em> algorithm is given by</p> \[\tilde V_\pi(s) = \frac{1}{m} \sum G_i^s\] <p>Where \(G_i^s\) is the total reward after \(s\) first appears in the episode. Even though the number of states is large, it’s nonetheless finite. Using this fact, we can show theoretically that the value above can be bounded. The convergence time is also associated with the underlying transition probabilities (rare states require more episodes to appear in the trajectory).</p> <p>There are more questions to answer. For example, is this a biased or unbiased estimator?</p> \[\begin{align*} \mathbb E(\tilde V_\pi(s)) &amp;= \frac{1}{m} \sum_i \mathbb E(G_i^s) \\ &amp;= V_\pi(s) \quad \because \text{ Markovian assumption} \end{align*}\] <p>How about doing a second-visit algorithm or every-visit algorithm? They are valid approaches too, the theoretical analysis slightly varies. The estimators may not be unbiased but they use the data more efficiently. That is useful in cases where sampling is expensive. The <em>every-visit algorithm</em> typically has a higher bias (due to dependencies in an episode for the occurrences) but lower variance and higher efficiency.</p> <p>These differences are important to understand. For example, in the latest <a href="https://openai.com/index/openai-o3-mini/" rel="external nofollow noopener" target="_blank">o3-mini</a> model, they observed that the every-visit variant of an RL algorithm obtained much better performance than the first-visit variant.</p> <p>In contrast, consider estimating the \(Q_\pi(s, a)\) function. A policy might choose a certain action for a given state. However, to get all the Q-values, the policy must account for exploring all actions at different states to get a good estimate. The exploration probability is captured by the <strong>epsilon-greedy class of algorithms</strong>.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/data-systems-for-ml/">Data Systems for Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/NumAn/">Numerical Analysis Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ipl/">IPL Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ai-agents/">AI Agents</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/dbms/">DiBS Notes</a> </li> </div> <script>document.querySelectorAll("#table-of-contents a").forEach(function(e){e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substring(1);document.querySelectorAll(".content-section").forEach(function(e){e.classList.add("hidden")});var n=document.getElementById(t);n&&n.classList.remove("hidden")})});</script> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Sudhansh Peddabomma. Last updated: February 05, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0K9MLG0V24");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <div class="chat-toggle-container"> <button id="chat-toggle-btn" class="chat-toggle-btn"> <i class="fas fa-comments"></i> </button> </div> <div id="chat-window" class="chat-window"> <div class="chat-header"> <h5 class="mb-0">Talk to my AI</h5> <button id="close-chat" class="btn-close"> <i class="fas fa-times"></i> </button> </div> <div id="chat-messages" class="chat-messages"></div> <div class="chat-input-container"> <input type="text" id="chat-input" class="form-control" placeholder="Type a message..."> <button id="send-btn" class="btn btn-primary"> <i class="fas fa-paper-plane"></i> </button> </div> </div> </body> </html>