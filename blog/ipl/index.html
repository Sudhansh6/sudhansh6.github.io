<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> IPL Notes | Sudhansh Peddabomma </title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="A course covering the details of the inner workings of a compiler. We start off with scanning ,parsing and semantic analysis to generate ASTs. Then, we discuss IR generator to create TAC. Lastly, we discuss a few register allocation algorithms."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?d0883cd261a1387cbdd04fa5cb9cc690" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/ipl/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/chat.js?e73db4280bae3cbae4d78219277155b9"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Sudhansh Peddabomma </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Articles </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">IPL Notes</h1> <p class="post-meta"> Created in January 05, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/category/notes"> <i class="fa-solid fa-tag fa-sm"></i> Notes</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="lecture-1">Lecture 1</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">05-01-22</code></p> </blockquote> <h2 id="binding">Binding</h2> <p>Binding refers to finding certain attributes of certain objects. For example, when we consider the example of developing a program, we have the following steps.</p> <p><img src="/assets/img/IPL\image-20220105111530405.png" alt="image-20220105111530405"></p> <p>At each step, the number of unbound objects decrease along with time.</p> <h2 id="implementation-mechanisms">Implementation Mechanisms</h2> <p>The important point to note is that the “execution” is done after “translation” in the case of <strong>compilers</strong> (Analysis + Synthesis). However, <strong>interpreters</strong> (Analysis + Execution) themselves are programs which take the source code as input and translate/execute the user-program.</p> <p>Compilers/Translators lower the level of the program specification, whereas the interpreters do the opposite. That is why interpreters are able to directly execute the source code.</p> <p><img src="/assets/img/IPL\image-20220105112702885.png" alt="image-20220105112702885"></p> <blockquote> <p>Why is the arrow in the reverse direction? Think of python. The python program itself is a low-level code which is able to execute a high-level user code.</p> </blockquote> <table> <thead> <tr> <th>High Level</th> <th>Low Level</th> </tr> </thead> <tbody> <tr> <td>C statements</td> <td>Assembly statement</td> </tr> </tbody> </table> <p><strong>Note.</strong> A practical interpreter does partial translation/compilation but not completely as it is redundant.</p> <blockquote> <p>Compilers performs bindings <strong>before</strong> runtime, and an interpreter performs bindings <strong>at</strong> runtime.</p> </blockquote> <blockquote> <p>Why interpreters if compilers are fast? Compilers are used for programs which need to be executed multiple times with speed. However, we must also consider the time needed to compiler the codes in case of compilers. Considering this, the turnaround time in both cases is almost the same. The user needs to think whether they want to reduce the execution time or reduce the turnaround time.</p> </blockquote> <table> <thead> <tr> <th>Language</th> <th>Mode of execution</th> </tr> </thead> <tbody> <tr> <td>C++</td> <td>Compiler</td> </tr> <tr> <td>Java/ C#</td> <td>Compiler + Interpreter</td> </tr> <tr> <td>Python</td> <td>Interpreter</td> </tr> </tbody> </table> <p>Java programs run on a <em>virtual machine</em> - JIT (just in time) compilation. the virtual machine invokes a native compiler for instances of code which are repeated often inside the code. This speeds up the execution in case of Java.</p> <p>Compilers like GCC can be used for multiple machines and languages. <img src="/assets/img/IPL/image-20220105120200389.png" alt="image-20220105120200389"></p> <h2 id="simple-compiler">Simple compiler</h2> <p><img src="/assets/img/IPL\image-20220105120528314.png" alt="image-20220105120528314"></p> <p>The first step in the translation sequence of the compiler is <strong>scanning and parsing</strong>. A parse-tree is built from the <u>grammar rules, terminals, and non-terminals</u>. Scanning involves finding special characters to create the parse tree, whereas parsing is essentially creating the parse tree itself.</p> <p>The next step involves <strong>semantic analysis</strong>. The semantic analyzer performs <strong>type checking</strong> to check the validity of the expressions. Therefore, we create an <u>abstract syntax tree</u> (<strong>AST</strong>) is created from the parse tree.</p> <blockquote> <p>Only for type-checking? Yes, that is the primary purpose, but it has other uses too. Sometimes, type checking is done on the fly while generating the parse tree itself. The advantage for generating the AST is that we can write a generic code (interpreter) to parse the abstract trees (since it involves only tokens).</p> </blockquote> <p>Then, we generate a <strong>TAC list</strong> (three address code) in the <strong>IR generation</strong> step. This step helps in separating data and control flow, and hence simplifies optimization. The control flow is linearized by flattening nested control constructs.</p> <p>In the <strong>Instruction Selection</strong> step, we generate the <strong>RTL list</strong> which is like pseudo-assembly code with all the assembly instruction names. We try to generate as few instructions as possible using temporaries and local registers.</p> <blockquote> <p>What is the purpose of this step? We like to divide the problem into many steps to leave room for optimization.</p> </blockquote> <p>In the final step of <strong>Emitting Instructions</strong>, we generate the assembly code by converting the names of assembly instructions to the actual code.</p> <p>The whole process is summarized by the following image.</p> <p><img src="/assets/img/IPL\image-20220105123323838.png" alt="image-20220105123323838"></p> <h2 id="observations">Observations</h2> <p>A compiler bridges the gap between source program and target program. Compilation involves gradual lowering of levels of the IR of an input program. The design of the IRs is the most critical part of a compiler design. Practical compilers are desired to be retargetable.</p> <h1 id="lecture-2">Lecture 2</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">07-01-22</code></p> </blockquote> <p>Write about retargetable compilers. A <strong>retargetable compiler</strong> does not require manual changes, the backend is generated according to the specifications.</p> <h2 id="why-are-compilers-relevant-today">Why are compilers relevant today?</h2> <p>In the present age, very few people write compilers. However, translation and interpretation are fundamental to CS at a conceptual level. It helps us understand stepwise refinement - building layers of abstractions and bridging the gaps between successive layers without performance penalties. Also, knowing compilers internals makes a person a much better programmer.</p> <h2 id="modularity-of-compilation">Modularity of Compilation</h2> <p>A compiler works in phases to map source features to target features. We divide each feature into multiple levels for convenience. This will be clear through the assignments (language increments).</p> <h2 id="course-plan">Course Plan</h2> <p>We shall cover scanning, parsing, static semantics, runtime support, and code generations in the theory course. In the lab course, we shall do incremental construction of SCLP.</p> <p><img src="/assets/img/IPL/image-20220107114750855.png" alt="image-20220107114750855"></p> <h1 id="lecture-3">Lecture 3</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">12-01-22</code></p> </blockquote> <h2 id="compilation-models">Compilation Models</h2> <p><img src="/assets/img/IPL/image-20220112111235980.png" alt="image-20220112111235980"></p> <p>Registers are introduced earlier in the Davidson Fraser model. Therefore, it becomes machine dependent. The target independent IR in <code class="language-plaintext highlighter-rouge">gcc</code> is called ‘gimple’.</p> <h3 id="typical-front-ends">Typical Front ends</h3> <p>How does a front end work? Usually, the <em>parser</em> calls the <em>scanner</em> that reads the source program to extract <strong>tokens</strong>. These tokens are then used by the parser to create the parse tree. We can say that the scanner is a subordinate routine of the parser. On the other side, the parser sends the parse tree to the <em>semantic analyzer</em> which generates the AST. Finally, we get the AST/ Linear IR along with a symbol table.</p> <h3 id="typical-back-ends-in-aho-ullman-model">Typical Back ends in Aho Ullman Model</h3> <p>The backend receives the machine independent IR from the front end. It then proceeds to optimize the instructions via compile time evaluations and elimination of redundant computations. There are techniques such as constant propagation and dead-code elimination that are used.</p> <p>After optimization, the code generator converts the machine independent IR to machine dependent IR. The essential operations done by the code generator are instruction selection, local register allocation, and the choice of order of evaluation.</p> <p>Finally, we have the machine dependent optimizer which outputs assembly code. It involves register allocators, instruction schedulers and peephole optimizer.</p> <h3 id="gnu-tool-chain-for-c">GNU Tool Chain for C</h3> <p>We know that the <code class="language-plaintext highlighter-rouge">gcc</code> compiler take source program as input and produces a target program. Internally, this is done via a compiler called ‘cc1’ (C compiler 1) which generates the assembly program. This assembly code is taken by the assembly component of the compiler to generate an object file. Finally, the loader takes this object file along with <code class="language-plaintext highlighter-rouge">glibc/newlib</code> to generate the target program. The assembly program and loader together are known as <strong><code class="language-plaintext highlighter-rouge">binutils</code></strong>.</p> <h3 id="llvm-tool-chain-for-c">LLVM Tool Chain for C</h3> <p>The LLVM tool chain looks similar to the GNU tool chain. We have <code class="language-plaintext highlighter-rouge">clang -cc1</code>, <code class="language-plaintext highlighter-rouge">clang -E</code>, <code class="language-plaintext highlighter-rouge">llvm-as</code> (assembly program) and <code class="language-plaintext highlighter-rouge">lld</code>(loader). The re-targetability mechanism is done via table generation code.</p> <h2 id="modern-challenges">Modern Challenges</h2> <p>Currently, most of the implementation of compilers has been more or less done. What is left? Some applications and architectures demand special programming languages and compilers that are <em>non-standard</em>. Also, there are some design issues with compilers. It is the IR that breaks or makes a compiler. Including these there are many places of optimization such as</p> <ul> <li>Scaling analysis to large programs without losing precision</li> <li>Increasing the precision of analysis</li> <li>Combining static and dynamic analysis.</li> </ul> <p><em>Full Employment Guarantee Theorem for Compiler Writers</em>.</p> <ul> <li>Software view is stable, hardware view is disruptive.</li> <li>Correctness of optimizations</li> <li>Interference with security</li> <li>Compiler verification and Translation Validation</li> <li>Machine Learning?</li> <li> \[\dots\] </li> </ul> <h2 id="-lexical-analysis">~ Lexical Analysis</h2> <h2 id="scanners">Scanners</h2> <p>To discover the structure of the program, we first get a sequence of <strong>lexemes</strong> or <strong>tokens</strong> using the smallest meaningful units. Once this is done, we remove spaces and other whitespace characters in the <strong>lexical analysis</strong> or <strong>scanning</strong> step.</p> <p>In the second step, we group the lexemes to form larger structures (parse tree). This is called as <strong>syntax analysis</strong> or <strong>parsing</strong>. There are tools like <code class="language-plaintext highlighter-rouge">Antlr</code> that combine scanning with parsing.</p> <h3 id="lexemes-tokens-and-patterns">Lexemes, Tokens, and Patterns</h3> <p><strong><em>Definition.</em></strong> <em>Lexical Analysis</em> is the operation of dividing the input program into a sequence of lexemes (tokens). <strong>Lexemes</strong> are the smallest logical units (words) of a program. Whereas, <strong>tokens</strong> are sets of similar lexemes, i.e. lexemes which have a common syntactic description.</p> <p>How are tokens and lexemes decided? What is the basis for grouping lexemes into tokens? Generally, lexemes which play similar roles during syntax analysis are grouped into a common token. Each keyword plays a different role and are therefore a token by themselves. Each punctuation symbol and each delimiter is a token by itself All comments are uniformly ignored and hence are grouped under the same token. All identifiers (names) are grouped in a common token.</p> <p>Lexemes such as comments and white spaces are not passed to the later stages of a compiler. These have to be detected and ignored. Apart from the token itself, the lexical analyzer also passes other information regarding the token. These items of information are called <strong>token attributes</strong>.</p> <p>In conclusion, the lexical analyzer</p> <ul> <li>Detects the next lexeme</li> <li>Categorizes it into the right oken</li> <li>Passes to the syntax analyzer. The token name for further syntax analysis and the lexeme itself.</li> </ul> <h1 id="lecture-4">Lecture 4</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">19-01-22</code></p> </blockquote> <p>Creating a lexical analyzer</p> <ul> <li>Hand code - possible more efficient but seldom used nowadays.</li> <li>Use a generator - Generates from formal description. Less prone to errors.</li> </ul> <p>A formal description of the tokens of the source language will consist of</p> <ul> <li>A regular expression describing each token, and</li> <li>A code fragment called an <strong>action routine</strong> describing the action to be performed, on identifying each token.</li> </ul> <p>Some regex notation -</p> <ul> <li>Alternation is represented using <code class="language-plaintext highlighter-rouge">|</code> </li> <li>Grouping is done via parentheses</li> <li> <code class="language-plaintext highlighter-rouge">+</code> is positive closure and <code class="language-plaintext highlighter-rouge">*</code> is Kleene closure</li> </ul> <p>The generator, such as Lex, puts together</p> <ul> <li>A <strong>DFA</strong> constructed from the token specification</li> <li>A code fragment called a <strong>driver routine</strong> which can traverse any DFA</li> <li> <strong>Action routines</strong> associated with each regular expression.</li> </ul> <p>Usually, the lexeme with the longest matching pattern is taken into consideration.</p> <h2 id="regular-expressions">Regular Expressions</h2> <p>A regular expression is a set of strings (<em>a language</em>) that belongs to set formed the following rules</p> <ul> <li>\(\epsilon\) and single letters are regular expressions.</li> <li>If \(r, s\) are regular expressions, then \(r\vert s\) is a regular expression. That is, \(L(r\vert s) = L(r) \cup L(s)\)</li> <li>If \(r,s\) are regular expressions, then \(rs\) is a regular expression. That is, \(L(rs) =\) concatenation of strings in \(L(r)\) and \(L(s)\).</li> <li>If \(r\) is a regular expression then \(r^*\) is a regular expression. That is, \(L(r^*)\) is the concatenation of zero or more strings from \(L(r)\). Similarly, for \(r^+\)</li> <li>If \(r\) is a regular expression, then so is \((r)\).</li> </ul> <p>The syntax of regular expressions according to Lex is given as</p> <p><img src="/assets/img/IPL/image-20220129164112391.png" alt="image-20220129164112391"></p> <p>The return statements in the action routines are useful when a lexical analyzer is used in conjunction with a parser. Some conventions followed by lexical analyzers are</p> <ul> <li>Starting from an input position, detect the longest lexeme that could match a pattern.</li> <li>If a lexeme matches more than one pattern, declare the lexeme to have matched the earliest pattern.</li> </ul> <h3 id="lexical-errors">Lexical errors</h3> <p>There are primarily three kinds of errors.</p> <ul> <li>Lexemes whose length exceeds the bound specified by the language. Most languages have a bound on the precision of numeric constants. This problem is due to the finite size of parse tables.</li> <li>Illegal characters in the program.</li> <li>Unterminated strings or comments.</li> </ul> <p>We could issue an appropriate error message to handle errors.</p> <h1 id="lecture-5">Lecture 5</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">21-01-22</code></p> </blockquote> <h2 id="tokenizing-the-input-using-dfas">Tokenizing the input using DFAs</h2> <p>The format to show a trace of scanning is given by</p> <p>| Step No | State | Matched String | Buffer | <code class="language-plaintext highlighter-rouge">NextChar</code> | <code class="language-plaintext highlighter-rouge">LastFinalState</code> | <code class="language-plaintext highlighter-rouge">MarkedPos</code> | Action | | ——- | —– | ————– | —— | ———- | —————- | ———– | —— |</p> <p>Here, <code class="language-plaintext highlighter-rouge">MatchedString</code> is the prefix of the buffer matched to identify a lexeme. <code class="language-plaintext highlighter-rouge">NextChar</code> is the next character in the input; it will be shifted to the buffer if there is a valid transition in the DFA. <code class="language-plaintext highlighter-rouge">MarkedPos</code> is the position of the character (in the buffer) just after the last seen lexeme. The important point to note is that when there is no transition on <code class="language-plaintext highlighter-rouge">Nextchar</code>,</p> <ul> <li>if <code class="language-plaintext highlighter-rouge">MarkedPos</code> is -1, no final state was seen, the first character in the buffer is discarded, and the second character becomes <code class="language-plaintext highlighter-rouge">NextChar</code>.</li> <li>otherwise, the lexeme up to <code class="language-plaintext highlighter-rouge">MarkedPos</code> (excluding it) is returned, the character at <code class="language-plaintext highlighter-rouge">MarkedPos</code> becomes <code class="language-plaintext highlighter-rouge">NextChar</code>.</li> </ul> <p>In either case, the <code class="language-plaintext highlighter-rouge">LastFinalState</code> is set to -1 and the state is set to 0. See the following example for clarity.</p> <p><img src="/assets/img/IPL/image-20220129172907863.png" alt="image-20220129172907863"></p> <p>This sort of parsing has quadratic behavior in the worst case. Consider the following two questions.</p> <ul> <li>Is \(S\) always equal to <code class="language-plaintext highlighter-rouge">LastFinalState</code>?</li> <li>Is <code class="language-plaintext highlighter-rouge">MP</code> always equal to the length of the last lexeme?</li> </ul> <p>The answer to both questions is “No”. Think.</p> <h2 id="-introduction-to-parsing-using-lexflex-and-yaccbison">~ Introduction to Parsing using Lex/Flex and Yacc/Bison</h2> <p>Lex is a program that takes regular expressions as input and generates a scanner code. A Lex script can be decoded as follows. It has three parts separated by “%%”.</p> <ul> <li>The first part consists of declarations. These are copied to <code class="language-plaintext highlighter-rouge">lex.yy.c</code> and are contained in the pair “%{” and “%}”. Declarations outside of this pair are directives to lex or macros to be used in regular expressions.</li> <li>The second part consists of rules and actions. These are basically a sequence of “Regex SPACE Action” lines.</li> <li>Finally, the third part consists of auxiliary code.</li> </ul> <p>Yacc is a program that takes tokens from the scanner and parses them. A yacc script is very similar to that of lex. It has three parts which are separated by “%%”.</p> <ul> <li>The first part has declarations that are to be copied to <code class="language-plaintext highlighter-rouge">y.tab.c</code> contained in the pair “%{” and “%}”. Declarations outside of this pair are directives to yacc or specifications of tokens, types of values of grammar symbol, precedence, associativity etc.</li> <li>The second part has rules and actions. That is, a sequence of grammar productions and actions.</li> <li>The last part has the auxiliary code.</li> </ul> <p>The yacc and lex scripts are combined together using gcc. The typical workflow is</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>yacc <span class="nt">-dv</span> &lt;file-name&gt;.y
gcc <span class="nt">-c</span> y.tab.c
lex &lt;file-name&gt;.l
gcc <span class="nt">-c</span> lex.yy.c
gcc <span class="nt">-o</span> &lt;obj-file-name&gt; lex.yy.o y.tab.o <span class="nt">-ly</span> <span class="nt">-ll</span>
</code></pre></div></div> <h1 id="lecture-6">Lecture 6</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">29-01-22</code></p> </blockquote> <p>A parser identifies the relationship between tokens.</p> <h2 id="constructing-dfas-for-multiple-patterns">Constructing DFAs for Multiple Patterns</h2> <ul> <li>Firstly, join multiple DFAs/NFAs using \(\epsilon\) transitions.</li> <li>This creates an NFA. This NFA can be converted to a DFA by subset construction. Each state in the resulting DFA is a set of “similar” states of the NFA. The start state of the DFA is a union of all original start states (of multiple patterns). The subsequent states are identified by finding out the sets of states of the NFA for each possible input symbol.</li> </ul> <h2 id="constructing-nfa-for-a-regular-expression">Constructing NFA for a Regular Expression</h2> <p>Consider a regular expression \(R\).</p> <ol> <li>If \(R\) is a letter in the alphabet \(\Sigma\), create a two state NFA that accepts the letter.</li> <li>If \(R\) is \(R_1.R_2\), create an NFA by joining the two NFAs \(N_1\) and \(N_2\) by adding an epsilon transition from every final state of \(N_1\) to the start state of \(N_2\).</li> <li>If \(R\) is \(R_1\vert R_2\), create an NFA by creating a new start state \(s_0\) and a new final state \(s_f\). Add an epsilon transition from \(s_0\) to the start state of \(R_1\) and similarly for \(R_2\). Also, add an epsilon transition from every final state of \(N_1\) to \(s_f\) and similarly for \(N_2\).</li> <li>If \(R\) is \(R_1^*\), create an NFA by adding an epsilon transition from every final state of \(R_1\) to the start state of \(R_1\).</li> </ol> <blockquote> <ul> <li>In the 2nd rule, all the final states in \(N_1\) must be made into normal states?</li> <li>In the 4th rule, \(R\) must accept \(S = \epsilon\) too.</li> <li>Where is the rule for \(R = (R_1)\).</li> </ul> </blockquote> <p>Recall the following rules</p> <ul> <li> <table> <tbody> <tr> <td> <em>First matching rule preferred</em>. For example, if we write the rule $$L(L</td> <td>D)^*\(- ID before\)int\(- INT, then the lexeme\)int$$ will be taken as ID token and not INT token.</td> </tr> </tbody> </table> </li> <li> <em>Longest match preferred.</em> For example, consider identifiers and int and the lexeme \(integer\). Then the lexeme will be treated as a single identifier token, and not an INT followed by ID.</li> </ul> <p>These rules are implicitly a part of DFAs in a way. The construction ensures the longest match is preferred. The accepted pattern is chosen from the possible patterns based on the first matching rule. <u>To ensure our grammar works as intended, special patterns must be written before general patterns.</u></p> <h2 id="representing-dfas-using-four-arrays">Representing DFAs using Four Arrays</h2> <p>A parsing table is also represented in a similar way. This is a general efficient representation for sparse data. The representation is explained through an example. Consider the following DFA</p> <table> <thead> <tr> <th> </th> <th>a</th> <th>b</th> <th>c</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>1</td> <td>0</td> <td>3</td> </tr> <tr> <td>1</td> <td>1</td> <td>2</td> <td>3</td> </tr> <tr> <td>2</td> <td>1</td> <td>3</td> <td>3</td> </tr> <tr> <td>3</td> <td>1</td> <td>0</td> <td>3</td> </tr> </tbody> </table> <p>with \(2\) being the final state. We use the following character codes</p> <table> <thead> <tr> <th>Char</th> <th>Code</th> </tr> </thead> <tbody> <tr> <td>a</td> <td>0</td> </tr> <tr> <td>b</td> <td>1</td> </tr> <tr> <td>c</td> <td>2</td> </tr> </tbody> </table> <p>Notice that states 0 and 3 have identical transitions. States 1 and 2 differ only on the ‘b’ transition. We shall use these similarities to exploit compact representation. The four arrays we consider are <strong>default</strong>, <strong>base</strong>, <strong>next</strong> and <strong>check</strong>. We follow the given steps.</p> <ol> <li>We choose to fill the entries for state 0 first.</li> <li>The ‘check’ array contains 0 to confirm that the corresponding entries in the ‘next’ array are for state 0. ‘Base’ is the location from which the transitions of state are stored in the ‘next’ array.</li> <li>For state 1, we reuse the transition on a and c from state 0 but we need to enter transition on b explicitly. We do this using the next free entry in the next array and back calculating the base of state 1.</li> <li>State 2 is filled in the same way.</li> <li>State 3 is identical to state 0. We keep its base same as that of state 0.</li> </ol> <p>The transition function in pseudocode is given by</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">nextState</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="kt">char</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">Check</span><span class="p">[</span><span class="n">Base</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+</span> <span class="kt">char</span><span class="p">]</span> <span class="o">==</span> <span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Next</span><span class="p">[</span><span class="n">Base</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+</span> <span class="kt">char</span><span class="p">];</span>
    <span class="k">else</span>
    	<span class="k">return</span> <span class="n">nextState</span><span class="p">(</span><span class="n">Default</span><span class="p">[</span><span class="n">state</span><span class="p">],</span> <span class="kt">char</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div> <blockquote> <p>How do we prevent clashes in the next array?</p> </blockquote> <p>We can further compress the tables using <strong>equivalence class</strong> reduction. In the above code, instead of <code class="language-plaintext highlighter-rouge">char</code>, we can have <code class="language-plaintext highlighter-rouge">class</code>. So for example, instead of defining transitions separately for 26 characters, we can define a single transition for all the letters. Further optimization can be done via <em>meta-equivalence classes</em>.</p> <h1 id="lecture-7">Lecture 7</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">02-02-22</code></p> </blockquote> <h2 id="syntax-analysis">Syntax Analysis</h2> <p>We had seen Lexical analysis so far. Syntax analysis discovers the larger structures in a program.</p> <p>A syntax analyzer or parser</p> <ul> <li>Ensures that the input program is well-formed by attempting to group tokens according to certain rules. This is <strong>syntax checking</strong>.</li> <li>Creates a hierarchical structure that arises out of such grouping - <strong>parse-tree</strong>.</li> </ul> <p>Sometimes, parser itself implements <strong>semantic-analysis</strong>. This way of compiler organization is known as parser driven front-end. On the other hand, if there is a separate semantic analyzer, then the organization is known as <code class="language-plaintext highlighter-rouge">parser driven back-end</code>?. Also, if the the entire compilation is interleaved along with parsing, then it is known as parser driven compilation. However, this organization is ineffective as optimizations are not possible.</p> <p>Till the early 70s, parsers were written manually. Now, plenty of parser generating tools exist such as</p> <ul> <li>Yacc/Bison - Bottom-up (LALR) parser generator</li> <li>Antlr - Top-down (LL) scanner cum parser generate.</li> <li>PCCTS, COCO, JavaCC, …</li> </ul> <p>To check whether a program is well-formed requires the specification to be unambiguous, correct and complete, convenient</p> <p>A <a href="https://sudhansh6.github.io/posts/automata#context-free-grammar"><strong>context free grammar</strong></a> meets these requirements. Each rule in the grammar is called as a <em>production</em>. The language defined by the grammar is done via the notion of a derivation. That is, the set of all possible <em>terminal strings</em> that can be derived from the start symbol of a CFG is the language of the CFG.</p> <p>For example, consider the grammar to define the syntax for variable declaration</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>declaration -&gt; type idlist;
type -&gt; integer | float | string
idlist -&gt; idlist, id | id
</code></pre></div></div> <p>Now, the parser can check if a sentence belongs to the grammar to check the correctness of the syntax. However, sometimes our algorithms to check whether a word is present in a grammar don’t work. When the derivations are unambiguous, most of the algorithms work in all cases.</p> <p>Human language is context-sensitive not context-free. For example, “Kite flies boy” and “boy files kite” are both syntactically correct. Such problems also arise in case of programming languages, but these are easier to deal with. We shall see this issue in Semantic Analysis.</p> <p>Equivalence of grammars in NP complete.</p> <h3 id="why-context-free">Why “Context Free”?</h3> <p>The only kind of productions permitted are of the form <code class="language-plaintext highlighter-rouge">non-terminal -&gt; sequence of terminals and non-terminals</code>. In a derivation, <u>the replacement is made regardless of the context</u> (symbols surrounding the non-terminal).</p> <h3 id="derivation-as-a-relation">Derivation as a relation</h3> <p>A string \(\alpha, \alpha \in (N \cup T)^*\), such that \(S \xrightarrow{*} \alpha\), is called a <strong>sentential form</strong> of \(G\).</p> <p>During a derivation, there is a choice of non-terminals to expand at each sentential form. We can arrive at leftmost or rightmost derivations.</p> <h1 id="lecture-8">Lecture 8</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">04-02-22</code></p> </blockquote> <h3 id="parse-trees">Parse Trees</h3> <p>If a non-terminal \(A\) is replaced using a production \(A \to \alpha\) in a left-sentential form, then \(A\) is also replaced by the same rule in a right-sentential form. The previous statement is only true when there is no ambiguity in derivations. The commonality of the two derivations is expressed as a <em>parse tree</em>.</p> <p>A <em>parse tree</em> is a pictorial form of depicting a derivation. The root of the tree is labeled with \(S\), and each leaf node is labeled b a token on by \(\epsilon\). An internal node of the tree is labeled by a non-terminal. If an internal node has \(A\) as its label and the children of this node from left to right are labeled with \(X_1, X_2, \dots, X_n\), then there must be a production \(A \to X_1X_2\dots X_n\) where \(X_i\) is a grammar symbol.</p> <h3 id="ambiguous-grammars">Ambiguous Grammars</h3> <p>Consider the grammar</p> \[E \to E + E \ \vert \ E*E \ \vert \ id\] <p>and the sentence <code class="language-plaintext highlighter-rouge">id + id * id</code>. We can have more than one leftmost derivation for this sentence.</p> \[\begin{align} E &amp;\implies E + E \\ &amp;\implies id + E \\ &amp;\implies id + E * E \\ &amp;\implies id + id * id \end{align}\] <p>The other leftmost derivation is -</p> \[\begin{align} E &amp;\implies E * E \\ &amp;\implies E + E * E \\ &amp;\implies id + E * E \\ &amp;\implies id + id * id \end{align}\] <p>A grammar is <strong>ambiguous</strong>, if there is a sentence for which there are</p> <ul> <li>more than one parse trees, or equivalently</li> <li>more than one leftmost/right most derivations.</li> </ul> <p>We can disambiguate the grammar while parsing (easier choice) or the grammar itself.</p> <p><strong>Grammar rewriting</strong></p> <p>Ambiguities can be eradicated via</p> <ul> <li>Precedence</li> <li>Associativity</li> </ul> <h3 id="parsing-strategies">Parsing Strategies</h3> <p>We have top-down and bottom-up parsing techniques. We shall only see bottom-up parsing in this course.</p> <p>Consider the grammar</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>D -&gt; var list : type;
tpye -&gt; integer | float;
list -&gt; list, id | id
</code></pre></div></div> <p>The bottom-up parse and the sentential forms produced for the string <code class="language-plaintext highlighter-rouge">var id, id : integer ;</code> is -</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>var id, id : integer ;
var list, id : integer ;
var list : integer ;
var list : type ;
D
</code></pre></div></div> <p>The sentential forms happen to be a right most derivation reverse order. The basic steps of a bottom-up parser are</p> <ul> <li>to identify a <em>substring</em> within a rightmost sentential form which matches the rhs of a rules</li> <li>when this substring is replaced by the lhs of the matching rule, it must produce the previous rm-sentential form.</li> </ul> <p>Such a substring is called <strong>handle</strong>.</p> <p><strong>Handle</strong> - A handle of a right sentential form \(\gamma\), is</p> <ul> <li>a production rule \(A \to \beta\), and</li> <li>an occurrence of substring \(\beta\) in \(\gamma\).</li> </ul> <p>Bottom-up parsing is an LR parsing as it amounts to reading the input from left to right, and placing the right most derivation in reverse.</p> <p><strong>Note.</strong> Only terminal symbols can appear to the right of a handle in a right most sentential form.</p> <h2 id="-syntax-analysis">~ Syntax Analysis</h2> <p>We shall assume that we know how to detect handles in a string, and proceed with parsing algorithms.</p> <h2 id="shift-reduce-parsing">Shift Reduce Parsing</h2> <p>Basic actions of the shift-reduce parser are -</p> <ul> <li> <strong>Shift</strong> - Moving a single token form the input buffer onto the stack till a handle appears on the stack.</li> <li> <strong>Reduce</strong> - When a handle appears on the stack, it is popped and replaced by the lhs of the corresponding production rule.</li> <li> <strong>Accept</strong> - When the stack contains only the start symbol and input buffer is empty, then we accept declaring a successful parse.</li> <li> <strong>Error</strong> - When neither shift, reduce or accept are possible, we throw an error (syntax).</li> </ul> <h1 id="lecture-9">Lecture 9</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">09-02-22</code></p> </blockquote> <h3 id="properties-of-shift-reduce-parsers">Properties of shift-reduce parsers</h3> <p>Is the following situation possible?</p> <ul> <li>\(\alpha \beta \gamma\) is the stack content, and \(A\to \gamma\) is the handle.</li> <li>The stack content reduces to \(\alpha \beta A\)</li> <li>Now, \(B \to \beta\) in the next handle.</li> </ul> <p>Notice that the handle is buried in the stack. The search for the handle can be expensive. If this is true, then there is a sequence of rightmost derivations</p> \[S \xrightarrow{*rm} \alpha BAxyz \xrightarrow{rm} \alpha \beta Axyz \xrightarrow{rm} \alpha \beta \gamma xyz\] <p>However, this is not a valid rightmost derivation. Therefore, the above scenario is not possible.</p> <p>This property does not ensure unique reductions with SR parser. For example, we can have the following.</p> <p><img src="/assets/img/IPL/image-20220209113043082.png" alt="image-20220209113043082"></p> <p>These problems are collectively grouped as shift-reduce conflicts and reduce-reduce conflicts. Given a parsing table, each (state, char) pair can have two possible valid actions. These conflicts are resolved by conventionally choosing one action over the other.</p> <h2 id="simple-right-to-left-parsing-slr1">Simple Right to Left Parsing (SLR(1))</h2> <p>Shift reduce parsing is a precursor to this parsing method. As in, shift reduce parsing does not have a definitive algorithm as such, and it is formalized using SLR parsing.</p> <h3 id="shift-reduce-parsing-formal-algorithms">Shift Reduce Parsing: Formal Algorithms</h3> <p>The first step involves disambiguating the grammar. Shift reduce conflicts are resolved using <strong>precedence</strong> and <strong>associativity</strong>. As we have seen, we trace right most derivations in reverse by identifying handles in right sentential forms and <em>pruning</em> them for constructing the previous right sentential form.</p> <p>How do we identify handles? <u>We need to discover a prefix of right sentential form that ends with a handle.</u></p> <p>A <strong>viable prefix</strong> of a right sentential form that does not extend beyond the handle. It is either a string with no handle, or it is a string that ends with a handle. By suffixing terminal symbols to the viable prefix of the second kind, we can create a right sentential form. <u>The set of viable prefixes forms a regular language (as they are prefixes), thus they can be recognised by a DFA</u>. We keep pushing prefixes on the stack until the handle appears on the top of the stack.</p> <p>The occurrence of a potential handle does not mean it should be reduced, the next terminal symbol decides whether it is an actual handle. In general, the set of viable prefixes need not be finite and the DFA to recognise them may have cycles.</p> <p>An item is a grammar production with a dot (\(\bullet\)) in it somewhere in the RHS. The dot separates what has been seen from what may be seen in the input . It can be used to identify a set of items for a viable prefix. A terminal to the left of the dot indicates a <em>complete item</em>.</p> <p>Now, we shall see how to compute <strong>LR(0) item sets</strong>. <code class="language-plaintext highlighter-rouge">L</code> means that it reads the input left to right, <code class="language-plaintext highlighter-rouge">R</code> denotes that it traces the rightmost derivation in reverse, and <code class="language-plaintext highlighter-rouge">0</code> tells that an item does no contain any lookahead symbol. Consider the grammar</p> \[\begin{align} E \to E + E \mid E*E \mid id \end{align}\] <p>We augment the grammar by adding a synthetic start symbol. Then we construct the start state by putting a dot at the start of the start symbol and taking a closure. We then identify the transitions on every symbol that has a dot before it to construct new states. For every state identified in this manner, we take a closure and identify the transitions on every symbol that has a dot before it.</p> <h1 id="lecture-10">Lecture 10</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">11-02-22</code></p> </blockquote> <blockquote> <p>Does a rule of the form \(T \to \epsilon\) always lead to a shift-reduce conflict?</p> </blockquote> <p>Consider \(\beta Aw \xrightarrow{rm} \beta \alpha w\). When do we reduce occurrence of \(\alpha\) in \(\beta\alpha\) using \(A \to \alpha\) using LR(\(k\))? (When do we decide that \(\alpha\) and \(A \to \alpha\) form a handle in \(\beta \alpha\)).</p> <ul> <li>As soon as we find \(\alpha\) in \(\beta \alpha\) - LR(0) items and no lookahead in the input - <strong>SLR(0) Parser</strong> </li> <li>As soon as we find \(\alpha\) in \(\beta \alpha\) and the next input token can follow \(A\) in some right sentential form - LR(0) items and 1 lookahead in the input - <strong>SLR(1) Parser</strong> </li> <li>As soon as we find \(\alpha\) in \(\beta \alpha\) and the next input token can follow \(A\) in \(\beta \alpha\) - LR(1) items and 1 lookahead in the input - <strong>CLR(1) Parser</strong> </li> </ul> <p>To formalise the notion of follow, we defined <code class="language-plaintext highlighter-rouge">FIRST</code> and <code class="language-plaintext highlighter-rouge">FOLLOW</code> sets.</p> <p><code class="language-plaintext highlighter-rouge">FIRST</code>\((\beta)\) contains the terminals that may begin a string derivable from \(\beta\). If \(\beta\) derives \(\epsilon\) then \(\epsilon \in FIRST(\beta)\).</p> <h1 id="lecture-12">Lecture 12</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">18-02-22</code></p> </blockquote> <h2 id="role-of-semantic-analysis">Role of Semantic Analysis</h2> <ul> <li> <p>We have seen <strong>lexical errors</strong> and <strong>syntax errors</strong>. Name and scope analysis cannot be done using scanner and parser (as our grammar is context free). These are detected by semantic analyser and are known as <strong>semantic errors</strong>.</p> <p>Compilers usually try to list all the errors in the program by trying to recover from each error, and continue compiling the remaining code.</p> </li> <li>C++ requires references to be initialised.</li> <li> <p>Overflow shows up as a warning and not as an error. Why? This is because type conversions are allowed in C++. These sort of errors are <strong>runtime errors</strong>.</p> </li> <li> <p>Suppose we have the following program</p> <div class="language-cpp highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">40</span><span class="p">;</span>
  
<span class="n">cout</span><span class="o">&lt;&lt;</span> <span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">i</span> <span class="o">&lt;=</span> <span class="mi">5</span> <span class="o">&lt;&lt;</span><span class="n">endl</span><span class="p">;</span>
</code></pre></div> </div> <p>The output of this code is <code class="language-plaintext highlighter-rouge">1</code> and not <code class="language-plaintext highlighter-rouge">0</code>. This is because <code class="language-plaintext highlighter-rouge">1 &lt;= i</code> is evaluated as <code class="language-plaintext highlighter-rouge">True</code>(<code class="language-plaintext highlighter-rouge">1</code>) and <code class="language-plaintext highlighter-rouge">1 &lt;= 5</code> is <code class="language-plaintext highlighter-rouge">True</code>.</p> <p>This is a runtime activity and the error cannot be identified by a compiler (unless constant propagation optimisation is performed). This error is known as a <strong>logical error</strong>.</p> <p>Runtime and Logical errors are usually not detected by the compiler.</p> </li> <li> <p>Does <code class="language-plaintext highlighter-rouge">int a[3] = {1, 2, 3, 4};</code> give an error? Yes! (Too many initialisers) It’s a semantic error. What if we had <code class="language-plaintext highlighter-rouge">int a[4] = {1, 2, 3};cout&lt;&lt;a[3];</code>? It does not give an error or a warning. It gives a <em>runtime error</em> in the form of a <strong>segmentation fault</strong>.</p> </li> <li> <p>Suppose we had the following code</p> <div class="language-cpp highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">f</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">a</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">)</span> <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div> </div> <p>Does this give an error? No, the compiler gives a warning that says “control reaches end of non-void function.” Such functions can be used to generate pseudorandom numbers. However, does <code class="language-plaintext highlighter-rouge">f(-5)</code> give a runtime error? No, it returns <code class="language-plaintext highlighter-rouge">-5</code>. Why is that so?</p> <p>Also, if the definition of <code class="language-plaintext highlighter-rouge">f</code> was modified to the following, then we get random numbers</p> <div class="language-cpp highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">f</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">a</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">)</span> <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">a</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div> </div> <p>But again, if <code class="language-plaintext highlighter-rouge">f</code> is modified to below, we start getting <code class="language-plaintext highlighter-rouge">-5</code> everytime.</p> <div class="language-cpp highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">f</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">)</span>
<span class="p">{</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">a</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">)</span> <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
  <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">a</span> <span class="o">&lt;&lt;</span> <span class="n">endl</span><span class="p">;</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="mi">5</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// Or the following f</span>
<span class="kt">int</span> <span class="nf">f</span><span class="p">(</span><span class="kt">int</span> <span class="n">a</span><span class="p">)</span>
<span class="p">{</span>
	<span class="k">if</span> <span class="p">(</span><span class="n">a</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">)</span> <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
	<span class="n">g</span><span class="p">(</span><span class="n">a</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div> </div> <p>Many such variations are possible with different behaviours. Also, the output of the program depends on the compiler flags too!</p> <p>The language specifications say that a variable must be declared before its use but may not be defined before its use.</p> </li> <li> <p>Consider the following code</p> <div class="language-cpp highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="kt">float</span> <span class="n">inc</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">while</span> <span class="p">(</span><span class="n">inc</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">sum</span> <span class="o">+=</span> <span class="n">inc</span><span class="p">;</span>
	<span class="n">inc</span> <span class="o">+=</span> <span class="mf">0.1</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div> </div> <p>This program can go into an infinite loop due to <strong>floating imprecision</strong>. This is a runtime activity and cannot be detected by a compiler. We will classify this as a logical error and not a runtime error. Remember that <code class="language-plaintext highlighter-rouge">0</code> does not cause any imprecision!</p> </li> <li> <p>Consider the following code</p> <div class="language-cpp highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">f</span><span class="p">(</span><span class="kt">short</span> <span class="n">a</span><span class="p">)</span> <span class="p">{</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">"short"</span><span class="p">;}</span>
<span class="kt">void</span> <span class="nf">f</span><span class="p">(</span><span class="kt">long</span> <span class="n">a</span><span class="p">)</span> <span class="p">{</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">"long"</span><span class="p">;}</span>
<span class="kt">void</span> <span class="nf">f</span><span class="p">(</span><span class="kt">char</span> <span class="n">a</span><span class="p">)</span> <span class="p">{</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">"char"</span><span class="p">;}</span>
<span class="kt">int</span> <span class="nf">main</span> <span class="p">()</span>
<span class="p">{</span>
	<span class="n">f</span><span class="p">(</span><span class="mi">100</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div> </div> <p>How does the compiler decide which function to use? There is a difficulty in resolving function overloading. This is a <strong>semantic error</strong>. If we had a definition for <code class="language-plaintext highlighter-rouge">int</code>, then there would not be any issue, as integer is a default type.</p> <p>Also, comparison of string and integer will give a <strong>syntax error</strong>.</p> </li> <li> <code class="language-plaintext highlighter-rouge">main</code> in C can be recursive!</li> </ul> <h1 id="lecture-13">Lecture 13</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">02-03-22</code></p> </blockquote> <h2 id="why-separate-semantic-analysis-from-syntax-analysis">Why separate semantic analysis from syntax analysis?</h2> <p>The constraints that define semantic validity cannot be described by context free grammars. Also, using context sensitive grammars for parsing is expensive. Practical compilers use CFGs to admit a superset of valid sentences and prune out invalid sentences by imposing context sensitive restrictions. For example, \(\{wcw \mid w \in \Sigma^*\}\) is not a CFG. We accept all sentences in \(\{xcy \mid x, y \in \Sigma^*\}\), enter \(x\) in a symbol table during declaration processing, and when ‘variable uses’ are processed, lookup the symbol table and check if \(y = x\).</p> <p>We identify some attributes of the context-free grammar, and apply some constraints on them to simulate context-sensitivity.</p> <h3 id="terminology">Terminology</h3> <ul> <li> <strong>Undefined behaviour</strong> - Unchecked prohibited behaviour flagged by the language. These are not a responsibility of the compiler or its run time support. They have unpredictable outcomes, and the compiler is legally free to do anything. Practical compilers try to detect these and issue warnings (not errors).</li> <li> <strong>Unspecified behaviour</strong> (aka implementation-defined behaviour) - These refer to a valid feature whose implementation is left to the compiler. The available choices do no affect the result by mat influence the efficiency. For example, the order of evaluation of subexpressions is chosen by the compiler. Practical compilers make choices based on well defined criteria,</li> <li> <strong>Exceptions</strong> - Prohibited behaviour checked by the runtime support. Practical compilers try to detect these at compile time.</li> </ul> <h2 id="different-forms-of-semantic-analysis">Different forms of Semantic Analysis</h2> <p><img src="/assets/img/IPL/image-20220302113554943.png" alt="image-20220302113554943"></p> <h2 id="syntax-directed-definitions-sdds">Syntax Directed Definitions (SDDs)</h2> <p>The augmented CFG with attributes is given by</p> <p>| \(A \to \alpha\) | \(b = f(c_1, c_2, \dots, c_k)\) | | —————– | ——————————- |</p> <p>where \(b\) is an attribute of \(A\) and \(c_i\), \(1 \leq i \leq k\) are attributes of \(\alpha\). These semantic rules are evaluated when the corresponding grammar tules is use for derivation/reduction.</p> <p><strong>Note.</strong> The associativity decision is not decided through the order of execution, but through the order in which variables are ‘seen’.</p> <h1 id="lecture-14">Lecture 14</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">04-03-22</code></p> </blockquote> <h2 id="sdd-for-generating-ir-for-expression">SDD for generating IR for expression</h2> <p>For example, consider the input statement \(x = (a - b) * ( c+ d)\). The expected IR output is</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>t0 = a - b
t1 = c + d
t2 = t0 * t1
x = t2
</code></pre></div></div> <p>To achieve this, we can have a “code” attribute for each <code class="language-plaintext highlighter-rouge">id</code>, and then concatenate the codes to obtain the IR.</p> <p>Let us consider the example for generating an IR for a ternary expression. We have \(E_1 \to E_2\ ?\ E_3 : E_4\). Then, we have the following template for the code of \(E_1\).</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>E_2.code
t_1 = !E_2.place
if t_1 goto I_1
E_3.code
t_2 = E_3.place
goto I_2
I_1 : E_4.code
			t_2 = E_4.place
I_2 :
--------------------------
E_1.place = t_2
</code></pre></div></div> <blockquote> <p>Why are we using <code class="language-plaintext highlighter-rouge">!t1</code> instead of <code class="language-plaintext highlighter-rouge">t1</code>? It is just a design decision.</p> </blockquote> <p>Now, we shall use the following SDD for generating the above code.</p> <p><img src="/assets/img/IPL/image-20220304112033935.png" alt="image-20220304112033935"></p> <p>There are two representations of a 2D array - row major representation and column major representation. FORTRAN uses column major representation for arrays.</p> <p>Should the compiler decide the representation or should the language do it? Languages specify the representation!</p> <p>In general, we use row-major representation where the address of the cell \((i, j)\) is given by <code class="language-plaintext highlighter-rouge">base + i1*n2 + i2</code>. For a general \(k\)-D array, the offset is given by the recurrence</p> \[\begin{align} O_1 &amp;= i_1 \\ O_{j + 1} &amp;= O_j \times n_j + i_j \end{align}\] <p>We add the attributes <code class="language-plaintext highlighter-rouge">name, offset, ndim</code> etc for IR of array accesses.</p> <p>Check the slides for the pseudocode to generate IR for unary/binary expressions, while loop etc.</p> <h1 id="lecture-15">Lecture 15</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">09-03-22</code></p> </blockquote> <h3 id="generating-ir-for-field-accesses-in-a-structure">Generating IR for Field Accesses in a Structure</h3> <p>The ‘access’ expressions become a little more non-trivial in case of structures as compared to variables. The memory offsets of a structure can be computed at compile time unlike an array offset. For example, the offset in <code class="language-plaintext highlighter-rouge">A[x]</code> cannot be calculated at compile time. Hence, it is easier to work on structures. A float is considered to occupy 8 bytes.</p> <p>The general IR code will have the following steps</p> <ul> <li>Obtain the base address</li> <li>Add the offset to the base</li> <li>Dereference the resulting address</li> </ul> <p>We will assume <code class="language-plaintext highlighter-rouge">offset(t, f)</code> gives the offset of field <code class="language-plaintext highlighter-rouge">f</code> in structure <code class="language-plaintext highlighter-rouge">t</code>. Similarly, <code class="language-plaintext highlighter-rouge">type(t, f)</code> gives the type of <code class="language-plaintext highlighter-rouge">f</code> in <code class="language-plaintext highlighter-rouge">t</code>. The TAC generation code is given by</p> <p><img src="/assets/img/IPL/image-20220323084445565.png" alt="image-20220323084445565"></p> <p>We need to add the <code class="language-plaintext highlighter-rouge">code</code> and <code class="language-plaintext highlighter-rouge">place</code> attributes to the above rules. We shall assume that the members of a class are stored sequentially based on the declaration.</p> <p><strong>Note.</strong> We cannot have a circular dependancy among classes without pointers.</p> <p>We shall introduce pointers in our grammar now.</p> <p><img src="/assets/img/IPL/image-20220323085151934.png" alt="image-20220323085151934"></p> <p><img src="/assets/img/IPL/image-20220323085134456.png" alt="image-20220323085134456"></p> <p><strong>Note.</strong> The new code for \(F \to id_1.id_2\) generates different code as compared to our previous implementation.</p> <h2 id="syntax-directed-translation-schemes">Syntax Directed Translation Schemes</h2> <p>Given a production \(X \to Y_1Y_2 \dots Y_k\),</p> <ul> <li>If an attribute \(X.a\) is computed from those of \(Y_i\), \(1 \leq i \leq k\), the \(X.a\) is a <strong>synthesised attribute</strong>.</li> <li>If an attribute \(Y_i.a\), \(1 \leq i \leq k\) is computed from those of \(X\) or \(Y_j\), \(1 \leq j \leq i\), then \(Y_i.a\) is an <strong>inherited attribute</strong>.</li> </ul> <p>Using these definitions, we will come up with an alternate approach for generating the IR for structures that is more expressive.</p> <h1 id="lecture-16">Lecture 16</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">11-03-22</code></p> </blockquote> <p>Previously, we had calculated the cumulative offset for a structure access in the IR. As in, we computed the final offsets at compile time and consequently used <code class="language-plaintext highlighter-rouge">F.offset</code> attribute instead of the <code class="language-plaintext highlighter-rouge">F.code</code> attribute. Alternately, we did not do ‘compile time offset calculation’ for the pointer IR code.</p> <p>Let us now continue the discussion on inherited and synthesised attributes. A synthesised attribute is an attribute derived for a non-terminal on the lhs of a production from the non-terminals on the rhs of the same production. Inherited attributes are derived for non-terminals on the rhs of a production from the non-terminal on the rhs or other non-terminals on the rhs. So, why do we require inherited attributes?</p> <p>For example, consider type analysis</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Decl</span> <span class="o">-&gt;</span> <span class="n">Type</span> <span class="n">VarList</span> <span class="p">{</span><span class="err">$</span><span class="mi">2</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="o">=</span> <span class="err">$</span><span class="mi">1</span> <span class="o">-&gt;</span> <span class="n">name</span><span class="p">}</span>
<span class="n">Type</span> <span class="o">-&gt;</span> <span class="kt">int</span> <span class="o">|</span> <span class="kt">float</span> <span class="p">{</span><span class="err">$$</span> <span class="o">-&gt;</span> <span class="n">name</span> <span class="o">=</span> <span class="err">$</span><span class="mi">2</span><span class="p">;}</span>
<span class="n">VarList</span> <span class="o">-&gt;</span> <span class="n">VarList</span><span class="p">,</span> <span class="n">id</span> <span class="p">{</span><span class="err">$</span><span class="mi">1</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="o">=</span> <span class="err">$$</span> <span class="o">-&gt;</span> <span class="n">type</span><span class="p">;</span> <span class="err">$</span><span class="mi">3</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="o">=</span> <span class="err">$$</span> <span class="o">-&gt;</span> <span class="n">type</span><span class="p">;}</span>
<span class="n">Varlist</span> <span class="o">-&gt;</span> <span class="n">id</span> <span class="p">{</span><span class="err">$</span><span class="mi">1</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="o">=</span> <span class="err">$$</span> <span class="o">-&gt;</span> <span class="n">type</span><span class="p">;}</span>
</code></pre></div></div> <p>Here, the attribute <code class="language-plaintext highlighter-rouge">type</code> is inherited.</p> <p>However, how do we allow flow of attributes concurrently with parsing? Clearly, there would be non-terminals that are not generated at a given step during parsing, and inherited attributes cannot be computed if they depend on a symbol not yet seen. All these problems can be avoided by no allowing inherited attributes to be derived from the symbols to the right of the current symbol. So once the left non-terminals are derived, we can derive the attribute for the current non-terminal.</p> <p>In summary, given a production \(X \to Y_1, \dots, Y_k\)</p> <ul> <li>\(Y_i.a\) is computed only from the attributes of \(X\) or \(Y_j\), \(j &lt; i\).</li> <li>\(X.a\) would have been computed from the grammar symbols that have already been seen (i.e, in some production of the form \(Z \to \alpha X\beta\))</li> </ul> <h3 id="more-definitions">More definitions</h3> <p>An SDD is <strong>S-attributed</strong> if it uses only synthesised attributes, and an SDD is <strong>L-attributed</strong> if it uses synthesised attributes or inherited attributes that depend on some symbol to the left. That is, given a production \(X \to Y_1\dots Y_k\) attribute \(Y_i.a\), of some \(Y_i\) is computed only from the attributes of \(X\) or\(Y_j\), \(j &lt; i\).</p> <h2 id="syntax-directed-translation-schemes-sdts">Syntax Directed Translation Schemes (SDTS)</h2> <p>We generalise the notion of SDDs using SDTS. A Syntax Directed Translation Scheme is an SDD with the following modifications</p> <ul> <li>Semantic rules are replaced by actions possibly with side effects.</li> <li>The exact time of the action is specified; an action computing an inherited attribute of a non-terminal appears just before the non-terminal.</li> </ul> <p>For example, for the previous type analysis rules, we have</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">decl</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="p">{</span><span class="err">$</span><span class="mi">2</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="o">=</span> <span class="err">$</span><span class="mi">1</span> <span class="o">-&gt;</span> <span class="n">type</span> <span class="p">}</span> <span class="n">VarList</span>
</code></pre></div></div> <p>We similarly define <strong>S-Attributed SDTS</strong> as SDTS that only use synthesised attributes and all actions appear at the end of the RHS of a production. <strong>L-Attributed SDTS</strong> use only synthesises attributes or attributes that depend on a symbol towards the left. The actions may appear in the middle of the rules end and at the end of the RHS of a production.</p> <h2 id="type-analysis">Type Analysis</h2> <h3 id="type-expressions-and-representation">Type Expressions and Representation</h3> <p>A type expression describes types of all entities (variable, functions) in a program - basic types, user-defined types, and derived types.</p> <p><strong>Note.</strong> The size of an array is not a part of the type in C for validation. It is just used for memory allocation. \(\tau_1 \times \tau_2\) describes the product of two types, and \(\tau_1 \to \tau_2\) describes a function that takes arguments described by \(\tau_1\) and returns the result described by \(\tau_2\). Product is left-associative and has a higher precedence than \(\to\).</p> <h1 id="lecture-17">Lecture 17</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">16-03-22</code></p> </blockquote> <h3 id="marker-non-terminals">Marker Non-terminals</h3> <p>If we have a rule of the form \(X \to Y_1 \{\dots\} Y_2\), then we convert it to the following set of rules \(X \to Y_1 M Y_2 \\ M \to \epsilon \{\dots\}\) \(M\) is a marker non-terminal for \(Y_2\) in the grammar. \(Y_1.s\) and \(Y_2.s\) denote the synthesised attributes of \(Y_1\) and \(Y_2\) whereas \(Y_2.i\) denotes the inherited attribute of \(Y_2\).</p> <p>When \(M \to \epsilon \{\dots\}\) is about to be reduced, the parsing stack contains \(Y_1\), and the value stack contains \(Y_1.s\). Once the reduction is done, we add \(M\) to the parsing stack, and \(Y_2.i\) is added to the value stack.</p> <p>Marker non-terminals may cause reduce-reduce conflicts. It is possible to rewrite the yacc scripts to prevent these conflicts.</p> <h2 id="type-analysis-2">Type Analysis (2)</h2> <h3 id="type-equivalence">Type Equivalence</h3> <p>Consider two different with identical member variables and functions. How do we distinguish the structures themselves and pointers to these structures?</p> <p><strong>Name Equivalence</strong> - Same basic types are name equivalent. Derived type are name equivalent if they have the same name. every occurrence of a derived type in declarations is given a unique name.</p> <p><strong>Structure Equivalence</strong> - Same basic types are structurally equivalent. Derived types are structurally equivalent if they are obtained by applying the same type constructors to structurally equivalent types, or <em>one is type name that denotes the other type expressions?</em>.</p> <p>Name equivalence implies structural equivalence and not the other way around. C uses structural equivalence for everything except structures. For structures, it uses name equivalence.</p> <h3 id="type-inferencing">Type Inferencing</h3> <p>Functional languages do not require separate declarations for variables and types. Some sort of type inferencing is done for class during runtime in C.</p> <h2 id="name-and-scope-analysis">Name and Scope Analysis</h2> <p>We maintain a stack of symbol tables. At the start of a new scope, we push a new symbol table on the stack. We start with the “global” scope symbol table. At the end of every scope, we pop the top symbol table from the stack. For use of a name, we look it up in the symbol table starting from the stack top</p> <ul> <li>If the name is not found in a symbol table, search in the symbol table below</li> <li>If the same name appears in two symbol tables, the one closer to the top hides the one below</li> </ul> <p>However, with this setup, how do we access variables in the outer scope?</p> <h3 id="static-and-dynamic-scoping">Static and Dynamic Scoping</h3> <p>Under <em>static scoping</em>/ lexical scoping, the names visible at line \(i\) in procedure \(X\) are</p> <ul> <li>Names declared locally within \(X\) before line \(i\)</li> <li>Name declared in procedures enclosing \(X\) upto the declaration of \(X\) in the program.</li> </ul> <p>Under <em>dynamic scoping</em>, the names visible at line \(i\) in a procedure \(X\) are</p> <ul> <li>Names declared locally within \(X\) before line \(i\)</li> <li>Name declared in procedures enclosing \(X\) in a call chain reaching \(X\).</li> </ul> <p>Dynamic scoping is difficult to comprehend is seldom used.</p> <h1 id="lecture-18">Lecture 18</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">23-03-22</code></p> </blockquote> <h2 id="declaration-processing">Declaration Processing</h2> <p>How do we process <code class="language-plaintext highlighter-rouge">int **a[20][10]</code>? Do we take it as double pointer to an array of 20 rows where each row consists of 10 integers, or do we take it as a 2D-array of double arrays to integers with size \(20 \times 10\)?</p> <p>In order to do the former, we can use the following grammar</p> \[\begin{align} decl &amp;\to T \ item; \\ T &amp;\to int \mid double \\ item &amp;\to id \mid item [num] \end{align}\] <p>However, this is an inconvenient layout for 20 arrays of arrays of 10 ints. Suppose we correct it to the following</p> \[\begin{align} decl &amp;\to T \ item; \\ T &amp;\to int \mid double \\ item &amp;\to id \mid Array \\ Array &amp;\to [num] \mid [num]Array \end{align}\] <p>So basically, we have changed left recursive rule to a right recursive rule.</p> <p>We introduce the notion of base types and derived types for We similarly add the size and width attributes in the above rules.</p> <p>Addition of pointers is easy. We just add the following set of rules to the above set.</p> \[\begin{align} item &amp;\to * \{item2.bt = item.bt\} \\ &amp;\to item2 \\ &amp;\to \{item.dt = pointer(item2.dt); item.s = 4\} \end{align}x\] <h1 id="lecture-19">Lecture 19</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">25-03-22</code></p> </blockquote> <h2 id="runtime-support">Runtime support</h2> <p>The data objects come into existence during the execution of the program. The generated code must refer to the data objects using their addresses, which must be decided during compilation.</p> <p>Runtime support also helps in dynamic memory allocation, garbage collection, exception handling, and virtual function resolution.</p> <p>Virtual function resolution falls under the category of runtime support as pointer declarations are static but the objects pointed are dynamic.</p> <p>A programmer specifies the type of a data item, and also its role and allocation. Unnamed data resides on heap. We have the following properties of data</p> <p><img src="/assets/img/IPL/image-20220405185340593.png" alt="image-20220405185340593"></p> <blockquote> <p>Local and global static variables?</p> </blockquote> <p>A sequential language may allow procedures to be</p> <ul> <li>Invoked as subroutines - Stack and static memory suffices for organising data for procedure invocations.</li> <li>Invoked recursively - Stack memory is required for organising data for procedure invocations.</li> <li>Invoked indirectly through a function pointer or passed as a parameter. Access to non-local data of the procedure needs to be provided.</li> </ul> <h2 id="compiling-procedure-calls">Compiling Procedure Calls</h2> <h3 id="activation-records">Activation Records</h3> <p>Every invocation of a procedure requires creating an <strong>activation record</strong>. An activation record provides space for</p> <ul> <li>Local variables</li> <li>Parameters</li> <li>Saved registers</li> <li>Return value</li> <li>Return address</li> <li>Pointers to activation records of the calling procedures</li> </ul> <p>We maintain two parameters known as <strong>frame pointer</strong> and <strong>stack pointer</strong>. Activation record of the callee is partially generated by the caller. We store the return parameter in one of the registers itself. Function prologue and function epilogue are generated by the compiler. We reduce the value of stack pointer when we push something on the stack. The stack pointer <code class="language-plaintext highlighter-rouge">$sp</code> points to the lower address of the next free location. Unlike the stack pointer, the frame pointer <code class="language-plaintext highlighter-rouge">$fp</code> holds the address of an occupied word.</p> <p><strong>Reminder.</strong> We are talking about runtime activity.</p> <p>As discussed earlier, the compiler generates a function prologue and epilogue. It also generates a code before the call and after the call. This is all the boring assembly code we typically see for calling functions. We shall this code changes when the parameter passing mechanism changes. We will also see implicit sharing through scoping.</p> <h1 id="lecture-20">Lecture 20</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">30-03-22</code></p> </blockquote> <h2 id="using-staticaccess-link">Using Static/Access Link</h2> <p>Let us reiterate the difference between static and dynamic scoping. In static scoping, we can use variables that are declared in the outer scope of the current scope. We use static/access link for this purpose. On the other hand, dynamic scoping follows the call chain for variables. We use control/dynamic link for this purpose.</p> <p>To access the activation record of a procedure at level \(l_1\) from within a procedure at level \(l_2\), we traverse the access link \(l_2 - l_1\) times. Note that \(d_{callee} \leq d_{caller}+ 1\).</p> <h2 id="parameter-passing-mechanisms">Parameter Passing Mechanisms</h2> <ul> <li> <strong>Call by value</strong> - Copy the value of the actual parameter into the formal parameter. We use eager evaluation here. We do not know if the parameter is being used in the procedure, but we still evaluate it.</li> <li> <strong>Call by reference</strong> - Copy the address of the actual parameter into the formal parameter.</li> <li> <strong>Call by value-result (copy-restore)</strong> - Copy the value of the actual parameter and copy the final value of the formal parameter into the actual parameter. This method starts off with call by value. That is, the value of the actual parameter is copied into the formal parameter. However, in the end the value of the formal parameter that is evaluated inside the procedure is copied to the actual parameter. In the end we do <code class="language-plaintext highlighter-rouge">*(&amp;e) = x</code> and not <code class="language-plaintext highlighter-rouge">e = x</code> because <code class="language-plaintext highlighter-rouge">e</code> is an expression.</li> <li> <strong>Call by name</strong> - Textural substitution of formal parameter by the actual parameter. Evaluation of the actual parameter is delayed until the value of formal parameter is used. The evaluation is done every time where the formal parameter is used in the procedure. So, this is equivalent to textual substitution or using a macro. All the modifications to the formal parameter is written to the address of the actual parameter. This is implemented by a <strong>thunk</strong> which is a parameterless procedure per actual argument to evaluate the expression and return the address of the evaluation.ccc</li> <li> <strong>Call by need</strong> - Textual substitution of formal parameter by the actual parameter but evaluation only once. This is similar to call by name but the evaluation of the actual parameter is done only at the first use. For any later use, we use the precomputed values. This is known as <strong>lazy evaluation</strong>.</li> </ul> <p><img src="/assets/img/IPL/image-20220405205123349.png" alt="image-20220405205123349"></p> <p>We have the above to make the distinction between the different methods.</p> <h3 id="parameter-passing-mechanisms-for-procedure-as-parameters">Parameter Passing Mechanisms for Procedure as Parameters</h3> <p>Pass a closure of the procedure to be passed as parameter. A data structure containing a pair consisting of</p> <ul> <li> <p>A pointer to the procedure body</p> </li> <li> <p>A pointer to the external environment (i.e. the declarations of the non-local variables visible in the procedure)</p> <p>Depends on the scope rules (i.e., static or dynamic scope)</p> </li> </ul> <p>For C, there are no nested procedures so the environment is trivially global. So a closure is represented trivially by a function pointer. In C++, the environment of a class method consists of global declarations and the data members of the class. The environment can be identified from the class name of the receiver object of the method call.</p> <h3 id="representation-of-a-class">Representation of a Class</h3> <p>There is no distinction between the public and private data in memory. Public/Private is a compile time distinction. Every function with \(n\) parameters is converted to a function of \(n + 1\) parameter with the first parameter being the address the address of the object. Internally, the data space is separate for each object, but the code memory is same for all which contains the functions.</p> <h3 id="virtual-functions">Virtual functions</h3> <p>Non-virtual functions are inherited much like data members. There is a single class-wide copy of the code and the address of the object is the first parameter as seen earlier. In the case of <strong>virtual functions</strong>, a pointer to a base class object may point to an object of any derived class in the class hierarchy.</p> <h3 id="virtual-function-resolution">Virtual function resolution</h3> <p>Partially static and partially dynamic activity. At compile time, a compiler creates a virtual function table for each class. At runtime, a pointer may point to an object of any derived class, and the compiler-generated code is used to pick up the appropriate function by indexing into the virtual table for each class.</p> <p>We define a <strong>non-virtual function</strong> as a function which is not virtual in <em>any</em> class in a hierarchy. Resolution of virtual functions depends on the class of the pointee object - needs dynamic information. Resolution of non-virtual functions depends on the class of the pointer - compile time information is sufficient. In either case, a pointee cannot belong to a “higher” class in the hierarchy.</p> <h1 id="lecture-21">Lecture 21</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">01-04-22</code></p> </blockquote> <p>Compiler decides the virtual function for a class statically. Functions are not copied across objects but are used with reference. Functions of the inherited class override the corresponding virtual functions of the parent class. There is no search for the correct function implementation during the runtime. Instead, all the functions are correctly configured at compile time.</p> <p>The runtime support looks up the class of the receiver object. Then, it dereferences the class information to access the virtual function table. Following this, there is another dereference to invoke the function itself.</p> <p><strong>Note.</strong> A pointer of the parent class can hold an object of the inherited class but not the other way around. However, when the pointer for the parent class is used, the functions in the inherited class that are not present in the parent class cannot be invoked. That is, the pointer will only be able to invoke functions from the parent class.</p> <p><strong>Note.</strong> Functions of the parent class can be overridden without using virtual functions. Non-virtual functions are copied in each class. However, pointer references are used in the case of virtual functions. In the case of virtual functions, the pointer of the base class holding an inherited class object will call the functions of the inherited class and not the base class.</p> <p>For example, consider the following</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">A</span> <span class="p">{</span>
<span class="nl">public:</span>
        <span class="k">virtual</span> <span class="kt">int</span> <span class="n">f</span><span class="p">()</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">1</span><span class="p">;</span> <span class="p">}</span>
        <span class="kt">int</span> <span class="nf">g</span><span class="p">()</span> <span class="p">{</span><span class="k">return</span> <span class="mi">2</span><span class="p">};</span>
<span class="p">};</span>

<span class="k">class</span> <span class="nc">B</span> <span class="o">:</span> <span class="k">public</span> <span class="n">A</span> <span class="p">{</span>
<span class="nl">public:</span>
        <span class="kt">int</span> <span class="n">f</span><span class="p">()</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">10</span><span class="p">;</span> <span class="p">}</span>
        <span class="kt">int</span> <span class="nf">g</span><span class="p">()</span> <span class="p">{</span> <span class="k">return</span> <span class="mi">20</span><span class="p">;</span> <span class="p">}</span>
<span class="p">};</span>
<span class="p">...</span>
<span class="n">A</span><span class="o">*</span> <span class="n">p</span><span class="p">;</span> <span class="n">B</span> <span class="n">b</span><span class="p">;</span>
<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">p</span> <span class="o">-&gt;</span> <span class="n">f</span><span class="p">();</span> <span class="c1">// Prints 10</span>
<span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">p</span> <span class="o">-&gt;</span> <span class="n">g</span><span class="p">();</span> <span class="c1">// Prints 2</span>
</code></pre></div></div> <blockquote> <p>Where are all the unique implementations of virtual functions stored?</p> </blockquote> <h2 id="optimisations">Optimisations</h2> <h2 id="register-allocation">Register Allocation</h2> <p>Accessing values from the registers in much faster than from the cache.</p> <h3 id="glocal-register-allocation-using-graph-coloring">Glocal register allocation using Graph Coloring</h3> <p>We identify the live ranges for each variable, and then construct an interference graph between variables if there is any overlap in the live ranges. Then, we use graph coloring to register allocation. However, graph coloring with \(k\) colors is NP-complete general. We use some heuristics, and we shall study one of them called - Chaitin-Briggs allocator. The problem is decidable for <strong>chordal graphs</strong> - Every cycle of length 4 or more has a chord connecting two nodes with an edge that is not part of the cycle (applies recursively). Most practical interference graphs are chordal.</p> <h3 id="chaitin-briggs-register-allocator">Chaitin-Briggs Register Allocator</h3> <ol> <li> <p><strong>Coalescing</strong></p> <p>We eliminate copy statements <code class="language-plaintext highlighter-rouge">x = y</code> so that we use the sam register for both the variables. Then, the <strong>copy propagation optimisation</strong> replaces the used of <code class="language-plaintext highlighter-rouge">x</code> by <code class="language-plaintext highlighter-rouge">y</code>.</p> </li> <li> <p><strong>Identification of live ranges</strong></p> <p>It is the sequences of statements from a definition of a variable to its last use of that values. We shall discuss <strong>live variables analysis</strong> later.</p> </li> <li> <p><strong>Identification of interference and construction of interference graph</strong></p> <p>Live ranges \(I_1\) and \(I_2\)</p> </li> <li> <p>Simplification of interferences graph to identify the order in which the nodes should be colored.</p> </li> </ol> <h3 id="copy-propagation-optimisation">Copy Propagation Optimisation</h3> <p>We assume intra-procedural lines of code. That is, the parts of code without any function calls.</p> <blockquote> <p>Slides - Why <code class="language-plaintext highlighter-rouge">e3</code> and not <code class="language-plaintext highlighter-rouge">e</code>? The number <code class="language-plaintext highlighter-rouge">3</code> represents the statement number where <code class="language-plaintext highlighter-rouge">e</code>was generated.</p> </blockquote> <h3 id="discovering-live-ranges">Discovering Live Ranges</h3> <p>Live ranges are calculated by traversing the code from the end to the beginning. We depict the live range as a set of statement indices. Then we check set intersection for interferences.</p> <h1 id="lecture-22">Lecture 22</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">06-04-22</code></p> </blockquote> <p>Following the analysis of live ranges, we construct the interference graph. Along with the degree of each node, we note the number of loads and stores of each variable. We define <strong>spill cost</strong> as the sum of loads and stores for each variable.</p> <h3 id="chaitin-briggs-allocator">Chaitin-Briggs Allocator</h3> <p>Let \(k\) be the number of colors. We discuss Chaitin’s method first.</p> <ol> <li> <p>We simplify the graph by removing the nodes in an arbitrary order such that for each node \(n\), \(D(n) &lt; k\) and push them no a stack. Since \(D(n) &lt; k\), we are guaranteed to find a color for \(n\).</p> <blockquote> <p>How to prove?</p> </blockquote> </li> <li> <p>If the graph is not empty, we find the node with the lease spill cost, and then we spill it. That is, we further simplify the graph by removing nodes with the lowest spill cost. Then, we repeat the first step.</p> </li> <li> <p>Now, after all the simplification is done, we repeatedly pop the node from the top of the stack, plug it in the graph and give it a color distinct from its neighbour.</p> </li> </ol> <p>Now, let us see Briggs’ method</p> <ol> <li>Same as that of Chaitin’s method.</li> <li>Briggs’ conjectured that it is not necessary for all nodes with a degree greater than \(n\) not to have a color in the coloring. So, we mark the nodes as potentially spill-able and stack them.</li> <li>As in Chaitin’s method, we repeatedly color the nodes. If a node cannot be colored, we spill it and go back to the first step.</li> </ol> <h3 id="spilling-decisions">Spilling Decisions</h3> <ul> <li> <p>Spill cost is weighted by loop nesting depth</p> \[C(n) = (L(n) + D(n)) \times 10^d\] </li> <li> <p>Sometimes, we normalise \(C(n)\) and consider \(C(n)/D(n)\).</p> </li> </ul> <p>Chaitin’s method cannot color the square/diamond graph with 2 colors whereas Briggs’ method can.</p> <p><strong>Note.</strong> We consider the degree of nodes in the original graph. That is, when we simplify the graph in step 1, we don’t update the degrees of all the nodes at each iteration.</p> <h3 id="allocating-registers">Allocating registers</h3> <p>Once we allocate the colors, we replace each variable by a register corresponding to a color. After replacing the code with registers, there would be redundant statements like <code class="language-plaintext highlighter-rouge">r4 = r4</code>. We can get rid of such statements, and this is known as <strong>Peephole Optimisation</strong>. (This happens due to control transfer)</p> <h3 id="live-range-spilling">Live range spilling</h3> <p>Spilling a live range \(l\) involves keep the variable of \(l\) in the memory. For RISC architecture, load in a register for every read, store back in the memory for every write. For CISC architectures, we access directly from the memory.</p> <p>Spilling is necessary if the number of interfering live ranges at a program point exceeds the number of registers. <em>Flow sensitivity</em> (interval interferes with, say, 1 interval at any given time) vs <em>Flow insensitivity</em> (total number of overlaps is greater than 1).</p> <p>Splitting a live range \(l\) involves creating smaller live ranges \(l_1, \dots, l_k\) such that \(D(l_i) \leq D(l), 1 \leq i \leq k\). Live ranges \(l_i\) can participate in graph coloring if \(D(l_i) &lt; D(l)\). Therefore, we have a choice between splitting and spilling. The difference between the two approaches is shown here.</p> <p><img src="/assets/img/IPL/image-20220406114350123.png" alt="image-20220406114350123"></p> <h2 id="registers-across-calls">Registers across Calls</h2> <p>So far, we have seen allocation of local registers. Suppose we have the following situation. A function <code class="language-plaintext highlighter-rouge">g()</code> uses a register <code class="language-plaintext highlighter-rouge">r</code> and calls <code class="language-plaintext highlighter-rouge">f()</code> inside its body. Now, how do we manage <code class="language-plaintext highlighter-rouge">r</code> across the call?</p> <ul> <li>Procedure <code class="language-plaintext highlighter-rouge">g</code> saves it before the call and restores it after the call. However, <code class="language-plaintext highlighter-rouge">g()</code> does not know if <code class="language-plaintext highlighter-rouge">f()</code> requires <code class="language-plaintext highlighter-rouge">r</code>. Here, save and restore is redundant if <code class="language-plaintext highlighter-rouge">f</code> does not require <code class="language-plaintext highlighter-rouge">r</code>, and this is unavoidable. Also, it knows if the value in <code class="language-plaintext highlighter-rouge">r</code> is need after the call. Here, save and restore is redundant if <code class="language-plaintext highlighter-rouge">r</code> is not required across the call, and this is avoidable.</li> <li>Procedure <code class="language-plaintext highlighter-rouge">f</code> saves it at the start and restores it at the end. Now, <code class="language-plaintext highlighter-rouge">f</code> does not know if <code class="language-plaintext highlighter-rouge">g</code> uses <code class="language-plaintext highlighter-rouge">r</code> after the call, but it knows if <code class="language-plaintext highlighter-rouge">r</code> is required in <code class="language-plaintext highlighter-rouge">f</code>. Like before, the first problem is unavoidable but the second one is avoidable.</li> </ul> <p>Now, as both methods are functionally similar, the method used is a matter of convention. The <strong>architecture</strong> decides this for the system. So, we have</p> <ul> <li> <strong>Caller-saved register</strong> - Callee can use it without the fear of overwriting useful data</li> <li> <strong>Callee-saved register</strong> - Caller can use it without the fear of overwriting useful data.</li> </ul> <p>We use a caller-saved register <code class="language-plaintext highlighter-rouge">r</code> for values that are not live across a call. Then, <code class="language-plaintext highlighter-rouge">r</code> is not saved by the callee and need not be saved by the caller. We use a callee-saved register <code class="language-plaintext highlighter-rouge">r</code> for values that are live across a call. <code class="language-plaintext highlighter-rouge">r</code> is not saved by the caller, and it is saved by the callee only if it is needed.</p> <h3 id="integrating-with-graph-coloring">Integrating with graph coloring</h3> <p>To begin with, the live range of a callee saved register is the entire procedure body of <code class="language-plaintext highlighter-rouge">g</code>, and the live range of a caller saved register is the procedure body of <code class="language-plaintext highlighter-rouge">f</code>. We then construct the interference graphs with these additional live ranges.</p> <h1 id="lecture-23">Lecture 23</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">08-04-22</code></p> </blockquote> <h2 id="instruction-selection">Instruction Selection</h2> <p>We need to generate an assembly code from the “register code” we generated after register allocation. Floating point comparison takes 2 arguments whereas integer comparison takes 3 arguments.</p> <h3 id="integrated-instruction-selection-and-register-allocation-algorithms">Integrated Instruction Selection and Register Allocation Algorithms</h3> <ul> <li> <strong>Sethi-Ullman Algorithm</strong> - Used in simple machine models, and is optimal in terms of the number of instructions with the minimum number of registers and minimum number of stores. It is also linear in the size of the expression tree.</li> <li> <strong>Aho-Johnson Algorithm</strong> - This algorithm is applicable for a very general machine model, and is optimal in terms of the cost of execution. It is also linear in the size of the expression tree (exponential in the arity of instruction which is bounded by a small constant). The main motivation behind this idea is that a sequence of 4 instructions can be more efficient than 2 instructions.</li> </ul> <h3 id="sethi-ullman-algorithm">Sethi-Ullman Algorithm</h3> <p>WE have a finite set of registers \(r_0, \dots, r_k\) and countable memory locations. We will be using simple machine instructions like loads, store, and computation instructions (\(r_1 \; op \; k\), \(k\) can be a register or a memory location). The input to this algorithm is the expression tree (essentially the AST) without</p> <ul> <li>control flow (no ternary expressions)</li> <li>assignments to source variables inside an expression (so no side effects). Assignments are outside the expression.</li> <li>no function calls</li> <li>no sharing of values (trees, not DAGs). For example, we don’t have expressions like \(b \times c + d - b \times c\). Basically, we shouldn’t use <em>common subexpression elimination</em>.</li> </ul> <p>The key idea behind this algorithm is that the order of evaluation matters. Sometimes, the result is independent of the order of evaluation of some subtrees in the tree. For example, in \(b \times c + a /d\), the order of evaluation of \(b \times c\) and \(a/d\) does not matter.</p> <p>Therefore, we choose the order of evaluation that minimises the number of registers so that we don’t store an intermediate result in the memory.</p> <p>In the algorithm, we traverse the expression tree bottom up and label each node with the minimum number of registers needed to evaluate the subexpression rooted at the node. Then, we traverse the tree top down and generate the code. Suppose we have</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>				op
			/    \
		l1      l2
</code></pre></div></div> <p>Assume \(l_1 &lt; l_2\). If we evaluate the left subtree first, we need \(l_1\) registers to evaluate it, 1 register to hold its result and \(l_2\) registers to evaluate the right subtree. Therefore, the total registers used would be \(\max(l_1, l_2 + 1)\) = \(l_2 + 1\). If we follow the other order, we get the number of registers as \(\max(l_2, l_1 + 1) = l_2\). Therefore, we <strong>evaluate the subtree with larger requirements first</strong>. So, we have the following recursion</p> \[label(n) = \begin{cases} 1 &amp; n \text{ is a leaf and must be in a register n or it is a left child} \\ 0 &amp; n \text{ is a leaf and can be in memory or it is a right child} \\ max(label(n_1), label(n_2)) &amp; n \text{ has two child nodes with unequal labels} \\ label(n_1) + 1 &amp; n \text{ has two children with equal labels} \\ \end{cases}\] <p>If we generalise to instructions and trees of higher arity, then for node \(n\) with \(k\) children we get</p> \[label(n) = \max(l_j + j - 1), 1 \leq j \leq k\] <p>Note that the above algorithm generates a store free program. Why aren’t we interleaving the codes of two subtrees? There is a notion of contiguity. Also, we are assuming that memory does not add any additional overhead in terms of CPU cycles.</p> <p>How do we generate the code for the tree? <code class="language-plaintext highlighter-rouge">rstack</code> is a stack of registers, and <code class="language-plaintext highlighter-rouge">gencode(n)</code> generates the code such that the result of the subtree rooted at \(n\) is contained in <code class="language-plaintext highlighter-rouge">top(rstack)</code>. <code class="language-plaintext highlighter-rouge">tstack</code> is a stack of temporaries used when the algorithm runs out of registers. <code class="language-plaintext highlighter-rouge">swap(rstack)</code> swaps the two top registers in <code class="language-plaintext highlighter-rouge">rstack</code>. Finally, the procedure <code class="language-plaintext highlighter-rouge">emit</code> emits a single statement of the generated code.</p> <p>Then, we have</p> <table> <thead> <tr> <th>Cases</th> <th><code class="language-plaintext highlighter-rouge">gencode(n)</code></th> </tr> </thead> <tbody> <tr> <td>\(n\) is a left leaf</td> <td> <code class="language-plaintext highlighter-rouge">emit(top(rstack) = name)</code>. This invariant that the top of the <code class="language-plaintext highlighter-rouge">rstack</code> is the result is maintained.</td> </tr> <tr> <td>The right child of \(n\) is a leaf</td> <td><code class="language-plaintext highlighter-rouge">emit(top(rstack) = top(rstack) op r.name)</code></td> </tr> <tr> <td>\(l_1 \geq l_2\) for children \(l_1, l_2\) of \(n\)</td> <td> <code class="language-plaintext highlighter-rouge">gencode(n_1)</code><br><code class="language-plaintext highlighter-rouge">R = pop(rstack)</code><br><code class="language-plaintext highlighter-rouge">gencode(n_2)</code><br><code class="language-plaintext highlighter-rouge">emit(R = R op top(rstack))</code><br><code class="language-plaintext highlighter-rouge">push(R, rstack)</code> </td> </tr> <tr> <td>\(l_1 &lt; l_2\)</td> <td> <code class="language-plaintext highlighter-rouge">swap(rstack)</code><br><code class="language-plaintext highlighter-rouge">gencode(n_2)</code><br><code class="language-plaintext highlighter-rouge">R = pop(rstack)</code><br><code class="language-plaintext highlighter-rouge">gencode(n_1)</code><br><code class="language-plaintext highlighter-rouge">emit(top(rstack) = top(rstack) op R</code><br><code class="language-plaintext highlighter-rouge">push(R, rstack)</code><br><code class="language-plaintext highlighter-rouge">swap(rstack)</code> </td> </tr> <tr> <td>Both children need more registers than available</td> <td> <code class="language-plaintext highlighter-rouge">gencode(n_2)</code><br><code class="language-plaintext highlighter-rouge">T = pop(tstack)</code><br><code class="language-plaintext highlighter-rouge">emit(T = top(rstack)</code><br><code class="language-plaintext highlighter-rouge">gencode(n_1)</code><br><code class="language-plaintext highlighter-rouge">emit(top(rstack) = top(rstack) op T)</code><br><code class="language-plaintext highlighter-rouge">push(R, rstack)</code><br> </td> </tr> </tbody> </table> <p>In the above, <code class="language-plaintext highlighter-rouge">R</code> can be seen as a local static variable of the procedure. In the last case, we evaluated the right child first because only the right child can be a memory argument.</p> <h1 id="lecture-24">Lecture 24</h1> <blockquote> <p><code class="language-plaintext highlighter-rouge">13-04-22</code></p> </blockquote> <h2 id="analysis-of-sethi-ullman-algorithm">Analysis of Sethi-Ullman algorithm</h2> <p>The register usage in the code fragment for a tree rooted at \(n\) can be described by</p> <ul> <li>\(R(n)\) the number of registers used by the code</li> <li>\(L(n)\) the number of registers live after the code (the intermediate results that are required later)</li> <li>The algorithm minimises \(R(n)\) to avoid storing intermediate results.</li> </ul> <p>If the code computes the left child \(n_1\) first then, \(R(n) = \max(R(n_1), L(n_1) + R(n_2))\) and vice versa for the right child \(n_2\). In order to minimise \(R(n)\), we minimise \(L(n_1)\) and \(L(n_2)\). How do we do that?</p> <ul> <li> <p><strong>Contiguous Evaluation</strong> - We evaluate \(n_1\) completely before evaluating \(n_2\) or vice versa. The reason this minimises the registers is that when we evaluate a subtree completely, we need to hold only the final result in a register during the evaluation of the other subtrees. Otherwise, we may have to hold multiple intermediate results in a register during the evaluation of the other subtree.</p> </li> <li> <p><strong>Strongly Contiguous Evaluation</strong> All subtrees of the children are also evaluated contiguously.</p> </li> </ul> <p>In Sethi-Ullman algorithm, each node is processed exactly, and hence the algorithm is linear in the size of the tree. Also, recall that we always evaluate the lowermost subtrees first for optimisation as mentioned somewhere before.</p> <h3 id="arguing-the-optimality">Arguing the Optimality</h3> <p>We define a node \(n\) to be a</p> <ul> <li> <strong>dense node</strong> - if \(label(n) \geq k\)</li> <li> <strong>major node</strong> - if both of its children are dense. A major node falls in case 5 of the algorithm.</li> </ul> <p>where \(k\) is the number of registers, and \(label\) refers to the number of registers required at this node. Note that every major node is dense but not vice-versa. The parent of every dense node is dense but the parent of every major node need not be major! Also, these categories are dynamic. That is, when we store a dense node, the parent of this node can cease to be a major node. The major nodes decrease by <strong>at most 1</strong> when we store a node.</p> <p>Now, the algorithm generates</p> <ul> <li>exactly one instruction per operator node</li> <li>exactly one load per left leaf</li> <li>no load for any right leaf</li> </ul> <p>The algorithm is optimal with regards to these counts. The optimality now depends on not introducing extra stores. Consider an expression tree with \(m\) major nodes.</p> <ul> <li>A store can reduce the number of major nodes by at most one. This is because, the node that becomes non-major, still remains a dense node so its parents remain a major node.</li> <li>Hence, the tree would need at least \(m\) stores regardless of the algorithm used for generating code</li> <li>The algorithm generates a single store for every major node as part of Case 5, thus it generates exactly \(m\) stores</li> <li>Since this is the smallest number of stores possible, the algorithm is optimal.</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/machine-learning-systems/">Key Works in ML Systems</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/data-systems-for-ml/">Data Systems for Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ai-agents/">AI Agents</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/rl-theory/">Reinforcement Learning Theory</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/brains-and-ai/">Brains and AI</a> </li> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Sudhansh Peddabomma. Last updated: March 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script src="/assets/js/tooltips-setup.js?53023e960fbc64cccb90d32e9363de2b"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-0K9MLG0V24');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> <div id="chat-window" class="chat-window"> <div class="chat-header"> <h5 class="mb-0">Talk to my AI</h5> <button id="close-chat" class="btn-close"> <i class="fas fa-times"></i> </button> </div> <div id="chat-messages" class="chat-messages"></div> <div class="chat-input-container"> <input type="text" id="chat-input" class="form-control" placeholder="Type a message..."> <button id="send-btn" class="btn btn-primary"> <i class="fas fa-paper-plane"></i> </button> </div> </div> </body> </html>