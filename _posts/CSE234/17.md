Getting back to our context, let us talk about LLM inference. 

We mentioned KV-cache before - It stores the previously calculated attention values during the next-token generation to reuse the values -

![](assets/17417436164038.gif)


There are issues with this approach is
1. Compute: The arithmetic intensity in the GPU is decreased, and GPU may be under-utilized. When $$n = 1, s = 1$$, the GPU is underutilized. 
2. Memory: Need to store KV cache until the request is completed
3. Communication: Mostly same 


Note that the batch size during inference is dependent on the application request load - It is a tradeoff between efficiency and latency reduction as we have mentioned before. We have also seen how speculative decoding is used to decrease the latency in case of single requests. Now, we will consider the other side - how do we handle a large number of requests while keeping the cost-per-query less. Let us again consider the three elements to find the potential bottleneck during inference

1. Compute:
    1. Pre-fill - Requests have different lengths. How do we batch?
    2. Decode - We do not know the number of tokens that need to be generated ahead of time. 
2. Memory: KV-Cache is an issue for these large batch sizes.
3. Communication: Remains the same. 

Recent works have addressed these issues, and the requirements for LLMs have almost decreased $$60 \times$$. Speculative decoding does not work for large batch sizes - it is being explored in research currently. 

### Continuous Batching
Suppose we have two prompts that are passed to the server. 
1. We cannot batch the sequences for attention if they have different number of tokens. So, here arises a crucial loss in performance - whether to pad all the short requests to make everything equal.
2. However, since MLPs are token independent, the requests can be batched! No loss in performance here.

Due to this difference, the current implementations perform cyclic batching and de-batching to execute attention of different requests on different ranks, and then merge all requests to get the MLP output.

Suppose a new request enters while the earlier requests are being processed - the new request can be easily be batched for the MLP phases after the attention has been calculated separately. In standard pipelines without continuous batching, such behavior is not allowed - a batch is issued and run until completion. A new request in the queue cannot enter and a request that finished early cannot exit.

Recall that when we calculated the FLOPs for a transformer, the MLP requires the maximum number of FLOPs. So, batching these stages results in huge gains in performance.

*Note.* The pre-fill and decoding phases for attention are superficially the same for this setup - they are separately calculated on each kernel before being batched for MLP passes. 

So essentially, we have introduced a lot of flexibility into transformer forward passes which were previously treated as atomic. We have improved GPU utilization with two key insights 
1. Attention consumes small percentage of FLOPs for medium context
2. MLP kernels are agnostic to sequence dimension

### KV-Cache Key Insight
KV Cache needs to be efficiently managed for high-throughput LLM serving. The KV Cache can become super large for long sequences. 

![](assets/17417463235738.jpg)
The throughput of the GPU is proportional to the batch size. That is, the execution time for 40 batches would be same as that of 8 batches because of how a GPU functions!

> Note. Why is the throughput higher for higher batch sizes?


So, if we can reduce the memory usage of KV-Cache, we can process more batches in the same time! How do we do this?

1. **Internal Fragmentation** - Memory for KV Cache is over-allocated because we do not know the output length.
2. **Reservation** - Memory that is not used at the current step (but will be needed in the future) is wasted

![](assets/17417467240834.jpg)

There is also external fragmentation that demarcates areas of different requests. Due to all these different things - only 20-40% of KV cache is used to store the tokens!!

### vLLM - Paged Attention
Drawing inspiration from virtual memory and paging in the OS, we improve how we store the values in KV cache. 

![](assets/17417467446680.jpg)

Deep-learning workflows typically consider contiguous blocks of memories. Breaking that standard, we store KV cache using continuous keys and values in non-contiguous memory space. Similar to OS mechanisms, we use logical and physical token blocks connected through a *page table*. Refer to [my notes on OS](blog/operating-systems) to understand these better.

![](assets/17417470519278.jpg)

Through this, we allocated memory on demand and minimize the internal fragmentation! We use CPU to orchestrate all this. There is no external fragmentation at all! With this, we have the following results -

![](assets/17417471345506.jpg)

### Prefill-decode disaggregation

To better benchmark LLM-based services, we consider new service based metrics
1. TTFT - Time to first token
    For a chatbot, we need to have fast initial response. For tasks like summarization, it can be slower.
2. TPOT - Time per output token
    Generate as fast as human reading speed for chatbots. For summarization, needs to be super fast. 
    
Any requests completed within Service Latency Output (SLO) are factored into *goodput*. High throughput can still have low goodput. We discussed continuous batching, but it has the following limitations
1. Prefill - Compute bound, one prefill saturates compute.
2. Decode - Memory bound, need to batch many requests together to saturate compute.

![](assets/17417478372758.jpg)
