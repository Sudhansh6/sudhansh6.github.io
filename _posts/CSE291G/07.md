The convergence proofs for these algorithms is not mathematically rigorous. It is a good area of research to find better proofs or better yet, more efficient algorithms.

How do we execute these algorithms in practice? Theoretically, setting the upper limit of $$j$$ to $$\infty$$ is the best estimate. However, $$\lambda$$ acts like a discount factor, and if the rewards are sparse (towards the end of the episode), then the convergence would take a long time - *episodic* algorithms. 

In essence, this version of the algorithm requires many forward executions of the simulation. In many cases, since this is infeasible in practice, developers have started using a *backwards version* of the algorithm. 

We define the eligibility trace of a state $$e_t(s) = \gamma \lambda e_{t - 1}(s) + 1$$. The update equation becomes

$$
 V_{t + 1}(s) \to V_t(s) + \alpha \delta_t e_t(s)
$$

where $$\delta_T = R_t + \gamma V_{t + 1} (S_{t + 1}) - V_{t - 1}(S_t)$$. 

These equations are almost same as the previous algorithm but with better practice implementation. (When you expand the formulae, and interchange the summations, this is what you get).

