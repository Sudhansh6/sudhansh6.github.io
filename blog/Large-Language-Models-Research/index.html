<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Large Language Model Reasoning | Sudhansh Peddabomma</title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="A survey of papers to better understand the workings of Large Language Models."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/Large-Language-Models-Research/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> <script src="/assets/js/chat.js?e73db4280bae3cbae4d78219277155b9"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Sudhansh Peddabomma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Articles</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Large Language Model Reasoning</h1> <p class="post-meta">October 14, 2024</p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> ¬† ¬∑ ¬† <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> Research</a> ¬† </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>As a part of this article, we delve into the paradigm of Chain of Though reasoning in Large Language Models. The aim is to highlight the importance of this idea and summarize the main research in this area. The blog should provide enough context for the reader in the field of AI to understand the basic concepts and think about the potential research ideas addressing the limitations of the current models.</p> <h1 id="chain-of-thought-reasoning"><a href="https://arxiv.org/pdf/2201.11903" rel="external nofollow noopener" target="_blank">Chain of thought Reasoning</a></h1> <p>Chain of thought (CoT) refers to manifesting the human thought process in large language models by endowing language models with the ability to generate a chain of thought - a coherent series of intermediate reasoning steps.</p> <p>It is hypothesized that CoT prompting helps LLMs to tackle complex arithmetic, commonsense and symbolic reasoning tasks. The following demonstration highlights this improvement.</p> <p><img src="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&amp;w=1920&amp;q=75" alt="COT"></p> <p>However, there are some limitations with this paradigm of reasoning with the current models.</p> <ul> <li>Small models are unable to improve with CoT prompting. LLMs with more than 100B parameters show performance gains with CoT.</li> <li>The performance improvements are larger with larger models. In other words, the benefits of CoT scale with the size of the models.</li> <li>Sometimes the models arrive at the correct answers with the wrong reasoning. The errors have been classified as <ul> <li> <strong>Calculation error</strong> - LLMs are probabilistic models, predicting what token occurs next. So when an LLM tries to do \(3* 25* 8 =\), it does not really calculate the answer but probabilistically guesses the answer which is the next token. This highlights a fundamental limitation in the current architectures of LLMs.</li> <li> <strong>Symbol mapping error</strong> - When there are too many variables involved, LLMs sometimes mix up the variables and arrive at the wrong answer. Again, the problem arises from the fundamental architecture flaw highlighted in the previous point.</li> <li>Other than these major errors, the models also have semnatic understanding problems, missing steps, incoherent chain of thought errors</li> </ul> </li> </ul> <h1 id="large-language-models-are-human-level-prompt-engineers"><a href="https://arxiv.org/abs/2211.01910" rel="external nofollow noopener" target="_blank">Large Language Models are Human-level prompt engineers</a></h1> <p>The motivation of this paper is as follows -</p> <ul> <li> <p><strong>Human effort in prompt engineering</strong> - Crafting effective prompts for LLMs is time-consuming and requires significant human expertise.</p> </li> <li> <p><strong>Optimization challenge</strong> - Primpts greatly influence LLM performance, but users often lack insight into how to optimize them for specific tasks.</p> </li> <li> <p><strong>Scalability</strong> - As LLMs grow in size and capabilities, manuallt designing prompts becomes less deasible for a wide range of applications.</p> </li> <li> <p><strong>Automating promtp design</strong> - There is a growing need to automate the prompt engineering process to enhance LLM usability and performance.</p> </li> <li> <p><strong>Real-world impact</strong> - Applications in diverse domains (e.g., AI chatbots, automated content generation) can benefit from optimized and automated prompts.</p> </li> </ul> <p>This work promposes an <strong>Automatic Prompt Engineer (APE)</strong> - asystem that automates prompt generationg and selection for Large Language Models. This task is treated as a program synthesis task wherein the input-output pairs (natural language questions and answers) are given to the APE, and it has to generate the instruction needed to generate these pairs.</p> <p>In essence, the APE is trying to learn the prompts generated by humans. The framework is as follows -</p> <ol> <li> <p>Instruction Generation. An LLM is used as an ingeerence model where the ‚Äúinstruction candidates‚Äù are generated based on a small set of input-output demonstrations</p> <p>Example: The input to APE is of the form -</p> <p><em>Input 1</em> - Forward generation technique</p> <p>‚Äù‚Äù‚Äù</p> <p>I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs</p> <p>Input: [ ] Output: [ ]</p> <p>Input: [ ] Output: [ ]</p> <p>‚Ä¶</p> <p>The instruction was &lt;COMPLETE&gt;</p> <p>‚Äù‚Äù‚Äù‚Äù</p> <p><em>Input 2</em> - Reverse generation technique</p> <p>‚Äù‚Äù‚Äù</p> <p>I instructed my friend to &lt;INSERT&gt;</p> <p>The friend read the instructions and wrote an output for every one of the inptus. Here are the input-output pairs:</p> <p>Input: [ ] Output: [ ]</p> <p>Input: [ ] Output: [ ]</p> <p>‚Ä¶</p> <p>‚Äù‚Äù‚Äù</p> </li> <li> <p>Scoring Instructions. Evaluate each instruction by computing a score that reflects how well the instruction guides the target LLM for the task. This is simply the confidence score associated with the log likelihoods of token generation. The authors consider a <em>moving average</em> score considering the probabilities for a window of tokens.</p> <p>They also consider an <strong>execution accuracy</strong> - the success of an instruction by checking if the model produces the correct output (0-1 loss). However, this cannot. be used for all kinds of instructions.</p> <p>The top \(k\)-percentile prompts are selected and the rest are discarded.</p> </li> <li> <p>LLM as Resampling Model. They apply an Iterative Monte search method to resample more prompts. The LLM generates semnatically similar instructions variants to improve the top-performing candidates.</p> <p>Once the prompts are generated, the moving average scores are generated for each of the prompts and the better scoring prompts are selected again.</p> </li> </ol> <p>Can APE be used to guide LLMs?</p> <p><img src="/assets/img/LLMs/2024-10-23-10-41-54-image.png" alt=""></p> <p>Although this is a very simple example, the work shows potential in taking such framework forward to work with more complex applications.</p> <p>Another interesting approach is to not generate the prompts from scratch, but to help humans design better prompts. Essentially, augment with context from humans to generate better prompts. On the flipside, RLHF can be used to improve these APE.</p> <h1 id="tree-of-thoughts"><a href="https://arxiv.org/abs/2305.10601" rel="external nofollow noopener" target="_blank">Tree of Thoughts</a></h1> <p>The early Language Models were limited by their token-level, left-to-right decision making. However, some tasks require exploration, stratefic lookahead, planning, and backtracking. The vanilla architecture does not support such mechanisms.</p> <h2 id="framework">Framework</h2> <p><img src="/assets/img/LLMs/2024-10-30-09-36-33-image.png" alt=""></p> <p>Mathematically, these models are depicted as</p> <ul> <li> <p>Input output programming - \(y \sim p_\theta^{IO}(y \vert x)\)</p> </li> <li> <p>Chain</p> </li> </ul> <p>Theoretically, the model seems promising. However, there are some intricate details that need to be figured out -</p> <ul> <li> <p>How to decompose the porcess into steps</p> </li> <li> <p>How to generate the potential thoughts from each state</p> <ul> <li> <p>Need to be small enough so LMs can generate promising and diverse samples</p> </li> <li> <p>Big enough so LMs can evaluate the difference contributing to the results</p> </li> </ul> </li> <li> <p>How to heuristically evaluate each state</p> </li> <li> <p>How to navigate through the generated tree</p> </li> </ul> <h2 id="thought-decomposition">Thought decomposition</h2> <p><img src="/assets/img/LLMs/2024-10-30-09-41-30-image.png" alt=""></p> <h3 id="method-1---direct-prompting">Method 1 - Direct Prompting</h3> <p>The prompts themselves can ask the LM to segment the problem into multiple problems. Due to the voting mechanism, LM generates multiple possibilities for an answer and chooses the best model. This works better when thought space is rich and i.i.d samples lead to diversity.</p> <h3 id="method-2---backtracking">Method 2 - Backtracking</h3> <p>Propose thoughts sequentially using a ‚Äúpropose prompt‚Äù. When the thought space is constrained, this works better - proposing different thoughts in the same context avoids duplication.</p> <h2 id="state-evaluator">State Evaluator</h2> <p>There are two strategies to evaluate each generated state</p> <ul> <li> <p><strong>Value</strong> each state independently, where a value prompt reasons about the state \(s\) to generate a scalar value \(v\). This value is very context dependent.</p> </li> <li> <p><strong>Vote</strong> across states by deliberately comparing different states in \(S\) in a vote prompt.</p> </li> </ul> <h2 id="search-algorithm">Search Algorithm</h2> <ul> <li> <p><strong>BFS</strong> is helpful when the tree depth is limited and the initial thought steps can be evaluated and pruned to a small set</p> </li> <li> <p><strong>DFS</strong> explores longer trees well - subtrees are pruned to trade exploration for exploitation.</p> </li> <li> <p>More advanced approaches such as \(A^*\) and MCTS are left to future work in the paper.</p> </li> </ul> <p>An interesting summary of all thought paradigms - <a href="https://arxiv.org/pdf/2401.14295" rel="external nofollow noopener" target="_blank">Demytifying Chains, Trees, and Graphs of Thoughts</a>.</p> <h1 id="on-second-thought-lets-not-think-step-by-step-bias-and-toxicity-in-zero-shot-reasoning"><a href="https://urldefense.com/v3/__https://arxiv.org/abs/2212.08061__;!!Mih3wA!FUSiREHKHqULp_GaFY0sSmJRsiVZqYBdk9nJf8WWrKLI4UoxKUzc3ir1rIQaWXw6bk6_UVVe0kXW$" rel="external nofollow noopener" target="_blank">On Second Thought, Let‚Äôs Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</a></h1> <p>We have seen how chain of thought improves problem solving capabilities. However, in some cases, CoT actually causes issues -</p> <p><img src="/assets/img/LLMs/2024-10-30-10-17-51-image.png" alt=""></p> <p>The authors explore such effects in the paper. They consider</p> <h1 id="least-to-most-prompting-enables-complex-reasoning-in-large-language-models"><a href="https://arxiv.org/pdf/2205.10625" rel="external nofollow noopener" target="_blank">Least-to-most prompting enables complex reasoning in Large Language Models</a></h1> <p>The key motivators for the paper are as follows -</p> <ul> <li>Given a new task</li> </ul> <p>In the prior works, CoT reasoning has been effective for many tasks but struggled with ‚ÄúEasy-to-hard generalization‚Äù. Inspired from educational philosophies, the model is implemented by few-show ptompting in 2 stages - decomposition stage and subproblem solving stage.</p> <ul> <li> <p>Decomposition stage - The problem is divided into subtasks <em>once</em> before solving</p> </li> <li> <p>Subsequent solving stage - Solve the subsequent problems one by one.</p> </li> </ul> <p>The key difference from CoT prompting is that CoT starts each sub-problem from scratch and is unable to build from previous reasoning. This behaviour is depicted using the symbolic manipulation task in the paper - The performance of CoT progressively decreases as the length of the list increases.</p> <p>This method of prompting achieves significantly better results borrowing the context from the previous subproblems to arrive at the final answer.</p> <p>However, decomposition thoughts don‚Äôt generalize across domains. This limitation mainly shows up for math problems where the subproblems need to be correctly decomposed to solve the original problem.</p> <h1 id="chain-of-thoughtlessness"><a href="https://arxiv.org/pdf/2405.04776" rel="external nofollow noopener" target="_blank">Chain of Thoughtlessness</a></h1> <p>CoT prompting sounds too good to be true. The paper aims to test this paradigm rigorously to verify the claims. The paper also tries to identify the difference between complex reasoning and pattern matching - What seems like ‚Äúcomplex reasoning‚Äù may just be a case of pattern matching?</p> <p>Consider the chain of thought reasoning -</p> <ul> <li> <p>How specific do the prompt examples have to be to the original problem?</p> </li> <li> <p>How generalizable are these prompts or how specific do they need ot be?</p> </li> <li> <p>How much human effort is needed to craft prompts for each problem subclass?</p> </li> </ul> <p>Furthermore, there are issues with the test domains as well. For example, GSM8K</p> <ul> <li> <p>They are non scalable - problem instances cannot be scaled</p> </li> <li> <p>The problems are static and be easily found in the training data</p> </li> </ul> <p>The main point the paper is trying to address the question - ‚ÄúIs it really possible to teach an LLM how to solve a generalizable problem?‚Äù. To test this claim, the authors choose ‚ÄúBlocks world‚Äù as the problem domain - given an initial and end configuration, output a series of steps to reach the end configuration from the initial configuration.</p> <p>They perform the following experiments</p> <ul> <li> <p><strong>Zero shot CoT</strong> - Simply append ‚ÄúLet‚Äôs think step by step‚Äù to the prompts.</p> </li> <li> <p><strong>Progression proof</strong> - Specific to planning problems. Each example‚Äôs steps describe the init state, action taken, reason of the action and the final step.</p> </li> </ul> <p>They see that zero-shot CoT achieves insignificant performance gains from zero-shot prompting. The progression proof CoT achieves a lower performance - this may be due to overfitting to the training examples. The LLM fails to learn the <em>universal block algorithm</em> (break the tower and put everything back) even with multiple version of CoT prompting. The authors chose a planning domain on purpose because these problems can be scaled up very well.</p> <p>The authors just wanted to highlight that there is a need for more rigorous testing. One might argue that planning problems are way out of domain of LLMs. So, the authors test the findings with commonly tested problems, and they find similar trends.</p> <h1 id="chain-of-thought-without-prompting"><a href="https://arxiv.org/abs/2402.10200" rel="external nofollow noopener" target="_blank">Chain of Thought without prompting</a></h1> <p>Prompting techniques, while effective, often encode task-specific human priors, thereby making it difficult to assess a language model‚Äôs intrinsic reasoning abilities. Ideally, a language model should be able to reason independently and provide the optimal response, without requiring humans to tweak the prompts or refine repeatedly if the initial response is unsatisfactory. Model-tuning can be expensive and requires a substantial amount of supervised data. In this work, we explore a different perspective and ask: Can LLMs reason effectively without prompting? And to what extent can they reason? We find that, perhaps surprisingly, there exists a task-agnostic way to elicit CoT reasoning from pre-trained LLMs by simply altering the decoding procedure. Figure 1 illustrates this phenomenon: given a reasoning question, the LLM generates a wrong answer via the standard greedy decoding path, yet alternative top-ùëò token inspection unveiled inherent CoT paths (e.g., decoding paths 2 and 4), which accurately resolved the query. This decoding modification bypasses prompting and is entirely unsupervised without the need for model tuning.</p> <p><strong>Why can‚Äôt LLMs reason if we only consider greedy decoding path?</strong></p> <h1 id="large-language-models-are-zero-shot-reasoners"><a href="https://arxiv.org/abs/2205.11916" rel="external nofollow noopener" target="_blank">Large Language Models are Zero-shot reasoners</a></h1> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Statistical-NLP/">Statistical Natural Language Processing</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Design-and-Analysis-of-Algorithms/">Design and Analysis of Algorithms</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Object-Detection/">Object Detection</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Lattices-in-Cryptography/">Lattices in Cryptography and Quantum Computers</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/GANs-for-Compressed-Sensing/">Generative Adversarial Networks for Compressed Sensing</a> </li> </div> <script>document.querySelectorAll("#table-of-contents a").forEach(function(e){e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substring(1);document.querySelectorAll(".content-section").forEach(function(e){e.classList.add("hidden")});var n=document.getElementById(t);n&&n.classList.remove("hidden")})});</script> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Sudhansh Peddabomma. Last updated: January 18, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0K9MLG0V24");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <div class="chat-toggle-container"> <button id="chat-toggle-btn" class="chat-toggle-btn"> <i class="fas fa-comments"></i> </button> </div> <div id="chat-window" class="chat-window"> <div class="chat-header"> <h5 class="mb-0">Talk to my AI</h5> <button id="close-chat" class="btn-close"> <i class="fas fa-times"></i> </button> </div> <div id="chat-messages" class="chat-messages"></div> <div class="chat-input-container"> <input type="text" id="chat-input" class="form-control" placeholder="Type a message..."> <button id="send-btn" class="btn btn-primary"> <i class="fas fa-paper-plane"></i> </button> </div> </div> </body> </html>