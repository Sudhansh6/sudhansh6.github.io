<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Data Systems for Machine Learning | Sudhansh Peddabomma</title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content="Prompting ChatGPT is not enough. To build large-scale AI systems, it is imperative to understand how to design the proper systems to optimize all the computations. The following blog is a deep-dive into system/data design for Machine Learning frameworks."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/data-systems-for-ml/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> <script src="/assets/js/chat.js?e73db4280bae3cbae4d78219277155b9"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Sudhansh Peddabomma</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Articles</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Data Systems for Machine Learning</h1> <p class="post-meta">January 6, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/category/notes"> <i class="fa-solid fa-tag fa-sm"></i> Notes</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="introduction">Introduction</h1> <p>Machine Learning systems have played a pivotal role in the rapid adaptation of Ai in the world today. This domain is essential for solving future problems and also making the current architectures more efficient. This is crucial considering that companies are reactivating nuclear power plants to power AI in the real-world.</p> <p>Along with the progress in AI from small neural networks to large language models, there has been a development in the size of datasets as well. Big data arrived, and AI today relies on these internet-scale datasets. After all, doesn’t ChatGPT just do pattern-matching in the internet?</p> <p>Moreover, the compute capabilities have been scaling exponentially. Just last year (2024), NVIDIA released a new super-chip architecture <a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing" rel="external nofollow noopener" target="_blank">Blackwell</a> that has 97 billion transistors that can reach up to 1.4 exa-flops! The largest super-computer was barely able to reach 1 exa-flop. All this power in the palm of your hand…</p> <p>Richard Sutton once said, search and learning can scale unparalleled with growing computation power.</p> <p>Consider the year 2012 — AlexNet made waves showing SOTA capabilities with images. They use Stochastic Gradient Descent, dropout, convolution networks and initialization techniques. Without ML systems (CUDA, etc), the code would have been 44,000 lines with days of training! With these systems (Jax, PyTorch, TensorFlow) in place, you can achieve the same result in 100 lines within hours of training.</p> <h3 id="in-practice">In Practice</h3> <p>In industry, problems are typically of the form - improve the self-driving car’s pedestrian detection to be X-percent accurate at Y-ms latency budget. For an ML engineer is, the general approach is to design a better model with better learning efficiency followed by hyper-parameter running, pruning, distillation. An ML systems engineer would take the best model by ML researchers, specialize the implementation to target the H/W platform to reduce latency. <em>Streamlining the entire process from development to deployment</em>.</p> <h2 id="overview">Overview</h2> <p>From ad-hoc methods having diverse models and optimization algorithms with various data pre-processing techniques - we have arrived at an optimal algorithm that is <em>iterative and convergent</em>. As our models have become more and more specialized, the computation resources scaled exponentially.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17363053041704.jpg" alt=""></p> <p>Through the course of this article, we will cover deep learning basics, computational graphs, Autodiff, ML frameworks, GPUs, CUDA and collective communication.</p> <p>There are more related topics to the ones discussed here</p> <ul> <li>ML for systems</li> <li>ML Hardware design</li> </ul> <p>Unfortunately, the textbook for this content is just miscellaneous research papers.</p> <h1 id="background">Background</h1> <h2 id="dl-computation">DL Computation</h2> <p>The idea is to concatenate composable layers</p> \[\theta^{(t + 1)} = f(\theta^{(t)}, \nabla_L(\theta^{(t)}, D^{(t)})\] <p>A <strong>model</strong> is a parameterized function that describes how we map inputs to predictions. The parameters are optimized using optimization methods like <strong>SGD</strong>, Newton methods, etc. A <strong>loss function</strong> guides the model to give feedback on how well the model is performing.</p> <p>Having these basic definitions, we will build abstractions to map all the models being used today. It is not possible to build systems to support all models. A quick refresher of important models</p> <ul> <li> <strong>CNNs</strong> - Learnable filters to convolute across images to learn spatial features. The top 3 breakthrough architectures were - AlexNet, ResNet, U-Net. What are the important components in CNNs? <ul> <li>Convolution (1D, 2D, 3D)</li> <li>Matmul</li> <li>Softmax</li> <li>Element-wise operations - ReLU, add, sub, pooling, normalization, etc.</li> </ul> </li> <li> <strong>Recurrent Neural Networks</strong> - Many problems in nature are many-to-many. RNNs maintain an internal state that is updated as a sequence is processed. Arbitrary inputs and outputs can be generated, and any neural network can be used in the RNN architecture. The top 3 breakthrough architectures were - Bidirectional RNNs, LSTMs, GRU. What are the important components in RNNs? <ul> <li>Matmul</li> <li>Element-wise non-linear - ReLU, Sigmoid, Tanh</li> <li>Underlying MLP RNNs have a problem of forgetting (\(0.9*0.9*… \approx 0\)). Additionally, they lack <strong>parallelizability</strong> - both forward and backward passes have \(O(sequence length)\).</li> </ul> </li> <li> <strong>Transformers</strong> (Attention + MLP) - Treat representations of each element in the sequences as queries to access and incorporate information from a set of values. Transformers have an encoder part (BERT most famous) and a decoder part (GPT most famous). Along with these, DiT is one of the top 3 models. What are the important components in Transformers? <ul> <li>Attention - Matmul, softmax, Normalization</li> <li>MLP</li> <li>Layernorm, GeLU, etc.</li> </ul> </li> <li> <strong>Mixture of Experts</strong> - Voting from many experts is better than one expert. Latest LLMs are mostly MoEs - Grok, Mixtral, Deepseek-v3. A router (Matmul, softmax) is the novel component in MoE - it makes system design difficult.</li> </ul> <h2 id="machine-learning-systems">Machine Learning Systems</h2> <p>As mentioned before, the three pillars for the systems are data, model and compute. The foal is to express as manny as models as possible using one set of programming interface by connecting math primitives.</p> <h3 id="computational-dataflow-graph">Computational Dataflow Graph</h3> <p>A representation to show data flow in programs. A <strong>node</strong> represents the computation (operator) and an <strong>edge</strong> represents the data dependency (data flowing direction). A node can also represent the input/output tensor of the operator.</p> <h3 id="example-deep-learning-with-tensorflow-v1">Example: Deep learning with TensorFlow v1</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tinyflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span> <span class="c1"># Forward declaration 
</span><span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span><span class="n">tf</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">reduction_indices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># Loss function declaration 
</span><span class="n">W_grad</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">,</span> <span class="p">[</span><span class="n">W</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Automatic differentiation
</span><span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">assign</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">W_grad</span><span class="p">)</span> <span class="c1"># SGD update rule 
</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">})</span> <span class="c1"># Real-execution happens here
</span></code></pre></div></div> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17364786998993.jpg" alt=""></p> <p>This DAG representation opens up all possibilities of optimizations. However, creating such a graph doesn’t allow flexibility - once a graph is defined, it cannot be changed based on the input.</p> <h3 id="example-pytorch">Example: PyTorch</h3> <p>PyTorch also uses computational graphs, but it creates it on the fly. Previously, we had defined the graph and then executed it. Symbolic declaration vs imperative programming. Define-then-run vs Define-and-run. C++ vs Python.</p> <p>What are the pros and cons?</p> <table> <thead> <tr> <th> </th> <th>Good</th> <th>Bad</th> </tr> </thead> <tbody> <tr> <td>Symbolic</td> <td>Easy to optimize, much more efficient (can be 10x faster)</td> <td>The way of programming can be counter-intuitive, hard to debug and less flexible</td> </tr> <tr> <td>Imperative</td> <td>More flexible, easy to program and debug</td> <td>Less efficient and more difficult to optimize</td> </tr> </tbody> </table> <p>How does TensorFlow work in Python then? Tensorflow has Python as the interface language.</p> <p>Apart from these two famous frameworks, there were more like Caffe, DyNet, mxnet (has ability to switch between both), etc. Recently, Jax (derived from Tensorflow) has been getting more popular.</p> <h3 id="just-in-time-jit-compilation">Just-in-time (JIT) compilation</h3> <p>Ideally, we want define-and-run during development and define-then-run during deployment. However do we combine both? PyTorch introduced a deploy mode through a decorator <code class="language-plaintext highlighter-rouge">torch.compile()</code>. So is there an issue with JIT? It creates only static graphs, and cannot work with conditionals or loops in the code.</p> <h3 id="static-vs-dynamic-models">Static vs Dynamic models</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17364797895738.jpg" alt=""> Static graphs are defined and optimized only once. The execution follows a defined computation. On the other hand, dynamic graphs depend on the input. It is difficult to express complex flow-control logic and debug. The implementation is also difficult.</p> <p>As seen above, LSTMs are trying to replace the dynamics in the natural language problem.</p> <p><strong>How to handle dynamics?</strong></p> <ul> <li>Just do Define-and-run and forget about JIT - most popular unforunately :(</li> <li>Introduce Control Flow Ops - <ul> <li>Example: Switch and Merge. This can be added a computational primitive in the graph and introduce dynamics in the graph.</li> <li>These ideas are natural across all programming languages - conditionals and loops. However, the problem with this approach is that graphs becomes complex, and more importantly, how does we do back propagation? What is the gradient of “switch”? TensorFlow team has been working on this.</li> </ul> </li> <li>Piecewise compilation and guards - This approach is better adopted than control flow. <ul> <li>Case 1: A graph accepting input shapes of \([x, c1, c2]\) where \(x\) is variable. The solution is to compile for different values of \(x\) (powers of 2).</li> </ul> </li> </ul> <p>So far, we have seen representations that express the forward computations using primitives. But, how do we represent backward computations?</p> <h1 id="autodiff-ad">Autodiff (AD)</h1> <p>Derivative can be taken using the first order principles. However, this approach can be slow since we have to evaluate the function twice \(f(\theta + \epsilon) , f(\theta)\) and it is also error prone \(\theta(\epsilon^2)\).</p> <p>To optimize the derivative calculation, we pre store the gradients of primitives and map the derivative chain rules in the computational graph. There are two ways of doing this as well</p> <ol> <li>Calculating the derivative from left (inside) to right (outside) in a network - from inputs to outputs</li> <li>Calculating it from right to left - from outputs to inputs</li> </ol> <p>Both are valid approaches and we will discuss them in detail.</p> <h2 id="forward-mode-autodiff">Forward Mode Autodiff</h2> <p>We start from the input nodes, and derive the gradients all the way to the output nodes. <strong>Cons</strong> - - For \(f: R^n \to R^k\), we need \(n\) forward passes to get the gradients with respect to each input. - However, it is usually the case that \(k = 1\) (loss) and \(n\) is very large.</p> <blockquote> <p>If this is confusing, think of it this way - we want the gradient of output with respect to all parameters to update weights. However, forward mode calculates the gradient of inputs with respect to all parameters.</p> </blockquote> <h2 id="reverse-mode-autodiff">Reverse Mode Autodiff</h2> <p>We define the quantity <em>adjoint</em> \(\bar v_i = \frac{\partial y}{\partial v_i}\). We then compute each \(\bar v_i\) in the reverse topological order of the graph. This way, we can simply do one backward pass to get the necessary gradients.</p> <p>In some scientific scenarios, we can have \(k &gt;&gt; n\) where the forward mode can be more efficient.</p> <blockquote> <p>What are the size bounds of the backward graph as compared to the neural network?</p> </blockquote> <p>We construct backward graphs in a symbolic way to reuse it multiple times.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369099776396.jpg" alt=""></p> <h2 id="backpropagation-vs-reverse-mode-ad">Backpropagation vs. Reverse-mode AD</h2> <p>In old frameworks like Caffe/cuda-convnet, the backward computations were done through the forward graph itself. Newer frameworks like Tensorflow and PyTorch construct the backward graph explicitly. The reasons to do so are -</p> <ol> <li>Explicit graphs allow backward computation with any input values. They have flexibility to even calculate gradient of gradients.</li> <li>Having an explicit backward graph can help optimization!</li> <li>Gradient update rules can be efficiently implemented.</li> </ol> <h2 id="gradient-update-rules">Gradient update rules</h2> <p>Typically done via gradient descent, the weights are updated with the gradients with the following simplified rule</p> \[f(\theta, \nabla_l) = \theta - \eta \nabla_L\] <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369103443367.jpg" alt=""></p> <h1 id="architecture-overview-of-ml-systems">Architecture Overview of ML systems</h1> <p>The aim is to make the systems fast, scalable, memory-efficient, run on diverse hardware, energy efficient and easy to program/debug/deploy. Phew.</p> <p>We have discussed dataflow and Autodiff graphs. However, there are numerous things that can be added to these - graph optimization, parallelization, runtime memory, operator optimizations and compilation.</p> <h2 id="graph-optimization">Graph Optimization</h2> <p>The goal is to rewrite the original graph \(G\) as \(G’\) that is faster.</p> <p>Consider the following motivating example - Typically, convolution is followed by batch normalization. Instead of performing batch normalization, just update the weights in convolution to do everything in one step!</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369110087495.jpg" alt=""></p> <p>Note that some steps can become slower based on the hardware, but you get the general idea.</p> <p>Similarly, in attention calculations, the code is typically written with a concatenated vector of queries, keys and values. This version is optimal - it can be understood with <em>Arithmetic Intensity</em> which the ratio of #operations and #bytes. For example, an addition operation has intensity of \(1/3\) (2 loads and one store). However, fusing multiple arithmetic operations reduces the loads and stores by bringing all variables into memory once, improving the arithmetic intensity.</p> <h3 id="so-how-do-we-optimize-graphs">So how do we optimize graphs?</h3> <p>We write rules or templates for opportunities to simplify graphs. There is also implementation of <em>auto-discovering</em> optimizations in the latest libraries, we shall study these.</p> <h2 id="parallelization">Parallelization</h2> <p>The goal is to parallelize the graph computation over multiple devices. Note that devices can be connected with fast (memory communication NVLink) and slow connections (across GPUs), with up to 10x performance difference. Ideally, we do not want to describe partitioning rules for every new model that comes up. Based on these communication patterns, distributing the tasks is not an easy problem. So, we shall discuss how we partition the computational graph on a device cluster.</p> <h2 id="runtime-and-scheduling">Runtime and Scheduling</h2> <p>How do we schedule the compute, communication and memory in a way that execution is as fast as possible, communication is overlapped with compute and is subject to memory constraints?</p> <h2 id="operator-implementations">Operator Implementations</h2> <p>The goal is this layer is to get the fastest possible implementation of <code class="language-plaintext highlighter-rouge">matmul</code>s, for different hardware, different precision and different shapes.</p> <p>NVIDIA releases a GPU every 2 years, and they have rewrite all operations every time! Notably, previously, models were trained using 32-bit floating points, but now researchers are emphasizing on lower and lower precisions.</p> <p>Now, we shall delve into each of these architectures.</p> <h1 id="operator-optimization-and-compilation">Operator Optimization and Compilation</h1> <p>The goal is maximize arithmetic intensity. In general there are three ways to speed up operators</p> <h3 id="vectorization">Vectorization</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369120762344.jpg" alt=""></p> <p>The right version is faster because of the hardware - cache sizes, etc. Tensorflow and PyTorch have this built-in.</p> <h3 id="refactoring-data-layout">Refactoring data layout</h3> <p>This is again related to how data is stored in memory. For example, C++ stores matrices in row-major order. Accessing columns of a matrix can be 10x slower! Remember this while writing code to lower cache misses and reduce pointer movements.</p> <p>ML systems don’t store tensors in row or column major but in a new format called <strong>strides format</strong> - <code class="language-plaintext highlighter-rouge">A[i, j, …] = A.data[offset + i*A.strides[0] + j*A.strides[1] + …</code>. It is a generalization of row and column major storage, and it offers more flexibility - so based on the batch-sizes or other parameters in a neural network.</p> <p>Strides can separate the underlying storage and the view of the tensor. Consider the following operations</p> <ol> <li> <code class="language-plaintext highlighter-rouge">slice</code> - simply changing the offsets and shape will output the slice without any copying involved.</li> <li> <code class="language-plaintext highlighter-rouge">transpose</code> - modifying strides will transpose the tensor without any copying! For example, consider the following example <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>     <span class="n">M</span><span class="p">.</span><span class="nf">strides</span><span class="p">()</span> <span class="c1"># (24, 12, 4, 1)
</span>     <span class="n">M</span><span class="p">.</span><span class="nf">permute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
     <span class="n">M</span><span class="p">.</span><span class="n">t</span><span class="p">.</span><span class="nf">strides</span><span class="p">()</span> <span class="c1"># (12, 4, 1, 24)
</span></code></pre></div> </div> </li> <li> <code class="language-plaintext highlighter-rouge">broadcast</code> - Suppose we have to extend a tensor’s data across a dimension for performing operations with another tensor, then by simply adding <code class="language-plaintext highlighter-rouge">0</code> stride in the appropriate dimensions would be enough! Again, no copying</li> </ol> <p>Many more operations can be done without copying the data and simply modifying the strides. For example, consider the following example -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17370816868291.jpg" alt=""></p> <p>However, strides also has an issue - Memory access may become non-contiguous, and many vectorized ops require continuous storage.</p> <h3 id="summary">Summary</h3> <p>To make operators efficient, we have seen the following tactics -</p> <ol> <li>Vectorization - leverage platform-specific vectorized functions that reduce seek time</li> <li>Data layout - strides format that allow zero-copies enabling fast array-manipulations</li> <li>Parallelization on CPUs</li> </ol> <p>These were techniques for general operations. However, we can optimize certain operators with their special properties.</p> <h3 id="matmul-optimization">Matmul optimization</h3> <p>The brute-force approach takes \(\mathcal O(n^3)\). The best approach humans know is \(\mathcal O(n^{2.371552})\)!</p> <p>How to improve the speed in practice then? Recall that we are trying to increase AI = #ops/#bytes.</p> <blockquote> <p><strong>Memory Hierarchy</strong> If everything ran on registers, things would be super-fast. But, that is expensive. Remember that L1-Cache has 0.5ns latency, L2-Cache has 7ns and DRAM has 200ns (400x slower!)</p> </blockquote> <p>Let us analyze the AI of <code class="language-plaintext highlighter-rouge">matmul</code> considering the different layers of memory</p> <ol> <li>We can directly move data to registers in every iteration in inner loop</li> </ol> <h2 id="gpus-and-accelerators">GPUs and accelerators</h2> <p>Recall that parallelizing operations across threads is super useful! CPUs have some level of parallelism through SIMD operations (vectorization) but they are limited. Building on the same idea, GPUs were born.</p> <p>When we started out, the ALU units were limited by the physical space on the chips. As technology improved, we moved from 70nm process all the way 3nm process! That is, we can fit up to 20x more cores in the same area! The majority of the area on CPUs is consumed by Control and Cache, and Jensen thought, ditch those and put cores.</p> <p>Graphical Processing Unit (GPU) are tailored for matrix or tensor operations. The basic idea is to use tons of ALUs (weak but specialized) with massive parallelism (SIMD on steroids).</p> <p>There are other hardware accelerators like Tensor Processing Unit (TPU) or Application specific integrated circuit (ASIC), etc. The common theme across all these is the same - there are specialized cores. What are specialized cores? They can only compute certain computations. Specialized cores can be super powerful - <img src="/assets/img/2025-01-06-data-systems-for-ml/17370849151083.jpg" alt=""></p> <p>Companies also tried reducing precision and maintain the same performance. Additionally, they also tune the distribution of different components for specific workloads.</p> <blockquote> <p>Why does quantization work in ML systems?</p> </blockquote> <h2 id="recap">Recap</h2> <p>Consider the following question - What is the arithmetic intensity of multiplying two matrices?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Load A
Load B
C = matmul(A, B)
</code></pre></div></div> <p>Given \(A \in \mathbb{R}^{mxn}, B \in \mathbb{R}^{nxp}, C \in \mathbb{R}^{mxp}\), the number of I/O operations is \(mn + np + mp\), and the number of compute operations is \(2mnp\) since there are approximately \(mnp\) addition and multiplication operations. The arithmetic intensity is then \(\frac{\text{\#compute operations}}{\text{\#I/O operations}} = \frac{2mnp}{mn + np + mp}\). Setting \(m=n=p=2\), results in \(\frac{2x2x2x2}{2x2 + 2x2 + 2x2} = \frac{4}{3}\).</p> <p>\textit{Note.} The addition operation discussed in the previous lecture also has the same I/O operations. However, \texttt{matmul} is a denser operation that results in a higher arithmetic intensity. In practice, it takes the same time to execute matrix addition and multiplication on GPUs, which is why they are so powerful.</p> <p><code class="language-plaintext highlighter-rouge">matmul</code> is an important operation. <check></check></p> <p>Now, consider the following operations</p> <ul> <li><code class="language-plaintext highlighter-rouge">broadcast_to</code></li> <li><code class="language-plaintext highlighter-rouge">slice</code></li> <li><code class="language-plaintext highlighter-rouge">reshape</code></li> <li>Permute dimensions</li> <li><code class="language-plaintext highlighter-rouge">transpose</code></li> <li>indexing like <code class="language-plaintext highlighter-rouge">t[:, 1:5]</code> </li> </ul> <p>All these operations are optimized due to strided access in tensors. On the other hand <code class="language-plaintext highlighter-rouge">contiguous()</code> cannot take advantage of this.</p> <p>Just to recap, the strides of a tensor of shape <code class="language-plaintext highlighter-rouge">[2, 9, 1]</code> stored in row major order are <code class="language-plaintext highlighter-rouge">[9, 1, 1]</code></p> <p>Consider the cache tiling operation -</p> <ul> <li>It increases the memory allocated on Cache and memory transfers between cache and register</li> <li>It reuses the memory movement between Dram and Cache</li> <li>The arithmetic intensity <em>decreases</em> since there is more load and store</li> </ul> <h1 id="gpu-and-cuda">GPU and CUDA</h1> <p>We have seen that specialized cores offer much better performance over traditional CPUs. Consider the following basic architecture of a GPU</p> <p>Let us see the basic terminology for understanding the architecture -</p> <ul> <li> <strong>Threads</strong> - Smallest units to process a chunk of data.</li> <li> <strong>Blocks</strong> - A group of threads that share memory. Each block has many threads mapped to a <em>streaming multiprocessor</em> (SM/SMP).</li> <li> <strong>Grid</strong> - A collection of blocks that execute the same kernel.</li> <li> <strong>Kernel</strong> - CUDA program executed by many CUDA cores in parallel.</li> </ul> <p>A GPU can be made more powerful by</p> <ul> <li>Adding SMs</li> <li>Adding more cores per SM</li> <li>Making the cores more powerful - at a point of <em>diminishing rewards</em>.</li> </ul> <p>NVIDIA, the largest GPU company, has released P100, V100, A100, H100 and B100 (Blackwell) for ML development. K80, P4, T4 and L4 were a lower tier of GPUs. Let us analyze how the compute has changed across these versions</p> <ol> <li>V100 (2019 -) - 80SMs, 2048 threads/SM - $3/hour</li> <li>A100 (2020 -) - 108SMs, 2048 threads/SM - $4/hour</li> <li>H100 (2022 -) - 144SMs, 2048 threads/SM - $12/hour</li> <li>B100 and B200 (2025 -)-</li> </ol> <p>The numbers are not doubling, then how has the performance doubled? They decreased the precisions.. :(</p> <h2 id="cuda">CUDA</h2> <p><strong>What is CUDA?</strong> It is a C-like language to program GPUs, first introduced in 2007 with NVIDIA Tesla architecture. It is designed after the grid/block/thread concepts.</p> <p>CUDA programs contain a hierarchy of threads. Consider the following host code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">const</span> <span class="kt">int</span> <span class="n">Nx</span> <span class="o">=</span> <span class="mi">12</span><span class="p">;</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">Ny</span> <span class="o">=</span> <span class="mi">6</span><span class="p">;</span>

<span class="kt">dim3</span> <span class="nf">threadsPerBlock</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// 12</span>
<span class="kt">dim3</span> <span class="nf">numBlocks</span><span class="p">(</span><span class="n">Ns</span><span class="o">/</span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">x</span> <span class="p">,</span> <span class="n">Ny</span><span class="o">/</span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">y</span> <span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// (3, 2, 1) = 6</span>

<span class="c1">// the following call triggers execution of 72 CUDA threads</span>
<span class="n">matrixAddDoubleB</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">);</span>
</code></pre></div></div> <p>The GPUs are associated with constants such as</p> <ul> <li> <code class="language-plaintext highlighter-rouge">GridDim</code> - dimensions of the grid</li> <li> <code class="language-plaintext highlighter-rouge">blocking</code> - the block inter within the grid</li> <li> <code class="language-plaintext highlighter-rouge">blockDim</code> - the dimensions of a block</li> <li> <code class="language-plaintext highlighter-rouge">threadIdx</code> - the thread index within a block With these in mind, the CUDA kernel for the above code is designed as</li> </ul> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__device__</span> <span class="kt">float</span> <span class="nf">doubleValue</span><span class="p">(</span><span class="kt">float</span> <span class="n">x</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// kernel definition </span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">matrixAddDoubleB</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">])</span>
    <span class="p">{</span>
        <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="n">C</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">doubleValue</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
</code></pre></div></div> <p>The host code launched a grid of CUDA blocks, which then call the <code class="language-plaintext highlighter-rouge">matrixAdd</code> kernel. The function definition starts with <code class="language-plaintext highlighter-rouge">__global__</code> which denotes a CUDA kernel function that runs of the GPU. Each thread indexes its data using <code class="language-plaintext highlighter-rouge">blockIdx</code>, <code class="language-plaintext highlighter-rouge">blockDim</code>, <code class="language-plaintext highlighter-rouge">threadIdx</code> and execute the compute. It is the user’s responsibility to ensure that the job is correctly partitioned and the memory is handled correctly.</p> <p>The host code has a serial execution. However, the device code has SIMD parallel execution on the GPUs. When the kernel is launched, the CPU program <em>continues executing</em> without <em>halting</em> while the device code runs on the GPU. Due to this design, it is important that the device code does not have any return values - causes erroneous behavior. To get results from the GPU, <code class="language-plaintext highlighter-rouge">CUDA.synchronize</code> is used (an example will be shown later).</p> <p>It is the developers responsibility to map the data to blocks and threads. The blockDim, shapes etc should be statically declared. This is the reason why compilers like <code class="language-plaintext highlighter-rouge">torch.compile</code> requires static shapes. The CUDA interface provides a CPU/GPU code separation to the users.</p> <p>The SIMD implementation has a constraint for the control flow execution - it requires all ALUs/cores to process in the same pace. In a control flow, not all ALUs may do useful work and it can lead to up to 8 times lower peak performance.</p> <h3 id="coherent-and-divergent-execution">Coherent and Divergent execution</h3> <p>A coherent execution applied the same instructions to all data. Divergent executions do the opposite and they need to be minimized in CUDA programs. This distinction is important to note - even the latest models like the LLMs have this behavior. Concepts such as attention masking and sliding window attention are examples of divergent behavior and they need to be specially implemented to extract the most compute from the GPU.</p> <h2 id="cuda-memory-model">CUDA Memory model</h2> <p>CUDA device (SIMD execution on GPU) has its own memory called the <em>HBM</em>.</p> <p>Unlike host (CPU) memory that is stored as pages in the RAM, GPU memory does not use pages but has memory pools (bulk data) that are accessed all at once.</p> <p>Memory can be allocated <code class="language-plaintext highlighter-rouge">cudaMalloc</code> and populated with <code class="language-plaintext highlighter-rouge">cudaMemcpy</code> like usual. CUDA has a concept called <strong>pinned memory</strong> that is part of the host memory which is optimized for data transfer between CPU/GPU. Ig is not pagable by the OS and is locked, and only certain APIs can access it.</p> <p>Every thread has its own private memory space, and every block has a shared memory that all its threads can access. The HBM is the global device memory in the GPU that can be accessed by all threads. The memory complexity is to balance between speed and shared memory parallelism.</p> <p>For example, consider the program for window averaging -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mf">3.0</span>
</code></pre></div></div> <p>How can this be parallelized? Since every 3-element tuple reduction is independent, each reduction can be mapped to a CUDA core. So, each thread can compute the result for one element in the output array.</p> <p>The host code -</p> <pre><code class="language-C">int N = 1024*1024;
cudaMalloc(&amp;devInput, sizeof(float)*(N+2)); // To account for edge conditions
cudaMalloc(&amp;devOutput, sizeof(float)*N);

convolve&lt;&lt;&lt;N/THREADS_PER_BLK, THREADS_PER_BLK&gt;&gt;&gt;(N, devInput, devOutput); 
</code></pre> <p>The device code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="cp">#define THREADS_PER_BLK = 128
</span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolve</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
        <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">//thread-local variable</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span>
        <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span><span class="mf">3.</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div> <p>This program can be optimized - each element is read thrice!<br> Notice that the number of blocks assigned is much more than what a typical GPU has. This is a general practice in CUDA programming where the blocks are <em>oversubscribed</em>.</p> <p>How to optimize? The memory hierarchy can be utilized -</p> <p>The new device code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="cp">#define THREADS_PER_BLK = 128
</span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolve</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
        
        <span class="k">__shared__</span> <span class="kt">float</span> <span class="n">support</span><span class="p">[</span><span class="n">THREADS_PER_BLK</span><span class="p">];</span>
        <span class="n">support</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
        <span class="k">if</span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">){</span>
            <span class="n">support</span><span class="p">[</span><span class="n">THREADS_PER_BLK</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="n">THREADS_PER_BLK</span><span class="p">];</span>
        <span class="p">}</span>

        <span class="n">__syncthreads</span><span class="p">();</span>

        <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">//thread-local variable</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">3</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">+=</span> <span class="n">support</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">];</span>
            
        <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span><span class="mf">3.</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div> <p>We introduced a synchronization primitive here. <code class="language-plaintext highlighter-rouge">_syncthreads()</code> waits for all threads in a block to arrive at this point. Another primitive <code class="language-plaintext highlighter-rouge">cudasynchronize()</code> that syncs between host and the device.</p> <h2 id="compilation">Compilation</h2> <p>A CUDA program also needs to be converted to low-level instructions to be executed. A compiled CUDA device binary includes -</p> <ul> <li>Program text (instructions)</li> <li>Information about required resources - 128 threads per block, 8 types of local data per thread and 130 floats (520 bytes) of shared space per thread block.</li> </ul> <p>The issue is that different GPUs have different SMs. If the user asks for a static (large) number of blocks, how to handle this? The first solution is that GPUs have varying (limited) number of blocks.</p> <p>Furthermore, CUDA schedules the threadblocks to many cores using a dynamic scheduling policy that respects the resource requirements. It assumes that the thread blocks can be executed in any order. The blocks are assigned based on the available resources and the remaining ones are <em>queued</em>.</p> <h2 id="understanding-a-gpu">Understanding a GPU</h2> <p>Consider a NVIDIA GTX 980 (2014) that has the following specs -</p> <ul> <li>96KB of shared memory</li> <li>16 SMs</li> <li>2048 threads/SM</li> <li>128 CUDA cores/SM Note that the number of CUDA cores is not equal to the number of CUDA threads.</li> </ul> <p>As the GPUs became better, NVIDIA tried to increase the shared memory per SMM. This is similar to the SRAM which is very important for LLM inference.</p> <h1 id="matmul---case-study"> <code class="language-plaintext highlighter-rouge">matmul</code> - Case Study</h1> <p>Remember that over subscribing in GPUs is allowed, and identify that work can be performed in parallel. Developing the thought-process while working with CUDA is important</p> <ul> <li>Oversubscribe to keep the machine busy</li> <li>Balance workload with convergent workflows</li> <li>Minimize communication to reduce I/O</li> </ul> <p>Now, let us consider matrix multiplication. What can be parallelized? In our previous CUDA implementation, we let each thread compute one element in the result matrix. So, each thread has \(2N\) reads, and there are \(N^2\) threads, resulting in \(2N^3\) global memory access.</p> <p>We are not leveraging the fact that one element can be used to calculate many values in the result matrix. The trick is to use the shared memory space - thread tiling (similar to what we did in CPUs).</p> <p>We let each thread compute \(V \times V\) submatrix. The kernel is as follows</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mm</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">float</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">){</span>
        <span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">x</span><span class="p">,</span> <span class="o">:</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">y</span><span class="p">){</span>
            <span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">y</span><span class="p">];</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
                <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">k</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">C</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">ybase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="o">:</span><span class="p">];</span>
<span class="p">}</span>

</code></pre></div></div> <p>For this version, we have reduced the read per threads to \(NV + NV^2\) and number of threads to \(N^2/V^2\) - total reads reduce to \(N^3/V + N^3\) with \(V^2 + 2N\) float storage per thread.</p> <p>We can improve this using partial sum computations. With a small change, we get</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mm</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">float</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
        <span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">k</span><span class="p">];</span> <span class="c1">// Grabbing an area</span>
        <span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span><span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">y</span><span class="p">){</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">){</span>
                <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">y</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">C</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">ybase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="o">:</span><span class="p">];</span>
<span class="p">}</span>

</code></pre></div></div> <p>With memory read per thread reduced to \(NV^2\), total memory to \(2N^3/V\) and memory per thread to \(V^2 + 2V\). This version is pretty good for systems with a single layer of memory hierarchy. However, if we have shared memory, it can be made more efficient!</p> <p>Suppose we have an SRAM layer, we can tile hierarchically. Consider the following GPU <code class="language-plaintext highlighter-rouge">matmul</code> v3: SRAM Tiling:</p> <p>The idea is to use block shared memory to let a block compute a \(L \times L\) submatrix and each thread computes a \(V \times V\) submatrix reusing the matrices in the shared block memory.</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mm</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span> <span class="p">{</span>
    <span class="k">__shared__</span> <span class="kt">float</span> <span class="n">sA</span><span class="p">[</span><span class="n">S</span><span class="p">][</span><span class="n">L</span><span class="p">],</span> <span class="n">sB</span><span class="p">[</span><span class="n">S</span><span class="p">][</span><span class="n">L</span><span class="p">];</span>
    <span class="n">Float</span> <span class="n">c</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="n">Float</span> <span class="n">a</span><span class="p">[</span><span class="n">V</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">V</span><span class="p">];</span>
    <span class="n">Int</span> <span class="n">y</span> <span class="n">block</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="n">Iint</span> <span class="n">X</span> <span class="n">block</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">float</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
        <span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">k</span><span class="p">];</span> <span class="c1">// Grabbing an area</span>
        <span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span><span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">y</span><span class="p">){</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">){</span>
                <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">y</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">C</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">ybase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="o">:</span><span class="p">];</span>
<span class="p">}</span>

</code></pre></div></div> <blockquote> <p>Think about it this way. Initially, we performed tiling across one layer of memory balancing the tradeoffs between I/O reads and memory constraints of the threads. Now, we are adding one more layer of such tiling in a similar manner. The key is to understand the partial sums idea.</p> </blockquote> <p>Note that it is highly unlikely that the threads have a large range of execution times, but we have the <code class="language-plaintext highlighter-rouge">__syncthreads()</code> as a failsafe. The statistics of this algorithm are -</p> <ul> <li>\(2LN\) global memory access per thread block</li> <li>\(N^2/L^2\) threadblocks</li> <li>\(2N^3/L\) global memory access</li> <li>\(2VN\) shared memory access per thread</li> </ul> <p>The key addition here is was the shared memory space. For this algorithm to be efficient, the fetching from the memory has to be implemented <em>cooperatively</em> -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="kt">int</span> <span class="n">nthreads</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    
</code></pre></div></div> <p>These summarize the matrix multiplication codes implemented in the GPUs. Simple, isn’t it? Although, we have not addressed the optimal values for \(L, V\). It depends on the number of threads, registers and amount of SRAM available on the GPU - this process is called <strong>kernel tuning</strong>. There are profilers that optimize these values. It is a difficult problem since large ML models have various operations that need to be optimized together based on their processing and memory requirements. Furthermore, this is different for every GPU and there are hundreds of GPUs!</p> <p>One solution is to do brute-force - hire people and throw money at it. On the other side of things, ML researchers are building <strong>operator compilers</strong> that figure these out automatically.</p> <p>There are other GPU optimizations that are utilized in practice</p> <ul> <li>Global memory continuous read</li> <li>Shared memory bank conflict</li> <li>Pipelining - While some threads are reading, let other threads compute.</li> <li>Tensor core - Dedicated hardware component to accelerate matrix multiplications</li> <li>Lower precisions</li> </ul> <h1 id="ml-compilation">ML compilation</h1> <p>A super-hot topic during the late 2010s since there was a lot of inefficient code that was being identified. The goal is to automatically generate optimal configurations and code given users code and <em>target hardware</em>.</p> <p>Traditional compilers have to simply convert high-level code to binary instructions. The stack for ML compilers is</p> <ol> <li>Dataflow graph generation</li> <li>Optimize graphs - pruning, partitioning, distribution, etc</li> <li>Build efficient kernel code - parameter optimization</li> <li>Machine code (This step is fairly easy and already well implemented)</li> </ol> <p>The big problems in this big process are</p> <ol> <li>Programming level - Automatically transforming arbitrary (usually imperative) code into a compilable code (static dataflow graphs)</li> <li>Graph level - Automatic graph transformations to make it faster (recall how convolution and batch norm can be fused). Graph theory researchers are working on this.</li> <li>Operator level - How to use hardware and optimize standard operators like <code class="language-plaintext highlighter-rouge">matmul</code>.</li> </ol> <p>The big players in this big field are</p> <ol> <li>XLA - First compiler for ML, released along with TensorFlow in 2016 (those researchers aimed big). This turned out to be so good, that the current TensorFlow stack still uses this. Also works for PyTorch, and it is useful to deploy on TPUs.</li> <li>tvm (Tensor Virtual Machine) - It is one of the most successful open-source compiler in academia. They founded OctoML with 200M (got acquired by NVIDIA). There is no backward pass.</li> <li>2.0 - Torch based compiler, that isn’t that great in terms of optimization.</li> <li>Modular - They raised 300M, founded by the same person who created LLVM. The co-founders started swift at Apple! They had big claims - 20x faster than 2.0, not sure how true they are.</li> </ol> <p>You can think of TensorFlow and PyTorch as the front end, and the above mentioned compilers as the backend.</p> <h2 id="operator-compilation">Operator Compilation</h2> <p>Each user-level written code (for standard operations) has a library of low-level program variants, and the compiler chooses the fastest one for the given hardware.</p> <p>For example, consider a loop -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
        <span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
</code></pre></div></div> <p>Get converted to</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for xi in range(4):
    for xo in range(32):   
        C[xo * 4 + xi] A[xo * 4 + xi] + B[xo * 4 + xi]
</code></pre></div></div> <p>Which is then efficiently implemented in the GPU kernels.</p> <p>So, how do we make this happens?</p> <ul> <li>Enumerate all possibilities</li> <li>Enumerate all the (close-to-) optimal values for the hardware - register/cache</li> <li>Apply to all operators and devices</li> </ul> <p>How to search or reduce the search space and generalize?</p> <p>Note that for a certain kind of code and hardware, finding these optimal value <em>once</em> is enough.</p> <h3 id="search-via-learned-cost-model">Search via Learned Cost Model</h3> <p>The famous example is Autotvm -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17376893447233.jpg" alt=""> The code generator here is done with templates (not LLMs).We need a lot of experts to write the template to define the search space.</p> <p>To search in this parameters space, the compiler does beam search with early pruning. The cost model can be trained on the historical data.</p> <h3 id="high-level-ideas">High-level ideas</h3> <p>We represent the programs in an abstract way and build a search space with a set of transformations (that represent a good coverage of common optimizations like tiling). Then effective search with accurate cost models and transferability have to be deployed.</p> <p>So, how well are we doing in this field? If the compilers were that good, they would’ve discovered flash-attention. Okay, it’s not that bad, compilers have found good optimizations and it just goes to show how difficult this problem is.</p> <h1 id="high-level-dsl-for-cuda-triton">High-level DSL for CUDA: Triton</h1> <p>We have seen a device-specific DSL (domain-specific language). Programmers are able to squeeze the last bits of performance through this. However, it requires deep expertise and the performance optimization is very time-consuming. Maintaining codebases is complex.</p> <p>On the other hand, we have ML compilers. They prototype ideas very quickly (automatically) and the programmer does not have to worry about the low-level details. The problem is representing and searching through the search-space is difficult. Compilers were not able to find Flash-attention because the search-space wasn’t able to represent this possibility. Furthermore, code generation is a difficult problem that relies on heavy use of templates - lots of performance cliffs.</p> <p>So compared to these two extremes, Triton is in between - it is simpler than CUDA and more expressive than graph compilers. It was developed by OpenAI as a solution to the problems with CUDA and compilers.</p> <h3 id="triton-programming-model">Triton Programming Model</h3> <p>The users define tensors in SRAM directly and modify them using torch-like primitives.</p> <ul> <li>Embedded in Python - Kernels are defined in Python using triton.jit</li> <li>Supports pointer arithmetics - Users construct tensors of pointers and can (de)reference them element wise.</li> <li>However, it has shape constraints - must have power-of-two number of elements along each direction</li> </ul> <p>Consider the following example</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="kn">import</span> <span class="n">triton.language</span> <span class="k">as</span> <span class="n">tl</span>
    <span class="kn">import</span> <span class="n">triton</span>
    
    <span class="o">@</span> <span class="n">triton</span><span class="p">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">__add</span><span class="p">(</span><span class="n">z_ptr</span><span class="p">,</span> <span class="n">x_ptr</span><span class="p">,</span> <span class="n">y_ptr</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
</code></pre></div></div> <p>The triton kernel will be mapped to a single block (SM) of threads. The users are responsible for mapping to multiple blocks. Basically, the language is automating some parts (like compilers), and making the design process simpler for users (as compared to CUDA). These design philosophies are important because they help build newer mental models for users - because they offload some of the cognitive load for optimization, they can think of newer ways of optimizing with these restricted set of parameters. Consider the example of softmax calculation. This function would be slow if implemented using primitives. PyTorch implements an end-to-end kernel for softmax to increase its performance. With triton, we can construct such an end-to-end operation in a simpler manner while achieving slightly higher performance.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">triton.language</span> <span class="k">as</span> <span class="n">tl</span>
<span class="kn">import</span> <span class="n">triton</span>
<span class="nd">@triton.jit</span>
<span class="k">def</span> <span class="nf">_softmax</span><span class="p">(</span><span class="n">z_ptr</span><span class="p">,</span> <span class="n">x_ptr</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">BLOCK</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">):</span>
    <span class="c1"># Each program instance normalizes a row
</span>    <span class="n">row</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK</span><span class="p">)</span>
    <span class="c1"># Load a row of row-major X to SRAM
</span>    <span class="n">x_ptrs</span> <span class="o">=</span> <span class="n">x_ptr</span> <span class="o">+</span> <span class="n">row</span><span class="o">*</span><span class="n">stride</span> <span class="o">+</span> <span class="n">cols</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">x_ptrs</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">,</span> <span class="n">other</span> <span class="o">=</span>    <span class="nf">float</span><span class="p">(</span><span class="err">‘</span><span class="o">-</span><span class="n">inf</span><span class="err">’</span><span class="p">))</span>
    <span class="c1"># Normalization in SRAM, in FP32    
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">tl</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># This is to avoid vert large and small values
</span>    <span class="n">num</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">den</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">num</span> <span class="o">/</span> <span class="n">den</span><span class="p">;</span> 
    <span class="c1"># Write-back to HBM
</span>    <span class="n">tl</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">z_ptr</span> <span class="o">+</span> <span class="n">row</span><span class="o">*</span><span class="n">stride</span> <span class="o">+</span> <span class="n">cols</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">&lt;</span> <span class="n">N</span>
</code></pre></div></div> <p>Note that Triton achieves good performance with low time investment. However, since it is not as flexible as CUDA, achieving very high-performance is not possible with Triton.</p> <h2 id="recap-1">Recap</h2> <p>A <em>kernel</em> in the GPU is a function that is executed simultaneously by tens of thousands of threads on GPU cores. The shared memory during the GPU execution can be used as a <em>cache</em> that is used by more than one thread, avoiding multiple accesses to the global memory. <em>Over-subscribing</em> the GPU ensures that there are more blocks than SMPs present on the device, helping to hide (tail) latencies by ensuring high occupancy of the GPU.</p> <p>&lt;iNsert a point about GPU memory)</p> <p>Operations such as ReLU, batch normalization and max pooling are not arithmetically dense operations. So typically, operations such as linear layers (and layer normalization with large batches) are limited by arithmetic operations. To calculate which linear layer would have more operations, consider the FLOPs calculation for GEMM.</p> <h1 id="3-graph-optimization">(3) Graph Optimization</h1> <p>Our goal is to rewrite \(G\) as \(G’\) such that \(G’\) runs faster than \(G\) while outputting equivalent results. The straightforward solution is to use a template, wherein human experts write (sub-)graph transformation templates and an algorithm replaces these in the data flow graphs for reduction.</p> <h3 id="graph-optimization-templates-fusion">Graph Optimization Templates: Fusion</h3> <p>Fusing operators reduces I/O and kernel launching (CPU to GPU overhead, all the operations that the SM has to run). The disadvantages of this method is that creating various fused operations is difficult making the codebase unmanageable (e.g., TensorFlow).</p> <p>This also includes <strong>folding constants</strong> in a graph to replace expressions such as <code class="language-plaintext highlighter-rouge">(X + 3) + 4)</code> with <code class="language-plaintext highlighter-rouge">(X + 7)</code>.</p> <h3 id="cuda-graph">CUDA graph</h3> <p>NVIDIA allows users to capture the graph at the CUDA level.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381192844173.jpg" alt=""></p> <blockquote> <p>Is this define-then-run?</p> </blockquote> <h3 id="standard-compiler-techniques">Standard compiler techniques</h3> <ul> <li>Common subexpression elimination (CSE). The high-level idea is replacing expressions such as <code class="language-plaintext highlighter-rouge">a = b; b = c</code> with <code class="language-plaintext highlighter-rouge">a = c</code> </li> <li>Dead Code elimination (DCE). After the CSE hit, we eliminate the dead-code with unused variables.</li> </ul> <p>These both are run iteratively to reach an optimal code. These operations are every useful to eliminate parts of graph based on, say default arguments -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381197256019.jpg" alt=""></p> <h2 id="how-to-ensure-performance-gain">How to ensure performance gain?</h2> <p>When we greedily apply graph optimizations, we may miss some options that initially decrease the performance but massively increase it later. Furthermore, the same optimizations could lead to an improvement in one hardware and reduction in other. Due to the existence of hundreds of operators (200-300), thousands of graph architectures and tens of hardware backends, it is infeasible to manually design graph optimizations for all cases.</p> <p>There are other issues with template based optimizations</p> <ol> <li>Robustness - Heuristics are not generalizable across architectures and hardware</li> <li>Scalability - New operators and graph structures require newer rules</li> <li>Performance - Misses subtle optimizations specific to DNNs/hardware.</li> </ol> <p>What’s the solution?</p> <h2 id="automate-graph-transformation">Automate Graph Transformation</h2> <p>The main idea is to replace manually-designed graph optimizations with automated generation and verification of graph substitutions for tensor algebra. Basically, generate all possible substitutions and verify if they generate the same output.</p> <p>We start by enumerating all possible graphs up to a fixed size using available operators.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381201066754.jpg" alt=""> There are up to 66M graphs with 4 operators!</p> <p>Then, with a graph substitution generator, we compute the output with random input tensors. For 4 operators, we can still generate up to 28744 substitutions!</p> <p>These are further pruned based on <em>variable renaming</em> and <em>common subgraphs</em>.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381203016208.jpg" alt=""></p> <p>These substitutions are formally verified to ensure that they are equivalent mathematically for all inputs. This verification is done by using the properties of operators. For example, convolution with concatenated kernels is same as concatenation of convolutions of the same kernels.</p> <p>So this <em>automated theorem prover</em> can be used to generate valid substitutions scaling up. It takes up to 5 minutes to verify 750 substitutions and there are about 45 rules for the operators which takes about 10 minutes. Adding a new operator is easy - just provide its specifications!</p> <h3 id="incorporating-substitutions">Incorporating substitutions</h3> <p>How do we apply verified substitutions to obtain an optimized graph? The cost is based on the sum of individual operator’s cost and the cost on the target hardware. We greedily apply the substitutions to improve the performance.</p> <p>This approach can be further improved to train a model to learn which kind of substitutions optimize the graph. This was successfully implemented by TASO and it showed good results for real-life models -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381207164574.jpg" alt=""></p> <h2 id="summary-1">Summary</h2> <p>In summary for graph optimization,</p> <ol> <li>We first construct a search space</li> <li>Enumerate all possibilities for substitutions</li> <li>Prune the candidates, and select the top ones based on profile/cost model</li> <li>Apply the transformations to iteratively improve the performance.</li> </ol> <p>What could go wrong with this? The search may be slow and the evaluation of various graphs can be expensive.</p> <p>Sometimes the substitutions may only be partially equivalent, but can be orders of magnitude faster. In such cases, we can trade off accuracy for performance. E.g., Convolution vs Diluted convolution.</p> <p>Consider the following example. Suppose we have to use the same kernel to perform convolution on two different tensors. Then, we could concatenate these tensors, apply the convolution, and then apply a correction to achieve the correct result. These transformations use partial equivalent transformations yielding some speed up. These are not explorable in the previous case with fully equivalent operators.</p> <h2 id="partially-equivalent-transformations">Partially Equivalent Transformations</h2> <p>Like the previous example, we <em>mutate</em> the programs and correct them to get an optimized graph.</p> <p>The steps to do this automatically, we do something similar to before</p> <ol> <li>Enumerate all possible programs up to a fixed size using available operators</li> <li>Only consider transformations with equal shapes (in contrast with equal results as compared to before)</li> </ol> <p>With this, all the crux of the algorithm comes to the correction of the mutant programs - how do we detect which part is not equivalent and how to correct it?</p> <p>By enumeration - For each possible input and position, check if the values match. For complete correctness, this search would be \(m \times n\) for \(m\) possible inputs and \(n\) output shape. We reduce the effort by reducing \(m, n\)</p> <ul> <li> <p>Reducing \(n\) - Since neural networks are mostly multi-linear, we can make such assumptions.</p> <p>Theorem: For two multi-linear functions \(f\) and \(g\), if \(f = g\) for \(O(1)\) positions in a region, then \(f = g\) for all positions in the region.</p> <p>As a consequence, the search reduces from \(\mathcal O(mn)\) to \(\mathcal O(mr)\)</p> </li> <li> <p>Reducing \(m\) - Theorem - If \(\exists l, f(l)[p] \neq g(l)[p]\), then the probability that \(f\) and \(g\) give identical results on \(t\) random inputs is \(2^{-31t}\).</p> <p>Using this, we can run \(t\) random tests with random inputs, and if all \(t\) pass then it is very likely that \(f\) and \(g\) are equivalent.</p> </li> </ul> <p>The search space reduces to $\mathcal O(tr)$$. How does this relate to correct?</p> <h1 id="ml-compiler-retrospective">ML Compiler Retrospective</h1> <p>This field started in 2013 with Halide. It was a compiler for rendering, but since the workflow is very similar to neural networks, the later compilers draw motivation from here.</p> <p>Then came XLA in 2016-17, that has good performance but had very difficult to understand code. Companies tried other operations such as TensorRT, cuDNN and ONNX for template based graph substitution. CuDNN is still popularly used but no one understands the code since it was written in a very low level language.</p> <p>Then came <code class="language-plaintext highlighter-rouge">tvm</code> in 2018 that we’ve discussed before. In 2019-20, MLIR and Flexflow were introduced - these are layers in the compiler that provided specific optimizations. Then came 2.0 and Torch Dynamo.</p> <p>However, the community is shifting away from compilers. Why? One part is that many optimizations have been found. The main reason is that we’ve seen a certain class of neural networks architectures that work really well. For example, transformers are all the rage. So instead of focusing on compilers, people can focus on just building fused kernels for the attention mechanisms. That’s how we got flash-attention that no compiler is able to beat.</p> <h1 id="runtime">Runtime</h1> <h2 id="memory-and-scheduling">Memory and Scheduling</h2> <p>Our goal is to fit the workload on limited memory and ensure that the peak memory usage is less than the available memory.</p> <blockquote> <p>Need to add some stuff here.</p> </blockquote> <p>Consider the GPT-3 architecture -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17382918001192.jpg" alt=""></p> <p>For every model, we check the precision and multiply the number of parameters by 2 or 4 to calculate the total memory consumption.</p> <p>For the main model with 175B parameters, if each parameter is 2 bytes, then we require 350Gb of memory! How did we make this work?</p> <p>Why does this rule of thumb work? Let us check the activation sizes for different layers</p> <ol> <li>For 2D convolution: The input has the size \((bs, nc, wi, hi)\), and the output has \((bs, nc, wo, ho)\). The activation size is \(bs*nc*wo*ho*\text{sizeof(element)}\)</li> <li>For an MLP with input size \((bc, m, n)\) and output size \((bs, m, p)\), the activation size is \(bs*m*p*\text{sizeof(element)}\)</li> <li>For a transformer (ignoring activation layers and other FFLs) - the input size is \((bs, h, seq\_len)\) and the output size is \((bs, h, seq\_len). The activation size is\)bs<em>h</em>seq_len*\text{sizeof(element)}$$</li> </ol> <p>So for GPT-3, the per-layer activation assuming sequence length 1 comes to 78 or 156 Gb. Let us add some more elements to this calculation.</p> <p>The Adam Optimizer estimates the first and second moment vectors with parameters for exponential decays. It also has a step-size or learning rate. The algorithm is given by</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17382924545817.jpg" alt=""></p> <p>Along with the learning rate, since it also stores the moments, it has to store two more values for each parameter! The memory usage becomes thrice of what it should be!</p> <h3 id="lifetime-of-activations-at-training">Lifetime of activations at training</h3> <p>Because we need to store the intermediate values for the gradient steps, training an \(N\)-layer neural network would require \(O(N)\) memory. This is the main difference between training and inference. In inference, we wouldn’t need to store the parameters at all layers, so we would just need \(O(1)\) memory.</p> <p>So we’ve seen for GPT-3, we require 350 or 700 Gb. So for a sequence length of 96, we would require 7488 or 14976 Gb! These numbers are just crazy! We haven’t even considered the composite layers.</p> <p>Therefore, it is important to take care of memory.</p> <h3 id="single-device-execution">Single Device execution</h3> <p>How do we reduce the memory usage?</p> <p>Idea 1 - the input or the activation is not needed until the backward pass reaches the layer. So, we can discard some of them and recompute the missing intermediate nodes in small segments. This technique is called <em>recomputation, rematerialization, checkpoint activation, etc</em>. It’s essentially the time-space tradeoff.</p> <p>For an \(N\) layer neural network, if we checkpoint every \(K\) layers, then the memory cost reduces to</p> \[\text{Memory cost} = \mathcal O\left(\frac{N}{K}\right) + \mathcal O(K)\] <p>To minimize this, we can pick \(K = \sqrt{N}\). The total recomputation increases by \(N\) - essentially another forward pass. In PyTorch, this feature can be activated using <code class="language-plaintext highlighter-rouge">torch.utils.checkpoint</code>.</p> <p>So when do we use this? When memory is a constraint and time of training is not a concern. The memory usage also depends on the layer being checkpointed - the layers can have different out sizes. In transformers, the layer boundary is typically checkpointed. The disadvantage is that this only works for activations.</p> <blockquote> <p>why?</p> </blockquote> <p>The second idea is <strong>gradient accummulation</strong>. The activation memory is linear to batch size. The idea is to compute the gradient for the batch but will limited memory. We split the original batch into micro-batches and accumulate the gradients at each layer. We then update the weights for the complete batch.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17382936900407.jpg" alt=""></p> <p>The disadvantage of this strategy is that over-subscribing of GPUs is difficult since we have smaller matrices.</p> <p>An alternative method to save on GPU memory is to use the memory hierarchy. We have <code class="language-plaintext highlighter-rouge">SwapIn</code> (swap from CPU DRAM to HBM) and <code class="language-plaintext highlighter-rouge">SwapOut</code> (swap from HBM to CPU DRAM) that can be applied to both weights and activation. As we do a forward pass, we swap in the next layers and swap out the passed layers. You can be a bit more intelligent about it and pre-fetch the layers based on the computation and swap latencies. This strategy is becoming more practical as more companies are adopting the unified memory architecture. The memory hierarchy seems to be breaking.</p> <p>All these strategies can be used together to probably train GPT-3 on a single device but it would take forever.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387232533206.jpg" alt=""> Why do we start with gradient accumulation instead of gradient checkpointing? Checkpointing greatly increases the computation time, so we try the other alternatives first.</p> <h2 id="memory-and-compute">Memory and Compute</h2> <h3 id="quantization">Quantization</h3> <p>All our memory usage is a multiple of \(\text{sizeof(element)}\). What if we reduce that parameter?</p> <p>Quantization is the process of constraining an input from a continuous or otherwise large set of value to a discrete set. We use a lower-precision representation for data while preserving ML performance (accuracy), speeding up compute, reducing memory, saving energy, etc. Most of the edge models use quantization.</p> <p>To understand this better, let’s understand the representation of data in memory -</p> <ul> <li>An unsigned integer has the range \([0, 2^n - 1]\)</li> <li>A signed integer with \(n\)-bit has the range \([-2^{n-1} - 1, 2^{n - 1} - 1]\). To avoid saving 0 twice by storing a sign bit, computer architects decided to use <em>Two’s complement representation</em>.</li> <li>Fixed point number - An arbitrary bit is chosen as the boundary for the integer and the decimal. This representation is mainly used in security applications now.</li> <li> <p>Floating point representation - We use a sign bit, 8-bit exponent and 23 bit fraction. That is the value is, \((-1)^{sign} \times (1 + \text{ fraction}) \times 2^{\text{exponent} - 127}\).</p> <p>How do we represent 0 then? Representation-wise, we technically cannot represent 0, so we make a special representation - <em>normal vs subnormal values</em>. Whenever the exponent bits are zero, we remove the bias term \(1\) that is added to the fraction, and represent the value as \((-1)^{sign} \times \text{ fraction} \times 2^{\text{exponent} - 127}\). This expressions is only used with the exponent is \(0\). This way, we also extend the range of the representation and the smallest positive number we can represent is \(2^{-149}\).</p> <p>How about special values? Exponent with all set bits is infinity and sign is decided by the sign bit. NaN is represented in the subnormal range with exponent bits set to 1. In summary, we have</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387239819544.jpg" alt=""></p> </li> </ul> <blockquote> <p>Calculate model sizes from parameters table.</p> </blockquote> <p>Notice that the precision of floating point numbers is much higher when the values themselves are small. This is a tradeoff we make based on the applications. Here is a summary of other floating point representations -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387241256605.jpg" alt=""></p> <p>The BF16 system has been observed to provide much better performance over FP16 for neural network training. The representation lowers the precision for a higher range. It could be that the higher range can stabilize the training during exploding or vanishing gradients. During inference, the precision matters more so FP16 might perform better in some cases.</p> <p>For practice, consider the following examples</p> <ul> <li>The FP16 bit set <code class="language-plaintext highlighter-rouge">1 10001 1100000000</code> represents -7.1. Why? The bias in the exponent is always the median value subtracted by 1. Here it is \(2^4 - 1 = 15\). The exponent is then \(17 - 15 = 2\), and the fraction is \(0.5 + 0.25 = 0.75\)</li> <li>The decimal 2.5 is represented as <code class="language-plaintext highlighter-rouge">0 10000000 0100000</code>. The bias is \(2^7 - 1 = 127\)</li> </ul> <p>After these representations, newer ones came up to improve the performance in deep-learning. <img src="/assets/img/2025-01-06-data-systems-for-ml/17387248528126.jpg" alt=""> The rule of thumb is that we require higher range for training and higher precision for inference.</p> <p>As if this were not enough, for even lower compute, NVIDIA has been pushing for INT4 and FP4 to keep up with the Moore’s law.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387250679255.jpg" alt=""></p> <p>There has been no successful model with these representations, making it a very promising avenue for research.</p> <p>Alright, with these things in mind, let us come back to quantization. There have been two approaches for quantization</p> <ol> <li> <p><strong>K-Means based quantization</strong> - widely used, almost all mobile devices have it.</p> <p>Consider we have a weight matrix that we are trying to quantize. The idea is to cluster close values together into an integer or a lower-precision representation. The number of clusters is chosen based on the chosen requirements - for 2-bit quantization, number of clusters is 4.<br> To do so, we simply apply K-means. The centroids are stored in a code book whereas the matrix is simply stored in the lowest possible representation to further reduce storage. <img src="/assets/img/2025-01-06-data-systems-for-ml/17387254097673.jpg" alt=""> How do we perform a backward pass on these matrices? The gradients are accumulated based on the classes and are applied to the centroids to fine-tune them. In practice, quantization starts affecting the performance sharply only after a certain threshold. Therefore, quantization becomes a hyper-parameter tuning problem, and we can achieve significantly lower memory consumption. The number of bits used can vary with layers as well!</p> <blockquote> <p>Try coding a library with variable quantization layers. Shared code books across layers</p> </blockquote> <p>How is the run-time affected? The computations are still FP arithmetic. There is an added computation cost for weight compression/decompression and code book lookup. K-means has been quite effective with convolution networks.</p> </li> <li> <p><strong>Linear quantization</strong> - The idea is to determine a linear mapping of integers to real numbers. It can be seen as a linear optimization problem.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387262709675.jpg" alt=""></p> <p>So, we need to determine zero-point and scale parameters. These parameters are often also publicly shared on platforms such as HuggingFace. The parameters can be chosen for the whole model or for each layer based on our performance tradeoff appetite. For many popular models like Llama, the quantization is done tensor-wise.</p> <p>The parameters can easily be determined as follows</p> \[\begin{align*} S &amp;= \frac{r_{max} - r_{min}}{q_{max} - q_{min}} Z = \text{round}(q_{min} - \frac{r_{min}{S}}) \end{align*}\] <p>The bit-width \(n\) determines \(q_{min} = -2^{n -1}\) and \(q_{max} = 2^{n - 1} - 1\).</p> <p>Suppose we apply linear quantization to <code class="language-plaintext highlighter-rouge">matmul</code> -</p> \[\begin{align*} Y &amp;= WX \\ S_Y(q_Y - Z_Y) &amp;= S_W(q_W - Z_W)S_X(q_X - Z_X) \\ Q_Y = \frac{S_W S_X}{S_Y} (q_Wq_X - Z_Wq_X - \underset{Z_Xq_W + Z_WZ_X) + Z_Y}_{\text{precomputed for inference}} \end{align*}\] <p>Empirically, the factor \(\frac{S_W S_X}{S_Y}\) is between 0 and 1. Instead of using floating point multiplications, it is represented as fixed point multiplication and bit shift. Also, empirically \(Z_W\) follows normal distribution and can be approximated as 0. Thus, the heavy lifting operation is \(q_Wq_X\) which is simply integer multiplication.</p> <p>Therefore, we reduced both storage and computation time (integer arithmetic is much faster and cheaper). This can also be used to reduce FP16 to FP8 rather than integers.</p> </li> </ol> <p>In summary, we have <img src="/assets/img/2025-01-06-data-systems-for-ml/17387273312564.jpg" alt=""></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/NumAn/">Numerical Analysis Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/ipl/">IPL Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/dbms/">DiBS Notes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/rl-theory/">Reinforcement Learning Theory</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/automata/">Automata Notes</a> </li> </div> <script>document.querySelectorAll("#table-of-contents a").forEach(function(e){e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substring(1);document.querySelectorAll(".content-section").forEach(function(e){e.classList.add("hidden")});var n=document.getElementById(t);n&&n.classList.remove("hidden")})});</script> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Sudhansh Peddabomma. Last updated: February 05, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0K9MLG0V24");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <div class="chat-toggle-container"> <button id="chat-toggle-btn" class="chat-toggle-btn"> <i class="fas fa-comments"></i> </button> </div> <div id="chat-window" class="chat-window"> <div class="chat-header"> <h5 class="mb-0">Talk to my AI</h5> <button id="close-chat" class="btn-close"> <i class="fas fa-times"></i> </button> </div> <div id="chat-messages" class="chat-messages"></div> <div class="chat-input-container"> <input type="text" id="chat-input" class="form-control" placeholder="Type a message..."> <button id="send-btn" class="btn btn-primary"> <i class="fas fa-paper-plane"></i> </button> </div> </div> </body> </html>