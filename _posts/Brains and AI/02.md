# Bias-Variance Tradeoff

This topic seems like a thing of the past. We have built very large models around large amounts of data, and everything seems to work. So, why worry about bias or variance? Bias is solved by large models and variance is solved by large data. If all’s good in the world, then why don’t we have zero loss?

> Recap: The bias-variance trade-off. When low complexity models cannot inherently capture the complexity of the data, they have a *bias* or they *underfits* the training data. It can arise from a poor architecture or features engineered from the data. On the other end, complex models with large amount of parameters may fit the training data really well, but they tend to perform poorly on the test loss. This error, called *variance error*, occurs due to *overfitting*. in these cases, models are said to not generalize well across different data distributions. This trade-off between bias and variance is still present at the core of modern models.

To counter these problems, early approaches involved **regularizing** models to prefer solutions with “simple” patterns - ones that have low-norm weights even in over-parameterized models. There are modern theoretical frameworks such as the [Neural Tangent Kernels (NTK)](https://arxiv.org/abs/1806.07572) that show that over-parameterized networks behave like kernel machines and converge to smooth solutions due to regularization. That is, interpolating regularized solutions seems to generalize well. 

I also mentioned *poor model architecture* as a source of these errors. Over-parameterized models can often be replaced with models designed with a good **inductive bias** - models that leverage the structures in the data to generate structured solutions. For example, CNNs leveraged that images have spatial-features, and replaced fully-connected layers with convolutional kernels that greatly reduced the number of parameters.     

> *Model architectures* designed with inductive bias are the best kind of models. Attention is also a product of such design philosophies. 

So far, we have
- Maybe over-parameterization works but we need to have appropriate regularization tricks such as dropout and weight decay
- Adaptive optimizers, early stopping, large batch training all seem to make sense

In 2018, [Belkin et al.](https://arxiv.org/abs/1812.11118), showed that test loss follows a “double descent” curve - it peaks at a critical model complexity, then decreases. This phenomenon has been seen to occur in CNNs, Transformers, and linear models with high dimensional features. The take away message is that more complexity does not mean worse generalization in the modern architectures.

![](/assets/img/BrainsAI/17377675747647.jpg)

How does data fit in all this? More the amount of data, the simpler the model becomes - we have seen that the inner layers of LLMs require sparse changes in the weights to fine-tune to different datasets (LoRA). It maybe seen as if large datasets prevent overfitting since larger models are able to absorb the noise in the data without harming the underlying signal.

In 2019, [Bartlett et al.](https://arxiv.org/abs/1906.11300), showed that models can memorize noisy data but still generalize if noise is structured or data has low intrinsic dimensions. High-dimensional mode old can separate signal from noise via implicit regularization. At the core of some of the large models we have built, we made a rather huge assumption - the noise in the data is Gaussian. The MSE loss is nothing but a negative likelihood over Gaussian noise. These assumptions must be carefully considered while building models for different applications.

So what do we make of all this? It’s new information that we didn’t have before while designing models. Maybe it’s because of this the model scaling laws are working.

# [Benign overfitting in linear regression](https://arxiv.org/abs/1906.11300)

The ultimate goal of machine learning is how to train a model that fits to a given data. We are approaching this by reducing the empirical training risk/error through a loss function. There is a mis-match between what are doing and the goal we are trying to achieve - reducing the test loss or generalize well to new data. Let us understand this better.

A model’s ability to fit a wide variety of functions or patterns in the data is a known as its *capacity*. As we have increased the models’ capacity, we seemed to have the cross the peak in the double descent curve - they are over-fitting but it seems to be benign. That is, they seem to have zero training risk and the test risk approaches the best possible value. Why do we think this is benign? As the model capacity increases, the test loss seems to be decreasing even more. So how do we reach this benign overfitting region?

The authors tested this with linear regression and significant over-parameterization. For a linear regression model the minimum norm solution is given by 

$$
    \hat \theta = X^T = (XX^T)^{-1}y  \; X\theta = y
$$

The authors define the excess risk of the estimator as 

$$
T(\theta) := \mathbb E_{x, t} [(y - x^T \theta)^2 - (y - X^T \theta^*)^2]
$$

How do you over-parameterize a linear regression model? The authors consider the number of eigenvectors of the covariance of the data. They consider quantities from the PCA theory. They concluded that if the decay of the eigenvalues of the covariance is sharp, then we can reach the benign overfitting region.