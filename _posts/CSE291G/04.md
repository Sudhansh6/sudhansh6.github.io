## Summary
The equation we’ve been working with for value iteration is

$$
Q_{t + 1}(s, a) = L(Q_t) = R(s, a) + \mathbb E_{s’ \sim P(. \ver s, a)} \max_{a’} Q_{t}(s’, a’)
$$ 

The key step here is to extract a policy from the current $$Q$$ function. We noted that the optimal function $$Q^*$$ satisfies the optimality equation. 

Aside from the optimality equation, we have the expectation equation that every $$Q$$ function satisfies

$$
    Q_\pi(s, a) = R(s, a) + \mathbb E_{s’} \mathbb E_{a’ \sim \pi(\cdot \vert s’)} Q(s’, a’)
$$

The advantage of looking at the optimality equation as an operation $$Q_{t + 1} = L(Q_{t})$$ is that we can apply the contraction concepts to arrive at $$Q^*$$ with Banach’s fixed point theorem. This way, we prove that there is a unique optimal $$Q$$ function. 

Now we show that, after enough number of iterations, we can also get the value function $$V$$ arbitrarily close the optimal value. All these subtleties together show that the value iteration algorithm works!

How do obtain these bounds based on iterations? We need to find an upper bound for $$\| Q_{t} - Q^* \|_\infty$$. We can show that this value is $$leq \|Q^* - Q_0 \|_\infty$$. Assuming we start with a $$Q$$ with all zeroes, the maximum value of $$Q^*$$ is simply $$R_{\max}/(1 - \gamma) = (\max_{s, a} \vert R(s, a)\vert)/(1 - \gamma)$$. 


*Lemma.* $$\|V_m(s) - V^*(s)\| \leq \frac{2}{1 - \gamma}\| Q_m - Q^*\|_\infty$$. 

*Proof.* 
$$
\begin{align*}
V^*(s) - V_m(s) &= Q^*(s, \pi^*(s)) - Q_m(s, a = \pi_m(s)) \\
    &= Q^*(s, \pi^*(s)) - Q^*(s, a) + Q^*(s, a) - Q_m(s, a) \\
    &= Q^*(s, \pi^*(s)) - Q^*(s, a) + \gamma \mathbb E_{s’} (V^*(s’) - V_m(s’))
    &\leq  Q^*(s, \pi^*(s)) - Q^*(s, a) + \gamma \|V^* - V_m\|_\infty \\
    &\leq (Q^*(s, \pi^*(s)) - Q_m(s, \pi^*(s))) + (Q_m(s, \pi^*(s)) - Q^*(s, a)) + \gamma \|V^* - V_m \|_\infty \\
    &\leq (Q^*(s, \pi^*(s)) - Q_m(s, \pi^*(s))) + (Q_m(s, a) - Q^*(s, a)) + \gamma \|V^* - V_m \|_\infty \\
    &\leq 2\|Q^* - Q_m\|_\infty + \gamma \|V^* - V_m\|_\infty \\
   \|V^* - V_m \|_\infty \leq \frac{2}{1 - \gamma} \|Q^* - Q_m \|_\infty
    
\end{align*}
$$

# Policy Iteration
Instead of modifying the policy based on the current value, why not do it the other way round? Iterate over the policy, get its value and improve it again? There is a subtle different as compared to the previous algorithm, and it turns out that this method is much more efficient!

$$
    \pi_0 \underset{\longrightarrow}{Q_0} \to \pi_1 \to \cdots \to \pi_k \underset{\longrightarrow}{Q_k} \pi_{k + 1}
$$

Policy iteration takes $$\mathcal O(\vert S\vert^3 + \vert S \vert^2 \vert A\vert)$$ whereas value iteration is $$\mathcal O(\vert S \vert^2 \vert A\vert)$$.





