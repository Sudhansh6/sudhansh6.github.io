The positively correlated random variables we choose are

$$
    \begin{align*}
        X &= G_t = Q(S_t, A_t) = R_t + \gamma T_{t + 1} + \gramma^2 R_{t + 1} \\
        Y &= b(S_t) 
    \end{align*}
$$

where $$b(S_t)$$ is $$V(S_t)$$ obtained from $$TD(\lambda)$$. The term $$Q(S_t, A_t) - V(S_t)$$ is called the **advantage**! The equation is modified to

$$
\begin{align*}
\nabla J(\theta) &= \mathbb E \left[\sum_{t = 0}^L (G_t - b(S_t)) \frac{\partial}{\partial \theta} \ln \pi(S_t, A_t, \theta) \right]\\
&= \mathbb E \left[\sum_{t = 0}^L G_t \frac{\partial}{\partial \theta} \ln \pi(S_t, A_t, \theta) - \sum_{t = 0}^L b(S_t) \frac{\partial}{\partial \theta} \ln \pi(S_t, A_t, \theta)\right] \\
&= \mathbb E \left[\sum_{t = 0}^L G_t \frac{\partial}{\partial \theta} \ln \pi(S_t, A_t, \theta)\right] - \underbrace{\sum_{t = 0}^L \sum_s P[S_t = s] b(S_t) \frac{\partial}{\partial \theta} \sum_a \pi(S_t, A_t, \theta \vert A_t = a, S_t = s)}_{=0} \\
&= \sum_{t = 0}^L G_t \frac{\partial}{\partial \theta} \ln \pi(S_t, A_t, \theta)
\end{align*}
$$

We can decrease the variance further but it may introduce bias in the estimator. There are many research works that explore these variations of the algorithm based on the outcome required. 

Apart from this, there are other implementation tricks that seemed to be more effective in practice. For example, removing the discount factor from the REINFORCE equation yields more stable and faster training. 


