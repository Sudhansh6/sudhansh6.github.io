Since intra-op requires more communication, this is an important aspect to consider. On the other hand, inter-op results in hardware bubble wherein some of the GPUs are idle (can be prevented to some extent with pipelining). 

In summary, inter-op parallelism requires point to point communication but results in idle devices. The devices are busy in intra-op parallelism but requires collective communication. 

> **A note on terminology**
> Previously, the literature used to talk about data and model parallelism. Data parallelism is general and precise but the term “model parallelism” is vague. That is why, we stick to the terms inter-op and intra-op parallelism which rely on the pillars of computational graph and device cluster. 
> Furthermore, we are mainly discussing about model parallelism because of the regime in which models do not fit on a single GPU anymore. 


With that, let us formulate our goal - “What is the most efficient way to execute a graph using combinations of inter-op and intra-op parallelism subject to memory and communication constraints?”

Let us equip ourselves with a metric to quantify this goal. Previously we had, Arithmetic Intensity. Now, we consider **Model FLOPs Utilization (MFU)**

$$
    MFU = \#FLOPs/t/\text{peak FLOPs}
$$

Where #FLOPs is the total FLOPs in the ML program, and $$t$$ is the time required to finish the program. The goal is to maximize this quantity. 

Why don’t we achieve the peak FLOPs? The Peak FLOPs values are typically achieved by matrix multiplications but a neural network model like a transformer involves many layer operations (memory bounded) that lower the performance. Moreover, we have seen how optimization can greatly reduce the time of execution and increase the operations within the same time. Unoptimized code can lower the value further. Adding to these communication like we have discussed before can reduce the quantity a lot too. Finally, all these parameters also depend on the precision, code and the GPU type being used. 

How do we calculate the MFU?
1. Count the number of operations (FLOPs) considering the shape of the data in the model. For example, we have seen the formula $$2mnp$$ for `matmul`. 
2. Run the model for one forward and backward pass on the GPU to get benchmark for the running time $$t$$ 
3. Check the GPU spec, type of cores, and their peak FLOPs
4. Calculate the MFU

What is the history of these numbers?
1. The best ML system on V100 a couple years ago got 40-50% MFU - only half the peak utilization! The peak FLOPs on V100 is 112 TFLOPs, so we were only able to use 50-60 FLOPs
2. With A100, we were still in the same range until FlashAttention came up, which took the MFU value to 60%! A100 has 312 FLOPs, so we are using ~160FLOPs at this stage!
3. With H100, we are able to use only 30-50% depending on the model size (Larger `matmul` is better for MFU over smaller). Why did it decrease? The peak value of H100 is very high (990 TFLOPs), and our software did not catch up to this. Remember that communication also plays an important role
4. This year, with B100 the peak FLOPs is 1.8 PFLOPs!

Besides MFU, we also define **Hardware FLOPs Utilization (HFU)**. This quantity is to consider operations that do not contribute to the model
For example, we can treat gradient checkpointing as 2 forward passes and 1 backward pass (each backward pass can be approximated as 2 forward passes due to gradient updates)

## Collective Communication 
In Machine Learning systems, there are usually two types of connections
1. Point-to-point communication - Comprises of a sender and a receiver. Very simple. 
2. Collective Communication - It’s a common concept in HPC (high performance computing). There are multiple forms of this
    1. Broadcast - One worker shares data with all the other workers
    2. Reduce(-to-one) - All the data among the workers is combined into one rank (one worker). Essentially reverse of broadcast in a way. 
    3. Scatter - The data from one rank is split and each of the splits is *scattered* across other ranks. That is, every worker gets on part of the data.
    4. Gather - The reverse operation of Scatter, where different parts of the data are *gathered* into one rank.
    5. All-gather - Essentially gather followed by broadcast. 
    6. Reduce-scatter - Essentially reduce followed by scatter.
    7. All-reduce - Reduce followed by scatter. 

    Collective communication is more expensive than P2P since it can be thought of as a combination of many P2Ps. However, collective communication has been highly optimized in the past 2 decades (`(x)ccl` libraries - NVIDIA has `NCCL` the best, Microsoft has `MCCL` and Intel has `OneCCL`). The important thing to note is that collective communication is not fault tolerant (if one device fails, then everything fails).

### Basics
Let us understand some more terminology for communication models. There is a terminology called as $$\alpha\beta$$ model that talks about latency and bandwidth. For example, if the model is $$\alpha + n \beta$$ then $$\alpha$$ refers to the latency and $$\beta = 1/B$$ is the reciprocal of bandwidth determined by the hardware. For smaller messages, latency ($$\alpha$$) is the dominant factor, and for larger messages (larger $$n$$), the bandwidth utilization ($$n\beta$$) dominates.

Based on these distinctions, the community works on two mainstream algorithms/implementations
1. For smaller messages, an MST based algorithm that emphasizes low latency
2. For larger messages, a **Ring algorithm** to emphasize on bandwidth utilization.

There are over 50+ algorithms in this area and the HPC community even got the 2023 Turing award!

The core principle for lower latency is to minimize the number of rounds needed for communication. For example, consider the Broadcast operation. 
1. We first split the ranks into half, and send the message to the half without the message
2. Then repeat broadcast recursively in each half

*Beautiful.*

For the core operations, we have the following communication models
- Reduce-to-one - $$\log(p) (\alpha + n \beta + n \gamma) $$
- Scatter and Gather- $$\log(p)\alpha + \frac{p - 1}{p} n\beta $$
- Broadcast - $$\log(p)(\alpha + n \beta)$$

The remaining composite operations can simply use these primitives. 

What are the problems with this approach? Since latency is prioritized over bandwidth, some links are idle. Building on the same idea, the core principle for high-bandwidth is to use all links between every two nodes. 

The **ring algorithm** essentially is a logical ring that can be embedded in a physical linear array with worm-hole routing such that the “wrap-around” message does not conflict. Look at the diagram below for clarity - 

![](/assets/img/2025-01-06-data-systems-for-ml/17395038648341.jpg)
At every instant, all the links are used ensuring full-bandwidth. For example, reduce-scatter can be implemented as consecutive reduce and propagate ring passes. 

For this regime of communication, the core primitives of communication are 
- Reduce-scatter
- Scatter
- Gather
- All-gather
and the other operations can be implemented as the composites of these
- Reduce(-to-one) is reduce-scatter followed by gather
- All-reduce is reduce-scatter followed by all-gather
- Broadcast is scatter followed by all-gather.

So, how does this all come back to ML? ML systems are usually composed of large data communications. Inter-op systems always result in P2P communications and intra-op result in collective communication. 

![](/assets/img/2025-01-06-data-systems-for-ml/17395041727515.jpg)

The key point to note is that communication always happens between nodes that are partitioned differently. Remember that with tensors there are more dimensions, and the above diagram is a simplified version.