# [Guest Lecture: Hongyang Zhang] Speculative Decoding

The vanilla autoregressive inference involves sampling tokens from a probability distribution that is obtained by the tokens generated so far. 

![](/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-18-35-07-image.png)

How do we move from this sequential sampling framework to a parallel one?



**Speculative sampling framework** (introduced in [Leviathan et al](https://arxiv.org/abs/2211.17192)) uses a much smaller model that is a good approximation of the original large model. This tiny **draft model** is used to create a *draft* of the response in the same sequential autoregressive manner. Since the model is tiny, the inference is faster. 

The tokens generated by the draft model are fed into the LLM to generate the distributions from these tokens in parallel. The draft model can be verified by comparing the distributions of both the models for a given token. 

1. We generate a number $$r$$ between 0 and 1 uniformly, and check if $$r < \min(1, p(t)/q(t))$$ where $$p$$ is the LLM distribution and $$q$$ is the draft model distribution. 

2. If the condition holds, we accept the token. Otherwise we correct the token by sampling $$t' \sim \norm(\max(0, p - q))$$. All the tokens following this corrected token are discarded, and the process is repeated again.

There is theoretical work that shows that the above procedure is equivalent to sampling from $$p$$.

Given this, how do we build the tiny model $$q$$? That is the crux of the methods in this domain. 

> Does this framework only work for greedy sampling? No, it can be extended to non-greedy sampling techniques with some modifications. 

> The tiny model used can have low rejection rates for tasks like summarization but may have high rejection rates for tasks involving reasoning. So, there's a trade-off with the draft model overhead and the rejection rate.

### Building $$q$$ with EAGLE-v1

As mentioned before, building $$q$$ is a trade-off between accuracy and efficiency. They have proposed EAGLE.

The authors of this paper observed that the initial distribution of the embeddings in the initial layers is very complicated and dynamic. However, for the top layers, the distribution becomes much simpler - that is, the distribution to predict next tokens from previous tokens. 

So, they tried to estimate the features rather than the tokens directly. In their first attempt, they trained a single transformer layer to learn the embeddings (features) in the last layer of LLMs. Essentially,  instead of using the first layer tokens, they used the last layer features for the speculative decoding framework. With this, they obtained a much higher speed-up and accuracy!

However, they noted that this model does not use feature uncertainty correctly. In the speculative decoding framework, the parallel decoding in the large LLM does not consider the previously generated 



To solve this, they tried another method by predicting the distribution by concatenating the initial token embedding with the feature from the last layer. They attribute this as adding a shifted token embedding. 

![](/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-19-01-18-image.png)



Essentially, we generate choices through the draft model rather than a single prediction. With this, we obtain a much higher speed-up and accuracy -

![](/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-18-55-53-image.png)

Note that the attention mask for thi has to be carefully design as follows - 

![](/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-19-07-29-image.png)

Because of this, the model cannot handle long-context settings.

The draft model can be around 1-3% (higher percentage for smaller models to account for the dimensionality of the latent space) of the larger model leading to a very high speed-up!

### EAGLE-v2

The authors noted that the method can be improved even further. They noted that the accuracy of the speculative decoding tree if higher to the top-left and lowest to bottom-right. So, they associated a confidence score to each node in the tree. Ideally, that would be $$\min(1, p(t)/q(t))$$, but they found that $$q(t)$$ is a good approximation and has a strong linear correlation with the actual quantity. 

With this, they build a context-aware dynamic draft tree. 

Essentially, instead of generating a tree, they do beam-search with the small model. 

![](/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-19-22-55-image.png)

As expected, this method obtained even higher speedup and accuracy!

They didn't stop there.

### EAGLE-v3

They published yesterday lol! With the new method that has a modified training routine and a loss function, they noticed a new scaling law that seems to perform better as the model grows. Essentially, they introduced draft model inference in the training routine to better simulate the inference routine.

They also observed that the performance is the best when the draft model architecture is the same as the large model. 

> The draft model just generates a draft and not the complete response. For example, it will generate 6 words, and these are used by the larger LLM. So it's different from knowledge distillation.






