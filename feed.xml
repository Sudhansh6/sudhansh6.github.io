<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://sudhansh6.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sudhansh6.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-17T19:27:04+00:00</updated><id>https://sudhansh6.github.io/feed.xml</id><title type="html">Sudhansh Peddabomma</title><entry><title type="html">Key Works in ML Systems</title><link href="https://sudhansh6.github.io/blog/machine-learning-systems/" rel="alternate" type="text/html" title="Key Works in ML Systems"/><published>2025-01-16T00:00:00+00:00</published><updated>2025-01-16T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/machine-learning-systems</id><content type="html" xml:base="https://sudhansh6.github.io/blog/machine-learning-systems/"><![CDATA[<blockquote> <p>Acknowledging use of Perplexity, ChatGPT and the reference books and papers to aid with the content.</p> </blockquote> <h1 id="introduction">Introduction</h1> <p>As with every other article, this one starts with the same remark too - We are at an inflection point in human history. Artificial Intelligence (AI) is everywhere, and its growth has been unparalleled. We are in the AI revolution.</p> <p>At times like these, it is important to think about the future - how do we create systems that work alongside humanity to tackle the biggest problems life faces? We must master a new field of engineering to maintain this unprecedented pace.</p> <h2 id="history-of-ai">History of AI</h2> <p><img src="assets/reading01/2025-01-16-17-56-14-image.png" alt=""/></p> <p>Starting with symbolic AI, one of the early approaches, STUDENT system developed by Daniel Bobrow in 1964, demonstrated natural language understanding by converting English text into algebraic equations. However, it was a rule-based system that was very brittle to the grammatical structure - although the solution may appear intelligent, it did not have a genuine understanding of language or the actions it was taking. This issue is the primary problem behind rule-based approaches.</p> <p>Then followed expert systems in 1970s, which focused on domain specific problems over generalized AI. For example, MYCIN developed by Stanford showed a major leap in medical AI with it’s 600 rules-based system. Nevertheless, scaling these rules with newer discoveries is infeasible.</p> <h3 id="statistical-learning">Statistical Learning</h3> <p>In the 1990s, something radical happened - the field moved away from rules to data-driven approaches. With the digital revolution, new capabilities got unlocked. From regression to neural networks, we allowed machines to discover patterns in the data leading to diverse applications. This new paradigm changed the way we approached machine learning. Quality of data, metrics to evaluate performance and trade-offs in design choices - all became relevant research questions.</p> <p>During the 2000s, algorithms like decision trees and KNNs made their way into practical applications. SVMs with their “kernel trick” became very popular. Human engineered features combined with statistical learning was the characteristic in the algorithms, and they had strong mathematical foundations as well. The models performed well with limited data, and produced reliable results. Even modalities like face detection with Viola-Jones algorithm (2001) became feasible.</p> <h3 id="deep-learning">Deep Learning</h3> <p>A double-edged sword, deep learning became the new kid in the block since the 2010s. Deep learning networks automatically discovered features in the data, doing away with feature engineering. Starting with AlexNet in 2012, the successes with deep learning models have never been shy. Researchers realized that bigger models equals better performance. Now, in the 2020s, we have entered the age of large models which have parameters reaching into few hundreds of billions! The datasets are well into the Petabytes stage.</p> <p>The three pillars required for these models to be successful, big data, better compute, and algorithmic breakthroughs, have successfully been put in place over the past few decades.</p> <p>These new depths have raised important questions about the future: how do we store and serve such models and datasets?!</p> <h2 id="ml-systems-engineering">ML Systems Engineering</h2> <p>It is the discipline of designing, implementing, and operating AI systems across computing scales. A machine learning system is an integrated computing system with three main parts - data, models and the compute infrastructure.</p> <p>Software Engineering as a field has been well established over the past decade. Even though the field is not yet mature, it has practices in place to enable reliable development, testing and deployment of software systems. However, these ideas are not entirely applicable to Machine Learning systems. Changes in the data distribution can alter the system behavior - this is a new paradigm we have not addressed before.</p> <p>More than often, the performance requirements guide the design decisions in the architecture. The complexities arise due to the broad spectrum across which ML is deployed today - from edge-devices to massive GPU-cloud clusters, each presents unique challenges and constraints. Operational complexities increase with system distribution. Some applications may have privacy requirements. The budget of the system acts as an important constraint. All these tradeoffs are rarely simple choices.</p> <p>Modern ML systems must seamlessly connect with existing software, process diverse data sources, and operate across organizational boundaries, driving new approaches to system design. FarmBeats by Microsoft, Alphafold by Deepmind and Autonomous vehicles are excellent examples of how proper systems in place can really push the extent of ML applicability.</p> <h2 id="challenges">Challenges</h2> <ol> <li> <p>Data Challenges - How do we store and process different kinds of data, and how to accommodate patterns with time?</p> </li> <li> <p>Model Challenges - How do we create efficient systems for different forms of learning, and test their performance across a wide range of scenarios?</p> </li> <li> <p>Systems Challenges - How do we set up pipelines in place to combine all of this in place? Systems that have monitoring and stats, that allow model updates and handle operational challenges.</p> </li> <li> <p>Ethical and Social Considerations - How do we address bias in such large-scale models? Can we do something about the “black-box” nature? Can we handle data privacy and handle inference attacks?</p> </li> </ol> <p>A major solution to address all these challenges has been to <em>democratize AI technology</em>. Similar to an “all hands-on deck” solution, with the involvement of a large amount of people in this evolution, we are tackling one of the most innovative problem’s humanity has ever faced - how do we achieve AGI?</p> <h1 id="dnn-architectures">DNN Architectures</h1> <p>Assuming the reader’s know enough about different model architectures, we shall now discuss the core computations involved in these models to design the systems around them.</p> <h2 id="architectural-building-blocks">Architectural Building Blocks</h2> <p>So far, we have the following major architectures summarized below -</p> <h3 id="multi-layer-perceptrons-mlps">Multi-Layer Perceptrons (MLPs)</h3> <ul> <li> <p>Purpose: Dense pattern processing</p> </li> <li> <p>Structure: Fully connected layers</p> </li> <li> <p>Key operation: Matrix multiplication</p> </li> <li> <p>System implications:</p> <ul> <li>High memory bandwidth requirements</li> <li>Intensive computation needs</li> <li>Significant data movement</li> </ul> </li> </ul> <h3 id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</h3> <ul> <li> <p>Purpose: Spatial pattern processing</p> </li> <li> <p>Key innovations: Parameter sharing, local connectivity</p> </li> <li> <p>Core operation: Convolution (implemented as matrix multiplication)</p> </li> <li> <p>System implications:</p> <ul> <li>Efficient memory access patterns</li> <li>Specialized hardware support (e.g., tensor cores)</li> <li>Opportunity for data reuse</li> </ul> </li> </ul> <h3 id="recurrent-neural-networks-rnns">Recurrent Neural Networks (RNNs)</h3> <ul> <li> <p>Purpose: Sequential pattern processing</p> </li> <li> <p>Key feature: Maintenance of internal state</p> </li> <li> <p>Core operation: Matrix multiplication with state update</p> </li> <li> <p>System implications:</p> <ul> <li>Sequential processing challenges</li> <li>Memory hierarchy optimization for state management</li> <li>Variable-length input handling</li> </ul> </li> </ul> <h3 id="transformers-and-attention-mechanisms">Transformers and Attention Mechanisms</h3> <ul> <li> <p>Purpose: Dynamic pattern processing</p> </li> <li> <p>Key innovation: Self-attention mechanism</p> </li> <li> <p>Core operations: Matrix multiplication and attention computation</p> </li> <li> <p>System implications:</p> <ul> <li>High memory requirements</li> <li>Intensive computation for attention</li> <li>Complex data movement patterns</li> </ul> </li> </ul> <p>Some innovations like skip connections, normalization techniques, and gating mechanisms are highlighted as important building blocks that require specific architectures. With these in mind, we require the following primitives -</p> <ul> <li> <p><strong>Core Computational Primitives</strong> -Matrix multiplication, Sliding window operations, Dynamic computation</p> </li> <li> <p><strong>Memory Access Primitives</strong> - Sequential access, Strided access, Random access</p> </li> <li> <p><strong>Data Movement Primitives</strong> - Broadcast, Scatter, Gather and Reduction</p> </li> </ul> <p>We address these primitives, by designing efficient systems as well as hardware.</p> <ul> <li> <p><strong>Hardware</strong> - Development of specialized processing units (e.g., tensor cores), Complex memory hierarchies and high-bandwidth memory, Flexible interconnects for diverse data movement patterns</p> </li> <li> <p><strong>Software</strong> - Efficient matrix multiplication libraries, Data layout optimizations, Dynamic graph execution support</p> </li> </ul> <h2 id="conclusion">Conclusion</h2> <p>In summary, understanding the relationship between high-level architectural concepts is important for their implementation in computing systems. The future advancements in deep learning will likely stem from both novel architectural designs and innovative approaches to implementing and optimizing fundamental computational patterns.</p> <p>Now that we have understood the basics of Machine Learning Systems, let us delve into the two biggest frameworks that support the ML systems today.</p> <h1 id="tensorflow-a-system-for-large-scale-machine-learning"><a href="https://arxiv.org/pdf/1605.08695">TensorFlow: A system for large-scale machine learning</a></h1> <p>A product of Google brain built to tackle large scale systems in heterogeneous environments. TensorFlow uses data-flow graphs to represent computations, shared states and operations. These can be mapped across many machines giving a flexibility to the developers.</p> <p>TensorFlow is based on a previous product of Google Brain, DistChild, that was used for a large number of research and commercial tasks. The team recognized the recent advances in ML - CNNs that broke records reaching up to millions of parameters and GPUs that accelerated the training processes. They developed a framework that is well-optimized for general scenarios for both training and inference. They also meant for it to be <em>extensible</em> - to allow ability to experiment and scale in production with the same code.</p> <p>Prior to this work, Caffe, Theano and Torch were the major frameworks. Caffe was known for its high-performance, Theano for its data flow model and Torch for its imperative programming model. Along with these works, TensorFlow also draws inspiration from the map-reduce paradigm that improved the performance of data systems significantly.</p> <h2 id="tensorflow-execution-model">TensorFlow execution model</h2> <p>The computation and state of the algorithm, including the mathematical operations are represented in a single dataflow graph. The communications between the subcomputations are made explicitly to execute independent computations in parallel and partition the computation across distributed devices. The key point to note here is that the graph has mutable states for the individual vertices, meaning that data can be shared between different executions of the graph. Although this point may seem trivial now, architectures prior to TensorFlow worked with static computations graphs that did not allow changes to the weights - algorithms such as mini-batch gradient descent were not scalable to large parameter models. Due to this change in the architecture, developers can experiment with different optimization algorithms, consistency schemes and parallelization strategies.</p> <h3 id="dataflow-graph-elements">Dataflow graph elements</h3> <p>Each vertex in the dataflow graph is an atomic unit of computation and each edge is the output or input to a vertex - values flowing through tensors.</p> <p><strong>Tensors</strong> - Notably, <em>tensors</em> as a computational quantity were introduced in this paper. Their work assumes all tensors are dense - this allows simple implementations for memory allocation and serialization.</p> <blockquote> <p>Modern neural networks on the contrary have sparse tensors in many scenarios - can be optimized greatly</p> </blockquote> <p>TensorFlow does allow representing sparse tensors but at the cost of more sophisticated shape inference.</p> <p><strong>Operations</strong> - An operation can simply be thought of as a function that takes \(m \geq 0\) tensors as input and returns \(n \geq 0\) tensors as output. The number of arguments to these operators can be constant or variable (<em>variadic</em> operators). <strong>Stateful operations</strong> (variables and queues) contain mutable states that can be updated every time the operator executes. Variables are mutable buffers storing shared parameters and queues support advanced forms of coordination.</p> <h3 id="partial-and-concurrent-execution">Partial and concurrent execution</h3> <p>The key advantage of storing the dataflow as a graph is the ability to execute independent operations in parallel. Once a graph is defined by the user, the API allows for executing any sub-graph the user queries. Each invocation is a <em>step</em> and TensorFlow supports multiple <em>concurrent steps</em>. This ability shines for the batch-processing workflows in neural networks. Furthermore, TensorFlow has a checkpointing subgraph that runs periodically for fault tolerance.</p> <p>This functionality of running graphs partially and concurrently contribute much to TensorFlow’s flexibility.</p> <h3 id="distributed-execution">Distributed execution</h3> <p>Since the communication between subcomputations is explicit, the distribution of the dataflow becomes simpler. Each operation resides on a particular <em>device</em> (note that this feature has also been adapted in PyTorch), and a device is responsible for executing a <em>kernel</em> for each operation assigned to it. TensorFlow allows multiple kernels to be registered for a single operation.</p> <p>The placement algorithm computes a feasible set of devices for each operation, calculates the sets of operations that must be colocated and selects a satisfying device for each colocation group. In addition, the users can also specify their device preferences for a particular task. <em>Yes, TensorFlow was advanced since the beginning</em>.</p> <p>Once the operations are placed, the partial subgraphs are computed for a step, and are pruned and partitioned across the devices. The communication mechanisms between devices is also put in place with specialized implementations to optimize for latency with large-subgraphs running repeatedly. On the user-end, a <em>client session</em> maintains a mapping from step definitions to cached subgraphs. This computation model performs the best with static, reusable graphs but also supports dynamic computations with control flows.</p> <h3 id="dynamic-control-flow">Dynamic control flow</h3> <p>Although most evaluation in TensorFlow is <em>strict</em>, wherein it requires for all inputs to an operation to be computed before the operation executes, advanced algorithms (such as RNNs), require dynamic control flow, requiring non-strict evaluation. To aid with this, TensorFlow also supports primitive <strong>Switch</strong> (demultiplexer) and <strong>Merge</strong> (multiplexer) operations for dynamic dataflow architectures! These primitives can be used to build a non-strict conditional sub-graph that executes one of two branches based on the runtime values of a tensor. These primitives also support loops with additional structural constraints based on the dataflow!</p> <h2 id="extensibility">Extensibility</h2> <ul> <li> <p><strong>Differentiation</strong> - TensorFlow has a user-level library that <strong>automatically differentiates</strong> expressions. It performs a breadth-first search to identify all of the backwards paths from the target operation (loss function) to a set of parameters in the computation graph, and sums the partial gradients that each path contributes. With optimizations such as batch normalization and gradient clipping, the algorithm also supports backpropagation through conditional and iterative subcomputations! All of this done with memory management on GPU.</p> </li> <li> <p><strong>Optimization</strong> - SGD is a simple algorithm encoded in the framework. However, for more advanced optimization schemes like Momentum, TensorFlow relies on community driven implementations that are easily pluggable without modifying the underlying system. Such a modular framework was not available before.</p> </li> <li> <p><strong>Handling Large models</strong> - Even back then, the language models were too large to store in RAM of a single host. For the language specific case, TensorFlow has <em>sparse embedding layers</em> that is a primitive operation that abstracts storing and reading a tensor across different memory spaces. They are implemented with operators such as <code class="language-plaintext highlighter-rouge">gather</code>, <code class="language-plaintext highlighter-rouge">part</code> and <code class="language-plaintext highlighter-rouge">stitch</code>, and their gradients are also implemented. Along with innovations such as Project Adam, TensorFlow’s training was ultimately made efficient through community driven improvements.</p> </li> <li> <p><strong>Fault Tolerance</strong> - Since many learning algorithms do not require consistency and writing at every step is compute intensive, TensorFlow implements user-level checkpointing for fault tolerance. This design decision leaves it to the user to build their own best fault handling mechanisms. <em>I wish they had an automatic version as well</em>.</p> </li> <li> <p><strong>Synchronous replica coordination</strong> - SGD is robust to asynchrony, and TensorFlow is designed for asynchronous training. In the asynchronous case, each worker reads the current value when the step begins, applies its gradient to the different current value at the end. The synchronous cases use queues to coordinate execution, allowing multiple gradient updates together. Although the throughput is reduced, this way of training is more efficient. TensorFlow implements <em>backup workers</em> to improve the throughput of this synchronous case by 15%.</p> </li> </ul> <h2 id="implementation">Implementation</h2> <p>The core TensorFlow library is implemented in C++ for portability and performance with its implementation being open-source. It consists of</p> <ul> <li> <p>Distributed master - given a graph and a step definition, it prunes and partitions the graphs to each devices, and caches these subgraphs so that they may be reused in subsequent steps.</p> </li> <li> <p>Dataflow executor - Schedules the execution of the kernels that comprise a local subgraph - optimized for running large fine-grained graphs with low overhead.</p> </li> </ul> <p>The API can be accessed both via C++ and Python.</p> <p>The library does not provide huge gains for single-system training but has higher throughput for large models across multiple devices.</p> <h2 id="conclusion-1">Conclusion</h2> <p>When this paper was published, TensorFlow was still a work in progress. Later, the library transformed significantly with the introduction of v2, and other optimizations.</p> <h1 id="pytorch-an-imperative-style-high-performance-deep-learning-library"><a href="https://arxiv.org/abs/1912.01703">PyTorch: An Imperative Style, High-Performance Deep Learning Library</a></h1> <p>A product of Facebook research, PyTorch is the most popular deep-learning library. They addressed the biggest limitation in previous frameworks - usability. They targeted both performance and usability by designing an imperative-style ML framework.</p> <p>In contrast to PyTorch, the previous approaches created a static dataflow graph that represents the computation. Since the whole computation is visible ahead of time, it can be leveraged to improve performance and scalability. However, due to this, the usability is reduced and cannot be used to iteratively build experiments. PyTorch, a python library, performs immediate execution of dynamic tensor computations with automatic differentiation and GPU acceleration while maintaining comparable performance to the static libraries.</p> <h2 id="background">Background</h2> <p>There were four major trends in scientific computing that have become important for deep learning:</p> <ol> <li>Development of domain-specific languages and libraries for tensor manipulation (e.g., APL, MATLAB, NumPy made array-based productive productive)</li> <li>Automatic differentiation, making it easier to experiment with different machine learning approaches</li> <li>Shift towards open-source Python ecosystem from proprietary software. The network effects of Python contributed to its exponential growth.</li> <li>Availability of general-purpose parallel hardware like GPUs</li> </ol> <p>PyTorch builds on these trends by providing an array-based programming model accelerated by GPUs and differentiable via automatic differentiation integrated in the Python ecosystem.</p> <h2 id="design-principles">Design Principles</h2> <p>PyTorch’s design is based on four main principles:</p> <ol> <li>Be Pythonic: Integrate naturally with Python ecosystem, keep interfaces simple and consistent</li> <li>Put researchers first: Make writing models, data loaders, and optimizers easy and productive</li> <li>Provide pragmatic performance: Deliver compelling performance without sacrificing simplicity</li> <li>“Worse is better”: Prefer simple, slightly incomplete solutions over complex, comprehensive designs</li> </ol> <h2 id="usability-centric-design">Usability-centric Design</h2> <p>The approach to PyTorch starts by considering deep-learning models as just another Python program. By considering so, PyTorch maintains the imperative programming model inspired from Chainer and Dynet. Defining layers, composing models, loading data, running optimizers and parallelizing the training process can all be expressed using familiar Python syntax. It allows any new potential neural network architecture to be easily implemented with composability.</p> <p>Mainly, since PyTorch programs execute eagerly, all features of Python like print, debugging and visualization work as expected. In addition, PyTorch has -</p> <ul> <li> <p><strong>Interoperability</strong> - Integrated well with other libraries to allow bidirectional exchange of data (NumPy, Matplotlib, etc).</p> </li> <li> <p><strong>Extensibility</strong> - Supports custom differentiable functions and datasets. The abstractions take care of shuffling, batching, parallelization, and management of pinned CUDA memory to improve the throughput and performance. In general, PyTorch components are completely interchangeable!</p> </li> </ul> <h3 id="automatic-differentiation">Automatic differentiation</h3> <p>Python is a dynamic programming language that has most of the behavior defined at run-time, making it difficult to pre-calculate the differentiation. Instead, PyTorch uses operator overloading approach to build up a representation of the computed function every time it is executed. It notes the difference between forward mode and backward mode automatic differentiation and adopts the latter which is better suited for ML applications.</p> <p>Their system can differentiate through code with mutation on tensors as well. They also have a versioning system for tensors as a failsafe to track the modifications.</p> <h2 id="performance-focused-implementation">Performance-focused Implementation</h2> <p>With all these considerations to usability, the developers implemented many tricks to maintain the performance of the library. Since the models have to run on Python interpreter, which has its own limitations such as the global interpreter lock (ensures only one of any concurrent threads is running at a given time), PyTorch optimized every aspect of execution and also enabled users to add their own optimization strategies. Prior frameworks avoided these constraints by deferring the evaluation to their custom interpreters.</p> <p>Most of PyTorch is implemented C++ for high performance. The core <code class="language-plaintext highlighter-rouge">libtorch</code> library implements the tensor data structure, GPU and CPU operators, automatic differentiation system with gradient formulas for most built-in functions and basic parallel primitives. The pre-computed gradient functions allow computation of gradients of core PyTorch operators in a multithreaded evaluation evading the Python global interpreter lock. The bindings are generated using YAML meta-data files, that allowed the community to create bindings to other languages.</p> <p>PyTorch separates the control (program branches, loops) and the data flow (tensors and the operations). The Control flow handled by Python and optimized C++ code on CPU and Data flow can be executed on CPU or GPU. PyTorch is designed to run asynchronously leveraging the CUDA stream mechanism. This allows overlap of Python code execution on CPU with tensor operations on the GPU, effectively saturating GPU with large tensor operations. The main performance cover-up comes from this design.</p> <p>Since every operator needs to allocate an output tensor to hold the results, the speed of <em>dynamic memory allocators</em> needs to be optimized. CPU has efficient libraries to handle this. However, to avoid the bottleneck of <code class="language-plaintext highlighter-rouge">cudaFree</code> routine that blocks code until all previously queued work on GPU completes, PyTorch implements its custom allocator that incrementally builds up a cache of CUDA memory. It reassigns it to later allocations without CUD APIs for better interoperability allowing users to use other GPU enabled Python packages. This allocator is further optimized with memory usage patterns and its implementation is simplified with the <em>one-pool-per-stream</em> assumption.</p> <p>The <code class="language-plaintext highlighter-rouge">multiprocessing</code> library was developed to evade the global interpreter lock on Python. However, this is inefficient for large arrays, so it is extended as <code class="language-plaintext highlighter-rouge">torch.multiprocessing</code> to allow automatic movement of data to shared memory improving the performance significantly. It also transparently handles sharing of CUDA tensors to build analytical systems on top of this.</p> <p>Since users write models to consume all the memory resources, PyTorch treats memory as a scarce resource and handles it carefully. The overheads of garbage collection are too large. To solve this, PyTorch relies on a reference counting scheme to track the number of uses of each tensors, and frees the underlying memory <em>immediately</em> when this count reaches zero. This ensures immediate freeing of memory when tensors become unneeded.</p> <p>With all these optimizations, PyTorch achieves</p> <ul> <li> <p>ability to asynchronously execute dataflow on GPU with almost perfect device utilization</p> </li> <li> <p>custom memory allocator showing improved performance</p> </li> <li> <p>performance within 17% of the fastest framework across all benchmarks</p> </li> </ul> <h2 id="conclusion-2">Conclusion</h2> <p>The paper concludes by highlighting PyTorch’s success in combining usability with performance. Future work includes:</p> <ul> <li>Developing PyTorch JIT for optimized execution outside the Python interpreter</li> <li>Improving support for distributed computation</li> <li>Providing efficient primitives for data parallelism</li> <li>Developing a Pythonic library for model parallelism based on remote procedure calls</li> </ul> <p>##</p> <h1 id="deep-learning-performance-on-gpus">Deep Learning Performance on GPUs</h1> <blockquote> <p>Source: https://docs.nvidia.com/deeplearning/performance/index.html#performance-background</p> </blockquote> <p>GPUs accelerate computing by performing independent calculations in parallel. Since deep learning operations involve many such operations, they can leverage GPUs very well.</p> <p>It is useful to understand how deep learning operations are utilizing the GPU. For example, operations that are not matrix multiplies, such as activation functions and batch-normalization, often are <em>memory-bound</em>. In these cases, tweaking parameters to more efficiently use the GPU can be ineffective. In these cases, the data movement between the host and the device can limit the performance. By understanding such intricacies, we can design better code to leverage GPUs to the fullest.</p> <blockquote> <p>We shall discuss the concepts in context of NVIDIA GPUs because of the existing monopoly.</p> </blockquote> <h2 id="architecture-fundamentals">Architecture Fundamentals</h2> <p>The core paradigm of GPU is parallel compute. It has processing elements and a memory hierarchy. At a high level, GPUs consist of Streaming Multiprocessors (SMs), on-chip L2 cache, and high-bandwidth RAM.</p> <p>Each SM has its own instruction scheduler and execution pipelines. In a neural network, <em>multiply-add</em> is the most frequent operation. A typical spec sheet shows FLOPS rate of GPU operations - #SMs*SM clock rate*#FLOPs.</p> <h2 id="execution-model">Execution Model</h2> <p>GPUs execute functions in a 2-level hierarchy of threads - a given function’s threads are grouped into equally-sized <em>thread blocks</em>, and a set of thread blocks are launched to execute the function. The latency caused by dependent instructions is hidden by switching to the execution of other threads. As a result, the number of threads needed to effectively use a GPU is much higher than the number of cores or instruction pipelines.</p> <p>At runtime, each thread block is placed on an SM for execution. All the threads in the block communicate via a shared memory in the blocks and synchronize efficiently. To use the GPU to the maximum, we need to execute many SMs (since each threadblock only activates one SM). Furthermore, each SM can execute multiple threadblocks simultaneously. Therefore, we need to launch many threadblocks, a number several times higher than the number of SMs.</p> <p>These thread blocks running concurrently are termed as a <em>wave</em>. In effect, we need to launch functions that execute several waves of thread blocks to minimize the <em>tail effect</em> (towards the end of a function, only a few active thread blocks remain).</p> <h2 id="understanding-performance">Understanding performance</h2> <p>There are three factors determining performance - <strong>memory bandwidth, math bandwidth and latency</strong>. To analyze these better, let \(T_{mem}\) be the time spent in accessing memory and \(T_{math}\) is the time spent performing math operations.</p> \[\begin{align*} T_{mem} &amp;= \text{\# bytes accessed}/ \text{memory bandwidth} \\ T_{math} &amp;= \#\text{ math ops}/\text{math bandwidth} \end{align*}\] <p>If these functions can be overlapped, then the total time is approximately \(\max(T_{mem}, T_{math})\). And based on the dominance, we categorize the functions as <em>math-limited</em> and <em>memory-limited</em>. So if a function is math-limited, we have</p> \[\#\text{ops}/\#\text{bytes} &gt; BW_{math}/BW_{mem}\] <p>The left-hand side of this inequality is known as <strong>arithmetic intensity</strong>, and the right-hand side is called as <em>ops:byte</em> ratio. Based on this description, we have the following data for NVIDIA V100 GPU for FP16 data -</p> <table> <thead> <tr> <th>Operation</th> <th>Arithmetic Intensity</th> <th>Usually limited by…</th> </tr> </thead> <tbody> <tr> <td>Linear layer (4096 outputs, 1024 inputs, batch size 512)</td> <td>315 FLOPS/B</td> <td>arithmetic</td> </tr> <tr> <td>Linear layer (4096 outputs, 1024 inputs, batch size 1)</td> <td>1 FLOPS/B</td> <td>memory</td> </tr> <tr> <td>Max pooling with 3x3 window and unit stride</td> <td>2.25 FLOPS/B</td> <td>memory</td> </tr> <tr> <td>ReLU activation</td> <td>0.25 FLOPS/B</td> <td>memory</td> </tr> <tr> <td>Layer normalization</td> <td>&lt; 10 FLOPS/B</td> <td>memory</td> </tr> </tbody> </table> <p>Many common operations have low arithmetic intensities (memory-limited). <em>Note.</em> This analysis assumes that the workload is large enough to saturate the device and not be limited by latency.</p> <h2 id="operation-categories">Operation Categories</h2> <p>There are three main types of operations in a neural network</p> <ol> <li> <p>Element-wise operations - Unary/binary operations such as ReLU and element-wise addition. These layers tend to be memory-limited as they perform few operations per byte accessed.</p> </li> <li> <p>Reduction operations - Operations that produce values over a range of input tensor values, such as pooling, softmax and normalization layers. These layers usually have low arithmetic intensity (memory limited).</p> </li> <li> <p>Dot-product operations - Operations can be expressed as dot-product of two tensors, such as attention and fully-connected layers. These operations tend to be math-limited if the matrices are large. For smaller operations, these are memory limited.</p> </li> </ol> <h2 id="summary">Summary</h2> <p>The number of SMs in the GPU determines the ops:bytes ratio. To maximize the performance, ensure sufficient parallelism by oversubscribing the SMs. The most likely performance limiter is</p> <ul> <li>Latency if there is not sufficient parallelism</li> <li>Math if there is sufficient parallelism and algorithm arithmetic intensity is higher than the GPU ops:byte ratio</li> <li>Memory if there is sufficient parallelism and algorithm arithmetic intensity is lower than the GPU ops:byte ratio</li> </ul> <h1 id="matrix-multiplication">Matrix Multiplication</h1> <p>As we’ve observed previously, General Matrix Multiplications (GEMMs) is the most frequent operation in neural network layers. For the sake of the discussion, based on the conventions, let \(A \in \mathbb R^{m \times k}, B \in \mathbb R^{k \times n}\) and \(C = A \times \in \mathbb R^{m \times n}\). \(C\) would require a total of \(M*N*K\) fused multiply-adds (FMAs) (twice for the FLOPs). In general, for smaller multiplications, the operation is memory limited, and otherwise it is math-limited. As a consequence, vector products \(M = 1\) or \(N = 1\) are always memory limited and their arithmetic intensity is less than 1.</p> <h2 id="implementation-1">Implementation</h2> <p>Most of the GPUs implement matrix multiplication as a tiling operation - each thread block computes its output tile by stepping through the \(K\) dimension in tiles.</p> <p>The latest NVIDIA GPUs have Tensor Cores to maximize multiplications in tensors. The performance is better when \(M, n&lt; k\) are aligned to multiples of 16 bytes.</p> <p>To aid with the code design, NVIDIA has provided the cuBLAS library that contains its optimized GEMM implementations. The tradeoff arises between parallelism and memory reuse - Larger tiles have fewer memory I/O but reduced parallelism. This is, in particular, a concern for smaller matrices. The library contains a variety of tiling to techniques to best utilize the GPU.</p> <h2 id="dimension-quantization-effects">Dimension Quantization effects</h2> <p>A GPU function is executed by launching a number of thread blocks, each with the same number of threads. This introduces two potential effects on execution efficiency - tile and wave quantization.</p> <ul> <li><strong>Tile Quantization</strong> - Tile quantization occurs when matrix dimensions are not divisible by the thread block tile size.</li> </ul> <p><img src="/assets/img/Machine Learning Systems/2025-01-25-23-37-44-image.png" alt=""/></p> <ul> <li><strong>Wave quantization</strong> - The total number of tiles is quantized to the number of multiprocessors on the GPU.</li> </ul> <h1 id="mi300x-vs-h100-vs-h200">MI300X vs H100 vs H200</h1> <blockquote> <p>Source: <a href="https://semianalysis.com/2024/12/22/mi300x-vs-h100-vs-h200-benchmark-part-1-training/">MI300X vs H100 vs H200 Benchmark Part 1: Training; CUDA Moat Still Alive; SemiAnalysis</a></p> </blockquote> <p>A case-study to examine why numbers on paper do not translate to real-life performance.</p> <p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/10-H100-vs-H200-vs-MI300X-Basic-Specs-initial-1.jpg?resize=2184%2C1088&amp;ssl=1" alt=""/></p> <p>Comparing numbers on sheets is similar to comparing the megapixel count of cameras - the software is very important. NVIDIAs real world performance is also quite low compared to the papers. The analysis boils down to this - The potential on paper of AMD’s MI300X is not realized due to lack of AMDs software stack and testing. The gap between AMDs and NVIDIAs software is large - AMD is still catching up while NVIDIA is racing ahead. AMDs PyTorch is unfortunately broken.</p> <h2 id="matrix-multiplication-1">Matrix Multiplication</h2> <p>OpenAI provides a <code class="language-plaintext highlighter-rouge">do_bench</code> function to benchmark matrix multiplications - it provides cache clearing between runs, ways to warmup and execute the benchmark multiple times and takes the median result. The results are as follows -</p> <p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/71-bf16-gemm-perf-for-real-world-shapes-w-amd-images.png?resize=1489%2C1084&amp;ssl=1" alt=""/></p> <p>Notice how the marketed value is much higher than the real-world performance! The main finding from this study is that AMDs software stack is riddled with bugs that lowered its performance quite a lot.</p> <p>It is also important to ensure that the underlying benchmark has no issues. For example, a popular benchmark for GEMM showed comparable performance for AMD and NVIDIA. However, on closer look, it had issues - it did not clear out L2 Cache and displayed the max performance rather than mean/median.</p> <h2 id="hbm-memory-bandwidth-performance">HBM Memory Bandwidth Performance</h2> <p>Higher HBM bandwidth leads to better inference performance. It is also helpful during training, specially with higher batch sizes.</p> <p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/115-HBM-Copy-Bandwidth-Chart.png?resize=1485%2C1047&amp;ssl=1" alt=""/></p> <h2 id="training-large-language-models">Training Large Language Models</h2> <p>MLPerf GPT3 175B training is a good metric to measure the time it takes to train a model. However, this benchmark turned out to be very difficult for the AMD GPUs. Instead, the authors designed a benchmark better representing a user workload. They noted that many AI practitioners do not use Megatron, NeMo and 3D parallelism (advances in modern networks architectures for faster inference) due to their rigidity and complexity.</p> <p>Overall for this test, on a single node, the authors obtained the following results -</p> <p><img src="https://i0.wp.com/semianalysis.com/wp-content/uploads/2024/12/121-bf16-single-node-8gpu-training-perf-with-new-AMD-images.png?resize=1491%2C1180&amp;ssl=1" alt=""/></p> <p>The public releases of NVIDIA perform much higher than AMDs bugs-resolved builds. Apart from these single node builds, these providers also have a server cluster availability. NVIDIAs GPUs are merged with the NVLink fabric whereas AMD has xGMI. They connect up to 8GPUs with up to 450GB/s of bandwidth per GPU. This network of GPUs synchronize via the map-reduce command, and NVIDIAs topology performs better than that of AMDs.</p> <p>Ironically, AMDs RCCI team has only 32 GPUs to experiment with the algorithms. Looking at this dismal conditions, TensorWave and SemiAnalysis sponsored some clusters to the team to aid the team in fixing their software.</p> <h2 id="conclusion-3">Conclusion</h2> <p>Due to poor internal testing and a lack of good testing suite internally at AMD, MI300 is not usable out of the box. There have been advanced in attention mechanisms such as the <strong>FlexAttention</strong> that can improve the speed of window attention by 10-20x. However, due to the lacking nature of AMDs software, they are 6 months behind NVIDIA which is a long time in the rapid AI age.</p> <p>In fact, many of AMDs libraries are forked off NVIDIA’s open-source libraries! The authors suggest AMD should hire more software talent to reduce the gap and reshape their user interface.</p> <h1 id="tvm-end-to-end-compiler-for-deep-learning"><a href="https://arxiv.org/pdf/1802.04799">TVM: End-to-end compiler for Deep Learning</a></h1> <p>Current ML frameworks require significant manual effort to deploy on different hardware. TVM exposes the graph-level and operator-level optimizations to provide performance portability. In the current frameworks, techniques such as computational graph representations, auto-differentiation and dynamic memory management have been well established. Optimizing these structures on different hardware is however often infeasible, and developers have resorted to highly engineered and vendor-specific operator libraries. These libraries require significant amount of manual tuning and cannot be ported across devices.</p> <p>How does the hardware matter? The input to hardware instructions are multi-dimensional, with fixed or variable lengths; they dictate different data layout and they have special requirements for memory hierarchy. These different constraints (memory access, threading pattern, hardware primitives - loop tiles and ordering, caching, unrolling) form a large search space that needs to be optimized to get efficient implementation.</p> <p>TVM, an end-to-end ML compiler, has been designed to take a high-level specification of a deep-learning program from existing frameworks and generate a low-level optimized code for a diverse set of hardware backends. To do so, they made the following contributions</p> <ul> <li> <p>A <em>tensor expression language</em> to build operators and provide program transformation primitives to generate equivalent programs</p> </li> <li> <p>An <em>automated program optimization framework</em> to find optimized tensor operators based on an ML cost model that adapts and improves based on the data received from the hardware backend.</p> </li> <li> <p>A <em>graph-rewriter</em> compiler together the high-level and operator-level optimizations.</p> </li> </ul> <p><img src="/assets/img/Machine Learning Systems/2025-02-04-17-41-07-image.png" alt=""/></p> <h2 id="graph-optimization">Graph Optimization</h2> <p>As mentioned previously, DL frameworks represent the data flow with graphs. This is different from the low-level compiler intermediate representation due to the presence of large, multi-dimensional tensors. Similar to compiler graph optimization, TVM also performs similar optimizations on this graph -</p> <ol> <li> <p><strong>Operator Fusion</strong> - Fuse multiple small operations together, reducing the activation energy for executing each operator. The commonly used operators can be classified as injective (one-to-one, e.g., add), reduction (many-to-one, e.g., sum), complex-out-fusable (element-wise map based output, e.g., convolution), and opaque (cannot be fused, e.g., sort).</p> <p>These classifications help us identify operators that can be combined. For example, in general</p> <ul> <li> <p>Multiple injective operators can be fused into one another</p> </li> <li> <p>A reduction operator can be fused with the input injective operators</p> </li> <li> <p>Complex-out-fusable operators can fuse element-wise operators to its output</p> </li> </ul> <p>These fusions generate <strong>up to 1.2x to 2x speedup</strong>!</p> </li> <li> <p><strong>Constant folding</strong> - Pre-compute graph parts that can be determined statically, saving execution costs</p> </li> <li> <p><strong>Static memory planning pass</strong> - Pre-allocate memory to hold each intermediate tensor</p> </li> <li> <p><strong>Data Layout transformations</strong> - Transform internal data layouts into back-end friendly forms. For example, a DL accelerator might exploit \(4\times4\) matrix operations, requiring data to be tiled into \(4\times4\) chunks. The memory hierarchy constraints are also taken into consideration.</p> </li> </ol> <p>These optimizations is where the search-space complexity arises. With more operators being introduced on a regular basis, the number of possible fuses can grow significantly considering the increasing number of various hardware back-ends as well. Doing these optimizations manually is infeasible and we need to automate the process.</p> <h2 id="tensor-operations">Tensor Operations</h2> <p>TVM produces efficient code for each operator by generating many valid implementations for each hardware back-end and choosing an optimized implementation. This process is built on Halide’s idea of decoupling descriptions from computation rules and extends it to support new optimizations (nested parallelism, tensorization and latency hiding).</p> <h3 id="tensor-expression-and-schedule-space">Tensor Expression and Schedule Space</h3> <p>Unlike high-level computation graph representations, where the implementation of tensor operations is opaque, each operation is described in an index formula expression language. For example, matrix multiplication is written as</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="sh">'</span><span class="s">m</span><span class="sh">'</span><span class="p">),</span> <span class="n">t</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="sh">'</span><span class="s">n</span><span class="sh">'</span><span class="p">),</span> <span class="n">t</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="sh">'</span><span class="s">h</span><span class="sh">'</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">A</span><span class="sh">'</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">B</span><span class="sh">'</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">reduce_axis</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="n">h</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">compute</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> 
                <span class="n">t</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">k</span><span class="p">))</span>
</code></pre></div></div> <p>Each compute operation specifies both shape and expression for output. Since the language does not specify the loop structure and other execution details, it provides the flexibility for adding hardware-aware optimizations. <em>Schedules</em> are built incrementally with basic transformations to preserve the program’s logical equivalence. To achieve high performance on many back-ends, the schedule primitives must be diverse enough to cover various hardware backends.</p> <h3 id="nested-parallelism">Nested Parallelism</h3> <p>A common paradigm based on fork-join concept, nested parallelism is present in many existing solutions. Based on memory hierarchies, solutions also consider <em>shared memory spaces</em> and <em>fetching data cooperatively</em>. TVM builds on this idea with the concept of <strong>memory scopes</strong> to the schedule space so that a compute stage can be marked as shared. These shared tasks have <em>memory synchronization barriers</em> to ensure proper read/writes.</p> <h3 id="tensorization">Tensorization</h3> <p>To ensure high arithmetic intensity, hardware developers have started implementing <em>tensor compute primitives</em> that consider the common operations and design specific hardware for their execution. They can improve the performance (similar to vectorization for SIMD architectures), but with more and more accelerators with their own variants of tensor instructions, there is a need for an extensible solution that can seamlessly integrate with all these.</p> <p>To do so, TVM extends the tensor expression language to include hardware intrinsics. Doing so decouples the schedule from specific hardware primitives. The generated schedules are broken into a sequence of micro-kernel calls which can also use the <em>tensorize</em> primitive to make use of accelerators.</p> <h3 id="explicit-memory-latency-hiding">Explicit Memory Latency Hiding</h3> <p>Latency hiding refers to the process of overlapping memory operations with computation to maximize utilization of memory and compute resources. These strategies depend on the underlying hardware -</p> <ul> <li> <p>On CPUs, memory latency hiding is achieved implicitly with simultaneous multithreading or hardware prefetching.</p> </li> <li> <p>GPUs rely on rapid context switching of many warps of threads</p> </li> <li> <p>Specialized DL accelerators such as the TPU usually favor leaner control with a decoupled access-execute (DAE) architecture and offload the problem of fine-grained synchronization to software. This is difficult to program because it requires explicit low-level synchronization. To solve this, TVM introduces a virtual threading scheduling primitive that lets programmers specify a high-level data parallel program. TVM automatically lowers the program to a single instruction stream with low-level explicit synchronization.</p> </li> </ul> <p>Empirically, they observed that peak utilization increased by 25% for ResNet with latency hiding.</p> <h2 id="automating-optimization">Automating Optimization</h2> <p>Now that we have a set of schedule primitives, TVM needs to find the optimal operator implementations for each layer of the DL model. An <em>automated schedule optimizer</em> automates optimizing the hardware parameters through a high-dimensional search space. It consists of</p> <ol> <li> <p>A schedule explorer that proposes new configurations. A <em>schedule template specification</em> API is used to let a developer define the changeable parameters in a code for a given hardware. TVM also has a <em>generic master template</em> to automatically extract possible knobs based on the tensor expression language. More the number of configurations, better optimization is possible.</p> </li> <li> <p>A machine learning cost model that predicts the performance of a given configuration. Defining a perfect cost model is difficult - we need to consider memory access patterns, data reuse, pipeline dependencies, threading patterns, etc for every hardware. So, TVM uses a statistical approach by training an ML model that keeps getting better with newer data. The model has been chosen for <em>quality</em> and <em>speed</em>. The model uses a rank objective to predict the relative order of runtime costs. The model itself is a <em>gradient tree boosting model</em> (based on XGBoost) that makes predictions based on features extracted from the loop program.</p> </li> </ol> <p>The explorer starts with random configurations, and, at each step, randomly walks to a nearby configuration. This exploration method is more efficient than random exploration.</p> <p>A <em>distributed device pool</em> scales up the running of on-hardware trials and enables fine-grained resource sharing among multiple optimization jobs.</p> <h2 id="summary-1">Summary</h2> <p>The core of TVM is implemented in C++ with approximately 50k lines of code. It achieves significantly higher performance as compared to earlier works. However, even with this extensive approach, the models can be designed carefully to achieve much better performance. For example, as seen in Flash Attention, the optimization is a result of human intellectual effort rather than a manual exploration of a partially defined search space by an automated compiler.</p> <h1 id="triton-an-intermediate-language-and-compiler-for-neural-network-computations"><a href="https://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf">Triton: An intermediate language and compiler for Neural Network computations</a></h1> <p>As with the previous motivation, Triton was developed to provide a way for users to test new models efficiently without needing to manually optimize it for the hardware. Triton is a language and a compiler centered around the concept of <em>tile</em>, statically shaped multi-dimensional sub-arrays. It is a C-based language for expressing tensor programs in terms of operations on parametric tile variables and provides novel tile-level optimization passes for compiling programs into efficient GPU code.</p> <p>Previous approaches to tackle the wide-variety of deep-learning models include <strong>Domain-Specific Languages (DSLs)</strong> based on polyhedral machinery (tenor comprehensions) and/or loop synthesis techniques (Halide, TVM, PlaidML, etc). These systems perform well for models such as depthwise-separable convolutions, but they are much slower than vendor-based libraries like cuBLAS and cuDNN. The problem with vendor based libraries is that they support only a restricted set of tensor operations.</p> <p>These issues have been addressed by the use of micro-kernels - hand-written tile-level intrinsics, but it requires significant manual labour and cannot be generalized. Compilers do not support these tile-level operators and optimizations. The prior approaches can be summarized as</p> <ol> <li> <p>Tensor level IRs - XLA, Flow to transform tensor programs into predefined LLVM-IR and CUDA-C operation templates (e.g., tensor contractions, element-wise operations, etc) using pattern matching. Triton provides more flexibility.</p> </li> <li> <p>The polyhedral model - Tensor Comprehensions (TC), Diesel to parameterize and automate the compilation of one of many DNN layers into LLVM-IR and CUDA-C programs. Triton supports non-affine tensor indices.</p> </li> <li> <p>Loop synthesizers - Halide, TVM to transform tensor computations into loop nests that can be manually optimized using user-defined schedules. Automatic inference of possible execution schedule in Triton.</p> </li> </ol> <h2 id="triton-c">Triton-C</h2> <p>A C-like language for expressing tensor programs in terms of parametric tile variables. It provides a stable interface for existing DNN trans-compilers and programmers familiar with CUDA. Here is an example of matrix multiplication</p> <div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Tile shapes are parametric and can be optimized</span>
<span class="c1">// by compilation backends</span>
<span class="k">const</span> <span class="n">tunable</span> <span class="kt">int</span> <span class="n">TM</span> <span class="o">=</span> <span class="p">{</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">};</span>
<span class="k">const</span> <span class="n">tunable</span> <span class="kt">int</span> <span class="n">TN</span> <span class="o">=</span> <span class="p">{</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">};</span>
<span class="k">const</span> <span class="n">tunable</span> <span class="kt">int</span> <span class="n">TK</span> <span class="o">=</span> <span class="p">{</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">};</span>

<span class="c1">// C = A * B.T</span>
<span class="n">kernel</span> <span class="kt">void</span> <span class="nf">matmul_nt</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">b</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">c</span><span class="p">,</span> <span class="kt">int</span> <span class="n">M</span><span class="p">,</span> <span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">int</span> <span class="n">K</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// 1D tile of indices</span>
    <span class="kt">int</span> <span class="n">rm</span><span class="p">[</span><span class="n">TM</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_global_range</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">rn</span><span class="p">[</span><span class="n">TN</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_global_range</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">rk</span><span class="p">[</span><span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">...</span><span class="n">TK</span><span class="p">;</span>
    
    <span class="c1">// 2D tile of accumulators</span>
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">TM</span><span class="p">,</span> <span class="n">TN</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>

    <span class="c1">// 2D tile of pointers</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">pa</span><span class="p">[</span><span class="n">TM</span><span class="p">,</span> <span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">rm</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">]</span> <span class="o">+</span> <span class="n">rk</span> <span class="o">*</span> <span class="n">M</span><span class="p">;</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">pb</span><span class="p">[</span><span class="n">TN</span><span class="p">,</span> <span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span> <span class="o">+</span> <span class="n">rn</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">]</span> <span class="o">+</span> <span class="n">rk</span> <span class="o">*</span> <span class="n">K</span><span class="p">;</span>
    
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="n">K</span><span class="p">;</span> <span class="n">k</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">-=</span> <span class="n">TK</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">bool</span> <span class="n">check_k</span><span class="p">[</span><span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="n">rk</span> <span class="o">&lt;</span> <span class="n">k</span><span class="p">;</span>
        <span class="n">bool</span> <span class="n">check_a</span><span class="p">[</span><span class="n">TM</span><span class="p">,</span> <span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">rm</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">)[</span><span class="o">:</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">]</span> <span class="o">&amp;&amp;</span> <span class="n">check_k</span><span class="p">;</span>
        <span class="n">bool</span> <span class="n">check_b</span><span class="p">[</span><span class="n">TN</span><span class="p">,</span> <span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">rn</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">)[</span><span class="o">:</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">]</span> <span class="o">&amp;&amp;</span> <span class="n">check_k</span><span class="p">;</span>
        
        <span class="c1">// Load tile operands</span>
        <span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">TM</span><span class="p">,</span> <span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="n">check_a</span> <span class="o">?</span> <span class="o">*</span><span class="n">pa</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
        <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">TN</span><span class="p">,</span> <span class="n">TK</span><span class="p">]</span> <span class="o">=</span> <span class="n">check_b</span> <span class="o">?</span> <span class="o">*</span><span class="n">pb</span> <span class="o">:</span> <span class="mi">0</span><span class="p">;</span>
        
        <span class="c1">// Accumulate</span>
        <span class="n">C</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">trans</span><span class="p">(</span><span class="n">B</span><span class="p">));</span>
        
        <span class="c1">// Update pointers</span>
        <span class="n">pa</span> <span class="o">=</span> <span class="n">pa</span> <span class="o">+</span> <span class="n">TK</span> <span class="o">*</span> <span class="n">M</span><span class="p">;</span>
        <span class="n">pb</span> <span class="o">=</span> <span class="n">pb</span> <span class="o">+</span> <span class="n">TK</span> <span class="o">*</span> <span class="n">N</span><span class="p">;</span>
    <span class="p">}</span>
    
    <span class="c1">// Write-back accumulators</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">pc</span><span class="p">[</span><span class="n">TM</span><span class="p">,</span> <span class="n">TN</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span> <span class="o">+</span> <span class="n">rm</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">]</span> <span class="o">+</span> <span class="n">rn</span> <span class="o">*</span> <span class="n">M</span><span class="p">;</span>
    <span class="n">bool</span> <span class="n">check_c</span><span class="p">[</span><span class="n">TM</span><span class="p">,</span> <span class="n">TN</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">rm</span> <span class="o">&lt;</span> <span class="n">M</span><span class="p">)[</span><span class="o">:</span><span class="p">,</span> <span class="n">newaxis</span><span class="p">]</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">rn</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">);</span>
    <span class="err">@</span><span class="n">check_c</span> <span class="o">*</span> <span class="n">pc</span> <span class="o">=</span> <span class="n">C</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p>It is a front-end to describe CUDA like syntax with Numpy like semantics. The syntax is based on ANSI C, and has the following changes</p> <ul> <li> <p>Tile declarations: Syntax for multi-dimensional arrays that can be made parametric with <code class="language-plaintext highlighter-rouge">tunable</code> keyword. Ranges with ellipses.</p> </li> <li> <p>Broadcasting using <code class="language-plaintext highlighter-rouge">newaxis</code> keyword</p> </li> <li> <p>Predicated statements in tiles with <code class="language-plaintext highlighter-rouge">@</code></p> </li> </ul> <p>A tile is an abstraction to hide details involving intra-tile memory coalescing, cache management and specialized hardware utilization.</p> <p>The triton programming model is similar to that of CUDA - each kernel corresponds to a single thread execution.</p> <h2 id="triton-ir">Triton-IR</h2> <p>An LLVM-based Intermediate Representation (IR) that provides an environment suitable for tile-level program analysis, transformation and optimization. Triton-IR programs are constructed directly from Triton-C during parsing, but automatic generation from embedded DSLs is unimplemented. Here is an example for the <code class="language-plaintext highlighter-rouge">max</code> operation</p> <pre><code class="language-C">define kernel void @relu ( float * %A , i32 %M , i32 % N ) {
prologue :
% rm = call i32 &lt;8 &gt; get_global_range (0) ;
% rn = call i32 &lt;8 &gt; get_global_range (1) ;
; broadcast shapes
%1 = reshape i32 &lt;8 , 8 &gt; % M;
% M0 = broadcast i32 &lt;8 , 8 &gt; %1;
%2 = reshape i32 &lt;8 , 8 &gt; % N;
% N0 = broadcast i32 &lt;8 , 8 &gt; %2;
; broadcast global ranges
%3 = reshape i32 &lt;8 , 1 &gt; % rm;
% rm_bc = broadcast i32 &lt;8 , 8 &gt; %3;
%4 = reshape i32 &lt;1 , 8 &gt; % rn;
% rn_bc = broadcast i32 &lt;8 , 8 &gt; %4;
; compute mask
% pm = icmp slt % rm_bc , % M0;
% pn = icmp slt % rn_bc , % N0;
% msk = and % pm , % pn;
; compute pointer
% A0 = splat float * &lt;8 , 8 &gt; % A;
%5 = getelementptr % A0 , % rm_bc ;
%6 = mul % rn_bc , % M0;
% pa = getelementptr %5 , %6;
; compute result
% a = load % pa;
% _0 = splat float &lt;8 , 8 &gt; 0;
% result = max % float %a , % _0;
; write back
store fp32 &lt;8 , 8 &gt; % pa , % result
}
</code></pre> <p>It is similar to LLVM-IR, but it includes the necessary extensions for tile-level data-flow and control-flow.</p> <p>It constructs a data-flow graph including nodes for multi-dimensional tiles and instructions made from basic blocks of code. The control-flow is handled with the use of <em>Predicated SSA</em> and \(\psi\)<em>-functions</em> (compare and merge).</p> <h2 id="triton-jit-compiler">Triton-JIT compiler</h2> <p>A Just-In-Time (JIT) compiler and code generation backend for compiling Triton-IR programs into efficient LLVM bitcode. It includes</p> <ul> <li> <p>A set of tile-level, machine-independent passes aimed at simplifying input compute kernels independently of any compilation target. It involves operations such as pre-fetching to reduce cache misses and tile-level peephole optimization that implement algebraic tricks with tensors.</p> </li> <li> <p>A set of tile-level, machine dependent passes for generating efficient GPU-ready LLVM-IR. It involves</p> <ul> <li> <p>Hierarchical Tiling - The tiles defined by the user are further broken down to the machine’s constraints.</p> </li> <li> <p>Memory coalescing - Making sure that adjacent threads simultaneously access nearby memory locations.</p> </li> <li> <p>Shared memory allocation - To improve memory reuse.</p> </li> <li> <p>Shared memory synchronization - Barriers to preserve program correctness in parallel execution.</p> </li> </ul> </li> <li> <p>An auto-tuner that optimizes any meta-parameters associated with the above passes. Traditional auto-tuners rely on hand-written templates. In contrast, Triton-JIT extracts optimization spaces from Triton-IR (hierarchical tiling parameters only - up to 3 per dimension per tile) and optimizes using an exhaustive search. <strong>This needs to be improved in the future</strong></p> </li> </ul> <h2 id="summary-2">Summary</h2> <p>Triton defeats the other prior solutions achieving performance close to DSLs. However, the authors have highlighted many areas where this framework can be improved in the future - support for tensor cores (the ones TVM talked about), implementation of quantized kernels and integration into higher-level DSLs.</p> <h1 id="deep-compression-compressing-deep-neural-networks-with-pruning-trained-quantization-and-huffman-coding"><a href="https://arxiv.org/pdf/1510.00149">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a></h1> <p>Released in 2016, Deep Compression is a three stage-pipeline involving pruning, trained quantization and Huffman coding that reduce the storage of neural networks by \(35\times\) to \(49\times\) without affecting their accuracy! Here are these steps in detail</p> <ul> <li> <p><strong>Network Pruning</strong>- Leveraging the previous successes of pruning CNNs, the authors prune the network by removing all connections with weights below a certain threshold from the network. The model is retrained to learn optimized weights. This step reduces the number of parameters by almost \(10\times\) for AlexNet and \(35\times\) for VGG-16.</p> <p>For further storage savings, these weights are stored as a compressed sparse row/column. Moreover, the authors recommend storing the index different instead of absolute position, and they assign the bits very parsimoniously.</p> </li> <li> <p><strong>Trained Quantization and Weight Sharing</strong> - A quantization technique during training, that essentially shares the weights across multiple neurons. That is, the updates and computes are grouped as centroids. The compression rate with this technique is given by</p> \[r = \frac{nb}{n \log_2(k) + kb}\] <p>where \(k\) is the number of clusters, \(n\) is the number of connections in the network, and \(b\) is the number of bits used to encode these connections. The weights are not shared across layers, and the quantization is done so that the loss across all the weights is minimum. The weights centroids can be accessed through a hash function.</p> <p>The authors go into some more detail about initializing these centroids.</p> <ul> <li> <p>Forgy (Random) - Randomly chooses \(k\) observations from the dataset. These centroids tend to concentrate near the peaks of the weight distribution.</p> </li> <li> <p>Density based initialization - Starts with a uniform distribution of the centroids and then adapts to the PDF of the data distribution. The centroids still gather near the peaks but have a better spread than random initialization.</p> </li> <li> <p>Linear initialization - Most scattered initialization</p> </li> </ul> <p>Why do these matter? In many cases, the larger weights are important but they usually are fewer in number. So, the first two initializations that are centered around the distribution peaks (which are not the large values) can result in poor accuracy. So, linear initialization usually works the best. They also experimentally confirm these hypotheses.</p> </li> <li> <p><strong>Huffman Coding</strong> - Huffman encoding is the optimal encoding standard that relies on a common prefix code for lossless data compression. The intuition is that more common symbols are represented with fewer bits. The authors of this paper use this idea to encode the sparse matrix formed by the quantized weights. The distributions are usually bi-modal and Huffman encoding saves 20-30% in storage with these non-uniform distributions.</p> </li> </ul> <p>Overall, with all these methods combined, the pipeline saves over \(30\times\) in storage for many image models! What’s more? The accuracy is more or less the same as the original models!</p> <p>The authors analyze a bit more on how these methods work together with one another. They noticed that the accuracy of the networks starts dropping sharply after a certain threshold in each method.</p> <p><img src="/assets/img/Machine Learning Systems/2025-02-12-18-35-58-image.png" alt=""/></p> <p>They also point out that quantization works well with pruned networks due to lesser number of parameters to quantize. In fact, they notice that pruning virtually does not affect the performance obtained after quantization. Basically, with more savings, you get the same performance.</p> <p>The target of this paper is to allow models to work on edge devices. They perform experiments on latency, especially with the fully connected layers since they occupy 90% of the parameters in the networks. One must note that benchmarking quantized models is difficult since there are no hardware primitives to support the lookup architecture for centroid codes. They have the following observations</p> <ol> <li> <p>Matrix vector multiplications are more memory-bound than matrix matrix multiplications, and so reducing the memory-footprint for non-batched inferences is important.</p> </li> <li> <p>Pruned networks had \(3\times\) speed-up since larger matrices are able to fit into the caches.</p> </li> <li> <p>They also note that the effect of the quantization codebook is negligible as compared to the other operations.</p> </li> </ol> <p>In the future, they emphasize that to take full-advantage of the method, hardware must support indirect matrix entry lookup and the relative indices in CSC and CSR formats. This can be achieved via custom software kernels or hardware primitives.</p> <h1 id="a-survey-of-quantization-methods-for-efficient-neural-network-inference"><a href="https://arxiv.org/abs/2103.13630">A survey of Quantization methods for Efficient Neural Network Inference</a></h1> <p>The research and development of efficient ML has been a constant interest alongside developing new ML architectures. The earlier works in this space include:</p> <ul> <li> <p>Optimizing the micro-architecture (such as depth-wise convolution, low-rank factorization) and macro-architecture (residual connections, inception). These methods have been a result of manual search that is not scalable. A new line of work called AutoML and Neural Architectural Search methods was developed to automate the search of the right ML architecture under the given constraints of model size. Another line of work in ML compilers tried to optimize a particular architecture for a specific hardware.</p> </li> <li> <p><strong>Pruning</strong> the models by removing the neurons with small saliency (sensitivity - that do not affect the result of the network largely). These methods can be categorized as</p> <ul> <li> <p>Unstructured - Removes neurons with smaller saliency wherever they occur - this tends to be aggressive while having little impact to performance. However, this approach leads to sparse matrix operations that are harder to accelerate and are memory bound.</p> </li> <li> <p>Structured - A group of parameters (e.g., entire convolutional filter) is removed so that the operations are still on dense matrices.</p> </li> </ul> </li> <li> <p><strong>Knowledge distillation</strong> refers to training large models and using it as a teacher to train a compact model. The idea is to use the soft-probabilities generated by the teacher to better train a small model, resulting in high compression. This class of methods can significantly reduce the model size with little to no performance degradation.</p> </li> <li> <p><strong>Quantization</strong> has showed consistent success for both training and inference of Neural Networks. It allowed breakthroughs such as half-precision and mixed prevision training to have a high throughput in AI accelerators.</p> <p>Quantization is loosely related to some works in neuroscience which suggest that information stored in continuous form will inevitably get corrupted by noise. However, discrete signal representations can be more robust.</p> </li> </ul> <p>Quantization is mainly useful for deploying models on edge devices. Quantization, combined with efficient low-precision logic and dedicated deep learning processors can really push edge processors.</p> <h2 id="history-of-quantization">History of Quantization</h2> <p>Quantization maps input values in a large (often continuous) set to output values in a small (often finite) set. The effect of quantization and its use in coding theory was formalized with the seminal paper by Shannon in 1948. It evolved into different concepts such as Pulse Code Modulation in signal processing.</p> <p>In digital systems, numerical optimization methods showed that having quantization effects produces roundoff errors in applications that we know to have closed form solutions. These realizations led to a shift towards approximation algorithms.</p> <p>Neural Network (NN) quantization is different from these earlier considerations. Since inference and training of NNs is expensive and NNs are typically over-parameterized, there is a huge opportunity to reduce the precision without impacting accuracy. The nature of NNs allows high error/distance between quantized and non-quantized models. Also, the layered nature of NNs allows exploration of more types of quantizing techniques.</p> <p>There are two kinds of quantization</p> <ol> <li> <p><strong>Uniform quantization</strong> - \(Q(r) = Int(r/S) - Z\) where \(Q\) is the quantization operator, \(r\) is a real valued input (activations and weights), \(S\) is a real-valued scaling factor and \(Z\) is an integer zero-point. The resulting quantized values are uniformly spaced. An important factor is the choice of the scaling factor \(S\). It is determined by the clipping range that can be determined from calibration. Based on the clipping range, symmetric quantization results in easier implementation since it results in \(Z = 0\). These calibration ranges can be computed dynamically for every activation map or done statically during inference.</p> </li> <li> <p><strong>Non-uniform quantization</strong> - Essentially, the quantized values are not necessarily uniformly spaced. It typically achieves higher accuracy for cases involving non-uniform distributions. Neural networks usually have bell-shaped distributions with long tails. These methods can be generally considered as optimizing the difference between the original tensor and quantized counterpart. The quantizer itself can also be jointly trained with the model parameters.</p> <p>These prior approaches can be classified as rule-based and optimization-based quantizations. In addition, there can be clustering based quantizations such as K-means to minimize the performance loss.</p> </li> </ol> <p>The granularity of quantization is also an important choice -</p> <ol> <li> <p>Layerwise quantization - The clipping range is determined by considering all the weights in convolutional filters of a layer.</p> </li> <li> <p>Groupwise quantization - The clipping range is determined by grouping multiple channels inside a layer to account for cases where the distribution of the parameters across a single convolution/activation varies a lot.</p> </li> <li> <p>Channelwise quantization - Each channel is assigned a dedicated scaling factor.</p> </li> <li> <p>Sub-channelwise quantization - Each channel is partitioned into groups that have their own scaling factor.</p> </li> </ol> <p>These quantization methods can also be applied during training and after training</p> <ol> <li> <p><strong>Quantization-aware training (QAT)</strong> - The NN is re-trained with quantized parameters to improve the accuracy that degrades due to the perturbations caused by quantization. In one such method, the forward and backward passes are performed on the quantized model in floating point and then quantized after each gradient update. The back-propagation through a quantization operator is approximated via <em>Straight-Through Estimator (STE)</em> that ignores the rounding operation and approximates with an identity operation. It often works well in practice except for binary quantization. Other approaches include combinatorial optimization, target propagation or Gumbel-softmax. There has been a lot of work with regards to this aspect of quantization.</p> <p>The other kind of approaches tried learning quantization parameters during the training. These are recent works and there is not much to summarize here. In summary QAT methods require retraining efforts which may not be worth it for short-lived models.</p> </li> <li> <p><strong>Post-training quantization (PTQ)</strong> - In contrast to QAT, the overhead is very low, does not require retraining, and it can be applied in situations where data is limited or unlabeled. However, it comes at a cost of lower accuracy. To prevent this, many approaches were proposed - bias correction methods for post-quantization ranges, analytically optimizing the clipping ranges, L2 distance optimization, outlier channel splitting, adaptive rounding methods, etc.</p> </li> <li> <p><strong>Zero-shot quantization</strong> - In cases where the original data is not available to find the clipping range, zero shot quantization is used. It can again be classified into two levels based on if the approach has fine-tuning after quantization. Some approaches try to generate synthetic data to mimic the real data (earlier approaches included GANs for this) and calibrate the clipping ranges. In summary, these methods are useful for Machine Learning as a Service providers where security and privacy are a concern.</p> </li> </ol> <p>Finally, there is a notion of stochastic quantization wherein the floating numbers are mapped up or down with a probability associated to the magnitude of the weight update. Phew, that finishes the background.</p> <h2 id="quantization-below-8-bits">Quantization below 8 bits</h2> <ul> <li> <p><strong>Simulated and Integer-only Quantization</strong> - Simulated quantization refers to storing parameters as lower precision but carrying out calculations as floating point. This method can lead to higher stability while training and better inference performance, but results in mixed representation calculations. It is useful for problems that are bandwidth-bound rather than compute-bound such as in recommendation systems. The latter method which uses only a single format can exploit some hardware optimizations and be more energy efficient.</p> </li> <li> <p><strong>Mixed-Precision Quantization</strong> - In this method, each layer is quantized with different bit precision to improve the accuracy. Searching the mixed-precisions for the layers is essentially a search problem. Researchers have tried RL abased exploration methods or using a neural network to achieve this, but these can be computationally intensive with their performance being sensitive to hyperparameters and initialization.</p> <p>Another class of mixed-precision methods uses periodic function regularization by automatically distinguishing different layers and their importance wrt accuracy to learn their bandwidths. Some approaches also proposed a linear programming formulation for this approach. With hardware supporting these types of methods, it can be a feasible option for many cases.</p> </li> <li> <p><strong>Hardware Aware Quantization</strong> - Essentially considering hardware factors such as on-chip memory, bandwidth and cache hierarchy to choose the quantization parameters. Again, it is a search problem.</p> </li> <li> <p><strong>Distillation-Assisted Quantization</strong> - Some approaches tried to use model distillation to boost quantization accuracy. This method has to be studied separately since it was not covered in the survey.</p> </li> <li> <p><strong>Extreme Quantization</strong> - Binarization. Many researchers have been bold enough to try this. Important works include</p> <ul> <li> <p>Binary Connect - Weights are +1 or - 1</p> </li> <li> <p>Binarized NN - Binarizes both activations and weights. Has improved latency due to matrix multiplications being replaced by XNORs and bit-counting.</p> </li> <li> <p>XNOR-Net - Same as before but the weights have a scaling factor. Further works noted that weights tend to be close to 0, and adopted a ternarization (yes, that’s a word).</p> </li> </ul> <p>Nevertheless, these result in significant performance degradation. The works in this field can be classified as Quantization error minimization, Improved loss function formulations, and improved training methods. People tried to do this with BERT and GPT models as well!</p> </li> <li> <p><strong>Vector Quantization</strong> - Borrowing ideas from signal processing, these approaches include clustering weights or methods such as product quantization that subgroups the vectors.</p> </li> </ul> <h2 id="future-directions-summary-and-conclusion">Future Directions, Summary and Conclusion</h2> <ol> <li> <p>Quantization should be provided as a plug-and-play module in software packages to improve the accessibility.</p> </li> <li> <p>Hardware and Neural networks should be co-designed to optimize the computations.</p> </li> <li> <p>As mentioned in the previous article, coupling compression methods can have a huge impact!</p> </li> </ol> <h1 id="distributed-training-and-efficient-finetuning">Distributed Training and Efficient Finetuning</h1> <p>The growth of machine learning is much faster than what hardware can catch up with. We cannot fit our models in a single GPU anymore, and we need to learn how to use distributed training systems to satisfy our training needs.</p> <p>Typically, we deal with large model sizes (10B+) and large dataset sizes (1T+ tokens while pre-training, 1M+ in supervised fine-tuning). The goal is to maximize <em>throughput</em> by using smart distributed training strategies, where each GPU worker only deals with the fraction od training state and date. The main strategies used for this are -</p> <ol> <li> <p><strong>Data Parallelism</strong> (DP) - Each GPU worker gets a fraction of the total mini-batch of data, and computes the gradients on that data. these are then averaged across the workers to update the weights. In the most basic form, each worker has a copy of model weights, optimizer state and gradients for the fraction of the data it’s working on</p> </li> <li> <p><strong>Model Parallelism</strong> (MP, Vertical) - Models are <em>vertically sliced</em> with different layers placed on different GPUs. In the naïve form, the GPUs wait for the previous GPU to finish the computation. To improve this, people use <strong>pipeline parallelism</strong> where the execution is pipelined across micro-batches.</p> </li> <li> <p><strong>Tensor Parallelism</strong> (TP) - Each GPU processes only a slice of a tensor by <em>horizontally slicing</em> the model across GPU workers. Each worker process the same batch of data but for different activations. They exchange parts when needed.</p> </li> </ol> <p>These are the core strategies but there can be hybrid approaches based on the needs.</p> <h2 id="zero-powered-data-parallelism">ZeRO-powered Data Parallelism</h2> <p><a href="[[1910.02054] ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)">DeepSpeed’s ZeRO</a> (Zero-Redundancy) is one of the most efficient and popular strategies for distributed training. It is a data parallelization strategy that leverages memory redundancy in data-parallel training and the inter-GPU connects to improve throughput. It comprises of two components - ZeRO-DP (data parallelism) and ZeRO-R (residual memory). The team has also proposed newer architectures such as ZeRO-Offload/Infinity (offloading computation to CPU/ NVMe disk) and ZeRO++ (with flexible multi-node training and quantized weights).</p> <p>It boasts up to 64x memory reduction for a specific example across different hardware setups and model sizes!</p> <p>The base method is PyTorch DDP that has been described before. Each GPU worker has <strong>a copy</strong> of the model weights, optimizer state and gradients. To average the gradients, <strong>all-reduce</strong> step is used. All-reduce is a two-step approach - <em>reduce-scatter</em> operation to reduce different parts of the data on different processes and an <em>all-gather</em> operation to gather the reduced data on all the processes. It requires \(2\psi\) amount of communication cost for \(\psi\) number of parameters. The paper suggests three ways of doing this -</p> <ul> <li> <p><strong>ZeRO Stage 1 /</strong> \(P_{os}\) <strong>(Optimizer state partitioning)</strong> - The optimizer state is partitioned/sharded across the workers, with model weights and gradients replicated across all the workers. Previously, the all-reduce step gets the average gradient value across all the workers. Now, each worker updates the optimizer state with the Adam equations that is in it’s partition. The key savings come from using a sharded optimizer state rather than having copies. Recall that Adam optimizer uses 2x as many weight parameters as the model.</p> </li> <li> <p><strong>ZeRO Stage 2 /</strong> \(P_{os + g}\) <strong>(Optimizer State + Gradient Partitioning)</strong> - As the name suggests the gradients are sharded along with the optimizer state. So each worker calculates its gradient and the gradients are averaged with reduce-scatter at the worker (instead of all-reduce).</p> </li> <li> <p><strong>ZeRO Stage 3 /</strong> \(P_{os + g}\) <strong>(Optimizer State + Gradient Partitioning)</strong> - The partitioning of the model weights is done horizontally. That is, each layer of the model is split across the GPUs and there is a parameter communication on-demand. The communication volume increases (since there is an extra all-gather for model parameters, the communication becomes 3x the number of parameters). As a consequence, the memory consumption is cut down by the number of GPU workers \(N\) which is huge! So if you have enough number of GPUs, you can get good savings on the memory.</p> </li> </ul> <p>These were the base methods. The authors proposed a wide-array of variations on top of this.</p> <ol> <li> <p><strong>ZeRO-R</strong> - Improves on ZeRO-DP by reducing the memory consumptions by activations and managing memory fragmentation. It essentially shards the activations as well. it also uses some temporary buffers to store intermediate results during gradient accumulation and reduction across workers.</p> </li> <li> <p><a href="https://arxiv.org/abs/2101.06840"><strong>Zero-Offload</strong></a> - The idea is to offload optimizer and computation from GPUs to the host CPU. Back in 2021, this obtained magnitudes of higher performance over PyTorch DDP. CPU computation is much slower, so only the less-intensive operations are offloaded so that the total compute complexity stays the same. So, operations such as norm calculations, weight updates, etc are done on the CPU, while the forward and backward pass are done on the GPU. It works with all stages of ZeRO (1, 2 and 3).</p> </li> <li> <p><strong>Zero-Infinity</strong> - An improvement on the previous approach to allow offloading to disk and some more improvements to CPU offloading. It could fit 10-100T parameters on just one DGX-2 node! This method is specifically built on top of ZeRO-3, and they achieved 49 Tflops/GPU for a 20 trillion model spread across 512 GPUs. This is insane!</p> </li> <li> <p><a href="https://arxiv.org/abs/2306.10209"><strong>ZeRO++</strong></a> - The latest improvement in this saga of approaches. The major changes are quantized weights (reduces communication by half), hierarchical partitioning (a hybrid sharing technique) and quantized gradients.</p> </li> </ol> <h2 id="fully-sharded-data-parallel">Fully-Sharded Data Parallel</h2> <p>FSDP is another data-parallelism technique aimed at improving memory efficiency with limited communication overhead. It’s based on the previous approaches and has two sharding strategies - Full Sharding and Hybrid Sharding.</p> <ul> <li><strong>Full-sharding</strong> - Similar to ZeRO-3, the parameters, optimizer state and gradient are sharded across workers.</li> </ul> <p>    The high level procedure is -</p> <blockquote> <p>In forward path</p> </blockquote> <blockquote> <ul> <li>Run all_gather to collect all shards from all ranks to recover the full parameter in this FSDP unit</li> <li>Run forward computation</li> <li>Discard parameter shards it has just collected</li> </ul> </blockquote> <blockquote> <p>In backward path</p> </blockquote> <blockquote> <ul> <li>Run all_gather to collect all shards from all ranks to recover the full parameter in this FSDP unit</li> <li>Run backward computation</li> <li>Run reduce_scatter to sync gradients</li> <li>Discard parameters</li> </ul> </blockquote> <ul> <li><strong>Hybrid Sharding</strong> - It consists of both sharding and replication based on the tradeoff between communication latency and memory savings. This option is useful when the only way out is sharding parameters.</li> </ul> <h2 id="implementation-in-practice">Implementation in Practice</h2> <p>ZeRO and FSDP integrate well with existing architectures. ZeRO is implemented in Microsoft’s DeepSpeed library and is integrated into the 🤗 Accelerate library. FSDP is a part of PyTorch itself, and again has an integration in the 🤗 Accelerate library.</p> <p>Pipeline parallelism requires architectural changes in the forward pass of the model. For this, the best option right now is Megatron-LM that is discussed next. A recent update in 2024 pushed nanotron that has 3D parallelism support.</p> <h2 id="efficient-fine-tuning">Efficient Fine-tuning</h2> <p>Some popular optimizations -</p> <ol> <li> <p><strong>Mixed precision</strong> - It has been widely adopted form LLMs wherein there is a master copy that is updated from the gradients of quantized copies.</p> </li> <li> <p><strong>Parameter Efficient Fine Tuning (PEFT)</strong> - Various methods to reduce the finetuning effort.</p> <ol> <li> <p><a href="https://arxiv.org/abs/2106.09685">LoRA</a> - Low-rank version of weight updates to the model parameters. Works as well as full fine-tuning.</p> </li> <li> <p><a href="https://huggingface.co/docs/peft/conceptual_guides/ia3">\(IA^3\)</a> - Injects trainable vectors into key, value and feed forward layers. Is working as well as LoRA with a lower order of parameters but requires more experimentation.</p> </li> </ol> </li> <li> <p><strong>Flash-attention</strong> - It is a fast, memory efficient, exact, IO-aware attention mechanism. <a href="https://tridao.me/publications/flash2/flash2.pdf">FlashAttention 2</a> achieves 220+ TFLOPS on A100! FlashAttention 1 achieved 124 TFLOPs before. However, these only support Ampere, Ada and Hopper NVIDIA GPUs and half prevision data types.</p> </li> <li> <p><strong>Gradient and Activation Checkpointing</strong> - A technique wherein only a subset of intermediate activations are stored and others are recomputed when needed. However, it can slow down the training by 20% for \(O(\sqrt N)\) memory savings. <a href="https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9">Check this site</a> for better info.</p> </li> <li> <p><strong>Quantization</strong> - Post-training quantization refers to savings at inference since weights don’t change. During training, there can be updates with quantized parameters. <a href="https://arxiv.org/abs/2305.14314">QLoRA</a> is one such technique that quantizes the base model and trains half precision low-rank weights on this. The throughput actually decreases due to the de-quantization step for each activation. Nevertheless, it can reach the performance of full fine-tuning.</p> </li> <li> <p><strong>Gradient accumulation</strong> - The intuition behind this approach is to run the network in micro-batches, accumulate the gradients across these micro-batches and update the model for a full batch-size. The idea is to get the same output as with large batches but with much lower memory consumption (the time increases accordingly). Larger batches usually have lower convergence time and the technique is especially useful when using multiple GPUs. However, one should be conservative with this approach. <a href="https://openreview.net/forum?id=H1oyRlYgg">A pre-transformers era paper</a> showed larger batch sizes can reduces the generalization abilities of a model.</p> </li> </ol> <h2 id="summary-3">Summary</h2> <p>The final tips for training these large models from this article are -</p> <blockquote> <ul> <li> <p>BF16/ FP16 by default. BF16 comes with basically no other config parameters and usually without any overflow issues (as opposed to FP16, where you can get different results with different loss scaling factors and have more overflow issues because of smaller dynamic range), so it’s very convenient.</p> </li> <li> <p>Use (Q)LoRA with trainable parameters added to all the linear layers.</p> </li> <li> <p>Use Flash Attention if your GPU supports it. Currently, Flash Attention 2 is supported for Llama 2 and Falcon in HuggingFace, with other models requiring monkey patches.</p> </li> <li> <p>Use Gradient/Activation Checkpointing. This will reduce throughput slightly. If you’ve got Flash attention, gradient checkpointing might not be required (they use a selective gradient checkpointing in the softmax of the attention layer).</p> </li> <li> <p>Use an efficient sampler in your dataloader, like the <a href="https://github.com/imoneoi/multipack_sampler">multi-pack sampler</a>.</p> </li> <li> <p>If you have multiple GPUs, always try BF16 + LoRA + Gradient Checkpointing + DeepSpeed ZeRO 3 first. <strong>Megatron stuff from the next part too</strong></p> </li> <li> <p>Use quantization when you have very limited GPU memory. QLoRA-style training currently works for DeepSpeed ZeRO 1/2 only. Thus, even though it is memory efficient when it comes to model parameters, you still have parameter redundancy with ZeRO 1/2, and you also get reduced throughput.</p> </li> <li> <p>With more and more GPUs (say 8 V100s or A100s), DS ZeRO-3 should be the way to go. DS ZeRO-2 is also good, but you can start to hit CPU RAM limits (during model initialization) because of model parameters being replicated across all workers.</p> </li> <li> <p>In a small-scale multi-node setup, with a few nodes, the best option seems to be DeepSpeed ZeRO-3 with hierarching partitioning enabled (or FSDP with hybrid sharding). If you’ve got Infiniband interconnect, you can mostly use plain DeepSpeed ZeRO-3 and push for larger model sizes as well.</p> </li> <li> <p>Gradient accumulation should be used if you’re still short on batch size after all the above optimizations. Training times with gradient accumulation can be faster with large models and multi-GPU/ multi-node settings.</p> </li> <li> <p>If you’re really short on GPU memory, then you would activate CPU/ disk offloading (and by disk, this has to be NVMe for ZeRO-Infinity). With the advent of Flash Attention 2, we need another study on the throughput gap between plain GPU-based training and GPU + NVMe/CPU-offloading. ZeRO-Infinity is better than ZeRO-Offload.</p> </li> <li> <p>Calculate the effective batch size and adjust hyperparameters accordingly. A general guideline is to scale up the learning rate with the effective batch size. This seems to hold true even for 100B+ models, <a href="https://platform.openai.com/docs/guides/legacy-fine-tuning/hyperparameters">as seen in OpenAI’s finetuning docs</a>.</p> </li> <li> <p>Finally, when you do start training, monitor <code class="language-plaintext highlighter-rouge">htop</code> to check on RAM usage (sometimes RAM OOM can be an issue), along with <code class="language-plaintext highlighter-rouge">nvidia-smi</code> to make sure GPUs aren’t bottlenecked by data preprocessing (you should aim for close to 100% volatile GPU utilization, even if GPU memory usage is lesser).</p> </li> <li> <p>Lower learning rate during pre-training works better in practice for GPT-3 and Bloom paper. The intuition/explanation for this effect is still pending.</p> </li> </ul> </blockquote> <h1 id="megatron-lm">Megatron-LM</h1> <p>A simple model parallel approach that is orthogonal and complementary to pipeline model parallelism. They also show that larger models do indeed result in higher performance by demonstrating larger models trained with their approach.</p> <p>Their approach exploits the inherent structure of transformer based language models that trains efficiently with PyTorch. Compared to the baseline that trains a 1.2 billion parameter model on a single NVIDIA V100 32GB GPU that sustains 39 TFLOPs (30% of the peak possible value), their method obtains 15.1 PetaFLOPs with an 8.3 billion model on 512 GPUs with 8-way parallelism - 76% scaling efficiency.</p> <p>The paper mentions the previous approaches <em>gradient accummulation</em> (Valiant, 1990) and <em>gradient/activation checkpointing</em> (Chen et. al., 2016), and that they are constrained to fit the model on a single GPU. They also mention that layer parallelism suffers from pipeline bubbles that reduce efficient or changes to the optimizer that impacts accuracy. Distributed tensor computation is a more general approach, and the authors use this technique for transformer’s attention heads to parallelize. No framework, no compiler, simple PyTorch modifications.</p> <h2 id="model-parallel-transformers">Model Parallel Transformers</h2> <p>The core idea is to parallelize the transformer architecture with some synchronization primitives.</p> <ol> <li> <p>The MLP part is a GEMM followed by a GeLU non-linearity.</p> \[\begin{align*} Y &amp;= GeLU(XA) \\ &amp;= GeLU (\begin{bmatrix} X_1 &amp; X_2\end{bmatrix}\begin{bmatrix}A_1 \\A_2\end{bmatrix}) \\ &amp; \neq GeLU(X_1A_1) + GeLU(X_2A_2) \end{align*}\] <p>The matrix \(A\) can be split across rows and \(X\) across columns. Due to the non-linearity of \(GeLU\), this method would need a synchronization point before the non-linear layer.</p> <p>The other approach is to split \(A\) across columns, and then \(GeLU\) can be applied independently. It doesn’t require any synchronization point.</p> \[\begin{align*} Y &amp;= GeLU(XA) \\ &amp;= GeLU (X \begin{bmatrix} A_1 &amp; A_2\end{bmatrix}) \\ &amp;= GeLU(XA_1) + GeLU(XA_2) \end{align*}\] <p>This approach splits both GEMMs in the MLP block across GPUs and requires a single all-reduce operation in the forward and backward passes.</p> </li> <li> <p>For the self-attention block, the partitioning of GEMMs is done in a similar column parallel fashion for \(K, Q, V\). These outputs can directly be passed into the linear layer to parallelize across rows evading communication latency here as well. The final answers after the FFN can be fused within two all-reduce operations.</p> </li> </ol> <p>Furthermore, since vocabulary sizes can be large, it is beneficial to parallelize the output embedding GEMM. However, since the output embedding layer shares weights with the input embedding, the input embedding matrix is also parallelized across the vocabulary dimension. These splits would add two all-reduce operations. However, in the output layer the all-gather operation can be optimized by fusing with the cross-entropy loss - reduces parameter communication from \(b\times s\times v\) to \(b \times s\). That is, communicating scalar losses instead of logits is a huge reduction in communication.</p> <p>The paper further reduces the communication by replicating dropouts, layer normalization, and residual connections across GPUs and only the results are communicated.</p> <blockquote> <p>Specifically, each worker maintains duplicate copies of layer normalization parameters on each GPU, and run dropout and residual connection on the output of the model parallel region before feeding them as input to the next model parallel regions. To optimize the model, each worker optimizes its own set of parameters. Since all values are either local to or duplicated on a GPU, there is no need for communicating updated parameter values in this formulation.</p> </blockquote> <h2 id="conclusion-4">Conclusion</h2> <p>The authors provide comprehensive set of results to demonstrate their method with GPT-2 and BERT, showcasing that they are able to scale the compute much higher with these simple optimizations while achieving higher accuracy with larger models. They also found that BERT like models perform better with layer normalization.</p> <h1 id="gpipe"><a href="https://arxiv.org/pdf/1811.06965">GPipe</a></h1> <p>GPipe is an attempt to democratize model parallelism through pipeline algorithms for layered networks. It was proposed towards the end of 2018, when transformers started getting popular and the hardware could not catch up. The high-level idea is that each model can be specified as a sequence of layers and consecutive groups of layers can be partitioned into cells that are each placed on a separate accelerator. A novel pipeline parallelism algorithm with micro-batch splitting is used to maximize the utility of the hardware.</p> <p>It was released to the public as an open-source library. The interface consists of user-selectable parameters such as estimate of the computation cost \(C_k\), number of partitions \(K\), number of micro-batches \(M\), and layers \(L\) that define the model etc.</p> <p>GPipe is a synchronous pipeline based parallelism technique. There are other approaches that split the operators across the accelerators (GPipe splits the layers), akin to SIMD approach for data parallelism. They call this the Single Program Multiple Data (<strong>SPMD</strong>) paradigm, and Mesh-Tensorflow is one method existing at that time. They note that these methods have high communication overhead and there are variety of ways of splitting the operators some of which may not be generalizable or scalable. Along similar lines, PipeDream pipelines the execution of forward passes and intersperses them with backward passes to maximize hardware utilization. However, it suffers from update staleness introduced by asynchronous backward updates. Some versions suggested maintaining multiple versions of the model, but that greatly hampers the scaling abilities of the technique.</p> <h2 id="algorithm">Algorithm</h2> <p>GPipe partitions the network into \(K\) sequential cells and each of them are placed on the available devices. Communication primitives are inserted at the boundaries to allow data transfer. The partitions are split so that the variance across all the cells is minimized to sync the computation time across all partitions.</p> <p>During the forward pass, every mini-batch of size \(N\) is split into \(M\) micro-batches that are pipelined through \(K\) accelerators. During the backward pass, gradients for each micro-batch are computed with the weights used for the forward pass, and these gradients are accumulated to update the model parameters across all accelerators.</p> <p>For batch-normalization layers, the statistics of inputs across micro-batches and moving averages across the mini-batch are calculated and communicated.</p> <p><img src="/assets/img/Machine Learning Systems/2025-02-25-22-13-11-image.png" alt=""/></p> <p>To further decrease the memory consumption, GPipe supports gradient checkpointing - each accelerator only stores the output activates at the partition boundaries. As a result, the peak activation memory is reduced to \(O(N + \frac{L N }{KM})\) from \(O(NL)\) (without gradient checkpointing and partitioning) where \(N/M\) is the micro-batch size and \(L/K\) is the number of layers per partition.</p> <p>The bubble overhead with this method is \(O(\frac{K - 1}{M + K - 1})\) amortized over the number of micro-steps \(M\). It is negligible in practice when \(M \geq 4\times K\). Note that increasing the number of micro-batches would underutilize the GPUs.</p> <p>GPipe has low communication overhead allowing efficient scaling on accelerators without high-speed interconnects. However, the memory requirements and computation flops at different layers are usually very imbalanced leading to inefficiencies. The authors left this problem of better partitioning algorithms to future works.</p> <h2 id="results-and-conclusion">Results and Conclusion</h2> <p>GPipe is able to reduce the memory requirements by half allowing training of almost 25x larger models. These savings are even more significant with architectures such as transformers where the savings can be 250 times. The speed-up is almost linear barring the pipeline bubbles and communication overheads. They obtained significant savings even with a low-speed communication device (PCI-E vs NVLink).</p> <p>These savings translate into the model performance as well. Giant networks can improve the accuracy, also allowing models to be trained on larger datasets. The authors tests this hypothesis with image classification and machine translation tasks. This work opened new avenues to analyze model architectures such as studying depth-width trade-offs in transformers (the deeper model outperforms by huge margins on low-resource languages, suggesting that it may be better for generalization).</p> <p>They also tested higher batch sizes that were not possible previously, and they observed that larger batch sizes improved the performance!</p> <p>GPipe assumes that a single layer fits within the memory requirements of a single accelerator and some layers (such as batch norm) require complicated computations and communications across accelerators which have been solved by future works.</p> <h1 id="alpa---automated-parallelism"><a href="https://arxiv.org/pdf/2201.12023">Alpa - Automated Parallelism</a></h1> <p>A given model can be parallelized in many ways. There are intra-operation and inter-operation splitting mechanisms that need to be considered based on the model architecture and the underlying hardware. These choices are interdependent, and it is difficult to consider all the variations to manually find the best way to parallelize every time.</p> <p>Alpa automates model-parallel training of deep-learning models by generating execution plans that unify data, operation and pipeline parallelism! They observed that all the parallelization techniques can be organized in a hierarchical space and be mapped to the hierarchical structure of the compute cluster. Different parallelization techniques have different bandwidth requirements for communication. On this note, they consider two types of parallelization</p> <ol> <li> <p><strong>Intra-operator parallelism</strong> - Partition ML operators along one or more tensor axes (batch or non-batch) and dispatch the partitions across the devices. Note that it involves data parallelism as well.</p> <p>Intra-operator parallelism has better device utilization, but results in communicating at every split and merge of partitioned operators in each training iteration.</p> </li> <li> <p><strong>Inter-operator parallelism</strong> - Slice the model into disjoint stages and pipeline the execution of stages on different devices.</p> <p>Inter-operator parallelism only requires communication between adjacent stages which can be light if sliced efficiently. However, it results in device idle time due to pipeline bubbles as we discussed before.</p> </li> </ol> <p><img src="/assets/img/Machine Learning Systems/2025-02-25-23-05-39-image.png" alt=""/></p> <p>Parallel to these tradeoffs, the communication bandwidth on the device cluster also has an asymmetric nature. The intra-operator parallelism can use the high communication bandwidth, while the inter-operator parallelism is handled between distance devices with relatively lower bandwidth. The joint execution plan may not be globally optimal, it obtains high performance across different large models.</p> <p>Alpa essentially is a compiler for parallel execution plans that partitions the cluster into device meshes based on the bandwidth hierarchy and performs inter-operator and intra-operator splitting.</p> <h2 id="background-1">Background</h2> <p>As mentioned previously, the authors consider a new view of ML parallelism that differs from the conventional views. Previously, the parallelism was one of the following types</p> <ol> <li> <p>Data parallelism - Data is distributed, but the model is replicated. The parameter updates are synchronized before the next iteration.</p> </li> <li> <p>Operator parallelism - When the models are too large to fit in a device, the operators involved are split across the non-batch axes and the computations are performed in parallel across devices. This method usually requires large amount of communication across devices. When the tensors are partitioned evenly, i.e., SPMD, all devices follow the same communication patterns.</p> </li> <li> <p>Pipeline parallelism - The model is split into stages which are placed on different workers. The training batch is split into microbatches for overlapped iterations of forward and backward passes across workers.</p> </li> <li> <p>Hybrid parallelism - Researchers manually combined the above techniques to obtain much higher performance. One notable work is the Megatron-LM that optimizes the parallelization for transformer models (3D Parallelism).</p> </li> <li> <p>Automatic combination of parallelism - Prior to this work, auto-parallelization was limited to combining data parallelism with at most one model parallelism approach missing improvement opportunities.</p> </li> </ol> <p>In contrast to this view, the authors consider inter-op and intra-op parallelisms as described before. With such a view, techniques such as Megatron-LM, ZeRO (that involves updates sharding) can be viewed as intra-op parallelism. On the other hand, techniques such as GPipe, 1F1B and PipeDream are all pipeline optimizations that come under inter-op parallelism. The authors of this paper adopt synchronous 1F1B for pipeline parallelism.</p> <h2 id="method-overview">Method Overview</h2> <p>As mentioned previously, Alpa considers two-level planning</p> <ol> <li> <p>At the intra-op level, minimize the cost of executing a stage of the computational graph on a given device mesh (that have high bandwidth between each other)</p> </li> <li> <p>At the inter-op level, minimize the inter-op parallelization latency with respect to slicing the model and mapping the stage-mesh pairs. The inter-op optimization depends on knowing the execution cost of each stage-mesh pair reported by the intra-op optimizer.</p> </li> </ol> <p>Alpa does this by generating an intermediate representation similar to Jax which is used by the optimization steps.</p> <p>A device mesh is a 2-dimensional logical view of a set of physical devices (all having same compute capability) which communicate with different bandwidths in each dimension. We will delve deeper into the two stages below.</p> <h2 id="intra-operator-parallelism">Intra-operator Parallelism</h2> <p>Alpa adopts the SPMD-style intra-op parallelism that partitions operators evenly across devices and executes the same instructions on all devices assuming that the devices in the mesh have equivalent compute capability. This decision greatly reduces the search space and unifies approaches such as data parallelism, ZeRO and Megatron LM. Alpa formalizes the problem as an integer linear programming problem.</p> <p>The goal of intra-op pass is to pick one parallel algorithm for every operator to minimize the execution time of the entire graph. The authors use a <strong>sharding spec</strong> to define the layout of a tensor - each dimension of a tensor is <em>S</em> (sharded) or <em>R</em> (replicated). The partitioned tensor axes must be mapped to mesh axes.</p> <p>Through this, we can succinctly represent different ways of partitioning a matmul.</p> <p><img src="/assets/img/Machine Learning Systems/2025-02-25-23-31-35-image.png" alt=""/></p> <p>The model graph is represented in XLA’s HLO format, that summarizes common DL operators into less than 80 primitive operators whose parallel algorithms can be manually enumerated.</p> <p>The ILP then considers the node and edge communication costs which can be solved by an off-the-shelf solver.</p> <p>The equation is given by</p> \[\min_s \sum_{v \in V} s^T_v (c_v + d_v) + \sum_{(v, u) \in E} s^T_v R_{vu} s_u\] <p>Although these different costs can be obtained by profiling, the authors suggest heuristic calculations to prevent executing multiple operators on the cluster wasting resources. There can be post-ILP optimizations similar to the ones in ZeRO to further improve the efficiency.</p> <h2 id="inter-op-parallelism">Inter-op Parallelism</h2> <p>Previous works consider a simplified problem where the device for each stage is pre-assigned and all stages have fixed data or operator parallelism plan. Alpa jointly optimizes everything considering a much general scenario.</p> <p>The computational graph is split based on the topology order, and each stage has its own intra-op cost obtained from the intra-op pass. The cost is minimized considering the micro-batches inn the pipeline. The latency is given by</p> \[T^* = \min_{s_1, \dots, s_S} \left\{ \sum_{i = 1}^S t_i + (B - 1) \cdot \max_{1 \leq j \leq S} \{t_j\}\right\}\] <p>The first term is the total latency of all stages, interpreted as the latency of the first micro-batch groin through the pipeline; the second term is the pipelined execution time for the rest of the micro-batches which is bounded by the slowest stage in the 1F1B pipeline.</p> <p>Additionally, for an operator in the forward pass of the graph, the corresponding backward operator should be in the same submesh - the backward propagation usually uses the similar set of tensors during the forward pass, reducing the communication.</p> <p>The authors formulate this as a DP formulation, cleverly considering primitive sub-mesh sizes to reduce the search-space significantly! These were the high level ideas, and there are considerations for cases where the operators do not fit into one device, etc. They introduce further techniques to prune the search space</p> <ol> <li> <p>Early pruning - Get rid of states that have higher cost than the current best</p> </li> <li> <p>Operator clustering - Element-wise operators can be merged without much change in the cost. Greatly reduces the search space.</p> </li> </ol> <h2 id="parallelism-orchestration">Parallelism Orchestration</h2> <p>Once the optimal assignment is found, XLA and GSPMD are used to generate parallel executables. Alpa implements additional parallelism orchestration pass to address cross-mesh communications. They introduce cross-mesh resharding for different sharding specs - P2P send/recv primitives.</p> <h1 id="language-models-are-few-shot-learners">Language Models are Few-Shot Learners</h1> <p>The paper from OpenAI that started it all. The after-effects of this work are still rapidly progressing. Let’s dive in.</p> <p>The focus in NLP shifted from specialized models to task-agnostic systems for downstream applications. What started out as single layer representations, got converted to RNNs with multiple layers of representations and contextual states for stronger representations, and finally in the form of a transformer language model that seem to have amazing generalizing capabilities.</p> <p>When this paper was proposed, people were still training these models on labeled datasets, and with the unavailability of large scale general datasets outside task-specific ones, developing a large generalizable model was a challenge. Furthermore, the narrowness of the datasets can also inhibit the capabilities of the model. The authors of this paper argue that humans do not require such large scale datasets to perform these general capabilities, and quickly learn from only a few demonstrations. To counter these limitations, the previous works experimented with meta-learning and scaling the size of the language models. The authors note the smooth trend in improvement with scale, and they double down on this hypothesis.</p> <p>The largest model trained prior to this had 17B parameters. In this paper, the authors trained a 175B parameter model!</p> <h2 id="approach-model-and-dataset">Approach, Model and Dataset</h2> <p>The pre-training of GPT follows a similar procedure to previous models at a larger scale. With this paper, they introduce new paradigms to evaluate/tune the model for in-context learning</p> <ol> <li> <p>Fine-tuning - Curating datasets for specific tasks for better performance. The authors mention that they don’t use this paradigm with GPT-3 to test it’s generalizing capabilities.</p> </li> <li> <p>Few-Shot - Model is given a few input demonstrations as part of the prompt to adapt to a new task</p> </li> <li> <p>One-shot - Same as above, but only one example if provided</p> </li> <li> <p>Zero-shot - Testing the capabilities of the model for new tasks without providing any new demonstrations</p> </li> </ol> <p>The model architecture is same as GPT-2 that uses alternating dense and locally banded sparse attention patterns similar to Sparse Transformer.</p> <p>Many datasets for language models rapidly expanded and culminated into the Common Crawl dataset consisting of nearly a trillion words. The authors improved the dataset by</p> <ol> <li> <p>Filtering based on similarly to a range of high-quality reference corpora</p> </li> <li> <p>Fuzzy deduplication at the document level to prevent redundancy and preserve the integrity of the validation set</p> </li> <li> <p>Addition of high-quality reference corpora into the training mix</p> </li> </ol> <p>One concern people have with large datasets is the memorization of downstream test tasks. To reduce this effect, the authors tried to remove all benchmark test instances from the training set. They mention that due to a bug they did not do this perfectly, and there was contamination in some cases.</p> <h2 id="training-and-evaluation-details">Training and Evaluation Details</h2> <p>Previous works showed that larger datasets perform better with larger batch sizes and a smaller learning rate. The authors use a gradient noise scale during training to guide the choice of the batch size. They use model parallelism (both intra-op and inter-op) to reduce the peak memory usage. All models were trained on V100 GPUs from Microsoft.</p> <p><img src="/assets/img/Machine Learning Systems/2025-03-04-16-31-21-image.png" alt=""/></p> <p>With this experiment, a strong paradigm in this field of research has been established - <strong>scaling both compute and model size has a power-law effect on the performance</strong>.</p> <p>The authors test GPT-3 on multiple benchmarks carefully omitting cases that were a subset of the training dataset. They see that the few-shot setting achieves significantly higher performance that both one-shot and zero-shot. One-shot also shows improved performance with this model. in many cases, GPT-3 beat the SOTA model performance in multiple benchmarks. They test on generation, evaluation, reasoning and many other genres of tasks.</p> <h2 id="limitations">Limitations</h2> <p>The authors noticed that GPT-3 tends to repeat itself and in some cases the few-shot does not perform much better than one-shot inference. In a more architectural sense, they note that GPT-3 does not specifically cater to tasks with bidirectional language flow requirements. They also mention that large models such as these reach the limits of their pre-training objective that itself does not place importance on some task-specific needs. That is, more useful language systems may be better due to goal-directed actions rather than just making predictions. Furthermore, language models are not grounded in other forms of information such as video and audio.</p> <p>Compared to humans, GPT-3 has much lower sample efficiency (it does meh with much much more samples seen during pre-training). Finally, both training and inference on such large models is very expensive and inconvenient. This has been changing in the recent times due to the advancements in RLHF and distillation techniques.</p> <p>The authors also mention the broader impact of this paper, and warn the readers about the potential misuses of language models. The biases in the training datasets like certain stereotypes can propagate into the model and also about the energy due to the rise in this domain.</p> <h1 id="training-compute-optimal-large-language-models">Training Compute-optimal Large Language Models</h1> <p>The paper tries to answer an important dilemma in training language models - is it better to train bigger models that are potentially undertrained or train smaller models on more data? The authors noted the trends in model and tried to derive an empirical law for the trends observed. They note that the budget for model training is known apriori, and the training can only be done once, estimating the best hyperparameters is critical.</p> <p>So, the question we are concerned with is - Given a fixed FLOPs budget, how should one trade-off model size and the number of training tokens?</p> <p>There have been some previous works around this. A 10x increase in the computational budget should increase 5.5x while the number of training tokens should only increase 1.8x. However, the authors note that the previous works did not consider the variations in tokens, learning rate schedule and other hyper-parameters to better model the behaviors.</p> <p><strong>Note.</strong> This work is mainly for dense transformer architectures. They mention that there are orthogonal works like RAG and MoE that improve the models, but the motivation for this paper is to understand the behavior of dense transformers.</p> <h2 id="approach">Approach</h2> <p>In the first method, the authors tried to vary the number of training steps for a fixed family of models. They obtained the minimum loss achieved for a given number of training FLOPs. Using the interpolants, they obtain a mapping from any FLOP count C to most efficient model size N and training tokens D. They obtain the relationship \(N_{opt} \propto C^{0.5}\) and \(D_{opt} \propto C^{0.5}\) contradicting previous results.</p> <p>In their second approach, they vary the model size for a fixed set of training FLOP counts and consider the final training loss for each point. It essentially answers the question for a given FLOP budget, what is the optimal parameter count. These are the IsoFLOPs experiments, and they obtain \(N_{opt} \propto C^{0.49}\) and \(D_{opt} \propto C^{0.51}\).</p> <p>In the final approach, they modeled all the final losses considering the previous experiments, and considering a classical <strong>risk decomposition</strong>, they propose</p> \[\hat L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta}\] <ul> <li> <p>The first term captures the loss for an ideal generative process on the data distribution and corresponds to the entropy of the natural text.</p> </li> <li> <p>The second term captures the fact that a perfectly trained transformer with \(N\) parameters underperforms the ideal generative process.</p> </li> <li> <p>The final term captures the fact that the transformer is not trained to convergence, as we only make a finite number of optimisation steps, on a sample of the dataset distribution.</p> </li> </ul> <p>To fit the empirical observations, they minimize a Huber loss on this formulation and also consider possible local minimas. Finally, they obtain the following results -</p> <table> <thead> <tr> <th>Approach</th> <th>Coeff. \(a\) where \(N_{\text{opt}} \propto C^a\)</th> <th>Coeff. \(b\) where \(D_{\text{opt}} \propto C^b\)</th> </tr> </thead> <tbody> <tr> <td>1. Minimum over training curves</td> <td>0.50 (0.488, 0.502)</td> <td>0.50 (0.501, 0.512)</td> </tr> <tr> <td>2. IsoFLOP profiles</td> <td>0.49 (0.462, 0.534)</td> <td>0.51 (0.483, 0.529)</td> </tr> <tr> <td>3. Parametric modelling of the loss</td> <td>0.46 (0.454, 0.455)</td> <td>0.54 (0.542, 0.543)</td> </tr> <tr> <td>Kaplan et al. (2020)</td> <td>0.73</td> <td>0.27</td> </tr> </tbody> </table> <p>The key insights they obtain are -</p> <ol> <li> <p>The current language models are considerably over-sized given the compute budges.</p> </li> <li> <p>The amount of training data that is projected to be needed is far beyond what is current used to train LLMs.</p> </li> </ol> <p>So the authors recommend that for a given training compute budget, smaller models should be trained on more tokens to achieve the most performant model.</p> <h2 id="chinchilla">Chinchilla</h2> <p>Considering the optimal hyperparameters they obtained, the authors trained a language model called <em>Chinchilla</em> as a competitor to Gopher. It is 4\(\times\) smaller than Gopher, and similarly the memory footprint and inference costs are reduced too. They follow a mixed-precision training procedure.</p> <p>The results obtained are very impressive - Chinchilla significantly outperforms Gopher on many evaluation subsets.</p> <h2 id="conclusion-5">Conclusion</h2> <p>The authors assert that scaling data instead of scaling models is the right way to obtain the best performance. We need to curate high-quality data - current models (e.g., GPT-3, MT-NLG) use only ~300B tokens, far below optimal recommendations (e.g., 1.5T tokens for 70B models).</p> <p>They mention that such results and methodologies can also be extended to other kinds of models outside autoregressive language models.</p> <p>Overall, this paper pushes towards an experimental based theoretical analysis of models and establishes a framework for compute-optimal training, demonstrating that <strong>equal scaling of parameters and tokens</strong> maximizes model performance. Chinchilla validates this hypothesis, achieving SOTA results with reduced computational overhead. Future work must prioritize dataset quality and ethical considerations alongside scaling.</p> <h1 id="flash-attention">Flash Attention</h1> <p>One of the seminal papers that has significantly reduced the energy usage of these large transformer models. At the core of transformers is the attention mechanism that becomes a bottle neck for long sequences. Both the time and space complexity scale quadratically with the sequence length making it infeasible to work with long context. Some previous works tried to reduce these requirements using sparse-approximations, low-rank approximations and other niche techniques which did not gain traction. <strong>The issue is that they reduce the FLOPs but because of the behavior of GPUs, this does not necessarily translate to optimized run-times.</strong></p> <p>Flash Attention on the other hand leverages the various hierarchies in the GPUs to utilize the resources most efficiently (to their fullest extent) and reduce both time and space requirements. Due to the reduction of these requirements, they claim fast model training, higher quality models and SOTA attention speed.</p> <h3 id="background-2">Background</h3> <p>The GPU memory hierarchy has memory of different sizes and speeds. For example, the A100 GPU has 40-80GB of high bandwidth memory (HBM) with bandwidth 1.5-2.0TB/s and 192KB of on-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s.</p> <p><img src="/assets/img/Machine Learning Systems2025-03-10-19-19-19-image.png" alt=""/></p> <p>The on-chip SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size. As compute has gotten faster relative to memory speed, operations are increasingly bottlenecked by memory (HBM) accesses. With this motivations, the authors exploit fast SRAM for Flash Attention.</p> <p>As mentioned previously in these articles, GPU operations can be classified as compute bound (time is arithmetic operations dominant) or memory bound (time is memory access dominant). Memory bound operations have been optimized with the use of <strong>kernel fusion</strong> - multiple operations can be done with single load from HBM. It turns out that attention is a memory-bound operation since it mostly consists of element-wise operations.</p> <p><img src="/assets/img/Machine Learning Systems2025-03-10-16-14-20-image.png" alt=""/></p> <p>In standard attention, the matrices \(S = QK^T\) and \(P = \text{softmax}(S)\) are explicitly calculated using up \(\mathcal O(N^2)\) memory. In terms of GPU usage, it looks like this -</p> <ol> <li> <p>Load \(Q, K\) by blocks from HBM, compute \(S = QK^T\), write \(S\) <strong>to HBM</strong>.</p> </li> <li> <p>Read \(S\) <strong>from HBM</strong>, compute \(P = \text{softmax}(S)\), write \(P\) <strong>to HBM</strong>.</p> </li> <li> <p>Load \(P\) and \(V\) by blocks <strong>from HBM</strong>, compute \(O = PV\), write \(O\) to HBM.</p> </li> </ol> <p>We shall now see how Flash Attention improves on this with <strong>kernel fusion</strong></p> <h2 id="method">Method</h2> <p>The main idea is that, like matrix multiplication, \(Q, K, V\) can be split into blocks to load from slow HBM to fast SRAM.</p> <h3 id="forward-pass">Forward Pass</h3> <p><strong>Tiling</strong>. The main hurdle in the previous works was getting softmax to work with tiling mechanisms. how can softmax be fused across blocks? Since we apply softmax across columns of \(K\), for each column, we can maintain \(f(x) = e^{x - m(x)}, l(x) = \sum_i f(x)_i\) to calculate the softmax as \(f(x)/l(x)\) - they essentially keep track of the sum and maximum of elements (for numerically stable softmax). These two statistical quantities are calculated across blocks recursively all the way up to calculate the final softmax - forward pass of the flash attention algorithm.</p> <p><img src="/assets/img/Machine Learning Systems2025-03-10-19-25-38-image.png" alt=""/></p> <p>Let us dive into a bit more detail. The output matrix \(O\) is stored in the HBM, and updates are made to store the correct output at the end of all block computations. For simplicity, let us consider blocks of sizes \((n \times d)\) where \(n\) is some factor of \(N\).</p> <ol> <li> <p>In typical softmax \(O_ij = \underbrace{\sum_k Q_{ik} K_{kj}}_{S^i} \cdot V_j\) (using superscript for row and subscript for column). In Flash attention, we pick up block \(Q_{b_i}\) and \(K_{b_j}\), perform the multiplication to obtain a sub-matrix \(S_{b_ib_j}\) of size \(n \times n\) and store</p> <ol> <li> <p>\(m^k_{b_ib_j}\) - The maximum value of each row vector.</p> </li> <li> <p>\(l^k_{b_ib_j}\) - Sum of \(e^{S_{b_ib_j} - m^k}\) across all the columns in each row</p> </li> </ol> </li> <li> <p>We compute the result \(f(S_{b_ib_j})V_{b_i}\) with appropriate softmax values using the \(m, l\) values and update the corresponding \(n \times d\) block in \(O\). Remember that this is the partial result and needs to updated with further calculations.</p> </li> <li> <p>When another partial sum for the block in \(O\), the entry can be easily updated with the new \(m, l\) values (the math is super easy) to calculate the softmax of the complete partial sum so far.</p> </li> </ol> <p>Although the asymptotic time complexity is still the same, the constant associated would decrease significantly due to lower HBM accesses. The space usage increases by \(2*N\) while dropping the intermediate results that occupy \(\mathcal O(N^2)\)! This is huge! The time computation becomes \(\mathcal O(N^2d^2 M^{-1})\) where \(M\) is the size of SRAM, as compared to \(\mathcal \Omega (Nd + N^2)\).</p> <p><strong>Input:</strong></p> <ul> <li> <p>Matrices \(Q, K, V \in \mathbb{R}^{N \times d}\) in HBM.</p> </li> <li> <p>On-chip SRAM of size \(M\).</p> </li> <li> <p>Softmax scaling constant \(\tau \in \mathbb{R}\).</p> </li> <li> <p>Masking function \(mask\).</p> </li> <li> <p>Dropout probability \(p_{drop}\).</p> </li> </ul> <p><strong>Steps:</strong></p> <ol> <li> <p>Initialize the pseudo-random number generator state \(R\) and save to HBM.</p> </li> <li> <p>Set block sizes:</p> </li> </ol> <p>   - \(B_c = \left\lfloor \frac{M}{4d} \right\rfloor\),</p> <p>   - \(B_r = \min\left(\left\lfloor \frac{M}{4d} \right\rfloor, d'\right)\).</p> <ol> <li>Initialize in HBM:</li> </ol> <p>   - \(O = (0)^{N \times d} \in \mathbb{R}^{N \times d}\),</p> <p>   - \(\ell = (0)^N \in \mathbb{R}^{N}\),</p> <p>   - \(m = (-\infty)^N \in \mathbb{R}^{N}\).</p> <ol> <li>Divide \(Q\) into </li> </ol> <p>   \(T_r = \left\lceil \frac{N}{B_r} \right\rceil\) blocks </p> <p>   \(Q_1, Q_2, \dots, Q_{T_r}\) of size \(B_r \times d\) each, and divide both \(K\) and \(V\) into </p> <p>   \(T_c = \left\lceil \frac{N}{B_c} \right\rceil\) blocks </p> <p>   \(K_1, K_2, \dots, K_{T_c}\) and </p> <p>   \(V_1, V_2, \dots, V_{T_c}\) of size \(B_c \times d\) each.</p> <ol> <li>Divide:</li> </ol> <p>   - \(O\) into \(T_r\) blocks \(O_1, O_2, \dots, O_{T_r}\) of size \(B_r \times d\) each,</p> <p>   - \(\ell\) into \(T_r\) blocks \(\ell_1, \ell_2, \dots, \ell_{T_r}\) of size \(B_r\) each,</p> <p>   - \(m\) into \(T_r\) blocks \(m_1, m_2, \dots, m_{T_r}\) of size \(B_r\) each.</p> <ol> <li><strong>For</strong> \(j = 1 \text{ to } T_c\) <strong>do:</strong></li> </ol> <p>   1. Load \(K_j, V_j\) from HBM to on-chip SRAM.</p> <p>   2. <strong>For</strong> \(i = 1 \text{ to } T_r\) <strong>do:</strong></p> <p>      1. Load \(Q_i, O_i, \ell_i, m_i\) from HBM to on-chip SRAM.</p> <p>      2. On-chip, compute  </p> <p>         \(S_{ij} = \tau\, Q_i\, K_j^T \in \mathbb{R}^{B_r \times B_c}\).</p> <p>      3. On-chip, compute  </p> <p>         \(S^{masked}_{ij} = mask(S_{ij})\).</p> <p>      4. On-chip, compute:</p> <p>         - \(\tilde{m}_{ij} = \text{rowmax}(S^{masked}_{ij}) \in \mathbb{R}^{B_r}\),</p> <p>         - \(\tilde{P}_{ij} = \exp\Bigl(S^{masked}_{ij} - \tilde{m}_{ij}\Bigr) \in \mathbb{R}^{B_r \times B_c}\) (pointwise),</p> <p>         - \(\tilde{\ell}_{ij} = \text{rowsum}(\tilde{P}_{ij}) \in \mathbb{R}^{B_r}\).</p> <p>      5. On-chip, compute:</p> <p>         - \(m^{new}_i = \max(m_i, \tilde{m}_{ij}) \in \mathbb{R}^{B_r}\),</p> <p>         - \(\ell^{new}_i = e^{\,m_i - m^{new}_i}\,\ell_i + e^{\,\tilde{m}_{ij} - m^{new}_i}\,\tilde{\ell}_{ij} \in \mathbb{R}^{B_r}\).</p> <p>      6. On-chip, compute  </p> <p>         \(\tilde{P}^{dropped}_{ij} = dropout\Bigl(\tilde{P}_{ij},\, p_{drop}\Bigr)\).</p> <p>      7. Write to HBM:</p> <p>         \(O_i \gets \text{diag}(\ell^{new}_i)^{-1}\Bigl(\text{diag}(\ell_i)\,e^{\,m_i - m^{new}_i}\,O_i + e^{\,\tilde{m}_{ij} - m^{new}_i}\,\tilde{P}^{dropped}_{ij}\,V_j\Bigr)\).</p> <p>      8. Write to HBM:</p> <p>         \(\ell_i \gets \ell^{new}_i,\quad m_i \gets m^{new}_i\).</p> <p>   3. <strong>End For</strong> (over \(i\)).</p> <ol> <li> <p><strong>End For</strong> (over \(j\)).</p> </li> <li> <p>Return \(O, \ell, m, R\).</p> </li> </ol> <h3 id="block-sparse-flashattention">Block-Sparse FlashAttention</h3> <p>In vanilla attention mechanisms, a mask is typically applied to the output after all the intermediate computations are completed. If the matrices are manipulated during the calculations, we may underutilize the GPU and lead to a less efficient computation.</p> <p>Flash Attention solves this problem to some extent - it defines block sparsity masks for each block and extends the previous algorithm by simply considering blocks that have non-zero mask! This speeds-up computations a lot!</p> <h3 id="backward-pass">Backward Pass</h3> <p><strong>Recomputation.</strong> Instead of explicitly storing the intermediate matrices, the authors suggest gradient checkpointing for backward pass of attention. Peculiarly, gradient checkpointing speed-up the backward computation due to lower HBM accesses even though the intermediate matrices have to be recomputed.</p> <p>Essentially, again, instead of storing \(P, S\) both of which are \(\mathcal O(N^2)\), we simply load \(K, Q, V\) that have the size \(N \times d\) and recompute the required results for backward pass. The key ideas for gradient computations are very similar to the forward pass (more complicated because of too many computations).</p> <h3 id="summary-4">Summary</h3> <p>In accordance with their claims, the authors show the following results -</p> <ol> <li> <p>Fast models - BERT trains 15% faster and \(3\times\) end-to-end speedup compared to typical architectures for GPT-2. Only \(1.7\times\) speedup compared to Megatron-LM.</p> <blockquote> <p>Can FlashAttention be combined with Megatron-LM? I think so, need to read this up.</p> </blockquote> </li> <li> <p>Better Models with Longer Sequences - The context length of \(GPT-2\) can now be increased \(4\times\) and the training is still faster than Megatron-LM!</p> </li> <li> <p>Benchmarking Attention -</p> <p><img src="/assets/img/Machine Learning Systems2025-03-10-20-51-41-image.png" alt=""/></p> </li> </ol> <p>So that concludes our analysis on FlashAttention. The authors also mention some limitations -</p> <ol> <li> <p>Developing new CUDA kernels for each hardware! Projects like Triton and other DSLs like TVM are helping with these issues though.</p> </li> <li> <p>The authors did not discuss how to extend these techniques to distributed systems. Later works, <a href="https://www.adept.ai/blog/flashier-attention">FlashierAttention</a> and <a href="https://tridao.me/blog/2024/flash3/">FlashAttention3</a> extended this work to further improve the efficiency.</p> </li> </ol> <p>This is another good <a href="https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad">blog</a> explaining Flash Attention well. Look into this if what I wrote does not make sense!</p> <h1 id="paged-attention">Paged Attention</h1> <p>Running LLM services is very expensive. It is imperative we ensure the LLM serving systems are efficient and have a high throughput. KVCache made a profound impact on the efficiency of LLMs by storing the previously generated keys and values during the decoding process.</p> <p>An LLM generates tokens in an autoregressive manner, and this sequential generation process makes the workload memory-bound. We need to efficiently manage memory to load more number of batches and increase the throughput.</p> <p>In a serving system, typically 30% of the memory is used to store the states for the requests. The KVCache for these different requests are often stored precariously resulting in almost 60-80% of the storage going towards fragmentation and reservation spaces (since these tensors are stored in contiguous memory). As a result, the GPU is underutilized because the batch sizes are limited.</p> <p><img src="/assets/img/Machine Learning Systems2025-03-12-19-41-41-image.png" alt=""/></p> <p>This work draws inspiration from OS memory management to improve the efficiency of KV Cache memory storage, resulting in storage of higher batches and consequently higher throughput.</p> <p>Contiguous memory storage has become a standard in deep learning systems. However, the dynamic nature of the tokens generated over time make the existing systems inefficient due to pre-allocation and not being able to share the memory. For example, decoding algorithms such as parallel sampling and beam search, the generated sequences partially share the KV cache and the current systems do not support memory sharing due to contiguous storage.</p> <p>PagedAttention addresses these limitations by storing KV cache in a flexible manner similar to OS virtual memory - with logical memory and pages.</p> <h2 id="background-3">Background</h2> <ul> <li> <p><strong>Batching multiple requests</strong>. Since the input tokens and the generated tokens vary and are not known apriori, batching them together is a difficult task. However, the MLPs in the transformer layers are token-agnostic allowing different requests to be batched for this stage of the decoding process. In such manner, <strong>continuous batching</strong> is employed where the self-attention of the requests are processed independently on different GPUs and the requests are then merged for the forward pass through MLP.</p> </li> <li> <p>The lifetime of a request can be classified into prompt phase (where the prompt is processed as a whole) and the decoding phase (token generation in autoregressive manner). The prompt phase of the requests can be efficiently parallelized across GPUs.</p> </li> <li> <p>The size of the KV Cache grows very quickly. With more powerful GPUs, the FLOPs typically increase dramatically while the memory improves incrementally. Therefore, memory would be a significant bottleneck going forward.</p> </li> </ul> <h3 id="memory-management-in-kv-cache">Memory Management in KV Cache</h3> <p><img src="/assets/img/Machine Learning Systems2025-03-12-20-06-37-image.png" alt=""/></p> <p>As we mentioned before, KV Cache is stored as a contiguous tensor, and a chunk of memory is pre-allocated statically irrespective of actual input or eventual output length of the request. As a result, there are three primary sources of memory wastage</p> <ol> <li> <p>Internal fragmentation - Over-provisioning for potential maximum sequence lengths</p> </li> <li> <p>Reserved slots for future tokens. Although this maybe used in the future, it could be used now for holding other requests.</p> </li> <li> <p>External fragmentation from the memory allocator.</p> </li> </ol> <h2 id="method-1">Method</h2> <p>PagedAttention allows storing continuous keys and values in non-contiguous memory space. It partitions KV cache of each sequence into KV blocks and each block contains the key and value vectors for a fixed number of tokens based on the <em>Block Size B</em>. During the attention computation, the PagedAttention kernel fetches different KV blocks separately.</p> <p>The memory manager is analogous to virtual memory in operating systems - the KV cache is represented as a series of logical KV blocks, filled from left to right. The unfilled positions on each row are reserved for future generations. On GPU workers, a block engine allocates a contiguous chunk of GPU DRAM and divides it into physical KV blocks and maintains block tables (mapping between logical and physical KV blocks). Note that the block size is an important hyperparameter that is a trade-off between spaces reserved for the future and the block table overhead.</p> <p><img src="/assets/img/Machine Learning Systems2025-03-12-20-27-56-image.png" alt=""/></p> <blockquote> <p>The whole reason we maintain contiguous blocks is to ensure faster retrieval, right?</p> </blockquote> <p>New physical blocks are assigned only when all the previous blocks for a request are full.</p> <p>Due to the flexible nature of this memory management, other decoding techniques like parallel decoding, beam search and shared prefix become much more efficient.</p> <ol> <li> <p>In parallel decoding a single physical block and be mapped to multiple logical blocks (tracked through a <em>reference count</em>). Here only the initial prompt blocks are shared</p> </li> <li> <p>In beam search, blocks other than the prefill phase are shared and a block tree has to be generated similar to OS process tree. Previous systems maintained copies of KV cache for beam candidates and this technique prevents that.</p> </li> <li> <p>In case of system prompts (usually very long), LLM service providers usually have a single copy for the tokens, and the same system can be adapted to PagedAttention as well</p> </li> </ol> <p>When the number of requests surpass the system’s capacity, the authors adopt a FCFS policy. There are two main questions that need to be answered -</p> <ol> <li> <p>What blocks need to be evicted? They use an all-eviction policy where all the blocks relating to a request are evicted.</p> </li> <li> <p>How to recover evicted blocks? The evicted blocks are copied to the CPI memory, and swapped based on the requirement</p> </li> </ol> <p>They also adopt a recomputation technique if the latency is much lower.</p> <p><strong>Distributed Execution.</strong> When multiple GPUs are present, the memory manager needs to be capable of handling distributed memory. They implement the memory management in a Megatron-LM style tensor model parallelism. Each model shard still processes the same set of input tokens, thus requiring the KV cache for the same positions - so a single KV cache manager within the centralized scheduler suffices.</p> <h2 id="execution-and-results">Execution and Results</h2> <p>PagedAttention introduces memory access patterns that are not efficiently supported by the existing systems. They implemented their own GPU kernels</p> <p>1) Fused re-shape and block write</p> <p>2) Fusing block read and attention</p> <p>3) Fused block copy</p> <p>PagedAttention obtains significantly better throughput - It is able to handle 2-3x higher number of requests</p> <p><img src="/assets/img/Machine Learning Systems2025-03-12-20-45-50-image.png" alt=""/></p> <p>The authors also performed a detailed ablation study for</p> <ul> <li> <p>Custom kernel operations - 20-26% lower latency</p> </li> <li> <p>Block size - If the block size is too small, vLLM may not fully utilize the GPU’s parallelism for reading and processing KV cache. If the block size is too large, internal fragmentation increases and the probability of sharing decreases<img src="/assets/img/Machine Learning Systems2025-03-12-20-47-22-image.png" alt=""/></p> </li> <li> <p>Comparison of recomputation and swapping - depends on the PCIe bandwidth</p> </li> </ul> <h2 id="conclusion-6">Conclusion</h2> <p>It must be noted that these techniques work well for LLMs because they are memory bound. In other words, other DNN workloads may not see similar improvements because they tend to be compute-bound.</p> <p>Furthermore, due to the significance of the transformer architecture, numerous specialized serving systems for it have been developed. There are other works like Orca that developed orthogonal techniques to improve GPU utilization.</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Few keypoints from books other important papers in the field.]]></summary></entry><entry><title type="html">Data Systems for Machine Learning</title><link href="https://sudhansh6.github.io/blog/data-systems-for-ml/" rel="alternate" type="text/html" title="Data Systems for Machine Learning"/><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/data-systems-for-ml</id><content type="html" xml:base="https://sudhansh6.github.io/blog/data-systems-for-ml/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Machine Learning systems have played a pivotal role in the rapid adaptation of Ai in the world today. This domain is essential for solving future problems and also making the current architectures more efficient. This is crucial considering that companies are reactivating nuclear power plants to power AI in the real-world.</p> <p>Along with the progress in AI from small neural networks to large language models, there has been a development in the size of datasets as well. Big data arrived, and AI today relies on these internet-scale datasets. After all, doesn’t ChatGPT just do pattern-matching in the internet?</p> <p>Moreover, the compute capabilities have been scaling exponentially. Just last year (2024), NVIDIA released a new super-chip architecture <a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-platform-arrives-to-power-a-new-era-of-computing">Blackwell</a> that has 97 billion transistors that can reach up to 1.4 exa-flops! The largest super-computer was barely able to reach 1 exa-flop. All this power in the palm of your hand…</p> <p>Richard Sutton once said, search and learning can scale unparalleled with growing computation power.</p> <p>Consider the year 2012 — AlexNet made waves showing SOTA capabilities with images. They use Stochastic Gradient Descent, dropout, convolution networks and initialization techniques. Without ML systems (CUDA, etc), the code would have been 44,000 lines with days of training! With these systems (Jax, PyTorch, TensorFlow) in place, you can achieve the same result in 100 lines within hours of training.</p> <h3 id="in-practice">In Practice</h3> <p>In industry, problems are typically of the form - improve the self-driving car’s pedestrian detection to be X-percent accurate at Y-ms latency budget. For an ML engineer is, the general approach is to design a better model with better learning efficiency followed by hyper-parameter running, pruning, distillation. An ML systems engineer would take the best model by ML researchers, specialize the implementation to target the H/W platform to reduce latency. <em>Streamlining the entire process from development to deployment</em>.</p> <h2 id="overview">Overview</h2> <p>From ad-hoc methods having diverse models and optimization algorithms with various data pre-processing techniques - we have arrived at an optimal algorithm that is <em>iterative and convergent</em>. As our models have become more and more specialized, the computation resources scaled exponentially.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17363053041704.jpg" alt=""/></p> <p>Through the course of this article, we will cover deep learning basics, computational graphs, Autodiff, ML frameworks, GPUs, CUDA and collective communication.</p> <p>There are more related topics to the ones discussed here</p> <ul> <li>ML for systems</li> <li>ML Hardware design</li> </ul> <p>Unfortunately, the textbook for this content is just miscellaneous research papers.</p> <h1 id="background">Background</h1> <h2 id="dl-computation">DL Computation</h2> <p>The idea is to concatenate composable layers</p> \[\theta^{(t + 1)} = f(\theta^{(t)}, \nabla_L(\theta^{(t)}, D^{(t)})\] <p>A <strong>model</strong> is a parameterized function that describes how we map inputs to predictions. The parameters are optimized using optimization methods like <strong>SGD</strong>, Newton methods, etc. A <strong>loss function</strong> guides the model to give feedback on how well the model is performing.</p> <p>Having these basic definitions, we will build abstractions to map all the models being used today. It is not possible to build systems to support all models. A quick refresher of important models</p> <ul> <li><strong>CNNs</strong> - Learnable filters to convolute across images to learn spatial features. The top 3 breakthrough architectures were - AlexNet, ResNet, U-Net. What are the important components in CNNs? <ul> <li>Convolution (1D, 2D, 3D)</li> <li>Matmul</li> <li>Softmax</li> <li>Element-wise operations - ReLU, add, sub, pooling, normalization, etc.</li> </ul> </li> <li><strong>Recurrent Neural Networks</strong> - Many problems in nature are many-to-many. RNNs maintain an internal state that is updated as a sequence is processed. Arbitrary inputs and outputs can be generated, and any neural network can be used in the RNN architecture. The top 3 breakthrough architectures were - Bidirectional RNNs, LSTMs, GRU. What are the important components in RNNs? <ul> <li>Matmul</li> <li>Element-wise non-linear - ReLU, Sigmoid, Tanh</li> <li>Underlying MLP RNNs have a problem of forgetting (\(0.9*0.9*… \approx 0\)). Additionally, they lack <strong>parallelizability</strong> - both forward and backward passes have \(O(sequence length)\).</li> </ul> </li> <li><strong>Transformers</strong> (Attention + MLP) - Treat representations of each element in the sequences as queries to access and incorporate information from a set of values. Transformers have an encoder part (BERT most famous) and a decoder part (GPT most famous). Along with these, DiT is one of the top 3 models. What are the important components in Transformers? <ul> <li>Attention - Matmul, softmax, Normalization</li> <li>MLP</li> <li>Layernorm, GeLU, etc.</li> </ul> </li> <li><strong>Mixture of Experts</strong> - Voting from many experts is better than one expert. Latest LLMs are mostly MoEs - Grok, Mixtral, Deepseek-v3. A router (Matmul, softmax) is the novel component in MoE - it makes system design difficult.</li> </ul> <h2 id="machine-learning-systems">Machine Learning Systems</h2> <p>As mentioned before, the three pillars for the systems are data, model and compute. The foal is to express as manny as models as possible using one set of programming interface by connecting math primitives.</p> <h3 id="computational-dataflow-graph">Computational Dataflow Graph</h3> <p>A representation to show data flow in programs. A <strong>node</strong> represents the computation (operator) and an <strong>edge</strong> represents the data dependency (data flowing direction). A node can also represent the input/output tensor of the operator.</p> <h3 id="example-deep-learning-with-tensorflow-v1">Example: Deep learning with TensorFlow v1</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tinyflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span> <span class="c1"># Forward declaration 
</span><span class="n">cross_entropy</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">y</span> <span class="o">*</span><span class="n">tf</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">reduction_indices</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># Loss function declaration 
</span><span class="n">W_grad</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">gradients</span><span class="p">(</span><span class="n">cross_entropy</span><span class="p">,</span> <span class="p">[</span><span class="n">W</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># Automatic differentiation
</span><span class="n">train_step</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">assign</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span><span class="o">*</span><span class="n">W_grad</span><span class="p">)</span> <span class="c1"># SGD update rule 
</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
        <span class="n">sess</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">train_step</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">})</span> <span class="c1"># Real-execution happens here
</span></code></pre></div></div> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17364786998993.jpg" alt=""/></p> <p>This DAG representation opens up all possibilities of optimizations. However, creating such a graph doesn’t allow flexibility - once a graph is defined, it cannot be changed based on the input.</p> <h3 id="example-pytorch">Example: PyTorch</h3> <p>PyTorch also uses computational graphs, but it creates it on the fly. Previously, we had defined the graph and then executed it. Symbolic declaration vs imperative programming. Define-then-run vs Define-and-run. C++ vs Python.</p> <p>What are the pros and cons?</p> <table> <thead> <tr> <th> </th> <th>Good</th> <th>Bad</th> </tr> </thead> <tbody> <tr> <td>Symbolic</td> <td>Easy to optimize, much more efficient (can be 10x faster)</td> <td>The way of programming can be counter-intuitive, hard to debug and less flexible</td> </tr> <tr> <td>Imperative</td> <td>More flexible, easy to program and debug</td> <td>Less efficient and more difficult to optimize</td> </tr> </tbody> </table> <p>How does TensorFlow work in Python then? Tensorflow has Python as the interface language.</p> <p>Apart from these two famous frameworks, there were more like Caffe, DyNet, mxnet (has ability to switch between both), etc. Recently, Jax (derived from Tensorflow) has been getting more popular.</p> <h3 id="just-in-time-jit-compilation">Just-in-time (JIT) compilation</h3> <p>Ideally, we want define-and-run during development and define-then-run during deployment. However do we combine both? PyTorch introduced a deploy mode through a decorator <code class="language-plaintext highlighter-rouge">torch.compile()</code>. So is there an issue with JIT? It creates only static graphs, and cannot work with conditionals or loops in the code.</p> <h3 id="static-vs-dynamic-models">Static vs Dynamic models</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17364797895738.jpg" alt=""/> Static graphs are defined and optimized only once. The execution follows a defined computation. On the other hand, dynamic graphs depend on the input. It is difficult to express complex flow-control logic and debug. The implementation is also difficult.</p> <p>As seen above, LSTMs are trying to replace the dynamics in the natural language problem.</p> <p><strong>How to handle dynamics?</strong></p> <ul> <li>Just do Define-and-run and forget about JIT - most popular unforunately :(</li> <li>Introduce Control Flow Ops - <ul> <li>Example: Switch and Merge. This can be added a computational primitive in the graph and introduce dynamics in the graph.</li> <li>These ideas are natural across all programming languages - conditionals and loops. However, the problem with this approach is that graphs becomes complex, and more importantly, how does we do back propagation? What is the gradient of “switch”? TensorFlow team has been working on this.</li> </ul> </li> <li>Piecewise compilation and guards - This approach is better adopted than control flow. <ul> <li>Case 1: A graph accepting input shapes of \([x, c1, c2]\) where \(x\) is variable. The solution is to compile for different values of \(x\) (powers of 2).</li> </ul> </li> </ul> <p>So far, we have seen representations that express the forward computations using primitives. But, how do we represent backward computations?</p> <h1 id="autodiff-ad">Autodiff (AD)</h1> <p>Derivative can be taken using the first order principles. However, this approach can be slow since we have to evaluate the function twice \(f(\theta + \epsilon) , f(\theta)\) and it is also error prone \(\theta(\epsilon^2)\).</p> <p>To optimize the derivative calculation, we pre store the gradients of primitives and map the derivative chain rules in the computational graph. There are two ways of doing this as well</p> <ol> <li>Calculating the derivative from left (inside) to right (outside) in a network - from inputs to outputs</li> <li>Calculating it from right to left - from outputs to inputs</li> </ol> <p>Both are valid approaches and we will discuss them in detail.</p> <h2 id="forward-mode-autodiff">Forward Mode Autodiff</h2> <p>We start from the input nodes, and derive the gradients all the way to the output nodes. <strong>Cons</strong> - - For \(f: R^n \to R^k\), we need \(n\) forward passes to get the gradients with respect to each input. - However, it is usually the case that \(k = 1\) (loss) and \(n\) is very large.</p> <blockquote> <p>If this is confusing, think of it this way - we want the gradient of output with respect to all parameters to update weights. However, forward mode calculates the gradient of inputs with respect to all parameters.</p> </blockquote> <h2 id="reverse-mode-autodiff">Reverse Mode Autodiff</h2> <p>We define the quantity <em>adjoint</em> \(\bar v_i = \frac{\partial y}{\partial v_i}\). We then compute each \(\bar v_i\) in the reverse topological order of the graph. This way, we can simply do one backward pass to get the necessary gradients.</p> <p>In some scientific scenarios, we can have \(k &gt;&gt; n\) where the forward mode can be more efficient.</p> <blockquote> <p>What are the size bounds of the backward graph as compared to the neural network?</p> </blockquote> <p>We construct backward graphs in a symbolic way to reuse it multiple times.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369099776396.jpg" alt=""/></p> <h2 id="backpropagation-vs-reverse-mode-ad">Backpropagation vs. Reverse-mode AD</h2> <p>In old frameworks like Caffe/cuda-convnet, the backward computations were done through the forward graph itself. Newer frameworks like Tensorflow and PyTorch construct the backward graph explicitly. The reasons to do so are -</p> <ol> <li>Explicit graphs allow backward computation with any input values. They have flexibility to even calculate gradient of gradients.</li> <li>Having an explicit backward graph can help optimization!</li> <li>Gradient update rules can be efficiently implemented.</li> </ol> <h2 id="gradient-update-rules">Gradient update rules</h2> <p>Typically done via gradient descent, the weights are updated with the gradients with the following simplified rule</p> \[f(\theta, \nabla_l) = \theta - \eta \nabla_L\] <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369103443367.jpg" alt=""/></p> <h1 id="architecture-overview-of-ml-systems">Architecture Overview of ML systems</h1> <p>The aim is to make the systems fast, scalable, memory-efficient, run on diverse hardware, energy efficient and easy to program/debug/deploy. Phew.</p> <p>We have discussed dataflow and Autodiff graphs. However, there are numerous things that can be added to these - graph optimization, parallelization, runtime memory, operator optimizations and compilation.</p> <h2 id="graph-optimization">Graph Optimization</h2> <p>The goal is to rewrite the original graph \(G\) as \(G’\) that is faster.</p> <p>Consider the following motivating example - Typically, convolution is followed by batch normalization. Instead of performing batch normalization, just update the weights in convolution to do everything in one step!</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369110087495.jpg" alt=""/></p> <p>Note that some steps can become slower based on the hardware, but you get the general idea.</p> <p>Similarly, in attention calculations, the code is typically written with a concatenated vector of queries, keys and values. This version is optimal - it can be understood with <em>Arithmetic Intensity</em> which the ratio of #operations and #bytes. For example, an addition operation has intensity of \(1/3\) (2 loads and one store). However, fusing multiple arithmetic operations reduces the loads and stores by bringing all variables into memory once, improving the arithmetic intensity.</p> <h3 id="so-how-do-we-optimize-graphs">So how do we optimize graphs?</h3> <p>We write rules or templates for opportunities to simplify graphs. There is also implementation of <em>auto-discovering</em> optimizations in the latest libraries, we shall study these.</p> <h2 id="parallelization">Parallelization</h2> <p>The goal is to parallelize the graph computation over multiple devices. Note that devices can be connected with fast (memory communication NVLink) and slow connections (across GPUs), with up to 10x performance difference. Ideally, we do not want to describe partitioning rules for every new model that comes up. Based on these communication patterns, distributing the tasks is not an easy problem. So, we shall discuss how we partition the computational graph on a device cluster.</p> <h2 id="runtime-and-scheduling">Runtime and Scheduling</h2> <p>How do we schedule the compute, communication and memory in a way that execution is as fast as possible, communication is overlapped with compute and is subject to memory constraints?</p> <h2 id="operator-implementations">Operator Implementations</h2> <p>The goal is this layer is to get the fastest possible implementation of <code class="language-plaintext highlighter-rouge">matmul</code>s, for different hardware, different precision and different shapes.</p> <p>NVIDIA releases a GPU every 2 years, and they have rewrite all operations every time! Notably, previously, models were trained using 32-bit floating points, but now researchers are emphasizing on lower and lower precisions.</p> <p>Now, we shall delve into each of these architectures.</p> <h1 id="operator-optimization-and-compilation">Operator Optimization and Compilation</h1> <p>The goal is maximize arithmetic intensity. In general there are three ways to speed up operators</p> <h3 id="vectorization">Vectorization</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17369120762344.jpg" alt=""/></p> <p>The right version is faster because of the hardware - cache sizes, etc. Tensorflow and PyTorch have this built-in.</p> <h3 id="refactoring-data-layout">Refactoring data layout</h3> <p>This is again related to how data is stored in memory. For example, C++ stores matrices in row-major order. Accessing columns of a matrix can be 10x slower! Remember this while writing code to lower cache misses and reduce pointer movements.</p> <p>ML systems don’t store tensors in row or column major but in a new format called <strong>strides format</strong> - <code class="language-plaintext highlighter-rouge">A[i, j, …] = A.data[offset + i*A.strides[0] + j*A.strides[1] + …</code>. It is a generalization of row and column major storage, and it offers more flexibility - so based on the batch-sizes or other parameters in a neural network.</p> <p>Strides can separate the underlying storage and the view of the tensor. Consider the following operations</p> <ol> <li><code class="language-plaintext highlighter-rouge">slice</code> - simply changing the offsets and shape will output the slice without any copying involved.</li> <li><code class="language-plaintext highlighter-rouge">transpose</code> - modifying strides will transpose the tensor without any copying! For example, consider the following example <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>     <span class="n">M</span><span class="p">.</span><span class="nf">strides</span><span class="p">()</span> <span class="c1"># (24, 12, 4, 1)
</span>     <span class="n">M</span><span class="p">.</span><span class="nf">permute</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
     <span class="n">M</span><span class="p">.</span><span class="n">t</span><span class="p">.</span><span class="nf">strides</span><span class="p">()</span> <span class="c1"># (12, 4, 1, 24)
</span></code></pre></div> </div> </li> <li><code class="language-plaintext highlighter-rouge">broadcast</code> - Suppose we have to extend a tensor’s data across a dimension for performing operations with another tensor, then by simply adding <code class="language-plaintext highlighter-rouge">0</code> stride in the appropriate dimensions would be enough! Again, no copying</li> </ol> <p>Many more operations can be done without copying the data and simply modifying the strides. For example, consider the following example -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17370816868291.jpg" alt=""/></p> <p>However, strides also has an issue - Memory access may become non-contiguous, and many vectorized ops require continuous storage.</p> <h3 id="summary">Summary</h3> <p>To make operators efficient, we have seen the following tactics -</p> <ol> <li>Vectorization - leverage platform-specific vectorized functions that reduce seek time</li> <li>Data layout - strides format that allow zero-copies enabling fast array-manipulations</li> <li>Parallelization on CPUs</li> </ol> <p>These were techniques for general operations. However, we can optimize certain operators with their special properties.</p> <h3 id="matmul-optimization">Matmul optimization</h3> <p>The brute-force approach takes \(\mathcal O(n^3)\). The best approach humans know is \(\mathcal O(n^{2.371552})\)!</p> <p>How to improve the speed in practice then? Recall that we are trying to increase AI = #ops/#bytes.</p> <blockquote> <p><strong>Memory Hierarchy</strong> If everything ran on registers, things would be super-fast. But, that is expensive. Remember that L1-Cache has 0.5ns latency, L2-Cache has 7ns and DRAM has 200ns (400x slower!)</p> </blockquote> <p>Let us analyze the AI of <code class="language-plaintext highlighter-rouge">matmul</code> considering the different layers of memory</p> <ol> <li>We can directly move data to registers in every iteration in inner loop</li> </ol> <h2 id="gpus-and-accelerators">GPUs and accelerators</h2> <p>Recall that parallelizing operations across threads is super useful! CPUs have some level of parallelism through SIMD operations (vectorization) but they are limited. Building on the same idea, GPUs were born.</p> <p>When we started out, the ALU units were limited by the physical space on the chips. As technology improved, we moved from 70nm process all the way 3nm process! That is, we can fit up to 20x more cores in the same area! The majority of the area on CPUs is consumed by Control and Cache, and Jensen thought, ditch those and put cores.</p> <p>Graphical Processing Unit (GPU) are tailored for matrix or tensor operations. The basic idea is to use tons of ALUs (weak but specialized) with massive parallelism (SIMD on steroids).</p> <p>There are other hardware accelerators like Tensor Processing Unit (TPU) or Application specific integrated circuit (ASIC), etc. The common theme across all these is the same - there are specialized cores. What are specialized cores? They can only compute certain computations. Specialized cores can be super powerful - <img src="/assets/img/2025-01-06-data-systems-for-ml/17370849151083.jpg" alt=""/></p> <p>Companies also tried reducing precision and maintain the same performance. Additionally, they also tune the distribution of different components for specific workloads.</p> <blockquote> <p>Why does quantization work in ML systems?</p> </blockquote> <h2 id="recap">Recap</h2> <p>Consider the following question - What is the arithmetic intensity of multiplying two matrices?</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Load A
Load B
C = matmul(A, B)
</code></pre></div></div> <p>Given \(A \in \mathbb{R}^{mxn}, B \in \mathbb{R}^{nxp}, C \in \mathbb{R}^{mxp}\), the number of I/O operations is \(mn + np + mp\), and the number of compute operations is \(2mnp\) since there are approximately \(mnp\) addition and multiplication operations. The arithmetic intensity is then \(\frac{\text{\#compute operations}}{\text{\#I/O operations}} = \frac{2mnp}{mn + np + mp}\). Setting \(m=n=p=2\), results in \(\frac{2x2x2x2}{2x2 + 2x2 + 2x2} = \frac{4}{3}\).</p> <p>\textit{Note.} The addition operation discussed in the previous lecture also has the same I/O operations. However, \texttt{matmul} is a denser operation that results in a higher arithmetic intensity. In practice, it takes the same time to execute matrix addition and multiplication on GPUs, which is why they are so powerful.</p> <p><code class="language-plaintext highlighter-rouge">matmul</code> is an important operation. <Check></Check></p> <p>Now, consider the following operations</p> <ul> <li><code class="language-plaintext highlighter-rouge">broadcast_to</code></li> <li><code class="language-plaintext highlighter-rouge">slice</code></li> <li><code class="language-plaintext highlighter-rouge">reshape</code></li> <li>Permute dimensions</li> <li><code class="language-plaintext highlighter-rouge">transpose</code></li> <li>indexing like <code class="language-plaintext highlighter-rouge">t[:, 1:5]</code></li> </ul> <p>All these operations are optimized due to strided access in tensors. On the other hand <code class="language-plaintext highlighter-rouge">contiguous()</code> cannot take advantage of this.</p> <p>Just to recap, the strides of a tensor of shape <code class="language-plaintext highlighter-rouge">[2, 9, 1]</code> stored in row major order are <code class="language-plaintext highlighter-rouge">[9, 1, 1]</code></p> <p>Consider the cache tiling operation -</p> <ul> <li>It increases the memory allocated on Cache and memory transfers between cache and register</li> <li>It reuses the memory movement between Dram and Cache</li> <li>The arithmetic intensity <em>decreases</em> since there is more load and store</li> </ul> <h1 id="gpu-and-cuda">GPU and CUDA</h1> <p>We have seen that specialized cores offer much better performance over traditional CPUs. Consider the following basic architecture of a GPU</p> <p>Let us see the basic terminology for understanding the architecture -</p> <ul> <li><strong>Threads</strong> - Smallest units to process a chunk of data.</li> <li><strong>Blocks</strong> - A group of threads that share memory. Each block has many threads mapped to a <em>streaming multiprocessor</em> (SM/SMP).</li> <li><strong>Grid</strong> - A collection of blocks that execute the same kernel.</li> <li><strong>Kernel</strong> - CUDA program executed by many CUDA cores in parallel.</li> </ul> <p>A GPU can be made more powerful by</p> <ul> <li>Adding SMs</li> <li>Adding more cores per SM</li> <li>Making the cores more powerful - at a point of <em>diminishing rewards</em>.</li> </ul> <p>NVIDIA, the largest GPU company, has released P100, V100, A100, H100 and B100 (Blackwell) for ML development. K80, P4, T4 and L4 were a lower tier of GPUs. Let us analyze how the compute has changed across these versions</p> <ol> <li>V100 (2019 -) - 80SMs, 2048 threads/SM - $3/hour</li> <li>A100 (2020 -) - 108SMs, 2048 threads/SM - $4/hour</li> <li>H100 (2022 -) - 144SMs, 2048 threads/SM - $12/hour</li> <li>B100 and B200 (2025 -)-</li> </ol> <p>The numbers are not doubling, then how has the performance doubled? They decreased the precisions.. :(</p> <h2 id="cuda">CUDA</h2> <p><strong>What is CUDA?</strong> It is a C-like language to program GPUs, first introduced in 2007 with NVIDIA Tesla architecture. It is designed after the grid/block/thread concepts.</p> <p>CUDA programs contain a hierarchy of threads. Consider the following host code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">const</span> <span class="kt">int</span> <span class="n">Nx</span> <span class="o">=</span> <span class="mi">12</span><span class="p">;</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">Ny</span> <span class="o">=</span> <span class="mi">6</span><span class="p">;</span>

<span class="kt">dim3</span> <span class="nf">threadsPerBlock</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// 12</span>
<span class="kt">dim3</span> <span class="nf">numBlocks</span><span class="p">(</span><span class="n">Ns</span><span class="o">/</span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">x</span> <span class="p">,</span> <span class="n">Ny</span><span class="o">/</span><span class="n">threadsPerBlock</span><span class="p">.</span><span class="n">y</span> <span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c1">// (3, 2, 1) = 6</span>

<span class="c1">// the following call triggers execution of 72 CUDA threads</span>
<span class="n">matrixAddDoubleB</span><span class="o">&lt;&lt;&lt;</span><span class="n">numBlocks</span><span class="p">,</span> <span class="n">threadsPerBlock</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">);</span>
</code></pre></div></div> <p>The GPUs are associated with constants such as</p> <ul> <li><code class="language-plaintext highlighter-rouge">GridDim</code> - dimensions of the grid</li> <li><code class="language-plaintext highlighter-rouge">blocking</code> - the block inter within the grid</li> <li><code class="language-plaintext highlighter-rouge">blockDim</code> - the dimensions of a block</li> <li><code class="language-plaintext highlighter-rouge">threadIdx</code> - the thread index within a block With these in mind, the CUDA kernel for the above code is designed as</li> </ul> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__device__</span> <span class="kt">float</span> <span class="nf">doubleValue</span><span class="p">(</span><span class="kt">float</span> <span class="n">x</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="c1">// kernel definition </span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">matrixAddDoubleB</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">Ny</span><span class="p">][</span><span class="n">Nx</span><span class="p">])</span>
    <span class="p">{</span>
        <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="n">C</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">doubleValue</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
</code></pre></div></div> <p>The host code launched a grid of CUDA blocks, which then call the <code class="language-plaintext highlighter-rouge">matrixAdd</code> kernel. The function definition starts with <code class="language-plaintext highlighter-rouge">__global__</code> which denotes a CUDA kernel function that runs of the GPU. Each thread indexes its data using <code class="language-plaintext highlighter-rouge">blockIdx</code>, <code class="language-plaintext highlighter-rouge">blockDim</code>, <code class="language-plaintext highlighter-rouge">threadIdx</code> and execute the compute. It is the user’s responsibility to ensure that the job is correctly partitioned and the memory is handled correctly.</p> <p>The host code has a serial execution. However, the device code has SIMD parallel execution on the GPUs. When the kernel is launched, the CPU program <em>continues executing</em> without <em>halting</em> while the device code runs on the GPU. Due to this design, it is important that the device code does not have any return values - causes erroneous behavior. To get results from the GPU, <code class="language-plaintext highlighter-rouge">CUDA.synchronize</code> is used (an example will be shown later).</p> <p>It is the developers responsibility to map the data to blocks and threads. The blockDim, shapes etc should be statically declared. This is the reason why compilers like <code class="language-plaintext highlighter-rouge">torch.compile</code> requires static shapes. The CUDA interface provides a CPU/GPU code separation to the users.</p> <p>The SIMD implementation has a constraint for the control flow execution - it requires all ALUs/cores to process in the same pace. In a control flow, not all ALUs may do useful work and it can lead to up to 8 times lower peak performance.</p> <h3 id="coherent-and-divergent-execution">Coherent and Divergent execution</h3> <p>A coherent execution applied the same instructions to all data. Divergent executions do the opposite and they need to be minimized in CUDA programs. This distinction is important to note - even the latest models like the LLMs have this behavior. Concepts such as attention masking and sliding window attention are examples of divergent behavior and they need to be specially implemented to extract the most compute from the GPU.</p> <h2 id="cuda-memory-model">CUDA Memory model</h2> <p>CUDA device (SIMD execution on GPU) has its own memory called the <em>HBM</em>.</p> <p>Unlike host (CPU) memory that is stored as pages in the RAM, GPU memory does not use pages but has memory pools (bulk data) that are accessed all at once.</p> <p>Memory can be allocated <code class="language-plaintext highlighter-rouge">cudaMalloc</code> and populated with <code class="language-plaintext highlighter-rouge">cudaMemcpy</code> like usual. CUDA has a concept called <strong>pinned memory</strong> that is part of the host memory which is optimized for data transfer between CPU/GPU. Ig is not pagable by the OS and is locked, and only certain APIs can access it.</p> <p>Every thread has its own private memory space, and every block has a shared memory that all its threads can access. The HBM is the global device memory in the GPU that can be accessed by all threads. The memory complexity is to balance between speed and shared memory parallelism.</p> <p>For example, consider the program for window averaging -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="mf">3.0</span>
</code></pre></div></div> <p>How can this be parallelized? Since every 3-element tuple reduction is independent, each reduction can be mapped to a CUDA core. So, each thread can compute the result for one element in the output array.</p> <p>The host code -</p> <pre><code class="language-C">int N = 1024*1024;
cudaMalloc(&amp;devInput, sizeof(float)*(N+2)); // To account for edge conditions
cudaMalloc(&amp;devOutput, sizeof(float)*N);

convolve&lt;&lt;&lt;N/THREADS_PER_BLK, THREADS_PER_BLK&gt;&gt;&gt;(N, devInput, devOutput); 
</code></pre> <p>The device code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="cp">#define THREADS_PER_BLK = 128
</span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolve</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
        <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">//thread-local variable</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="mi">2</span><span class="p">];</span>
        <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span><span class="mf">3.</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div> <p>This program can be optimized - each element is read thrice!<br/> Notice that the number of blocks assigned is much more than what a typical GPU has. This is a general practice in CUDA programming where the blocks are <em>oversubscribed</em>.</p> <p>How to optimize? The memory hierarchy can be utilized -</p> <p>The new device code -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>   <span class="cp">#define THREADS_PER_BLK = 128
</span>
    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">convolve</span><span class="p">(</span><span class="kt">int</span> <span class="n">N</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">index</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span> 
        
        <span class="k">__shared__</span> <span class="kt">float</span> <span class="n">support</span><span class="p">[</span><span class="n">THREADS_PER_BLK</span><span class="p">];</span>
        <span class="n">support</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
        <span class="k">if</span><span class="p">(</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">){</span>
            <span class="n">support</span><span class="p">[</span><span class="n">THREADS_PER_BLK</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">input</span><span class="p">[</span><span class="n">index</span> <span class="o">+</span> <span class="n">THREADS_PER_BLK</span><span class="p">];</span>
        <span class="p">}</span>

        <span class="n">__syncthreads</span><span class="p">();</span>

        <span class="kt">float</span> <span class="n">result</span> <span class="o">=</span> <span class="mf">0.0</span><span class="n">f</span><span class="p">;</span> <span class="c1">//thread-local variable</span>
        <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="mi">3</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">+=</span> <span class="n">support</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">i</span><span class="p">];</span>
            
        <span class="n">output</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span><span class="mf">3.</span><span class="n">f</span><span class="p">;</span>
    <span class="p">}</span>
</code></pre></div></div> <p>We introduced a synchronization primitive here. <code class="language-plaintext highlighter-rouge">_syncthreads()</code> waits for all threads in a block to arrive at this point. Another primitive <code class="language-plaintext highlighter-rouge">cudasynchronize()</code> that syncs between host and the device.</p> <h2 id="compilation">Compilation</h2> <p>A CUDA program also needs to be converted to low-level instructions to be executed. A compiled CUDA device binary includes -</p> <ul> <li>Program text (instructions)</li> <li>Information about required resources - 128 threads per block, 8 types of local data per thread and 130 floats (520 bytes) of shared space per thread block.</li> </ul> <p>The issue is that different GPUs have different SMs. If the user asks for a static (large) number of blocks, how to handle this? The first solution is that GPUs have varying (limited) number of blocks.</p> <p>Furthermore, CUDA schedules the threadblocks to many cores using a dynamic scheduling policy that respects the resource requirements. It assumes that the thread blocks can be executed in any order. The blocks are assigned based on the available resources and the remaining ones are <em>queued</em>.</p> <h2 id="understanding-a-gpu">Understanding a GPU</h2> <p>Consider a NVIDIA GTX 980 (2014) that has the following specs -</p> <ul> <li>96KB of shared memory</li> <li>16 SMs</li> <li>2048 threads/SM</li> <li>128 CUDA cores/SM Note that the number of CUDA cores is not equal to the number of CUDA threads.</li> </ul> <p>As the GPUs became better, NVIDIA tried to increase the shared memory per SMM. This is similar to the SRAM which is very important for LLM inference.</p> <h1 id="matmul---case-study"><code class="language-plaintext highlighter-rouge">matmul</code> - Case Study</h1> <p>Remember that over subscribing in GPUs is allowed, and identify that work can be performed in parallel. Developing the thought-process while working with CUDA is important</p> <ul> <li>Oversubscribe to keep the machine busy</li> <li>Balance workload with convergent workflows</li> <li>Minimize communication to reduce I/O</li> </ul> <p>Now, let us consider matrix multiplication. What can be parallelized? In our previous CUDA implementation, we let each thread compute one element in the result matrix. So, each thread has \(2N\) reads, and there are \(N^2\) threads, resulting in \(2N^3\) global memory access.</p> <p>We are not leveraging the fact that one element can be used to calculate many values in the result matrix. The trick is to use the shared memory space - thread tiling (similar to what we did in CPUs).</p> <p>We let each thread compute \(V \times V\) submatrix. The kernel is as follows</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mm</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">float</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">){</span>
        <span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">x</span><span class="p">,</span> <span class="o">:</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">y</span><span class="p">){</span>
            <span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">y</span><span class="p">];</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
                <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">k</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">C</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">ybase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="o">:</span><span class="p">];</span>
<span class="p">}</span>

</code></pre></div></div> <p>For this version, we have reduced the read per threads to \(NV + NV^2\) and number of threads to \(N^2/V^2\) - total reads reduce to \(N^3/V + N^3\) with \(V^2 + 2N\) float storage per thread.</p> <p>We can improve this using partial sum computations. With a small change, we get</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mm</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span> <span class="p">{</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">float</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
        <span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">k</span><span class="p">];</span> <span class="c1">// Grabbing an area</span>
        <span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span><span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">y</span><span class="p">){</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">){</span>
                <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">y</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">C</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">ybase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="o">:</span><span class="p">];</span>
<span class="p">}</span>

</code></pre></div></div> <p>With memory read per thread reduced to \(NV^2\), total memory to \(2N^3/V\) and memory per thread to \(V^2 + 2V\). This version is pretty good for systems with a single layer of memory hierarchy. However, if we have shared memory, it can be made more efficient!</p> <p>Suppose we have an SRAM layer, we can tile hierarchically. Consider the following GPU <code class="language-plaintext highlighter-rouge">matmul</code> v3: SRAM Tiling:</p> <p>The idea is to use block shared memory to let a block compute a \(L \times L\) submatrix and each thread computes a \(V \times V\) submatrix reusing the matrices in the shared block memory.</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">__global__</span> <span class="kt">void</span> <span class="nf">mm</span><span class="p">(</span><span class="kt">float</span> <span class="n">A</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">B</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">],</span> <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">N</span><span class="p">][</span><span class="n">N</span><span class="p">])</span> <span class="p">{</span>
    <span class="k">__shared__</span> <span class="kt">float</span> <span class="n">sA</span><span class="p">[</span><span class="n">S</span><span class="p">][</span><span class="n">L</span><span class="p">],</span> <span class="n">sB</span><span class="p">[</span><span class="n">S</span><span class="p">][</span><span class="n">L</span><span class="p">];</span>
    <span class="n">Float</span> <span class="n">c</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="n">Float</span> <span class="n">a</span><span class="p">[</span><span class="n">V</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">V</span><span class="p">];</span>
    <span class="n">Int</span> <span class="n">y</span> <span class="n">block</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    <span class="n">Iint</span> <span class="n">X</span> <span class="n">block</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
        <span class="kt">int</span> <span class="n">ybase</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
    
    <span class="kt">float</span> <span class="n">C</span><span class="p">[</span><span class="n">V</span><span class="p">][</span><span class="n">V</span><span class="p">]</span> <span class="p">{</span><span class="mi">0</span><span class="p">};</span>
    <span class="kt">float</span> <span class="n">a</span><span class="p">[</span><span class="n">N</span><span class="p">],</span> <span class="n">b</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">k</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="o">++</span><span class="n">k</span><span class="p">){</span>
        <span class="n">a</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">k</span><span class="p">];</span> <span class="c1">// Grabbing an area</span>
        <span class="n">b</span><span class="p">[</span><span class="o">:</span><span class="p">]</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span><span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">];</span>
        <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">y</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">y</span><span class="p">){</span>
            <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">V</span><span class="p">;</span> <span class="o">++</span><span class="n">x</span><span class="p">){</span>
                <span class="n">c</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">y</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">y</span><span class="p">];</span>
            <span class="p">}</span>
        <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">C</span><span class="p">[</span><span class="n">xbase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">xbase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">,</span> <span class="n">ybase</span> <span class="o">*</span> <span class="n">V</span><span class="o">:</span> <span class="n">ybase</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">V</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="o">:</span><span class="p">];</span>
<span class="p">}</span>

</code></pre></div></div> <blockquote> <p>Think about it this way. Initially, we performed tiling across one layer of memory balancing the tradeoffs between I/O reads and memory constraints of the threads. Now, we are adding one more layer of such tiling in a similar manner. The key is to understand the partial sums idea.</p> </blockquote> <p>Note that it is highly unlikely that the threads have a large range of execution times, but we have the <code class="language-plaintext highlighter-rouge">__syncthreads()</code> as a failsafe. The statistics of this algorithm are -</p> <ul> <li>\(2LN\) global memory access per thread block</li> <li>\(N^2/L^2\) threadblocks</li> <li>\(2N^3/L\) global memory access</li> <li>\(2VN\) shared memory access per thread</li> </ul> <p>The key addition here is was the shared memory space. For this algorithm to be efficient, the fetching from the memory has to be implemented <em>cooperatively</em> -</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="kt">int</span> <span class="n">nthreads</span> <span class="o">=</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="kt">int</span> <span class="n">tid</span> <span class="o">=</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    
</code></pre></div></div> <p>These summarize the matrix multiplication codes implemented in the GPUs. Simple, isn’t it? Although, we have not addressed the optimal values for \(L, V\). It depends on the number of threads, registers and amount of SRAM available on the GPU - this process is called <strong>kernel tuning</strong>. There are profilers that optimize these values. It is a difficult problem since large ML models have various operations that need to be optimized together based on their processing and memory requirements. Furthermore, this is different for every GPU and there are hundreds of GPUs!</p> <p>One solution is to do brute-force - hire people and throw money at it. On the other side of things, ML researchers are building <strong>operator compilers</strong> that figure these out automatically.</p> <p>There are other GPU optimizations that are utilized in practice</p> <ul> <li>Global memory continuous read</li> <li>Shared memory bank conflict</li> <li>Pipelining - While some threads are reading, let other threads compute.</li> <li>Tensor core - Dedicated hardware component to accelerate matrix multiplications</li> <li>Lower precisions</li> </ul> <h1 id="ml-compilation">ML compilation</h1> <p>A super-hot topic during the late 2010s since there was a lot of inefficient code that was being identified. The goal is to automatically generate optimal configurations and code given users code and <em>target hardware</em>.</p> <p>Traditional compilers have to simply convert high-level code to binary instructions. The stack for ML compilers is</p> <ol> <li>Dataflow graph generation</li> <li>Optimize graphs - pruning, partitioning, distribution, etc</li> <li>Build efficient kernel code - parameter optimization</li> <li>Machine code (This step is fairly easy and already well implemented)</li> </ol> <p>The big problems in this big process are</p> <ol> <li>Programming level - Automatically transforming arbitrary (usually imperative) code into a compilable code (static dataflow graphs)</li> <li>Graph level - Automatic graph transformations to make it faster (recall how convolution and batch norm can be fused). Graph theory researchers are working on this.</li> <li>Operator level - How to use hardware and optimize standard operators like <code class="language-plaintext highlighter-rouge">matmul</code>.</li> </ol> <p>The big players in this big field are</p> <ol> <li>XLA - First compiler for ML, released along with TensorFlow in 2016 (those researchers aimed big). This turned out to be so good, that the current TensorFlow stack still uses this. Also works for PyTorch, and it is useful to deploy on TPUs.</li> <li>tvm (Tensor Virtual Machine) - It is one of the most successful open-source compiler in academia. They founded OctoML with 200M (got acquired by NVIDIA). There is no backward pass.</li> <li>2.0 - Torch based compiler, that isn’t that great in terms of optimization.</li> <li>Modular - They raised 300M, founded by the same person who created LLVM. The co-founders started swift at Apple! They had big claims - 20x faster than 2.0, not sure how true they are.</li> </ol> <p>You can think of TensorFlow and PyTorch as the front end, and the above mentioned compilers as the backend.</p> <h2 id="operator-compilation">Operator Compilation</h2> <p>Each user-level written code (for standard operations) has a library of low-level program variants, and the compiler chooses the fastest one for the given hardware.</p> <p>For example, consider a loop -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
        <span class="n">C</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
</code></pre></div></div> <p>Get converted to</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for xi in range(4):
    for xo in range(32):   
        C[xo * 4 + xi] A[xo * 4 + xi] + B[xo * 4 + xi]
</code></pre></div></div> <p>Which is then efficiently implemented in the GPU kernels.</p> <p>So, how do we make this happens?</p> <ul> <li>Enumerate all possibilities</li> <li>Enumerate all the (close-to-) optimal values for the hardware - register/cache</li> <li>Apply to all operators and devices</li> </ul> <p>How to search or reduce the search space and generalize?</p> <p>Note that for a certain kind of code and hardware, finding these optimal value <em>once</em> is enough.</p> <h3 id="search-via-learned-cost-model">Search via Learned Cost Model</h3> <p>The famous example is Autotvm -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17376893447233.jpg" alt=""/> The code generator here is done with templates (not LLMs).We need a lot of experts to write the template to define the search space.</p> <p>To search in this parameters space, the compiler does beam search with early pruning. The cost model can be trained on the historical data.</p> <h3 id="high-level-ideas">High-level ideas</h3> <p>We represent the programs in an abstract way and build a search space with a set of transformations (that represent a good coverage of common optimizations like tiling). Then effective search with accurate cost models and transferability have to be deployed.</p> <p>So, how well are we doing in this field? If the compilers were that good, they would’ve discovered flash-attention. Okay, it’s not that bad, compilers have found good optimizations and it just goes to show how difficult this problem is.</p> <h1 id="high-level-dsl-for-cuda-triton">High-level DSL for CUDA: Triton</h1> <p>We have seen a device-specific DSL (domain-specific language). Programmers are able to squeeze the last bits of performance through this. However, it requires deep expertise and the performance optimization is very time-consuming. Maintaining codebases is complex.</p> <p>On the other hand, we have ML compilers. They prototype ideas very quickly (automatically) and the programmer does not have to worry about the low-level details. The problem is representing and searching through the search-space is difficult. Compilers were not able to find Flash-attention because the search-space wasn’t able to represent this possibility. Furthermore, code generation is a difficult problem that relies on heavy use of templates - lots of performance cliffs.</p> <p>So compared to these two extremes, Triton is in between - it is simpler than CUDA and more expressive than graph compilers. It was developed by OpenAI as a solution to the problems with CUDA and compilers.</p> <h3 id="triton-programming-model">Triton Programming Model</h3> <p>The users define tensors in SRAM directly and modify them using torch-like primitives.</p> <ul> <li>Embedded in Python - Kernels are defined in Python using triton.jit</li> <li>Supports pointer arithmetics - Users construct tensors of pointers and can (de)reference them element wise.</li> <li>However, it has shape constraints - must have power-of-two number of elements along each direction</li> </ul> <p>Consider the following example</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="kn">import</span> <span class="n">triton.language</span> <span class="k">as</span> <span class="n">tl</span>
    <span class="kn">import</span> <span class="n">triton</span>
    
    <span class="o">@</span> <span class="n">triton</span><span class="p">.</span><span class="n">jit</span>
    <span class="k">def</span> <span class="nf">__add</span><span class="p">(</span><span class="n">z_ptr</span><span class="p">,</span> <span class="n">x_ptr</span><span class="p">,</span> <span class="n">y_ptr</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
</code></pre></div></div> <p>The triton kernel will be mapped to a single block (SM) of threads. The users are responsible for mapping to multiple blocks. Basically, the language is automating some parts (like compilers), and making the design process simpler for users (as compared to CUDA). These design philosophies are important because they help build newer mental models for users - because they offload some of the cognitive load for optimization, they can think of newer ways of optimizing with these restricted set of parameters. Consider the example of softmax calculation. This function would be slow if implemented using primitives. PyTorch implements an end-to-end kernel for softmax to increase its performance. With triton, we can construct such an end-to-end operation in a simpler manner while achieving slightly higher performance.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">triton.language</span> <span class="k">as</span> <span class="n">tl</span>
<span class="kn">import</span> <span class="n">triton</span>
<span class="nd">@triton.jit</span>
<span class="k">def</span> <span class="nf">_softmax</span><span class="p">(</span><span class="n">z_ptr</span><span class="p">,</span> <span class="n">x_ptr</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">BLOCK</span><span class="p">:</span> <span class="n">tl</span><span class="p">.</span><span class="n">constexpr</span><span class="p">):</span>
    <span class="c1"># Each program instance normalizes a row
</span>    <span class="n">row</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK</span><span class="p">)</span>
    <span class="c1"># Load a row of row-major X to SRAM
</span>    <span class="n">x_ptrs</span> <span class="o">=</span> <span class="n">x_ptr</span> <span class="o">+</span> <span class="n">row</span><span class="o">*</span><span class="n">stride</span> <span class="o">+</span> <span class="n">cols</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">x_ptrs</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">,</span> <span class="n">other</span> <span class="o">=</span>    <span class="nf">float</span><span class="p">(</span><span class="err">‘</span><span class="o">-</span><span class="n">inf</span><span class="err">’</span><span class="p">))</span>
    <span class="c1"># Normalization in SRAM, in FP32    
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">tl</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">tl</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># This is to avoid vert large and small values
</span>    <span class="n">num</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">den</span> <span class="o">=</span> <span class="n">tl</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">num</span> <span class="o">/</span> <span class="n">den</span><span class="p">;</span> 
    <span class="c1"># Write-back to HBM
</span>    <span class="n">tl</span><span class="p">.</span><span class="n">store</span><span class="p">(</span><span class="n">z_ptr</span> <span class="o">+</span> <span class="n">row</span><span class="o">*</span><span class="n">stride</span> <span class="o">+</span> <span class="n">cols</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">cols</span> <span class="o">&lt;</span> <span class="n">N</span>
</code></pre></div></div> <p>Note that Triton achieves good performance with low time investment. However, since it is not as flexible as CUDA, achieving very high-performance is not possible with Triton.</p> <h2 id="recap-1">Recap</h2> <p>A <em>kernel</em> in the GPU is a function that is executed simultaneously by tens of thousands of threads on GPU cores. The shared memory during the GPU execution can be used as a <em>cache</em> that is used by more than one thread, avoiding multiple accesses to the global memory. <em>Over-subscribing</em> the GPU ensures that there are more blocks than SMPs present on the device, helping to hide (tail) latencies by ensuring high occupancy of the GPU.</p> <p>&lt;iNsert a point about GPU memory)</p> <p>Operations such as ReLU, batch normalization and max pooling are not arithmetically dense operations. So typically, operations such as linear layers (and layer normalization with large batches) are limited by arithmetic operations. To calculate which linear layer would have more operations, consider the FLOPs calculation for GEMM.</p> <h1 id="3-graph-optimization">(3) Graph Optimization</h1> <p>Our goal is to rewrite \(G\) as \(G’\) such that \(G’\) runs faster than \(G\) while outputting equivalent results. The straightforward solution is to use a template, wherein human experts write (sub-)graph transformation templates and an algorithm replaces these in the data flow graphs for reduction.</p> <h3 id="graph-optimization-templates-fusion">Graph Optimization Templates: Fusion</h3> <p>Fusing operators reduces I/O and kernel launching (CPU to GPU overhead, all the operations that the SM has to run). The disadvantages of this method is that creating various fused operations is difficult making the codebase unmanageable (e.g., TensorFlow).</p> <p>This also includes <strong>folding constants</strong> in a graph to replace expressions such as <code class="language-plaintext highlighter-rouge">(X + 3) + 4)</code> with <code class="language-plaintext highlighter-rouge">(X + 7)</code>.</p> <h3 id="cuda-graph">CUDA graph</h3> <p>NVIDIA allows users to capture the graph at the CUDA level.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381192844173.jpg" alt=""/></p> <blockquote> <p>Is this define-then-run?</p> </blockquote> <h3 id="standard-compiler-techniques">Standard compiler techniques</h3> <ul> <li>Common subexpression elimination (CSE). The high-level idea is replacing expressions such as <code class="language-plaintext highlighter-rouge">a = b; b = c</code> with <code class="language-plaintext highlighter-rouge">a = c</code></li> <li>Dead Code elimination (DCE). After the CSE hit, we eliminate the dead-code with unused variables.</li> </ul> <p>These both are run iteratively to reach an optimal code. These operations are every useful to eliminate parts of graph based on, say default arguments -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381197256019.jpg" alt=""/></p> <h2 id="how-to-ensure-performance-gain">How to ensure performance gain?</h2> <p>When we greedily apply graph optimizations, we may miss some options that initially decrease the performance but massively increase it later. Furthermore, the same optimizations could lead to an improvement in one hardware and reduction in other. Due to the existence of hundreds of operators (200-300), thousands of graph architectures and tens of hardware backends, it is infeasible to manually design graph optimizations for all cases.</p> <p>There are other issues with template based optimizations</p> <ol> <li>Robustness - Heuristics are not generalizable across architectures and hardware</li> <li>Scalability - New operators and graph structures require newer rules</li> <li>Performance - Misses subtle optimizations specific to DNNs/hardware.</li> </ol> <p>What’s the solution?</p> <h2 id="automate-graph-transformation">Automate Graph Transformation</h2> <p>The main idea is to replace manually-designed graph optimizations with automated generation and verification of graph substitutions for tensor algebra. Basically, generate all possible substitutions and verify if they generate the same output.</p> <p>We start by enumerating all possible graphs up to a fixed size using available operators.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381201066754.jpg" alt=""/> There are up to 66M graphs with 4 operators!</p> <p>Then, with a graph substitution generator, we compute the output with random input tensors. For 4 operators, we can still generate up to 28744 substitutions!</p> <p>These are further pruned based on <em>variable renaming</em> and <em>common subgraphs</em>.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381203016208.jpg" alt=""/></p> <p>These substitutions are formally verified to ensure that they are equivalent mathematically for all inputs. This verification is done by using the properties of operators. For example, convolution with concatenated kernels is same as concatenation of convolutions of the same kernels.</p> <p>So this <em>automated theorem prover</em> can be used to generate valid substitutions scaling up. It takes up to 5 minutes to verify 750 substitutions and there are about 45 rules for the operators which takes about 10 minutes. Adding a new operator is easy - just provide its specifications!</p> <h3 id="incorporating-substitutions">Incorporating substitutions</h3> <p>How do we apply verified substitutions to obtain an optimized graph? The cost is based on the sum of individual operator’s cost and the cost on the target hardware. We greedily apply the substitutions to improve the performance.</p> <p>This approach can be further improved to train a model to learn which kind of substitutions optimize the graph. This was successfully implemented by TASO and it showed good results for real-life models -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17381207164574.jpg" alt=""/></p> <h2 id="summary-1">Summary</h2> <p>In summary for graph optimization,</p> <ol> <li>We first construct a search space</li> <li>Enumerate all possibilities for substitutions</li> <li>Prune the candidates, and select the top ones based on profile/cost model</li> <li>Apply the transformations to iteratively improve the performance.</li> </ol> <p>What could go wrong with this? The search may be slow and the evaluation of various graphs can be expensive.</p> <p>Sometimes the substitutions may only be partially equivalent, but can be orders of magnitude faster. In such cases, we can trade off accuracy for performance. E.g., Convolution vs Diluted convolution.</p> <p>Consider the following example. Suppose we have to use the same kernel to perform convolution on two different tensors. Then, we could concatenate these tensors, apply the convolution, and then apply a correction to achieve the correct result. These transformations use partial equivalent transformations yielding some speed up. These are not explorable in the previous case with fully equivalent operators.</p> <h2 id="partially-equivalent-transformations">Partially Equivalent Transformations</h2> <p>Like the previous example, we <em>mutate</em> the programs and correct them to get an optimized graph.</p> <p>The steps to do this automatically, we do something similar to before</p> <ol> <li>Enumerate all possible programs up to a fixed size using available operators</li> <li>Only consider transformations with equal shapes (in contrast with equal results as compared to before)</li> </ol> <p>With this, all the crux of the algorithm comes to the correction of the mutant programs - how do we detect which part is not equivalent and how to correct it?</p> <p>By enumeration - For each possible input and position, check if the values match. For complete correctness, this search would be \(m \times n\) for \(m\) possible inputs and \(n\) output shape. We reduce the effort by reducing \(m, n\)</p> <ul> <li> <p>Reducing \(n\) - Since neural networks are mostly multi-linear, we can make such assumptions.</p> <p>Theorem: For two multi-linear functions \(f\) and \(g\), if \(f = g\) for \(O(1)\) positions in a region, then \(f = g\) for all positions in the region.</p> <p>As a consequence, the search reduces from \(\mathcal O(mn)\) to \(\mathcal O(mr)\)</p> </li> <li> <p>Reducing \(m\) - Theorem - If \(\exists l, f(l)[p] \neq g(l)[p]\), then the probability that \(f\) and \(g\) give identical results on \(t\) random inputs is \(2^{-31t}\).</p> <p>Using this, we can run \(t\) random tests with random inputs, and if all \(t\) pass then it is very likely that \(f\) and \(g\) are equivalent.</p> </li> </ul> <p>The search space reduces to $\mathcal O(tr)$$. How does this relate to correct?</p> <h1 id="ml-compiler-retrospective">ML Compiler Retrospective</h1> <p>This field started in 2013 with Halide. It was a compiler for rendering, but since the workflow is very similar to neural networks, the later compilers draw motivation from here.</p> <p>Then came XLA in 2016-17, that has good performance but had very difficult to understand code. Companies tried other operations such as TensorRT, cuDNN and ONNX for template based graph substitution. CuDNN is still popularly used but no one understands the code since it was written in a very low level language.</p> <p>Then came <code class="language-plaintext highlighter-rouge">tvm</code> in 2018 that we’ve discussed before. In 2019-20, MLIR and Flexflow were introduced - these are layers in the compiler that provided specific optimizations. Then came 2.0 and Torch Dynamo.</p> <p>However, the community is shifting away from compilers. Why? One part is that many optimizations have been found. The main reason is that we’ve seen a certain class of neural networks architectures that work really well. For example, transformers are all the rage. So instead of focusing on compilers, people can focus on just building fused kernels for the attention mechanisms. That’s how we got flash-attention that no compiler is able to beat.</p> <h1 id="runtime">Runtime</h1> <h2 id="memory-and-scheduling">Memory and Scheduling</h2> <p>Our goal is to fit the workload on limited memory and ensure that the peak memory usage is less than the available memory.</p> <blockquote> <p>Need to add some stuff here.</p> </blockquote> <p>Consider the GPT-3 architecture -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17382918001192.jpg" alt=""/></p> <p>For every model, we check the precision and multiply the number of parameters by 2 or 4 to calculate the total memory consumption.</p> <p>For the main model with 175B parameters, if each parameter is 2 bytes, then we require 350Gb of memory! How did we make this work?</p> <p>Why does this rule of thumb work? Let us check the activation sizes for different layers</p> <ol> <li>For 2D convolution: The input has the size \((bs, nc, wi, hi)\), and the output has \((bs, nc, wo, ho)\). The activation size is \(bs*nc*wo*ho*\text{sizeof(element)}\)</li> <li>For an MLP with input size \((bc, m, n)\) and output size \((bs, m, p)\), the activation size is \(bs*m*p*\text{sizeof(element)}\)</li> <li>For a transformer (ignoring activation layers and other FFLs) - the input size is \((bs, h, seq\_len)\) and the output size is \((bs, h, seq\_len). The activation size is\)bs<em>h</em>seq_len*\text{sizeof(element)}$$</li> </ol> <p>So for GPT-3, the per-layer activation assuming sequence length 1 comes to 78 or 156 Gb. Let us add some more elements to this calculation.</p> <p>The Adam Optimizer estimates the first and second moment vectors with parameters for exponential decays. It also has a step-size or learning rate. The algorithm is given by</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17382924545817.jpg" alt=""/></p> <p>Along with the learning rate, since it also stores the moments, it has to store two more values for each parameter! The memory usage becomes thrice of what it should be!</p> <h3 id="lifetime-of-activations-at-training">Lifetime of activations at training</h3> <p>Because we need to store the intermediate values for the gradient steps, training an \(N\)-layer neural network would require \(O(N)\) memory. This is the main difference between training and inference. In inference, we wouldn’t need to store the parameters at all layers, so we would just need \(O(1)\) memory.</p> <p>So we’ve seen for GPT-3, we require 350 or 700 Gb. So for a sequence length of 96, we would require 7488 or 14976 Gb! These numbers are just crazy! We haven’t even considered the composite layers.</p> <p>Therefore, it is important to take care of memory.</p> <h3 id="single-device-execution">Single Device execution</h3> <p>How do we reduce the memory usage?</p> <p>Idea 1 - the input or the activation is not needed until the backward pass reaches the layer. So, we can discard some of them and recompute the missing intermediate nodes in small segments. This technique is called <em>recomputation, rematerialization, checkpoint activation, etc</em>. It’s essentially the time-space tradeoff.</p> <p>For an \(N\) layer neural network, if we checkpoint every \(K\) layers, then the memory cost reduces to</p> \[\text{Memory cost} = \mathcal O\left(\frac{N}{K}\right) + \mathcal O(K)\] <p>To minimize this, we can pick \(K = \sqrt{N}\). The total recomputation increases by \(N\) - essentially another forward pass. In PyTorch, this feature can be activated using <code class="language-plaintext highlighter-rouge">torch.utils.checkpoint</code>.</p> <p>So when do we use this? When memory is a constraint and time of training is not a concern. The memory usage also depends on the layer being checkpointed - the layers can have different out sizes. In transformers, the layer boundary is typically checkpointed. The disadvantage is that this only works for activations.</p> <blockquote> <p>why?</p> </blockquote> <p>The second idea is <strong>gradient accummulation</strong>. The activation memory is linear to batch size. The idea is to compute the gradient for the batch but will limited memory. We split the original batch into micro-batches and accumulate the gradients at each layer. We then update the weights for the complete batch.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17382936900407.jpg" alt=""/></p> <p>The disadvantage of this strategy is that over-subscribing of GPUs is difficult since we have smaller matrices.</p> <p>An alternative method to save on GPU memory is to use the memory hierarchy. We have <code class="language-plaintext highlighter-rouge">SwapIn</code> (swap from CPU DRAM to HBM) and <code class="language-plaintext highlighter-rouge">SwapOut</code> (swap from HBM to CPU DRAM) that can be applied to both weights and activation. As we do a forward pass, we swap in the next layers and swap out the passed layers. You can be a bit more intelligent about it and pre-fetch the layers based on the computation and swap latencies. This strategy is becoming more practical as more companies are adopting the unified memory architecture. The memory hierarchy seems to be breaking.</p> <p>All these strategies can be used together to probably train GPT-3 on a single device but it would take forever.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387232533206.jpg" alt=""/> Why do we start with gradient accumulation instead of gradient checkpointing? Checkpointing greatly increases the computation time, so we try the other alternatives first.</p> <h2 id="memory-and-compute">Memory and Compute</h2> <h3 id="quantization">Quantization</h3> <p>All our memory usage is a multiple of \(\text{sizeof(element)}\). What if we reduce that parameter?</p> <p>Quantization is the process of constraining an input from a continuous or otherwise large set of value to a discrete set. We use a lower-precision representation for data while preserving ML performance (accuracy), speeding up compute, reducing memory, saving energy, etc. Most of the edge models use quantization.</p> <p>To understand this better, let’s understand the representation of data in memory -</p> <ul> <li>An unsigned integer has the range \([0, 2^n - 1]\)</li> <li>A signed integer with \(n\)-bit has the range \([-2^{n-1} - 1, 2^{n - 1} - 1]\). To avoid saving 0 twice by storing a sign bit, computer architects decided to use <em>Two’s complement representation</em>.</li> <li>Fixed point number - An arbitrary bit is chosen as the boundary for the integer and the decimal. This representation is mainly used in security applications now.</li> <li> <p>Floating point representation - We use a sign bit, 8-bit exponent and 23 bit fraction. That is the value is, \((-1)^{sign} \times (1 + \text{ fraction}) \times 2^{\text{exponent} - 127}\).</p> <p>How do we represent 0 then? Representation-wise, we technically cannot represent 0, so we make a special representation - <em>normal vs subnormal values</em>. Whenever the exponent bits are zero, we remove the bias term \(1\) that is added to the fraction, and represent the value as \((-1)^{sign} \times \text{ fraction} \times 2^{\text{exponent} - 127}\). This expressions is only used with the exponent is \(0\). This way, we also extend the range of the representation and the smallest positive number we can represent is \(2^{-149}\).</p> <p>How about special values? Exponent with all set bits is infinity and sign is decided by the sign bit. NaN is represented in the subnormal range with exponent bits set to 1. In summary, we have</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387239819544.jpg" alt=""/></p> </li> </ul> <blockquote> <p>Calculate model sizes from parameters table.</p> </blockquote> <p>Notice that the precision of floating point numbers is much higher when the values themselves are small. This is a tradeoff we make based on the applications. Here is a summary of other floating point representations -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387241256605.jpg" alt=""/></p> <p>The BF16 system has been observed to provide much better performance over FP16 for neural network training. The representation lowers the precision for a higher range. It could be that the higher range can stabilize the training during exploding or vanishing gradients. During inference, the precision matters more so FP16 might perform better in some cases.</p> <p>For practice, consider the following examples</p> <ul> <li>The FP16 bit set <code class="language-plaintext highlighter-rouge">1 10001 1100000000</code> represents -7.1. Why? The bias in the exponent is always the median value subtracted by 1. Here it is \(2^4 - 1 = 15\). The exponent is then \(17 - 15 = 2\), and the fraction is \(0.5 + 0.25 = 0.75\)</li> <li>The decimal 2.5 is represented as <code class="language-plaintext highlighter-rouge">0 10000000 0100000</code>. The bias is \(2^7 - 1 = 127\)</li> </ul> <p>After these representations, newer ones came up to improve the performance in deep-learning. <img src="/assets/img/2025-01-06-data-systems-for-ml/17387248528126.jpg" alt=""/> The rule of thumb is that we require higher range for training and higher precision for inference.</p> <p>As if this were not enough, for even lower compute, NVIDIA has been pushing for INT4 and FP4 to keep up with the Moore’s law.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387250679255.jpg" alt=""/></p> <p>There has been no successful model with these representations, making it a very promising avenue for research.</p> <p>Alright, with these things in mind, let us come back to quantization. There have been two approaches for quantization</p> <ol> <li> <p><strong>K-Means based quantization</strong> - widely used, almost all mobile devices have it.</p> <p>Consider we have a weight matrix that we are trying to quantize. The idea is to cluster close values together into an integer or a lower-precision representation. The number of clusters is chosen based on the chosen requirements - for 2-bit quantization, number of clusters is 4.<br/> To do so, we simply apply K-means. The centroids are stored in a code book whereas the matrix is simply stored in the lowest possible representation to further reduce storage. <img src="/assets/img/2025-01-06-data-systems-for-ml/17387254097673.jpg" alt=""/> How do we perform a backward pass on these matrices? The gradients are accumulated based on the classes and are applied to the centroids to fine-tune them. In practice, quantization starts affecting the performance sharply only after a certain threshold. Therefore, quantization becomes a hyper-parameter tuning problem, and we can achieve significantly lower memory consumption. The number of bits used can vary with layers as well!</p> <blockquote> <p>Try coding a library with variable quantization layers. Shared code books across layers</p> </blockquote> <p>How is the run-time affected? The computations are still FP arithmetic. There is an added computation cost for weight compression/decompression and code book lookup. K-means has been quite effective with convolution networks.</p> </li> <li> <p><strong>Linear quantization</strong> - The idea is to determine a linear mapping of integers to real numbers. It can be seen as a linear optimization problem.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17387262709675.jpg" alt=""/></p> <p>So, we need to determine zero-point and scale parameters. These parameters are often also publicly shared on platforms such as HuggingFace. The parameters can be chosen for the whole model or for each layer based on our performance tradeoff appetite. For many popular models like Llama, the quantization is done tensor-wise.</p> <p>The parameters can easily be determined as follows</p> \[\begin{align*} S &amp;= \frac{r_{max} - r_{min}}{q_{max} - q_{min}} \\ Z &amp;= \text{round}(q_{min} - \frac{r_{min}}{S}) \end{align*}\] <p>The bit-width \(n\) determines \(q_{min} = -2^{n -1}\) and \(q_{max} = 2^{n - 1} - 1\).</p> <p>Suppose we apply linear quantization to <code class="language-plaintext highlighter-rouge">matmul</code> -</p> \[\begin{align*} Y &amp;= WX \\ S_Y(q_Y - Z_Y) &amp;= S_W(q_W - Z_W)S_X(q_X - Z_X) \\ Q_Y = \frac{S_W S_X}{S_Y} \left( q_W q_X - Z_W q_X - \underset{Z_X q_W + Z_W Z_X) + Z_Y}{\text{precomputed for inference}} \right) \end{align*}\] <p>Empirically, the factor \(\frac{S_W S_X}{S_Y}\) is between 0 and 1. Instead of using floating point multiplications, it is represented as fixed point multiplication and bit shift. Also, empirically \(Z_W\) follows normal distribution and can be approximated as 0. Thus, the heavy lifting operation is \(q_Wq_X\) which is simply integer multiplication.</p> <p>Therefore, we reduced both storage and computation time (integer arithmetic is much faster and cheaper). This can also be used to reduce FP16 to FP8 rather than integers.</p> </li> </ol> <p>In summary, we have <img src="/assets/img/2025-01-06-data-systems-for-ml/17387273312564.jpg" alt=""/></p> <h1 id="large-language-models-on-cloud-and-edge">Large Language Models on Cloud and Edge</h1> <p>The first wave of revolution in ML systems was sparked by Big Data in 2010s (competitions like Netflix recommendation price), which resulted in systems such as XGBoost, Spark and GraphLab. Then in mid 2010s, deep-learning started gaining traction, and TensorFlow, TVM and Torch were created. Now, we’re in the third revolution with Generative AI. ML Systems is playing a much bigger role.</p> <p>The challenges involved are -</p> <ol> <li> <p>Memory - Llama-70B consumes 320GB VRAM just to store parameters in FP32</p> </li> <li> <p>Compute - The post-Moore era brings great demand for diverse specialized compute, system support becomes bottleneck</p> </li> </ol> <p>The design of systems depends on the paradigm that the industry is moving towards. Currently, we have cloud based models with a client-server architectures. This is how computers initially started before the age of personal computers. So, would we move towards personal AI in consumer devices?</p> <p>There are many engineering challenges involved in this. As we covered previously, there are specialized libraries and systems for each backend involving manually created optimizations. The area is very labor intensive with huge market-opportunities.</p> <p>Our approach to this has been in terms of composable optimizations to rewrite kernel codes. Furthermore, we added techniques such as parameter sharding, memory planning, operator fusion, etc to add to these optimizations. What have we learned from this journey?</p> <p>There are four abstractions we use</p> <ul> <li> <p>Computational Graphs - Graph and its extensions enable higher level program rewriting and optimization</p> </li> <li> <p>Tensor Programs - These abstractions focus on loop and layout transformation for fused operators</p> </li> <li> <p>Libraries and Runtimes - Optimizing libraries are built by vendors and engineers to accelerate key operators of interests</p> </li> <li> <p>Hardware Primitives - The hardware builders exposes novel primitives to provide native hardware acceleration</p> </li> </ul> <p>It has not been about a <strong>silver bullet system but continuous improvement and innovations</strong>. ML Engineering is going hand-in-hand with ML modeling.</p> <p>The developers of TVM are expanding it to TVMUnity to bring the compiler flow to the user. As we’ve studied, IRModule is the central abstraction in TVM. Once the user-level code is written in this form, TVM takes care of the hardware backend making it easy to the models on various architectures.</p> <p>TVM generates an optimized models, and features are continuously added with data to make it better over time.</p> <h2 id="tvm-unity">TVM Unity</h2> <h3 id="first-class-symbolic-shape-support">First-class symbolic shape support</h3> <p>Traditional models like ResNet have some key-characteristics. The compilers are being built under the assumption of these fixed parameters, but with the age of generative AI, these parameters keep changing continuously. TVMUnity leverages symbolic shift to incorporate variable sizes in the model. In essence, traditional compilers are unable to handle dynamic shapes, whereas TVMUnity allows it with symbolic support.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-47-07-image.png" alt=""/></p> <p>Knowing the dependencies between different variable parameters allows for better optimizations during runtime - static memory planning for dynamic shapes</p> <h3 id="composable-tensor-optimization">Composable Tensor Optimization</h3> <p>In the early ages, we had scalar computing. Then came the age of vector computing with SIMD. Now, NVIDIA has TensorCore and TPUs became a thing for tensor computing. How do we leverage these hardware developments in our programs? We want both loop based optimization and tensor-based programs.</p> <p>The first step is to isolate the internal computation tensorized computation from external loops to create a <code class="language-plaintext highlighter-rouge">Block</code>.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-52-50-image.png" alt=""/></p> <p>With this, TVMUnity does <em>Imperative Schedule Transformation</em> to search different variants of programs by changing blocks to create an optimized IR.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-53-13-image.png" alt=""/></p> <p>By providing this interface to the user, where the user can mention the parameters involved, TVMUnity can perform tensorization along with graph-optimization problem.</p> <h3 id="bringing-compilation-and-libraries-together">Bringing compilation and Libraries Together</h3> <p>We have seen this tradeoff with compilers and libraries.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-02-06-18-57-28-image.png" alt=""/></p> <p>TVMUnity provides interfaces like <code class="language-plaintext highlighter-rouge">Relax-BYOC</code> to offload the computational load to libraries such as TensoIR to leverage the library-optimized kernels. The native compilation works on top of the library offloading to increase the performance even more.</p> <p>By adding this flexibility, the compiler can update (live with library updates) to squeeze the best performance.</p> <p>Since libraries come out with data layout requirements, the compiler can use this information to optimize the remaining parts of the code.</p> <h2 id="ml-compilation-in-action">ML Compilation in Action</h2> <p>How does this architecture help with incremental developments? The users can try new optimizations in coordination with the compiler to scale to newer models.</p> <p>In language models, the compiler does targeted optimizations such a low-bit quantizations, dynamic shape planning, fusion and hardware aware optimizations, etc. Along with these, it works with KVCache etc to improve the performance.</p> <p>The <strong>MLCEngine</strong> is a universal LLM deployment engine being developed to deploy LLMs on any device. The key contribution here is <em>universal</em>. This development also revealed some key insights - iOS devices require custom Metal language kernels, Snapdragon devices require OpenCL - and this project is trying to take care of all of that.</p> <p>It also helps with structured generation with near zero overhead. This feature would be super useful for agent use cases. <a href="https://huggingface.co/spaces/mlc-ai/WebLLM-Structured-Generation-Playground">Surprisingly, MLCEngine does it with near zero overhead with something known as XGrammar!</a></p> <p>They also created <a href="https://webllm.mlc.ai/">WebLLM</a> with the new WebGPU standard to use the local compute to the browser. Yes, it uses your local GPUs and doesn’t send your data to any server! You can use this to build stuff on top of it with the typescript API and a Node package. It is an open source project too!</p> <p>Let us continue our discussion on Quantization. We discussed about quantization granularity (per-tensor, per-channel, group quantizations).</p> <p>Per-channel quantization is preferred to tensor quantization in some cases because the channels can have very different ranges, and using a single \(S\) can result in lopsided representations. Even then, some applications can lose too much on performance for per-channel quantization. In these cases, group quantization is useful.</p> <p>Group quantization is more fine-grained. E.g., per vector. It has more accuracy, less quantization error trading off some of the savings.</p> <blockquote> <p>Can we do some sort of statistical grouping here?</p> </blockquote> <p>The sweet-spot that works in practice is a two-level quantization that quantizes hierarchically. Instead of using \(r = S(q - Z)\) we use \(r = \gamma S_q (q = Z)\). \(\gamma\) is a gloating-point coarse grained scale factor, and \(S_Q\) is an integer per-vector scale factor. It can be further generalized into multi-level quantization with scale factors for each levels.</p> <p>So far, we have done quantization of weights. They are static during inference, and so everything works. What if we want to quantize the activations? The stats change with every input. This problem is solved in two ways:</p> <ol> <li>Moving average - Observed ranges are smoothed across thousands of training steps</li> <li>Calibration dataset - Figure out the range from subset of training set</li> </ol> <h2 id="mixed-precision">Mixed Precision</h2> <p>In the previous methods, we have done <em>uniform quantization</em> wherein every weight parameter is quantized to the same number of bits. However, what if we use different precision for different weights? Does this improve the performance a lot? The implementation is however very complicated since we have to deal with different data formats. Also, what formats give the best savings and high performance?</p> <p>These are hard questions. So we throw ML at the problem. We define a cost function to let a model discover the combination of formats that give the best parameters. It was a good research field, and the accuracy of models improved by 10% or so.</p> <p>After a while, <a href="https://arxiv.org/pdf/1710.03740">NVIDIA released a paper</a> (it’s a good read) that became the standard for mixed precision training. The intuition for this approach is as follows. Some layers are more sensitive to dynamic range and precision. For example, softmax and normalization layers have a large range of values. We identify such operations and assign them higher precision.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393289702270.jpg" alt=""/></p> <p>Since Adam calculates two moments and normalizes the gradients, we use FP32 for the weight updates.</p> <p><em>Note.</em> Deepseek changed the standard with new precisions.</p> <p>Let us see how the memory of models changes with this new precision system. Again, for the largest GPT, there are 175B parameters. So it occupies 350G with FP16 for all the weights. The activations occupy 7488G assuming checkpointing at each layer boundary.</p> <p>So in this system, the master model with FP32 weights occupies 4<em>175 = 700G. The gradients occupy 2</em>175 = 350G. The running copy of the model used for inference is 2<em>175 = 350G. Finally, we need Adam mean and variance (FP32) that is 2</em>4*175 = 1400G. The rule of thumb in general is \((4 + 2 + 2 + 4 + 4)N = 16N\) memory for LLMs.</p> <h3 id="scaling-down-ml">Scaling down ML</h3> <p>Running ML on edge devices is always strongly demanded, and the market is very fragmented. It is easy to build a startup and get acquired in this space. The possible research directions are quantization, pruning, ML energy efficiency, federated ML, etc.</p> <h1 id="parallelization-1">Parallelization</h1> <p>Moore’s law came to an end. However, ML models were scaling 3x every 18 months! Why? Bigger model gives better accuracy. People have also started seeing emergent capabilities in larger models.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393299103392.gif" alt=""/></p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393300313845.jpg" alt=""/></p> <p>So, the models are going to get bigger. Along with this, the memory demand is increasing too. It was back in 2019 when we last fit an entire model in one GPU. Now we require 100s of GPUs just to store the model parameters. The only way out of this is parallelization. Wait! Aren’t GPUs already doing that?</p> <blockquote> <p>GPUs did data parallelization. We are talking about having multiple GPUs and using them together. All the things we considered so far assumed the entire model is on one GPU. Now, we need to distribute the model training. We just seem to be creating problems for ourselves…</p> </blockquote> <p>Intuitively there are multiple ways we can go about this</p> <ol> <li>Parallelize along the layers (cutting through depth)</li> <li>Parallelize each layer (cutting through breadth) - this one is rather complicated with more data traveling between clusters.</li> </ol> <p>Apart from these considerations, a GPU cluster also has its own constraints. There are different latency communication channels (kind of like memory hierarchy).</p> <p>Let us look at the problem from a computational lens. A model involves parameters, weight updates, model spec and the data.</p> <ul> <li>Computing - The forward pass and backward pass require compute</li> <li>Memory - The data and parameters require memory. Between these, we require communication (typically done with interconnects or network, e.g., NVLink) which is the main bottleneck in this whole setup. How do we communicate parameters and activations?</li> </ul> <p>In <strong>data parallelism</strong>, we partition the data across GPUs and give each GPU a copy of the parameters and gradients. Then, we need to synchronize the updates together across the GPUs before the next iteration.</p> <p>In <strong>model parallelism</strong>, we partition the model across GPUs and use the data to update parts of the model. This method is more complicated. Let us delve deeper.</p> <p>How do we partition a computation graph on a device cluster?</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393313839159.jpg" alt=""/></p> <p>There are more strategies that consider hybrid variants - some parts of the model are inter-op, intra-op and some parameters are replicated across devices.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17393315930822.jpg" alt=""/></p> <p>These are the standard techniques being used for the models today.</p> <p>Let us delve deeper into these. <img src="/assets/img/2025-01-06-data-systems-for-ml/17393318019536.jpg" alt=""/> In the above example, intra-op is similar to what we discussed before for matmul on GPUs - each GPU computes a partial sum. Surprisingly, we can show mathematically that this is the best we can do with inter-op.</p> <p>What are the pros and cons?</p> <ul> <li>Inter-op parallelism requires point-to-point communication but results in devices being idle.</li> <li>Intra-op parallelism keeps the devices busy but requires collective communication.</li> </ul> <p>The all-reduce operation is computationally intensive.</p> <p>Since intra-op requires more communication, this is an important aspect to consider. On the other hand, inter-op results in hardware bubble wherein some of the GPUs are idle (can be prevented to some extent with pipelining).</p> <p>In summary, inter-op parallelism requires point to point communication but results in idle devices. The devices are busy in intra-op parallelism but requires collective communication.</p> <blockquote> <p><strong>A note on terminology</strong> Previously, the literature used to talk about data and model parallelism. Data parallelism is general and precise but the term “model parallelism” is vague. That is why, we stick to the terms inter-op and intra-op parallelism which rely on the pillars of computational graph and device cluster. Furthermore, we are mainly discussing about model parallelism because of the regime in which models do not fit on a single GPU anymore.</p> </blockquote> <p>With that, let us formulate our goal - “What is the most efficient way to execute a graph using combinations of inter-op and intra-op parallelism subject to memory and communication constraints?”</p> <p>Let us equip ourselves with a metric to quantify this goal. Previously we had, Arithmetic Intensity. Now, we consider <strong>Model FLOPs Utilization (MFU)</strong></p> \[MFU = \#FLOPs/t/\text{peak FLOPs}\] <p>Where #FLOPs is the total FLOPs in the ML program, and \(t\) is the time required to finish the program. The goal is to maximize this quantity.</p> <p>Why don’t we achieve the peak FLOPs? The Peak FLOPs values are typically achieved by matrix multiplications but a neural network model like a transformer involves many layer operations (memory bounded) that lower the performance. Moreover, we have seen how optimization can greatly reduce the time of execution and increase the operations within the same time. Unoptimized code can lower the value further. Adding to these communication like we have discussed before can reduce the quantity a lot too. Finally, all these parameters also depend on the precision, code and the GPU type being used.</p> <p>How do we calculate the MFU?</p> <ol> <li>Count the number of operations (FLOPs) considering the shape of the data in the model. For example, we have seen the formula \(2mnp\) for <code class="language-plaintext highlighter-rouge">matmul</code>.</li> <li>Run the model for one forward and backward pass on the GPU to get benchmark for the running time \(t\)</li> <li>Check the GPU spec, type of cores, and their peak FLOPs</li> <li>Calculate the MFU</li> </ol> <p>What is the history of these numbers?</p> <ol> <li>The best ML system on V100 a couple years ago got 40-50% MFU - only half the peak utilization! The peak FLOPs on V100 is 112 TFLOPs, so we were only able to use 50-60 FLOPs</li> <li>With A100, we were still in the same range until FlashAttention came up, which took the MFU value to 60%! A100 has 312 FLOPs, so we are using ~160FLOPs at this stage!</li> <li>With H100, we are able to use only 30-50% depending on the model size (Larger <code class="language-plaintext highlighter-rouge">matmul</code> is better for MFU over smaller). Why did it decrease? The peak value of H100 is very high (990 TFLOPs), and our software did not catch up to this. Remember that communication also plays an important role</li> <li>This year, with B100 the peak FLOPs is 1.8 PFLOPs!</li> </ol> <p>Besides MFU, we also define <strong>Hardware FLOPs Utilization (HFU)</strong>. This quantity is to consider operations that do not contribute to the model For example, we can treat gradient checkpointing as 2 forward passes and 1 backward pass (each backward pass can be approximated as 2 forward passes due to gradient updates)</p> <h2 id="collective-communication">Collective Communication</h2> <p>In Machine Learning systems, there are usually two types of connections</p> <ol> <li>Point-to-point communication - Comprises of a sender and a receiver. Very simple.</li> <li>Collective Communication - It’s a common concept in HPC (high performance computing). There are multiple forms of this <ol> <li>Broadcast - One worker shares data with all the other workers</li> <li>Reduce(-to-one) - All the data among the workers is combined into one rank (one worker). Essentially reverse of broadcast in a way.</li> <li>Scatter - The data from one rank is split and each of the splits is <em>scattered</em> across other ranks. That is, every worker gets on part of the data.</li> <li>Gather - The reverse operation of Scatter, where different parts of the data are <em>gathered</em> into one rank.</li> <li>All-gather - Essentially gather followed by broadcast.</li> <li>Reduce-scatter - Essentially reduce followed by scatter.</li> <li>All-reduce - Reduce followed by scatter.</li> </ol> <p>Collective communication is more expensive than P2P since it can be thought of as a combination of many P2Ps. However, collective communication has been highly optimized in the past 2 decades (<code class="language-plaintext highlighter-rouge">(x)ccl</code> libraries - NVIDIA has <code class="language-plaintext highlighter-rouge">NCCL</code> the best, Microsoft has <code class="language-plaintext highlighter-rouge">MCCL</code> and Intel has <code class="language-plaintext highlighter-rouge">OneCCL</code>). The important thing to note is that collective communication is not fault tolerant (if one device fails, then everything fails).</p> </li> </ol> <h3 id="basics">Basics</h3> <p>Let us understand some more terminology for communication models. There is a terminology called as \(\alpha\beta\) model that talks about latency and bandwidth. For example, if the model is \(\alpha + n \beta\) then \(\alpha\) refers to the latency and \(\beta = 1/B\) is the reciprocal of bandwidth determined by the hardware. For smaller messages, latency (\(\alpha\)) is the dominant factor, and for larger messages (larger \(n\)), the bandwidth utilization (\(n\beta\)) dominates.</p> <p>Based on these distinctions, the community works on two mainstream algorithms/implementations</p> <ol> <li>For smaller messages, an MST based algorithm that emphasizes low latency</li> <li>For larger messages, a <strong>Ring algorithm</strong> to emphasize on bandwidth utilization.</li> </ol> <p>There are over 50+ algorithms in this area and the HPC community even got the 2023 Turing award!</p> <p>The core principle for lower latency is to minimize the number of rounds needed for communication. For example, consider the Broadcast operation.</p> <ol> <li>We first split the ranks into half, and send the message to the half without the message</li> <li>Then repeat broadcast recursively in each half</li> </ol> <p><em>Beautiful.</em></p> <p>For the core operations, we have the following communication models</p> <ul> <li>Reduce-to-one - \(\log(p) (\alpha + n \beta + n \gamma)\)</li> <li>Scatter and Gather- \(\log(p)\alpha + \frac{p - 1}{p} n\beta\)</li> <li>Broadcast - \(\log(p)(\alpha + n \beta)\)</li> </ul> <p>The remaining composite operations can simply use these primitives.</p> <p>What are the problems with this approach? Since latency is prioritized over bandwidth, some links are idle. Building on the same idea, the core principle for high-bandwidth is to use all links between every two nodes.</p> <p>The <strong>ring algorithm</strong> essentially is a logical ring that can be embedded in a physical linear array with worm-hole routing such that the “wrap-around” message does not conflict. Look at the diagram below for clarity -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17395038648341.jpg" alt=""/> At every instant, all the links are used ensuring full-bandwidth. For example, reduce-scatter can be implemented as consecutive reduce and propagate ring passes.</p> <p>For this regime of communication, the core primitives of communication are</p> <ul> <li>Reduce-scatter</li> <li>Scatter</li> <li>Gather</li> <li>All-gather and the other operations can be implemented as the composites of these</li> <li>Reduce(-to-one) is reduce-scatter followed by gather</li> <li>All-reduce is reduce-scatter followed by all-gather</li> <li>Broadcast is scatter followed by all-gather.</li> </ul> <p>So, how does this all come back to ML? ML systems are usually composed of large data communications. Inter-op systems always result in P2P communications and intra-op result in collective communication.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17395041727515.jpg" alt=""/></p> <p>The key point to note is that communication always happens between nodes that are partitioned differently. Remember that with tensors there are more dimensions, and the above diagram is a simplified version. Just to summarize all ideas and the motivations behind our discussions, let us go through this again.</p> <h2 id="data-parallelism">Data Parallelism</h2> <p>Data parallelism was first suggested by Dean et. al. In DistBelief based on a Parameter server. This method was mainly based on sharding the data through batches. Then, this paradigm evolved, and integrated into PyTorch as <code class="language-plaintext highlighter-rouge">DDP</code>.</p> <p>When using data parallelism, there are two solutions to update the model parameters (assuming the models can fit inside one GPU worker)</p> <h3 id="parameter-server">Parameter server</h3> <p>As proposed in the original DistBelief paper, the parameter server is to help with gradient descent (not stochastic). The gradients are accumulated across the workers (have to wait for all workers) and are used to update the parameters. These parameters have to be replicated across all workers for further forward computations (server bottleneck). The compute to communication ratio for this is 1:10 in 2021.</p> <p>This approach can be improved to some extent by using a distributed parameter server (sharded KV stores). This ensures a redundancy across different PS shards. The savings essentially comes from using AllReduce - no server bottleneck and faster gradient sharing by workers. When the parameter server nodes are same as the worker nodes in the previous setup, the configuration simplifies a lot more. The operations then become Reduce-scatter followed by Allgather.</p> <p>Even then, one slow worker can slow down the whole system.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17399335217090.jpg" alt=""/> The problem is that we have been waiting for all workers to update the parameters. However, machine learning systems are error-tolerant to some extent. Deviating from the gradient can still lead to the same performance (similar to why stochastic gradient descent works).</p> <p>So adopting a form of asynchronous communication wherein the parameters are updated every time a worker finished the computation. We can set a staleness threshold to discard very old messages for improved performance. It turns out that the training stability is heavily dependent on this parameter. There were a lot of theorems for this line of work too!</p> <h3 id="all-reduce">All-Reduce</h3> <p><code class="language-plaintext highlighter-rouge">DDP</code> is a form of Allreduce type of data parallelism. Allreduce was initially implemented in Horovod (was designed by Uber!). It was further optimized by NVIDIA in NCCL. It was then adopted in PyTorch as DDP.</p> <p>Also, Allreduce is less fault-tolerant than parameter server! However, this paradigm is more popular since ML systems are somewhat robust to noisy updates. Allreduce does not have consistency mechanisms too!</p> <p>It became popular due to its simplicity as also because of the release of NVLink - more helpful for Allreduce.</p> <h2 id="model-parallelism">Model Parallelism</h2> <p>As we’ve discussed model parallelism is usually paired with data parallelism (once models stopped fitting in one worker). There are two kinds of sharding - inter-op and intra-op.</p> <h3 id="inter-operator-parallelism">Inter-operator Parallelism</h3> <p>Inter-op parallelism usually has smaller communication requirements since only outputs at stage boundaries have to be transferred. However, unless pipelining, there are many hardware bubbles. Without pipelining, we have a bubble percentage of \((D - 1)/D\) and with pipelining we have \((D - 1)/(D - 1 + N)\) where \(D\) is the number of devices and \(N\) is the number of inputs.</p> <p>Note that we are ignoring the computation models for starting GPU kernels and other overheads for simpler models.</p> <p>However, this is mainly for inference. For doing backward pass, the devices need to be executed in reverse! Pipelining won’t do. So, how do we reduce the bubbles for training?</p> <ol> <li> <p>Device Placement - Slice the branches of a neural network into multiple stages so they can be calculated concurrently.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17399350251810.jpg" alt=""/> This only works for models with branches such as the Inception model. Contrastive models also include branches and can be used. Other convolutional networks and transformers cannot be used with this.</p> </li> <li>Synchronous Pipeline Parallel Schedule - Modify pipeline schedule to improve efficiency, but keep the computation and convergence semantics exactly the same as if training with a single device. The advantage is that the training process is exactly the same. However, pipeline bubbles are always present and reducing pipeline bubbles typically requires splitting inputs into smaller components, and smaller inputs may not use the hardware to the fullest extent. <ol> <li><strong>GPipe</strong> (Google) - Partition the input batch into multiple “micro-batches”, and accumulate the gradients of the micro-batches. <img src="/assets/img/2025-01-06-data-systems-for-ml/17399351787754.jpg" alt=""/> The size of the microbatch has to be chosen so that each device is utilized to the full extent to maintain high arithmetic intensity. This choice is hard to consider. Due to the structure of GPipe, the memory usage of each device increases linearly to a peak and reduces. Each device has to store the outputs from the micro-batches until the backpropagation is called.</li> <li><strong>1F1B</strong> - To prevent this memory peak, the 1 forward, 1 backward schedule was introduced. The idea is to perform backward pass as early as possible. <img src="/assets/img/2025-01-06-data-systems-for-ml/17399357286232.jpg" alt=""/> It maintains the same latency as GPipe but with a lower memory usage. (Exam)</li> <li><strong>Interleaved 1F1B</strong> - The previous idea can be optimized further to slice the neural network into more fine-grained stages and assign multiple stages to reduce pipeline bubbles. <img src="/assets/img/2025-01-06-data-systems-for-ml/17399360266930.jpg" alt=""/> This is super hard to debug.</li> <li> <p><strong>TeraPipe</strong> - This method was mainly suggested for autoregressive models. The key idea is that the computation of an input token only depends on previous tokens but not the future tokens. The bubbles are reduced by pipelining with a sequence. <img src="/assets/img/2025-01-06-data-systems-for-ml/17399362100774.jpg" alt=""/></p> </li> <li><strong>Chimera</strong> - Store bi-directional stages and combine bidirectional pipeline to further reduce bubbles. Deepseek used this! Optimized 1F1B schedule <img src="/assets/img/2025-01-06-data-systems-for-ml/17399363005548.jpg" alt=""/> The only issue is that the model parameters have to be stored twice. But if you use a model with half the number of parameters, then you’re good!</li> </ol> </li> <li><strong>Asynchronous pipeline schedules</strong> - The idea is to start the next round of forward pass before the backward pass finishes. It breaks the synchronous training semantics, and training involves stalled gradients. The algorithms may have to store multiple versions of model weights for consistency. <ol> <li> <p><strong>AMPNet</strong> - Each device performs forward pass whenever free and updates the weights after every backward pass. It does not generalize well to larger datasets. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405374850797.jpg" alt=""/></p> </li> <li><strong>Pipedream</strong> - Enforce the same version of weight for a single input batch by storing multiple weight versions. Has a similar accuracy on Image Net with a 5x speed up compared to data parallel.</li> <li><strong>Pipedream-2BW</strong> - Reduced memory usage by only storing 2 copied and updating the weights less frequently. The weights are always stalled by 1 update. It achieved similar accuracy on larger models like BERT and GPT.</li> </ol> </li> </ol> <p><strong>Imbalanced pipelines</strong> - All these schedules work best when the stages are balanced. However, there can scenarios where the stages do not have equal workloads. There have been works for automatic stage partitioning that try to minimize maximum stage latency and maximize parallelization.</p> <p><strong>Placeto: RL-based partitioning algorithm</strong> - The state is given by the device assignment plan for a computation graph, and the actions are modifying the device assignment of a node. The rewards are latency differences between new and old placements.</p> <p>In summary, inter-operator parallelism assigns different operators of the computational graph to different devices and executed in a pipelined fashion.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17405378678788.jpg" alt=""/></p> <p>Furthermore, imbalanced stages cause further pipeline bubbles, and RL-based/optimization-based automatic stage partitioning techniques are being explored.</p> <h3 id="intra-op-parallelism">Intra-op parallelism</h3> <p>Let us understand some core ideas to implement techniques in this paradigm.</p> <p>Suppose we are parallelizing one operator.</p> <ol> <li>Element-wise operations - The operations involved are typically independent of one another and can be parallelized across devices. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405380474272.jpg" alt=""/> The splitting can be done based on the optimal parameters for the underlying hardware.</li> <li>Matrix multiplication - It is a 3-level nested for-loop. We have the following variations <img src="/assets/img/2025-01-06-data-systems-for-ml/17405381689985.jpg" alt=""/> <img src="/assets/img/2025-01-06-data-systems-for-ml/17405381942988.jpg" alt=""/> <img src="/assets/img/2025-01-06-data-systems-for-ml/17405382663939.jpg" alt=""/></li> <li>2D convolution - It is a 7-level nested for-loop as we’ve seen before. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405385743018.jpg" alt=""/></li> </ol> <p>Now, we consider how we parallelize multiple operators together. Note that data parallelism can be seen as a special case of intra-op parallelism. We will consider two main types of matrix multiplication parallelization strategies.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17405388302958.jpg" alt=""/></p> <p>Different parallelization strategies for operators require different partitioning format of the same tensor. For example, with type 1 matrix multiplications, a ReLU operator following matmul would not require any repartitioning. However, in Type 2, we would require an all-gather operation.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17405389681048.jpg" alt=""/></p> <p>So given these variations, our goal is to minimize the node costs (computation and communication) and the edge costs (repartitioning communication). There have been various solutions using manual design, randomized search, dynamic programming and integer linear programming. We discuss some important projects.</p> <p><strong>Model-specific strategies</strong></p> <ol> <li><strong>AlexNet</strong> - A convolution layer was split across 2 GPUs. It improved the accuracy by 2%, because the training improved on using 2 GPUs.</li> <li> <p><strong>Megatron-LM</strong> - A large language model with 8.3B parameters that outperforms SOTA results! They noted the following - performing Type 2 matmul followed by Type 1 matmul results in only 1 all-reduce operations for both forward and backward passes. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405394028552.jpg" alt=""/></p> </li> <li><strong>GShard MoE</strong> - A multi-language translation model with 600B parameters that outperforms SOTA. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405394573494.jpg" alt=""/> Deepseek used this kind of parallelization, and so they optimized all-to-all operations in their pipeline. These implementations were shared as part of their open-source week event.</li> </ol> <p><strong>Systems for Intra-op parallelism</strong></p> <ol> <li><strong>ZeRO Optimizer</strong> - The authors noticed that data parallelism replicates gradients, optimizer states and model weights on all devices. So, they partitioned gradients, optimizer states and model weights too! Here is a summary - <img src="/assets/img/2025-01-06-data-systems-for-ml/17405396936446.jpg" alt=""/> The memory cost in the above table is per device. ZeRO Stage 3 is part of PyTorch through <code class="language-plaintext highlighter-rouge">FSDP</code> (Fully Sharded Data Parallelism). <blockquote> <p>Recall why the optimizer, gradients and model weights have the mentioned memory costs. Optimizer states are stored in FP32 (factor of 4) and the rest are in FP16 (factor of 2).</p> </blockquote> <p>Due to the communication and memory costs, everyone prefers Stage 2 over Stage 1 today.</p> <ol> <li><strong>ZeRO Stage 2</strong> - The operation <code class="language-plaintext highlighter-rouge">all-reduce</code> is <code class="language-plaintext highlighter-rouge">reduce-scatter</code> followed by <code class="language-plaintext highlighter-rouge">all-gather</code>. Using this idea, ZeRO stage 2 does the following. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405399912790.jpg" alt=""/> We obtained the same results but with much larger memory savings.</li> <li><strong>ZeRO Stage 3</strong> - Extending this idea, we partition the model across the ranks. <img src="/assets/img/2025-01-06-data-systems-for-ml/17405402117787.jpg" alt=""/> The trade-off between Stage 2 and Stage 3 is memory vs communication cost.</li> </ol> </li> <li><strong>Mesh Tensorflow</strong> - The idea is to map tensor dimensions to mesh dimensions for parallelism. A mesh is essentially a matrix of devices.</li> <li><strong>GSPMD</strong> - Successor for Mesh Tensorflow (a compiler based approach) but generalizes it. The users do not have to partition every tensor, the compiler takes care of automatic partitioning across the code. That is, the annotations are propagated to the whole graph.</li> <li>There are more works like Tofu and FlexFlow which we do not cover here.</li> </ol> <p>Now that we have methods for both intra-op and inter-op, how do we combine the both of them? Such kind of a hybrid approach is compulsory to scale the parallelism to clusters that have 1000s of GPUs.</p> <p>Remember that these can be combined with the approaches we described before for the best outcome.</p> <ol> <li>System-level memory optimizations - Gradient checkpointing or memory swapping</li> <li>ML-level optimizations - Quantization, Sparsification and Low-rank approximation.</li> </ol> <h2 id="auto-parallelization">Auto-parallelization</h2> <p>The motivation to develop auto-parallelization is very similar to why we designed ML compilers for operator optimization. There are many parallelism models on different model architectures. It is difficult to keep track of which method is the best for a given model and the hardware cluster configuration.</p> <p>Again, the search space is huge because there are 100-10k operators in a model, 80-200 operator types and number of GPUs in a cluster vary from 10s to 1000s! There have been a multitude of methods for this problem -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17405409086956.jpg" alt=""/></p> <p>-<strong>Search-based methods</strong> simply search the best configuration for a given model and cluster. Most companies rely on this technique. It is similar to a brute-force approach.</p> <ul> <li><strong>Learning-based methods</strong> - Uses some sort of learned heuristic to navigate the search space. The most popular work is <a href="https://arxiv.org/abs/1706.04972">ColocRL by Mirhoseini et al.</a> That searches the space of inter-op strategies using an ML model to sample candidates and use a policy gradient algorithm to improve the model based on the execution cost. The method was able to obtain strategies that are 20-30% better than an expert-designed assignment. However, the method is not very efficient since it consumes a lot of resources, and RL is not very efficient as well.</li> <li> <p><strong>Optimization-based methods</strong> - Search methods but with better optimizations. One good method in this category is <a href="https://arxiv.org/abs/2201.12023">Alpa by Zheng et al.</a>. It tries to optimize considering both inter and intra-op parallelism considering the fast and slow connections in the GPU cluster. Intra-op parallelism strategies require communication, so splitting them across ranks with NVLink is a good heuristic. Inter-op can be done across slower communication methods. The method performs the search in a hierarchical manner - inter-op followed by intra-op.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17405414966691.jpg" alt=""/></p> <ul> <li>The inter-op pass, the graph is partitioned into multiple stages, and each stage is assigned different ranks. The states in the dynamic programming algorithm are formed by these choices - partitioning and the sub-mesh assignment. This step also considers the pipeline execution latency, due to different running times of each stages. The running times are given by the intra-op passes, and we have that information. Since dynamic partitioning can introduce imbalances in the pipeline increasing the bubbles, we use dynamic programming with the execution times of each stage to correctly optimize and reach the best strategy. Essentially, the inter-op pass is like the outer loop and we have the information from the inner-loops (intra-op passes) for optimization.</li> <li>The intra-op pass parallelizes the operators across sub-meshes and minimizes the communication. For example, consider the following - <img src="/assets/img/2025-01-06-data-systems-for-ml/17407105767310.jpg" alt=""/> <img src="/assets/img/2025-01-06-data-systems-for-ml/17407106048963.jpg" alt=""/> <blockquote> <p>What about the split cost?</p> </blockquote> </li> </ul> </li> </ul> <p>Essentially, within each stages, we try to color every node in the stage so that the execution latency of the stage on the assigned mesh is minimized such that the peak memory usage does not cross the memory budget. Note that this problem formulation has node costs (splitting the operator) and edge costs (layout conversions).</p> <p>With this hierarchical optimization, Alpa is able to</p> <ul> <li>Match specialized manual systems with GPT</li> <li>Outperform the manual baseline for Mixture of Experts by up to 8x</li> <li>Generalize to models without manual plans</li> </ul> <p>However, the paper is not popular because the industry has stuck to one type of model (transformers) and the manual expert design for these has been good enough.</p> <p>In summary, we have</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17407109626252.jpg" alt=""/></p> <p>How to choose parallelism then? Transformers have been heavily optimized manually, follow Hao’s ultimate guide below. Otherwise, use the automatic compilers.</p> <p>We consider the following factors - #GPUs, Model size, JCT (Job completion time), communication bandwidth, etc.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17407110650089.jpg" alt=""/></p> <blockquote> <p>What is 3D parallelism?</p> </blockquote> <h1 id="connecting-dots">Connecting dots…</h1> <p>We have studied all the concepts. Let us know see how they are used in practice.</p> <h2 id="a-bit-of-refresher">A bit of refresher</h2> <p>We’ll list out all aspects of an LLM that are relevant for implementation -</p> <p>LLMs do next-token prediction. They basically do Maximum Likelihood Estimation based on the tokens given so far (with the prompt or generated). Transformers at their core are seq2seq NNs built on top of attention mechanism.</p> <p>The attention score computes how relevant the position i’s token is to a given output token. With language, we choose the same tokens for input and output (hence, self-attention).</p> \[SelfAttention(Q, K, V) = \text{softmax} \left(\frac{QK^T}{d^{1/2}\right) V\] <p>Conceptually, we perform this calculation in two steps -</p> <ol> <li>Pre-softmax attention score $$s_{ti} = \frac{1}{\sqrt{d}} q_tk_i^T</li> <li>Weighted average via softmax - $$h = \sum_i \text{softmax}(s)_i v_i</li> </ol> <p>We also have multiple attention heads in models to calculate different features. We get \(Q, K, V\) using embedding layers \(W_Q, W_K, W_V\).</p> <p>During training, we also mask the attention so that a <em>causal relation</em> is maintained. Only attend to previous tokens. In naïve implementation, we make the entries of the attention matrix corresponding to the mask negative infinity. Can we be more smart about them and skip these calculations? Turns out masking later is more efficient due to GPUs behavior we’ve seen before - oversubscribe and don’t disturb the layout!</p> <p>Given all this, we summarize the following</p> <ol> <li>Self-attention is slow</li> <li>Layer-norm, residual - fast</li> <li>MLPs - slow</li> <li>Non-linear activations - fast</li> <li>Word and position embeddings - fast</li> <li>Loss function - fast</li> </ol> <p>So from a computing perspective, how do we make self-attention fast?</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17407127855541.jpg" alt=""/></p> <p>GPT-3 has 96 of these! So we definitely need to make it fast.</p> <p>These were the core parts of an LLM. Notably, only the fast operations have been changing a lot, and the core attention and MLP mechanisms have remained.</p> <table> <thead> <tr> <th>Feature</th> <th>Vaswani et al.</th> <th>LLaMA</th> </tr> </thead> <tbody> <tr> <td>Norm Position</td> <td>Post</td> <td>Pre</td> </tr> <tr> <td>Norm Type</td> <td>LayerNorm</td> <td>RMSNorm</td> </tr> <tr> <td>Non-linearity</td> <td>ReLU</td> <td>SiLU</td> </tr> <tr> <td>Positional Encoding</td> <td>Sinusoidal</td> <td>RoPE</td> </tr> </tbody> </table> <h2 id="training-llms">Training LLMs</h2> <p>As we have noted, the three main components of implementation are compute, memory and communication. We must know</p> <ol> <li>The number of parameters of an LLM - it tells us how much memory and communication bandwidth is needed</li> <li>The FLOPs needed to train the LLM - it tells us how much compute is needed</li> <li>The memory needed to train an LLM - again, tells us about memory and communication considering activations and gradients</li> </ol> <p>Let us delve into each of these questions</p> <h3 id="calculating-number-of-parameters">Calculating number of parameters</h3> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17407132381084.jpg" alt=""/></p> <ol> <li>The embedding layer has weight matrices of size $$</li> <li> <p>The SwiGLU layer has the following equation</p> \[\text{SwiGLU}(x) = \text{Swish{(xW_1 + b_1) \cdot (xW_2 + b_1)\] <p>So in comparison with vanilla feedforward, we have one extra set of parameters.</p> </li> <li>The linear transformation in the end essentially converts embeddings to token space and shares the weights with the embedding layer (<code class="language-plaintext highlighter-rouge">tie_embd</code> in HuggingFace)</li> </ol> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17407134503575.jpg" alt=""/></p> <p>Notice that the major bottleneck comes from the feedforward layers. It has \(12h^2\) parameters in LLaMa! The second bottleneck is from self-attention which has \(4h^2\) parameters.</p> <h3 id="calculating-flops">Calculating FLOPs</h3> <p>As we’ve noted multiple times, <code class="language-plaintext highlighter-rouge">matmul</code> is the most expensive operation in terms of FLOPs. We’ve seen that the total number of FLOPs is approximately \(2mnh\) for multiplying matrices of sizes \((m \times n)\) and \((n \times h)\).</p> <table> <thead> <tr> <th>Component</th> <th>Output Shape</th> <th>FLOPs</th> </tr> </thead> <tbody> <tr> <td><strong>Input:</strong></td> <td> </td> <td> </td> </tr> <tr> <td>X</td> <td>(b, s, h)</td> <td>0</td> </tr> <tr> <td><strong>Self Attention:</strong></td> <td> </td> <td> </td> </tr> <tr> <td>XW_Q, XW_K, XW_V</td> <td>(b, s, h)</td> <td>3 * 2bsh²</td> </tr> <tr> <td>RoPE</td> <td>(b, n, s, d)</td> <td>3bsnd</td> </tr> <tr> <td>P = Softmax(QKᵀ/√d)</td> <td>(b, n, s, s)</td> <td>2bs²nd + 3bs²n</td> </tr> <tr> <td>PV</td> <td>(b, n, s, d)</td> <td>2bs²nd</td> </tr> <tr> <td>AW_O</td> <td>(b, s, h)</td> <td>2bsh²</td> </tr> <tr> <td><strong>Residual Connection:</strong></td> <td>(b, s, h)</td> <td>bsh</td> </tr> <tr> <td><strong>Output from Self Attn:</strong></td> <td> </td> <td> </td> </tr> <tr> <td>X</td> <td>(b, s, h)</td> <td>0</td> </tr> <tr> <td><strong>Feed-Forward SwiGLU:</strong></td> <td> </td> <td> </td> </tr> <tr> <td>XW_gate / XW_up</td> <td>(b, s, i)</td> <td>2 * 2bshi</td> </tr> <tr> <td>Swish Activation</td> <td>(b, s, i)</td> <td>4bsi</td> </tr> <tr> <td>Element-wise *</td> <td>(b, s, i)</td> <td>bsi</td> </tr> <tr> <td>XW_down</td> <td>(b, s, h)</td> <td>2bshi</td> </tr> <tr> <td><strong>RMS Norm:</strong></td> <td>(b, s, h)</td> <td>4bsh + 2bs</td> </tr> </tbody> </table> <p>where</p> <ul> <li><strong>b</strong>: Batch size</li> <li><strong>s</strong>: Sequence length</li> <li><strong>n</strong>: Number of attention heads</li> <li><strong>d</strong>: Hidden state dimension of one head</li> <li><strong>h</strong>: Hidden state dimension</li> <li><strong>i</strong>: SwiGLU projection dimension</li> </ul> <p>Note that attention is a very expensive operation because we have \(s^2\) - it does not allow us to scale to large number of tokens!</p> <p>The final equation we get is</p> \[\text{Total FLOPs} = \#\text{num layers} *(6bsh^2 + 4bs^2h + 3bs^2 n + 2bsh^2 + \#\text{num layers} (6 bshi) + 2bshv\] <p>For Llama 7B, this comes out to be 63 TFLOPs! The MLP is \(\approx 55\%\) of the total FLOPs and attention is \(\approx 41 \%\)!</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17407142596268.jpg" alt=""/></p> <p>To reduce the bottleneck, one might try to decrease \(h\) (the hidden dimension) to scale up the model. The other way is to reduce the sequence length.</p> <p>Note the factor of 3 above that is because of training (the model parameters and optimizer parameters). Let us recall the memory usage in models</p> <ol> <li>Model Weights - \(2 \times M\) for FP16</li> <li>Intermediate activation values <ol> <li>If we checkpoint at transformers boundary, the activation memory is \(b \times s \times h \times n\_layers\). It is same as what we need to communicate in inter-op parallelism.</li> <li>The activation inside a transformer has the size \((b, n, s, s)\). It is the bottleneck for memory! We’ll see <strong>Flash Attention</strong> on how to avoid saving the complete attention matrix</li> </ol> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17411426493026.jpg" alt=""/></p> <p>In summary, these are the activations inside the transformer block.</p> </li> <li>Optimizer State - \(12 \times M\) for FP16-32 mixed-precision</li> <li>Weight Gradients + Activation Gradients - \(2 \times M\).</li> </ol> <p>Based on this analysis, we have the following observations</p> <ol> <li>All terms are at least linear with \(h\) - Useful observation for scaling up</li> <li>Dimensions of partitioning <ol> <li>\(b\) would be data parallelism</li> <li>\(d\) - Megatron based architecture</li> <li>\(s\) - Part of 4D parallelism</li> </ol> </li> </ol> <blockquote> <p>Advanced Topics (Recently published research) Megatron-style parallelism has been well adapted. How do we partition along \(s\)? We can partition \(n\) is attention (assign different heads to different GPUs) and \(s\) in MLP. Deepspeed Ulysses sequence parallelism is a recently proposed technique that partitions on \(n\) first and then partitions on \(s\). Moreover, it often happens that \(n \ll \#GPUs\) or is not a multiple of 8, then it is prudent to partition \(s\) in both attention and MLP. This is implemented in the form of <strong>ring attention</strong>.</p> </blockquote> <blockquote> <p>Rule of thumb: In many computer systems and algorithms, anything more complex than quadratic is less likely to be adapted at large scale.</p> </blockquote> <h2 id="scaling-laws">Scaling Laws</h2> <p>Continuing on our observations</p> <ol> <li>Computer is a function of \(h, i, b\)</li> <li>\(\#\)parameters is a function of \(h, i\)</li> <li>So the computer is a function \(\#\) parameters.</li> </ol> <p>But as we know, the compute budget is limited. Then we have to decide between</p> <ol> <li>Training models longer (on more data) vs training bigger models</li> <li>Collect more data vs get more GPUs</li> <li>How to choose exact \(h, i\), etc.</li> </ol> <p>Our motivation for scaling laws is to understand how large a model should we train and how much data should we use to achieve a given performance subject to a compute budget.</p> <p>For applications like statistical estimators, we can easily derive theoretical scaling laws based on mathematical axioms. However, that seems to be much difficult for language models.</p> <p>Then, how do we analyze these models? We perform empirical observations. Our analysis on machine learning has moved from mathematics based proofs to physics like experiments. Relying on observations. It became a strong trademark after the GPT-3 paper.</p> <p>Here is what we have got so far, observing based on empirical scaling laws:</p> <ul> <li>To conclude transformers are better than LSTMs, instead of spending tens of millions to train LSTM GPT-3, we extrapolated results and noticed that transformers outperform these models.</li> <li>Similarly for depth vs width of networks - 1 vs 2 layers make s a huge difference but more layers have diminishing returns after a certain number of parameters.</li> </ul> <p>Essentially, the scaling law way trains smaller models to establish a scaling law and select the optimal hyper-parameters based on the scaling law prediction. This allows us to get a heuristic on the effects of hyper-parameters in big LMs before training.</p> <p>Consider the question data vs compute trade-off - for a given compute is it better to train undertrained big models or well-trained small models?</p> \[N_{opt} (C), D_{opt}(C) = \arg_{N, D}\min_{FLOPs(N, D) = C} L(N, D)\] <p>where \(N\) is the number of parameters, \(D\) is the data and \(C\) is the compute. We obtained the following after spending billions of dollars for next-prediction loss</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17411445543561.jpg" alt=""/></p> \[L(N, D) = \frac{406.4}{N^{0.34}} \frac{410.7}{D^{0.29}} + 1.69\] <p>and every LLM seems to follow this…</p> <h2 id="moe-llms">MoE LLMs</h2> <p>The key idea like we discussed before is to make each expert focus on predicting the right answer for a subset of cases.</p> <p>Instead of choosing one expert each time, it is also possible to choose multiple experts for a token, and that becomes another hyper parameter. That is, we can choose to activate two experts each time and add the output of both of them.</p> <p>Let us consider the number of parameters and analyze MoE with scaling laws.</p> <ul> <li>The number of parameters increase drastically - MLP params \(\times N/2\) (if we choose to activate 2 experts each time) where \(N\) is the number of experts. For example, Deepseek V3 has 256 experts!</li> <li>Memory for parameters increases by the same factor. However, activations don’t consume too much additional memory. Even though the parameters increase drastically, the compute only increases mildly. This is why Deepseek blew up. For much higher number of experts, the attention parameters decrease largely (sparse activations in experts), and they were able to train dense model equivalent model with much lower compute.</li> </ul> <p>People also tried to get scaling laws for MoEs and noticed that MoE is a much more compute-efficient model (it has a better scaling law).</p> <p>Let us analyze the architecture again. We stuck to a Megatron like architecture for transformers. However, it would not work with MoE since all the weights are replicated number of expert times. So we perform parallelism in the dimension of experts. <img src="/assets/img/2025-01-06-data-systems-for-ml/17411455774654.jpg" alt=""/></p> <p>This way, each GPU would have different workloads (bubbles!) - A hot expert problem. Deepseek implemented many mechanisms to reduce this as much as possible.</p> <h2 id="inference">Inference</h2> <p>So far we’ve studied that LLMs are slow and expensive to serve. At least 10 A100-40GB GPUs are required to serve 175B GPT-3 in half precision and generating 256 tokens takes approximately 20 seconds.</p> <p>The key factor is that we perform autoregressive decoding - we require the current token to predict the next token. Note that we predict until the pre-defined maximum length or we reach the end of sequence.</p> <p>The inference stage can be divided into two stages</p> <ol> <li>Prefilling - Process all input tokens at once (prompt)</li> <li>Decoding phase - Process a single token generated from previous iteration (generating)</li> </ol> <p>The decoding phase is optimized with <strong>Key-value cache</strong> by saving attention keys and values during the decoding phase to avoid recomputations.</p> <p>In a broader sense, serving an LLM has more considerations during inference. For a larger group, it would be prudent to emphasize on throughput and if it is an individual then latency is a concern. How do we optimize for these scenarios?</p> <p>Different techniques have been proposed. For the single user case, a popular technique has been <strong>speculative decoding</strong>.</p> <p>The latency is essentially \(\text{latency} = \text{ step latency} \times \#\text{steps}\)</p> <p>With speculative decoding, we try and reduce the number of steps!</p> <h1 id="guest-lecture-hongyang-zhang-speculative-decoding">[Guest Lecture: Hongyang Zhang] Speculative Decoding</h1> <p>The vanilla autoregressive inference involves sampling tokens from a probability distribution that is obtained by the tokens generated so far.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-18-35-07-image.png" alt=""/></p> <p>How do we move from this sequential sampling framework to a parallel one?</p> <p><strong>Speculative sampling framework</strong> (introduced in <a href="https://arxiv.org/abs/2211.17192">Leviathan et al</a>) uses a much smaller model that is a good approximation of the original large model. This tiny <strong>draft model</strong> is used to create a <em>draft</em> of the response in the same sequential autoregressive manner. Since the model is tiny, the inference is faster.</p> <p>The tokens generated by the draft model are fed into the LLM to generate the distributions from these tokens in parallel. The draft model can be verified by comparing the distributions of both the models for a given token.</p> <ol> <li> <p>We generate a number \(r\) between 0 and 1 uniformly, and check if \(r &lt; \min(1, p(t)/q(t))\) where \(p\) is the LLM distribution and \(q\) is the draft model distribution.</p> </li> <li> <p>If the condition holds, we accept the token. Otherwise we correct the token by sampling \(t' \sim \norm(\max(0, p - q))\). All the tokens following this corrected token are discarded, and the process is repeated again.</p> </li> </ol> <p>There is theoretical work that shows that the above procedure is equivalent to sampling from \(p\).</p> <p>Given this, how do we build the tiny model \(q\)? That is the crux of the methods in this domain.</p> <blockquote> <p>Does this framework only work for greedy sampling? No, it can be extended to non-greedy sampling techniques with some modifications.</p> </blockquote> <blockquote> <p>The tiny model used can have low rejection rates for tasks like summarization but may have high rejection rates for tasks involving reasoning. So, there’s a trade-off with the draft model overhead and the rejection rate.</p> </blockquote> <h3 id="building-q-with-eagle-v1">Building \(q\) with EAGLE-v1</h3> <p>As mentioned before, building \(q\) is a trade-off between accuracy and efficiency. They have proposed EAGLE.</p> <p>The authors of this paper observed that the initial distribution of the embeddings in the initial layers is very complicated and dynamic. However, for the top layers, the distribution becomes much simpler - that is, the distribution to predict next tokens from previous tokens.</p> <p>So, they tried to estimate the features rather than the tokens directly. In their first attempt, they trained a single transformer layer to learn the embeddings (features) in the last layer of LLMs. Essentially, instead of using the first layer tokens, they used the last layer features for the speculative decoding framework. With this, they obtained a much higher speed-up and accuracy!</p> <p>However, they noted that this model does not use feature uncertainty correctly. In the speculative decoding framework, the parallel decoding in the large LLM does not consider the previously generated</p> <p>To solve this, they tried another method by predicting the distribution by concatenating the initial token embedding with the feature from the last layer. They attribute this as adding a shifted token embedding.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-19-01-18-image.png" alt=""/></p> <p>Essentially, we generate choices through the draft model rather than a single prediction. With this, we obtain a much higher speed-up and accuracy -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-18-55-53-image.png" alt=""/></p> <p>Note that the attention mask for thi has to be carefully design as follows -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-19-07-29-image.png" alt=""/></p> <p>Because of this, the model cannot handle long-context settings.</p> <p>The draft model can be around 1-3% (higher percentage for smaller models to account for the dimensionality of the latent space) of the larger model leading to a very high speed-up!</p> <h3 id="eagle-v2">EAGLE-v2</h3> <p>The authors noted that the method can be improved even further. They noted that the accuracy of the speculative decoding tree if higher to the top-left and lowest to bottom-right. So, they associated a confidence score to each node in the tree. Ideally, that would be \(\min(1, p(t)/q(t))\), but they found that \(q(t)\) is a good approximation and has a strong linear correlation with the actual quantity.</p> <p>With this, they build a context-aware dynamic draft tree.</p> <p>Essentially, instead of generating a tree, they do beam-search with the small model.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/2025-03-06-19-22-55-image.png" alt=""/></p> <p>As expected, this method obtained even higher speedup and accuracy!</p> <p>They didn’t stop there.</p> <h3 id="eagle-v3">EAGLE-v3</h3> <p>They published yesterday lol! With the new method that has a modified training routine and a loss function, they noticed a new scaling law that seems to perform better as the model grows. Essentially, they introduced draft model inference in the training routine to better simulate the inference routine.</p> <p>They also observed that the performance is the best when the draft model architecture is the same as the large model.</p> <blockquote> <p>The draft model just generates a draft and not the complete response. For example, it will generate 6 words, and these are used by the larger LLM. So it’s different from knowledge distillation.</p> </blockquote> <p>Getting back to our context, let us talk about LLM inference.</p> <p>We mentioned KV-cache before - It stores the previously calculated attention values during the next-token generation to reuse the values -</p> <p><img src="assets/17417436164038.gif" alt=""/></p> <p>There are issues with this approach is</p> <ol> <li>Compute: The arithmetic intensity in the GPU is decreased, and GPU may be under-utilized. When \(n = 1, s = 1\), the GPU is underutilized.</li> <li>Memory: Need to store KV cache until the request is completed</li> <li>Communication: Mostly same</li> </ol> <p>Note that the batch size during inference is dependent on the application request load - It is a tradeoff between efficiency and latency reduction as we have mentioned before. We have also seen how speculative decoding is used to decrease the latency in case of single requests. Now, we will consider the other side - how do we handle a large number of requests while keeping the cost-per-query less. Let us again consider the three elements to find the potential bottleneck during inference</p> <ol> <li>Compute: <ol> <li>Pre-fill - Requests have different lengths. How do we batch?</li> <li>Decode - We do not know the number of tokens that need to be generated ahead of time.</li> </ol> </li> <li>Memory: KV-Cache is an issue for these large batch sizes.</li> <li>Communication: Remains the same.</li> </ol> <p>Recent works have addressed these issues, and the requirements for LLMs have almost decreased \(60 \times\). Speculative decoding does not work for large batch sizes - it is being explored in research currently.</p> <h3 id="continuous-batching">Continuous Batching</h3> <p>Suppose we have two prompts that are passed to the server.</p> <ol> <li>We cannot batch the sequences for attention if they have different number of tokens. So, here arises a crucial loss in performance - whether to pad all the short requests to make everything equal.</li> <li>However, since MLPs are token independent, the requests can be batched! No loss in performance here.</li> </ol> <p>Due to this difference, the current implementations perform cyclic batching and de-batching to execute attention of different requests on different ranks, and then merge all requests to get the MLP output.</p> <p>Suppose a new request enters while the earlier requests are being processed - the new request can be easily be batched for the MLP phases after the attention has been calculated separately. In standard pipelines without continuous batching, such behavior is not allowed - a batch is issued and run until completion. A new request in the queue cannot enter and a request that finished early cannot exit.</p> <p>Recall that when we calculated the FLOPs for a transformer, the MLP requires the maximum number of FLOPs. So, batching these stages results in huge gains in performance.</p> <p><em>Note.</em> The pre-fill and decoding phases for attention are superficially the same for this setup - they are separately calculated on each kernel before being batched for MLP passes.</p> <p>So essentially, we have introduced a lot of flexibility into transformer forward passes which were previously treated as atomic. We have improved GPU utilization with two key insights</p> <ol> <li>Attention consumes small percentage of FLOPs for medium context</li> <li>MLP kernels are agnostic to sequence dimension</li> </ol> <h3 id="kv-cache-key-insight">KV-Cache Key Insight</h3> <p>KV Cache needs to be efficiently managed for high-throughput LLM serving. The KV Cache can become super large for long sequences.</p> <p><img src="assets/17417463235738.jpg" alt=""/> The throughput of the GPU is proportional to the batch size. That is, the execution time for 40 batches would be same as that of 8 batches because of how a GPU functions!</p> <blockquote> <p>Note. Why is the throughput higher for higher batch sizes?</p> </blockquote> <p>So, if we can reduce the memory usage of KV-Cache, we can process more batches in the same time! How do we do this?</p> <ol> <li><strong>Internal Fragmentation</strong> - Memory for KV Cache is over-allocated because we do not know the output length.</li> <li><strong>Reservation</strong> - Memory that is not used at the current step (but will be needed in the future) is wasted</li> </ol> <p><img src="assets/17417467240834.jpg" alt=""/></p> <p>There is also external fragmentation that demarcates areas of different requests. Due to all these different things - only 20-40% of KV cache is used to store the tokens!!</p> <h3 id="vllm---paged-attention">vLLM - Paged Attention</h3> <p>Drawing inspiration from virtual memory and paging in the OS, we improve how we store the values in KV cache.</p> <p><img src="assets/17417467446680.jpg" alt=""/></p> <p>Deep-learning workflows typically consider contiguous blocks of memories. Breaking that standard, we store KV cache using continuous keys and values in non-contiguous memory space. Similar to OS mechanisms, we use logical and physical token blocks connected through a <em>page table</em>. Refer to <a href="blog/operating-systems">my notes on OS</a> to understand these better.</p> <p><img src="assets/17417470519278.jpg" alt=""/></p> <p>Through this, we allocated memory on demand and minimize the internal fragmentation! We use CPU to orchestrate all this. There is no external fragmentation at all! With this, we have the following results -</p> <p><img src="assets/17417471345506.jpg" alt=""/></p> <h3 id="prefill-decode-disaggregation">Prefill-decode disaggregation</h3> <p>To better benchmark LLM-based services, we consider new service based metrics</p> <ol> <li>TTFT - Time to first token For a chatbot, we need to have fast initial response. For tasks like summarization, it can be slower.</li> <li>TPOT - Time per output token Generate as fast as human reading speed for chatbots. For summarization, needs to be super fast.</li> </ol> <p>Any requests completed within Service Latency Output (SLO) are factored into <em>goodput</em>. High throughput can still have low goodput. We discussed continuous batching, but it has the following limitations</p> <ol> <li>Prefill - Compute bound, one prefill saturates compute.</li> <li>Decode - Memory bound, need to batch many requests together to saturate compute.</li> </ol> <p><img src="assets/17417478372758.jpg" alt=""/></p> <p>To prevent these limitations, the idea of <em>disaggregating prefill and decode</em> phases has been proposed - Essentially, the prefill and decode steps are performed on different GPUs. There is an overhead of communication to transfer the KV cache from prefill phase to the decode phase, but the requests can be processed in parallel increasing the throughput. The result of this can be seen below -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17419162187482.jpg" alt=""/></p> <p>The curve for decoding in the disaggregate system is flatter since there is no prefill phase competing for resources.</p> <p>This system became a norm in the past few months. Deepseek-v2 uses prefill-decode disaggregation combined with different parallelisms for prefill and decoding instances.</p> <p>Furthermore, continuous batches and disaggregation are actually orthogonal to each other (doesn’t seem so at the first glance)</p> <ol> <li>Continuous batching improves GPU utilization and hence the throughput</li> <li>Disaggregation addresses goodput such that SLOs are satisfied</li> </ol> <p>The key insights from CB still carry to disaggregation.</p> <p>LLM Inference is a very hot research topic right now. We have seen many techniques from this field</p> <ol> <li>Scheduling - continuous batching, chunked prefill, disaggregated prefill and decoding</li> <li>Speculative Decoding</li> <li>Memory bottleneck of KV Cache - new attention mechanisms - paged, sparse, etc and sparse KV cache</li> <li>Kernel optimizations</li> </ol> <p>With that, we move to the final topic of this article.</p> <h1 id="flashattention">FlashAttention</h1> <p>Coming back to the computations in the transformer, we saw that the attention operation is limited by the sequence size (proportional to \(\mathcal O(n^2)\)).</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17419166000781.jpg" alt=""/></p> <p>The internal matrices have sizes proportional to square of the sequence length and we have to materialize these matrices. That is, we explicitly compute and store these matrices. Putting this into perspective with numbers we have</p> <p>For \(b = 1k, n = 32, h = 4k, s= 4k\),</p> <ul> <li>Memory = \(bs^2n = 512Gb\)</li> <li>Compute = \(4bs^2h = 256 TFLOPs\)</li> </ul> <p>The GPU memory is much more scarce than compute with our current hardware. Is there a way we can reduce the memory usage to improve the attention block?</p> <p>The implementations before FlashAttention explicitly calculated the internal matrices and moved them between HBM and kernel SRAM. These repeated reads/write add a lot of latency.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17419168508774.jpg" alt=""/></p> <p>The issue arises due to the softmax operation. It is not trivial to calculate the final softmax output without the intermediate matrices. Furthermore, the backward pass also needs to be done without the intermediate activations.</p> <p>We were able to calculate matrix multiplication results without materializing the complete input and output. Can we do the same here?</p> <p>Let us understand softmax first. Softmax is usually implemented in a <em>numerically stable</em> manner.</p> \[y_i = \frac{e^{x_i - \max_{k = 1}^V x_k}}{\sum_{j = 1}^V e^{x_j - \max_{k = 1}^V x_k}}\] <p>This introduces another level of complexity for calculating the attention output. We observe the following relation while iterating through the array elements to find the softmax</p> \[\begin{align*} d_i &amp;= \sum_{j = 1}^i e^{x_j - m_i} \\ d_i = d_{i - 1} e^{m_{i - 1} - m_i} + e^{x_i - m_i} \end{align*}\] <p>Now, we observe that the final output can be represented as a recurrence depending on some <em>precomputed values</em>. We move from two loops (one for maximum and one for exponentiation) to one single loop for softmax based on the recurrence relation. It is known as <strong>online softmax</strong> calculation.</p> <p>How do we use this to optimize attention?</p> <p>If we can find some recursion structure in \(o\), we can use it to optimize attention by preventing explicit calculation of \(a\), and fuse the two loops together.</p> \[\begin{align*} o’_i &amp;= \sum_{j = 1}^i \frac{e^{x_j- m_i}}{d_i’} V[j, :] \\ &amp;= \cdots \\ &amp;= o’_{i - 1} \frac{d’_{i - 1} e^{m_{i - 1} - m_i}}{d’_i} + \frac{e^{x_i - m_i}{d_i’} V[i, :] \end{align*}\] <p>Voila! That gives us FlashAttention</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17419183745135.jpg" alt=""/></p> <p>We never materialize the intermediate results, and transfer the matrices only once. That gives a huge speed up! FlashAttention also has tiling to utilize the GPUs to their fullest.</p> <p>What about the backward pass? FlashAttention resorts to recomputation of required results instead of storing the activations, and this still gives a speed up! That is how memory-bottlenecked we are.</p> <p>Overall, the method offers 104x speed up and 10-20x memory reduction!</p> <p>FlashAttention got very successful because it has a cascading effect. Due to this method, we were able to train with larger batch sizes leading to much higher throughput (better AI). Also, the cost of training decreased significantly because we can now turn off gradient check pointing (no need to save memory anymore).</p> <p>The method was further developed to FlashAttention2 and FlashAttention3 with more kernel-level optimizations with more aggressive fusion and memory access patterns. The success of FlashAttention is because of its unique insight into an opportunity that was created due to hardware limitations and rise of popularity of self-attention.</p> <h2 id="connecting-everything">Connecting everything!</h2> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17419187842427.jpg" alt=""/></p> <ol> <li>Outer Loop 1: There is inter-op parallelism with pipelining such as 1F1B</li> <li>Outer Loop 2: There is intra-op parallelism based on model architecture (Zero-2/3 and data parallelism or Megatron-LM tensor parallelism)</li> <li>Outer Loop 3: Gradient check pointing and recomputation at backward pass</li> <li>Inner Loop 4: Graph Fusion</li> <li>Inner Loop 5: Operator-level optimization: tiling, flash attention, etc</li> </ol> <h3 id="deepseek-v3">Deepseek-v3!</h3> <p>Deepseek-v3 was the first <strong>open-source</strong> model that reached the performance of GPT-4o! This was a big feat.</p> <p>Furthermore, they also made a bold claim of training the model with 100x lower cost! Many people argued that this is not a responsible claim since most of the budget is usually spent on the planning phase and not on the actual training phase. Previously it was all experimenting, and by the time Deepseek-v3 came into picture, many of these practices were set into stone. Either way, it was a big feat.</p> <h3 id="model-architecture">Model Architecture</h3> <p>People were familiar with Llama before this, but Deepseek had fundamentally changed things from minute-details to large architectural decisions. Firstly, they went big on <strong>Mixture-of-Experts (MoE)</strong>. Companies usually chose 8-10 MoEs in models, and Deepseek went ahead with 256 experts! This decision while introducing new challenges, significantly made model training and inference much more scalable.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17419191877713.jpg" alt=""/></p> <p>MoEs typically have much larger number of parameters requiring more communication between GPUs - we need expert parallelism with expert balancing.</p> <p>They introduced a new kind of attention known as <strong>Multi-head Latent Attention (MLA)</strong> that reduced the inference cost by a huge amount.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17419193632951.jpg" alt=""/></p> <p>They introduced a new vector \(c\) that essentially stores the KV values in a low-dimensional latent space. This trades memory for FLOPs and significantly reduces the KV Cache per token.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17419195081438.jpg" alt=""/></p> <p>They claim that MLA has much stronger ML capabilities.</p> <p>Another optimization in the model architecture is <strong>multi-token prediction</strong>. Inspired from the EAGLE series, they introduced speculative decoding in the pre-training phase. EAGLE claims 50% acceptance rate and the introduction of the speculative decoding into pre-training increases it to 80-85% acceptance! We essentially are getting a free speculation head.</p> <h3 id="system-optimization">System optimization</h3> <p>US infamously restricted the imports of the top NVIDIA GPUs (H100) to China, and Deepseek had to rely on a less powerful variant (H800) that does not have NVLink. Without NVLink, the communication overhead increases significantly. As a result, Deepseek cannot use tensor parallelism (all-reduce is \(N\times\) more complex than all-to-all). So they relied on pipeline parallelism, data parallelism and expert parallelism.</p> <p>We have also seen the issues with expert parallelism - need to have a good balancing. Typically, systems add an expert balancing loss. Instead, Deepseek argues that this loss hurts the pre-training and they use an alternative auxiliary-loss-free bias term.</p> <p>They use a sequence level balancing loss that encourages the expert load on each sequence to be balanced. (They said no loss but use another loss).</p> <p>So, overall, they have 16 way pipeline parallelism, 64 way expert parallelism, and ZERO-based data parallelism. Assuming they have 2048 GPUs (which they claimed), they have 16 PP, 64 EP and 2DP for a total of 2048 GPUs. Note that in each stage of inter-op parallelism, we need two all-to-all for EP and only one all reduce for 2DP.</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17419200930191.jpg" alt=""/> For pipeline parallelism, as we saw before, they introduced a new parallelism strategy on top of Chimera. Chimera required two copies of model weights in each rank. Chimera also has all-to-all communications that introduce some overhead. Let us look at the pipeline again -</p> <p><img src="/assets/img/2025-01-06-data-systems-for-ml/17419202751568.jpg" alt=""/></p> <p>To top all this, they made their own All-to-all communication kernel that does some extreme optimizations.</p> <p>Phew!</p> <p>Final set of innovations</p> <ul> <li>They made their own mixed-precision training standard different from the rest of the industry that reduced the usage from \(16N\) (we’ve seen this before) to \(13N\) using FP8 tensor core.</li> <li>Even more fine-grained quantization for FP8 kernels</li> <li>Prefill-decode disaggregation based inference.</li> </ul> <p>So, that is why Deepseek is big.</p> <h1 id="conclusion">Conclusion</h1> <p>LLMs are big, we’ve said this too many times now. But LLM systems is getting even bigger, and there are innovations happening much much faster in all fields both because of new paradigms and AI helping humans becoming more productive. All this article content can be replaced in 1-2 years. So, us, as researchers or developers, need to be able to identify the right problems, the ability to understand trends and the ability to predict the future! Identify the right papers and invest in the right companies.</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Prompting ChatGPT is not enough. To build large-scale AI systems, it is imperative to understand how to design the proper systems to optimize all the computations. The following blog is a deep-dive into system/data design for Machine Learning frameworks.]]></summary></entry><entry><title type="html">Reinforcement Learning Theory</title><link href="https://sudhansh6.github.io/blog/rl-theory/" rel="alternate" type="text/html" title="Reinforcement Learning Theory"/><published>2025-01-06T00:00:00+00:00</published><updated>2025-01-06T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/rl-theory</id><content type="html" xml:base="https://sudhansh6.github.io/blog/rl-theory/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>Through this article, we aim to understand the theory of reinforcement learning in the context of Large Language Models.</p> <p>Reinforcement learning addresses the domain of sequential decision problems, wherein an <em>agent</em> takes <em>actions</em> in an <em>environment</em>. These configurations are generally represented using <em>Markov Decision Processes</em> (MDP). An MDP is characterized by -</p> <ul> <li>Time - T - Discrete, infinite</li> <li>States - S - Discrete</li> <li>Actions - A - Discrete</li> <li> <p>Transitions - \(\tau : S \times A \to \Delta(S)\) - a probability distributions of states</p> \[P(s’ \vert s, a) = P_R[ \tau (s, a) = s’]\] <p>In more complicated setups, the transition function could be a function of the time \(T\) (stationarity and non-stationarity). In finite-time horizons, the time can be embedded inside the state itself, converting non-stationarity scenarios to stationarity ones.</p> </li> <li> <p>Reward - \(R: S \times A \times S \to \mathbb R: [-M, M]\). There is no need to make this non-deterministic since that is already generalized by \(\tau\). The rewards are usually bounded. The expected reward for an action is given by</p> \[R’(s, a) = \sum_{s’ \in S} R(s’, a, s) p(s’ \vert s, a)\] </li> <li>Initial state \(S_0\) - Can be a single state or a distribution over states.</li> <li>Discount Factor \(\gamma\) - A factor \(&lt;1\) to bound the total expected reward from the future. This will be better understandable from the text later.</li> </ul> <p>A <strong>policy</strong> \(\pi\) is a probability distributions over actions based on the current state. Consider a distribution \(\beta: S \to \delta(A)\), then a policy formally is</p> \[\pi(s, a) = P_R[\beta(s) = a]\] <p>A <strong>trajectory</strong> describes the sequence of states, actions of an agent in a <em>run</em> - \(s_0, a_0, r_0, s_1, a_1, r_1, \dots\).</p> <p>A policy is associated with a <strong>value function</strong> \(V_{\pi, t} (s) = \sum_{i = t}^\infty \gamma^{i - t} r_i\) Note that the dependence of the value of function on time is redundant in stationarity situations.</p> <p>The goal is to maximize this value function over all possible policies in an environment</p> \[\pi^* = \arg \max_{\pi} V_\pi (S_0)\] <p>This is known as the <em>Markov Decision Problem</em>. In a strict setting, the supremum may not exist (be a valid policy).</p> <p>How is Reinforcement Learning different from Supervised learning? The key differences are</p> <ul> <li>There is no sequentiality</li> <li>Every decision in SL has an associated reward, and there is no stochasticity</li> </ul> <p>Policies are optimized in multiple methods</p> <ul> <li>There are hill-climb methods - policy iteration, value iteration</li> <li>Learning based methods - think of a policy as a matrix that has to optimized to satisfy certain constraints.</li> </ul> <h2 id="bellman-equations">Bellman Equations</h2> <p>These are a recursive formulations of policies and value functions.</p> \[V_\pi(s) =3.142 s \pi(s, a)[ p(s’ \vert s, a) [R(s’, a, s) + \gamma V_\pi (s’)]\] <h2 id="the-markov-assumption">The Markov Assumption</h2> <p>Many processes in nature are <em>Markov</em> in nature - the action at the current state only depends on the current state, and not the history of states.</p> <p>A general policy is a function from trajectories \(H_t\) to a distribution over actions. However, such general representation is difficult to compute. Additionally, due to the Markov assumption, a stationary policy \(\pi: S \to \delta(A)\) is general enough for optimality. On surface, this may seem like we are limiting the power of an agent, but we will see that this is not the case. In fact, even a deterministic policy will do.</p> <p>How about reward? Are we limiting the reward function with a Markovian assumption? Since we consider expected rewards - the expectation value can be embedded within the state-based rewards as well. So in fact, both representations are the same.</p> <p>With a general policy, the probability of a trajectory is given by</p> \[P[S_0 = s_0, A_0 = a_0, \dots, S_t = s_t] = \mu(s_0) \pi(a_0 \vert s_0) P(s’ \vert s_0, a_0) \pi(A_1 \vert S_0, a_0, s_1) \dots\] <p>Continuing with the Bellman equations, our goal was to find a policy that maximizes the value function</p> \[\begin{align*} V_\pi(s) &amp;= \mathbb E_{P, \pi} \sum_{t = 0}^\infty \gamma^t [R(S_t, A_t) \vert S_0 = s] \max_{\pi} V_\pi(s) \\ &amp;= V^*(s) \\ \end{align*}\] <p>Firstly, does there exist a policy such that \(V_\pi(s) = V^*(s)\) for all \(s\)? An optimal policy for a particular state is guaranteed because of compactness, but is there such a policy for all states?</p> <p>To aid our analysis, we also define a <strong>Q-function</strong> \(Q: S \times A \to R\), that describes the expected reward \(Q(s, a)\) of taking an action \(a\) at state \(s\). This function is very similar to the value function but has an additional condition on the action as well. Why have both these functions? Convenience. Analogously, we also define \(Q^*(s, a) = \max_\pi Q_\pi (a, a)\).</p> <p><strong>Theorem (Existence theorem).</strong> There exists a stationary and deterministic policy that has the optimal value for all states.</p> <p>Therefore, Bellman equations are possible due to assumptions of Markovian nature of the policy and state transition functions. We have the following equations</p> \[\begin{align*} V_\pi(s) &amp;= R(s, \pi(s)) + \gamma \sum_{s’} P(s’ \vert a, \pi(s)) V_\pi (s’) \\ Q_\pi(s, a) &amp;= R(s, a) + \gamma \sum_{s’} P(s’ \vert s, a) \sum_{a’} \pi(a’ \vert s’) Q_\pi(s’, a’) \\ V_\pi(s) &amp;= \sum_a \pi(s) Q_\pi(s, a) \\ Q_\pi(s, a) &amp;= R(s, a) + \gamma \sum_{s’} P(s’ \vert s, a) V_\pi(s’) \end{align*}\] <h2 id="summary">Summary</h2> <p>The equation we’ve been working with for value iteration is</p> \[Q_{t + 1}(s, a) = L(Q_t) = R(s, a) + \mathbb E_{s’ \sim P(. \vert s, a)} \max_{a’} Q_{t}(s’, a’)\] <p>The key step here is to extract a policy from the current \(Q\) function. We noted that the optimal function \(Q^*\) satisfies the optimality equation.</p> <p>Aside from the optimality equation, we have the expectation equation that every \(Q\) function satisfies</p> \[Q_\pi(s, a) = R(s, a) + \mathbb E_{s’} \mathbb E_{a’ \sim \pi(\cdot \vert s’)} Q(s’, a’)\] <p>The advantage of looking at the optimality equation as an operation \(Q_{t + 1} = L(Q_{t})\) is that we can apply the contraction concepts to arrive at \(Q^*\) with Banach’s fixed point theorem. This way, we prove that there is a unique optimal \(Q\) function.</p> <p>Now we show that, after enough number of iterations, we can also get the value function \(V\) arbitrarily close the optimal value. All these subtleties together show that the value iteration algorithm works!</p> <p>How do obtain these bounds based on iterations? We need to find an upper bound for \(\| Q_{t} - Q^* \|_\infty\). We can show that this value is \(leq \|Q^* - Q_0 \|_\infty\). Assuming we start with a \(Q\) with all zeroes, the maximum value of \(Q^*\) is simply \(R_{\max}/(1 - \gamma) = (\max_{s, a} \vert R(s, a)\vert)/(1 - \gamma)\).</p> <p><em>Lemma.</em> \(\|V_m(s) - V^*(s)\| \leq \frac{2}{1 - \gamma}\| Q_m - Q^*\|_\infty\).</p> <p><em>Proof.</em></p> \[\begin{align*} V^*(s) - V_m(s) &amp;= Q^*(s, \pi^*(s)) - Q_m(s, a = \pi_m(s)) \\ &amp;= Q^*(s, \pi^*(s)) - Q^*(s, a) + Q^*(s, a) - Q_m(s, a) \\ &amp;= Q^*(s, \pi^*(s)) - Q^*(s, a) + \gamma \mathbb E_{s’} (V^*(s’) - V_m(s’)) &amp;\leq Q^*(s, \pi^*(s)) - Q^*(s, a) + \gamma \|V^* - V_m\|_\infty \\ &amp;\leq (Q^*(s, \pi^*(s)) - Q_m(s, \pi^*(s))) + (Q_m(s, \pi^*(s)) - Q^*(s, a)) + \gamma \|V^* - V_m \|_\infty \\ &amp;\leq (Q^*(s, \pi^*(s)) - Q_m(s, \pi^*(s))) + (Q_m(s, a) - Q^*(s, a)) + \gamma \|V^* - V_m \|_\infty \\ &amp;\leq 2\|Q^* - Q_m\|_\infty + \gamma \|V^* - V_m\|_\infty \\ \|V^* - V_m \|_\infty \leq \frac{2}{1 - \gamma} \|Q^* - Q_m \|_\infty \end{align*}\] <h1 id="policy-iteration">Policy Iteration</h1> <p>Instead of modifying the policy based on the current value, why not do it the other way round? Iterate over the policy, get its value and improve it again? There is a subtle different as compared to the previous algorithm, and it turns out that this method is much more efficient!</p> \[\pi_0 \underset{Q_0}{\longrightarrow} \to \pi_1 \to \cdots \to \pi_k \underset{Q_k}{\longrightarrow} \pi_{k + 1}\] <p>Policy iteration takes \(\mathcal O(\vert S\vert^3 + \vert S \vert^2 \vert A\vert)\) whereas value iteration is \(\mathcal O(\vert S \vert^2 \vert A\vert)\).</p> <h1 id="model-free-methods">Model-free methods</h1> <p>Policy iteration and Value iteration are closely related to each other. For both the algorithms, we need to evaluate a policy to find the corresponding value function. However, in many cases, we do not know the exact transition and reward functions. In other cases, the environment can have a large number of states, making it impossible to model it.</p> <p>For such situations, we rely on <strong>Monte-Carlo methods</strong>. Any method that solves a problem by generating suitable random numbers and observing that a fraction of numbers obey some property or properties, can be classified as a Monte Carlo method. The key ideas here are using a <em>sampling technique</em> for a heuristic <em>estimator</em>. These methods do not make use of the Markov assumption much, making them much more generalizable.</p> <p>The idea is to learn directly from episodes of experience without a prior knowledge of MDP transitions (rewards). Since the idea relies on episodes, one caveat is that it can only be applied to <em>episodic MDPs</em> - episodes have to terminate.</p> <h2 id="prediction-problem">Prediction Problem</h2> <p>Let us consider the first problem - estimating \(V_\pi(s)\) for a state. Instead of updating after every action, we update after each episode by taking the mean reward across all sample trajectories sampled from this stage. The <em>first-visit</em> algorithm is given by</p> \[\tilde V_\pi(s) = \frac{1}{m} \sum G_i^s\] <p>Where \(G_i^s\) is the total reward after \(s\) first appears in the episode. Even though the number of states is large, it’s nonetheless finite. Using this fact, we can show theoretically that the value above can be bounded. The convergence time is also associated with the underlying transition probabilities (rare states require more episodes to appear in the trajectory).</p> <p>There are more questions to answer. For example, is this a biased or unbiased estimator?</p> \[\begin{align*} \mathbb E(\tilde V_\pi(s)) &amp;= \frac{1}{m} \sum_i \mathbb E(G_i^s) \\ &amp;= V_\pi(s) \quad \because \text{ Markovian assumption} \end{align*}\] <p>How about doing a second-visit algorithm or every-visit algorithm? They are valid approaches too, the theoretical analysis slightly varies. The estimators may not be unbiased but they use the data more efficiently. That is useful in cases where sampling is expensive. The <em>every-visit algorithm</em> typically has a higher bias (due to dependencies in an episode for the occurrences) but lower variance and higher efficiency.</p> <p>These differences are important to understand. For example, in the latest <a href="https://openai.com/index/openai-o3-mini/">o3-mini</a> model, they observed that the every-visit variant of an RL algorithm obtained much better performance than the first-visit variant.</p> <p>In contrast, consider estimating the \(Q_\pi(s, a)\) function. A policy might choose a certain action for a given state. However, to get all the Q-values, the policy must account for exploring all actions at different states to get a good estimate. The exploration probability is captured by the <strong>epsilon-greedy class of algorithms</strong>.</p> <h2 id="temporal-difference-tdlambda-algorithms">Temporal Difference TD(\(\lambda\)) Algorithms</h2> <p>The idea of this class of algorithms is to improve the policy as we keep exploring the environment more.</p> <p>The first variant of these algorithms is TD(0) - We sample state transition from the trajectory (one from each) and update the existing value function based on the action and reward obtained. Formally, given \(V_t(.)\) and a sample from the trajectory \((s_t, a_t, r_t, s_{t + 1})\), how do we obtain \(V_{t + 1}(s_t)\)?</p> <p>From the Bellman’s equations, we have</p> \[V_\pi(s) = \mathbb E_{a \sim \pi(s)} [R(s, a) + \gamma \mathbb E_{s’ \in P(s, a)} V_\pi(s’)]\] <p>So, can we do</p> \[V_{t + 1}(s_t) \gets r_t + \gamma V_t(s_{t + 1})\] <p>This equation omits our previous estimate \(V_t(s_t)\). How do we use it? We can do a sort of an averaging or gradient descent</p> \[\begin{align*} V_{t + 1}(s_t) &amp;\gets V_t(s_t) + \alpha_{s_t} \delta_t \\ &amp;\delta_t = r_t + \gamma V_t(s_{t + 1}) - V_t(s_t) \end{align*}\] <p>In essence, we are averaging over the previous visits in the trajectory but with slightly different update rules.</p> <p>How do we choose the \(\alpha\)’s? For convergence purposes, we require \(\sum_{t = 0}^{\infty} \alpha(t) \to \infty\) and \(\sum_{t = 0}^\infty \alpha^2(t) &lt; \infty\). These parameters are important for controlling the bias and variance of the estimators as we will discuss later. Possible learning schedules include \(\alpha(t) = \frac{1}{t^{0.5 + \epsilon}}\) with \(0 &lt; \epsilon \leq 0.5\) and $$\alpha(t) = \frac{1}{\sqrt{t}\log t}.</p> <p>With these learning schedules, you can prove the convergence thinking of the process as a contraction. It converges to the Bellman’s equation for some new value \(\gamma\).</p> <p>To compare various statistical approaches (Monte Carlo methods), we need to compare the bias, variance, convergence and sampling complexity into account. To improve on some of these criteria, we have the general TD(\(\lambda\)) algorithms.</p> <p>Instead of considering a single transition, the general algorithm considers more transitions to update the value function.</p> \[V^k_t = r_t + \gamma r_{t + 1} + \dots + \gamma^{k} r_{t + k} +\gamma^{k + 1} V_t{s_t + k}\] <p>What is the advantage of this approach? We slowed the updates, which seemingly increases the bias but may decrease the variance. Another way to understand TD(\(\lambda\)) is to think of it as a combination of Temporal Difference and Monte Carlo learning. It is an average of \(k\)-step returns. Kind of <em>truncated Monte Carlo method</em>.</p> <p>What’s more? We can generalize the above mention equation a bit more. For different states, we can consider different \(k\)’s, and take the average of them.</p> \[V_t^\lambda(s_t) = (1 - \lambda)\sum_{i = 0}^\infty \lambda^i V_t^k(s_t)\] <p>where the update algorithm is</p> \[V_{t + 1} \gets V_t(s_t) + \alpha_t (V_t^{\lambda} (s_t) - V_t(s_t))\] <p>It reduces the variance because we are considering a a geometric weighted mean. Since longer windows \(k\) have higher variance, we reduce their weight in the average with \(lambda\).</p> <p>In practice, when we approach a new state, we take our current estimate of \(V_t\), update all our previous calculations to recompute the new value function. The convergence proofs for these algorithms is not mathematically rigorous. It is a good area of research to find better proofs or better yet, more efficient algorithms.</p> <p>How do we execute these algorithms in practice? Theoretically, setting the upper limit of \(j\) to \(\infty\) is the best estimate. However, \(\lambda\) acts like a discount factor, and if the rewards are sparse (towards the end of the episode), then the convergence would take a long time - <em>episodic</em> algorithms.</p> <p>In essence, this version of the algorithm requires many forward executions of the simulation. In many cases, since this is infeasible in practice, developers have started using a <em>backwards version</em> of the algorithm.</p> <p>We define the eligibility trace of a state \(e_t(s) = \gamma \lambda e_{t - 1}(s) + 1\). The update equation becomes</p> \[V_{t + 1}(s) \to V_t(s) + \alpha \delta_t e_t(s)\] <p>where \(\delta_T = R_t + \gamma V_{t + 1} (S_{t + 1}) - V_{t - 1}(S_t)\).</p> <p>These equations are almost same as the previous algorithm but with better practice implementation. (When you expand the formulae, and interchange the summations, this is what you get).</p> <h1 id="policy-gradient-methods">Policy Gradient Methods</h1> <p>The algorithms we have seen are for discrete MDPs; that is, the state-action space is discrete. However, many applications in real—life deal with high-dimensional spaces or continuous representations as we discussed before. In such cases, we need a more general case of algorithms arrive at the optimal policies in a computationally feasible manner.</p> <h2 id="policy-representations">Policy Representations</h2> <p>The generalized representation of a policy that seems to have worked for us is a parameterized distribution for \(V, Q\) or \(\pi\). For example, \(\pi\) could be a Gaussian probability distribution with \(\mu, \sigma\) as the parameters. With this, thus, we move on from the tabular representations to a more generalized one.</p> <p>However do we convert representations to probabilities? One simple idea is as follows. Consider a policy parameterized with \(\theta \in \mathbb R^d\). Then, for every state (continuous), think of mapping the actions to a real-space.</p> \[\phi_s : A \to \mathbb R^d\] <p>The probabilities can be obtained from \(\text{softmax}(Q_s(a)^T \cdot \theta)\). It is one of the simplest ideas, a linear approach, to arrive at the probabilities.</p> <p>There are some things to keep in mind when working with this new paradigm. If the representation space is not chosen properly, then a small change in the representation space, resulting in two different policies, can have a large difference in terms of the value functions. It can cause stability issues and other problems while using policy gradient approaches.</p> <h2 id="policy-updates">Policy updates</h2> <p>So, with this representation, how do we obtain the optimal policy?</p> <p>The loss function is of the form \(J(\theta) = \sum_s \mu(s) V_\pi(s)\). You can imagine, \(\theta\) being a neural network and how this approach scales up.</p> \[\begin{align*} \nabla J(\theta) &amp;= \frac{\partial J(\theta)}{\partial \theta} \\ &amp;= \sum_s \mu(s) \frac{\partial V_\pi(s)}{\partial \theta} \end{align*} \\ &amp;= \sum_s d^{\pi}(s) \sum_a Q_\pi(s, a) \frac{\partial}{\partial \theta} \pi(s, a, \theta)\] <p>where \(d^\pi(s) = \sum_{k \geq 0} \gamma^k Pr(S_k = s)\) - how likely does the state \(s\) appear in a Markov chain. Although it seems complicated, it can be computed easily.</p> <blockquote> <p>Do we sum across all states and actions again? We are trading space for computation time.</p> </blockquote> <p>Let us simplify the equation a bit more - \(\begin{align*} \nabla J(\theta) = \sum_s d^\pi(s) \sum_a \pi(s, a, \theta) Q_\pi(s, a) \frac{\partial}{\partial \theta} \ln \pi(s, a, \theta) \end{align*}\)</p> <p>This equation form helps us to think about it in terms of expectations of the policy. Let us derive these equations. We start with the definition of \(J(\theta)\)</p> \[\begin{align*} \frac{\partial}{\partial \theta} &amp;= \sum_s Pr(S_o = s) \frac{\partial}{\partial \theta} V(s) \\ \frac{\partial}{\partial \theta} V(s) &amp;= \sum_a Q(s, a) \frac{\partial}{\partial \theta} \pi(s, a, \theta) + \sum_a \pi(s, a, \theta) \frac{\partial}{\partial \theta} Q(s, a) \\ \frac{\partial}{\partial \theta} Q(s, a) \gamma &amp;= \sum_{s’} P(s, a, a’) \frac{\partial}{\partial \theta} V(s’) \\ \frac{\partial}{\partial \theta} V(s) &amp;= \sum_a Q(s, a) \frac{\partial}{\partial \theta} \pi(s, a, \theta) + \gamma \sum_a \pi(s, a, theta) \sum_{s’} P(s, a, s’) \frac{\partial}{\partial \theta} V(s’) \\ &amp;= \sum_a Q(s, a) \frac{\partial}{\partial \theta} \pi(s, a, \theta) + \gamma \sum_{s’} P[s_1 = s’ \vert s_0 = s] \frac{\partial}{\partial \theta} V(s’) \\ \frac{\partial}{\partial \theta} J(\theta) &amp;= \sum_s Pr(s_0 = s) (\sum_a Q(s, a) \frac{\partial}{\partial \theta} \pi(s, a, \theta) + \gamma \sum_{s’} P[s_1 = s’ \vert s_0 = s] \frac{\partial}{\partial \theta} V(s’)) \\ &amp;= \sum_{s, a} Pr(s_0 = s)Q(s, a)\frac{\partial}{\partial \theta} \pi(s, a, \theta) + \gamma \sum_{s’} P(s_1 = s’) \frac{\partial}{\partial \theta} V(s’)) \\ \end{align*}\] <p>We have unrolled the recursion once, but unrolling (tail recursion) it infinite times, would lead us to the equation we had before</p> \[\frac{\partial}{\partial \theta} J(\theta) &amp;= \sum_{s} d^\pi(s) \sum_a Q(s, a)\frac{\partial}{\partial \theta} \pi(s, a, \theta)\] <h2 id="optimal-policies">Optimal policies</h2> <blockquote> <p>Using this formula, how quickly do we reach the local/optimal policy? What is the inductive bias to design the neural network architecture? Can we formulate better loss functions? How do effectively perform the computations? <em>Need to research about this…</em></p> </blockquote> <p>Let us try answering the computational feasibility question. In a way, performing gradient ascent on the gradient formula we derived previously is analogous to performing gradient descent on the complete data set in supervised training. We can do better with a technique analogous to stochastic gradient descent (this is my understanding that could be incorrect), using Monte Carlo methods to perform the ascent on an estimate of the gradient \(\nabla J(\theta)\). How do we get a Monte Carlo method out of the equation?</p> \[\begin{align*} \nabla J(\theta) &amp;= \sum_{t geq 0} \mathbb E_{S_t, A_t} Q(S_t, A_t) \frac{\partial}{\partial \theta} \ln (S_t, A_t, \theta) \\ &amp;\approx \mathbb E_{S_t, A_t} \underbrace{\left(\sum_{t geq 0} Q(S_t, A_t) \frac{\partial}{\partial \theta} \ln (S_t, A_t, \theta) \right)}_{\text{samples}}\\ \end{align*}\] <p>So, we take the average of the quantity \(Q \cdot \ln (\pi)\) over multiple episodes using Monte Carlo sampling. These equations form the basis of the <strong>REINFORCE</strong> algorithm.</p> <p>We further analyze this equation - how do we reduce the variance of the <em>unbiased</em> estimator? We use something known as a <strong>controlled variate approach</strong> wherein for an estimator of the random variable \(X\), we build another random variable \(Y\) such that</p> \[\begin{align*} var(Y) &amp;&lt; 2cov(X, Y) \\ var(X - Y + \mathbb E(Y)) &amp;= var(X - Y) \\ &amp;= var(X) + var(Y) - 2cov(X, Y) &lt; var(X) \end{align*}\] <p>The positively correlated random variables we choose are</p> \[\begin{align*} X &amp;= G_t = Q(S_t, A_t) = R_t + \gamma T_{t + 1} + \gramma^2 R_{t + 1} \\ Y &amp;= b(S_t) \end{align*}\] <p>where \(b(S_t)\) is \(V(S_t)\) obtained from \(TD(\lambda)\). The term \(Q(S_t, A_t) - V(S_t)\) is called the <strong>advantage</strong>! The equation is modified to</p> \[\begin{align*} \nabla J(\theta) &amp;= \mathbb E \left[\sum_{t = 0}^L (G_t - b(S_t)) \frac{\partial}{\partial \theta} \ln \pi(S_t, A_t, \theta) \right]\\ &amp;= \mathbb E \left[\sum_{t = 0}^L G_t \frac{\partial}{\partial \theta} \ln \pi(S_t, A_t, \theta) - \sum_{t = 0}^L b(S_t) \frac{\partial}{\partial \theta} \ln \pi(S_t, A_t, \theta)\right] \\ &amp;= \mathbb E \left[\sum_{t = 0}^L G_t \frac{\partial}{\partial \theta} \ln \pi(S_t, A_t, \theta)\right] - \underbrace{\sum_{t = 0}^L \sum_s P[S_t = s] b(S_t) \frac{\partial}{\partial \theta} \sum_a \pi(S_t, A_t, \theta \vert A_t = a, S_t = s)}_{=0} \\ &amp;= \sum_{t = 0}^L G_t \frac{\partial}{\partial \theta} \ln \pi(S_t, A_t, \theta) \end{align*}\] <p>We can decrease the variance further but it may introduce bias in the estimator. There are many research works that explore these variations of the algorithm based on the outcome required.</p> <p>Apart from this, there are other implementation tricks that seemed to be more effective in practice. For example, removing the discount factor from the REINFORCE equation yields more stable and faster training.</p> <h1 id="language-models">Language Models</h1> <p>Recently, RL techniques have been applied extensively to language models to improve their performance. What are the ideas behind it?</p> <p>A language model (LLM) can be thought of as an agent in an environment. We (humans) give it feedback, and thereby we constitute the environment. The state of the LLM is the tokens generated so far, and the actions essentially are the set of tokens it can generate in the next instance. The probabilities associated with each token can be considered as the LLM policy (the language model itself encodes the policy). So, we have given an MDP formulation for the LLM. Can we use RL?</p> <p>The problem with RL is that it does not scale well to high dimensions. With LLMs, the number of tokens is very large and rewards are super-sparse. We only give feedback to the LLM at the end of a generated sentence rather than for each token. That makes the exploration of this space to find the optimal policy (language model) very intractable. Due to this, we have resorted to SFT (supervised fine-tuning) techniques to encode an initial policy. By training an LLM with internet text data, we are constraining the search domain implicitly by developing a really good initial policy. This policy (language model) can then be used with RL techniques to improve even further.</p> <p>The current state of research is at this intersection - How do we improve the credit assignment to make it more fine-grained (token-wise)? Obtaining human feedback continuously is not feasible. Human feedback is subjective, inconsistent with time and sometimes without any rational basis. So some works try to train a reward model learning from human feedback, and that can be used with the LLM for RL. There’s also the problem of catastrophic forgetting in smaller LLMs.</p> <h2 id="overview-of-llm-training">Overview of LLM training</h2> <p>There are three major steps involved in building models like Instruct-GPT.</p> <h3 id="pre-training">Pre-training</h3> <p>The model with randomized weights is trained on a large corpus of data in a self-supervised manner. This step essentially involves extracting large amounts of text, and training models like GPT for next-token prediction. Note that it does not require any labels. Intuitively, we are teaching the model human languages and making it learn how to generate meaningful sentences. The datasets have of the order trillions of tokens.</p> <h3 id="super-vised-fine-tuning">Super-vised Fine-tuning</h3> <p>In this step, we train the model with <em>prompts</em> and <em>answers</em>. We teach the model with specific set of questions and the corresponding answers. Again, there are many ways in which this step is done, some developers only train only a few layers of the model or use some other fine-tuning techniques like LoRA or Adapters. The datasets are much smaller but still consist millions of tokens.</p> <h3 id="reinforcement-learning-with-human-feedback-rlhf">Reinforcement-learning with Human Feedback (RLHF)</h3> <p>The most important aspect as we have discussed previously, is defining the reward. Many works are based on cognitive and neuroscience techniques. One popular observation is that humans are good at giving preferences over assigning numerical scores to choices. So, with this motivation, we ask the LLM to generate multiple responses to a given prompt. The human can then give a preference or a ranking order as a feedback to the model. These preferences can be related to scores for the reward model using the Bradley-Terry and Placket-Luce models.</p> <h1 id="alphazero">AlphaZero</h1> <p>Prior to 2014, researchers came up with specific rules to master board games. For example, they created very sophisticated Chess engines (e.g., <a href="https://stockfishchess.org">Stockfish</a>) and developed it iteratively. The issue with such games is that the state-space is very large (a computationally infeasible number). Training reinforcement learning algorithms (most suitable since these are sequential series of steps) is difficult since the rewards are usually given at the end of the plays. A single reward for a whole trajectory with states and actions is too sparse to train any useful policy.</p> <blockquote> <p>Hierarchical action space with rewards</p> </blockquote> <p>Addressing this, <a href="https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/">AlphaZero by Deepmind</a> was proposed in 2014 leveraging the advancements in neural networks. Given a state as an input, the neural network returns the optimal action and expected value for that state.</p> <p>We take the examples of trajectories from the game \(\{s_i, \pi(s_i), V(s_i)\}\) and minimize the following loss function \(\mathcal L = (V_\theta(s_i) - V(s))^2 - \pi(s_i)^T \log \pi_\theta (s_i) + c \|\theta\|^2\)</p> <blockquote> <p>Note that cross-entropy loss is the same as KL Divergence - we only consider the trainable part for the purposes of back-propagation.</p> </blockquote> <p>It is easy to see why this may work. Along with this, they introduced <em>self-play</em> to iteratively and autonomously improve the policy. For this to work, the agent has to explore the state-action space while exploiting the policy. The trade-off is implemented through a KL-UCB based action choice</p> \[U(s, a) = Q(s, a) + c \pi_\theta (s, a) \frac{\sqrt{\sum_b N(s, b)}}{N(s, a) + 1}\] <p>where \(N(s, a)\) represents the number of times we have chosen \(a\) in state \(s\).</p> <h1 id="multi-arm-bandit-problems">Multi-Arm Bandit Problems</h1> <p>The multi-armed bandit problem involves a decision maker iteratively selecting one of multiple fixed choices when the properties of each choice are only partially known at the time of allocation. More formally, given \(K\) bandits/actions/choices each returning a value from \(\{0, 1\}\) guided by a distribution \(\mu_k\). The goal of the decision maker is to choose the bandits so that the returned value is the highest.</p> <p>Since the player does not known the underlying distributions, they have to learn about the properties through experience. Ideally, they may greedily sample the bandit having the highest \(mu_k\), and the estimate for this distribution is built over time. The player has a tradeoff between exploitation of the choice that gives the highest expected payoff and exploration to get more information about the expected payoffs of the other machines.</p> <p>To quantify these choices, we introduce a quantity called <strong>regret</strong> - defined as the expected different between the reward sum associated with an optimal strategy and the sum of the collected rewards. So we define the following quantities</p> \[\begin{align*} N_k(T) &amp;= \sum_{t = 1}^T 1 [I(t) = k] \\ W_k(T) &amp;= \sum_{t = 1}^T X_k(t) 1 [I(t) = k] \\ \hat \mu_k(T) &amp;= \frac{W_k(T)}{N_k(T)} \\ W(T) &amp;= \sum_{k = 1}^K W_k(T) \\ \mathbb E(R(T)) = T \mu_1 - \sum_k \mu_k N_k(T) \end{align*}\] <p>We say \(k = 1\) is the best choice without any loss of generality. Our goal is minimize the expected regret \(\mathbb E(R(T))\).</p> <p>Is it possible to make the regret sub-linear? The Upper Confidence Bound (<strong>UCB</strong>) algorithm achieves logarithmic regret in terms of \(T\), and that is the best we have so far.</p> <p>Why do we define regret as the metric, and not something else? It could be that this is the simplest and the most flexible and intuitive metric for this problem. Furthermore, we have considered stationary distributions. What about non-stationary ones? There is one detailed work for this with very rigorous theory by <a href="https://arxiv.org/pdf/2101.08980">Wei et. al.</a>.</p> <p>So within our stationary setup, let us see why the UCB algorithm works. For each bandit we estimate a range \([a, b]\) for \(\hat \mu_k(t)\) such that \(P[\mu_k \in [a, b]] \geq 1 - \delta\). That is, we want to be able to estimate the properties of the bandits with a certain confidence. Given that we are dealing with Bernoulli variables, we have the following result</p> \[\begin{align*} P[\mu \geq \frac{1}{n} \sum Y_i + \epsilon] &amp;\leq e ^{-2n \epislon^2 \\ \end{align*}\] <blockquote> <p>The proof for the above Hoeffding’s inequality is to consider the probability nCk mu^k (1 - mu)^(n - k)</p> </blockquote>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[A deep dive of the basic RL theory and how we used them in modern ML systems.]]></summary></entry><entry><title type="html">AI Agents</title><link href="https://sudhansh6.github.io/blog/ai-agents/" rel="alternate" type="text/html" title="AI Agents"/><published>2025-01-06T00:00:00+00:00</published><updated>2025-01-06T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/ai-agents</id><content type="html" xml:base="https://sudhansh6.github.io/blog/ai-agents/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>The content on this article is based on the <a href="https://github.com/pearls-lab/ai-agents-course">course</a> by <a href="https://prithvirajva.com">Prof. Prithviraj</a> at UC San Diego. This <a href="https://www.kaggle.com/whitepaper-agents">whitepaper</a> about AI Agents by Google is also a good read.</p> <h2 id="what-is-an-agent">What is an agent?</h2> <p>Agent is an entity with <em>agency</em>. <a href="https://minecraft.wiki/w/Agent">A Minecraft agent?</a>. Agents see applications within the workspaces in the form of workflow automations, household or commercial robotics, software development and personal assistants. Generally, the theme is that <em>agents</em> take actions.</p> <p>Historically, the use of agents started in the early 1900s in the field of control theory. They were used for dynamic control of flight systems, and in 1940s it expanded to flight guidance, etc. By the 1950s the concepts of MDPs and dynamic programming were being expanded to many use cases. Surprisingly, one of the first natural language chatbots, Eliza, was created as a psychotherapist simulator in the 1960s! Finally, reinforcement learning became a field of study in the 1990s for sequential decision making.</p> <h2 id="sequential-decision-making">Sequential Decision Making</h2> <p>These tasks are different from other ML problems like classification. A model that has an accuracy of 99% at each step, has a cumulative accuracy of ~30% after 120 steps!</p> <p>These problems are formalized as a Markov Decision Process - an <strong>agent</strong> performs <strong>actions</strong> in an <strong>environment</strong>, and in turn receives <strong>rewards</strong> as feedback. These configurations are distinguished as <strong>states</strong>, and the whole process can be seen as sequential decision making.</p> <p>The core components of an agent, often agreed on, are</p> <ul> <li><strong>Grounding</strong> - Language is anchored to <em>concepts</em> in the world. Language can be grounded to different forms of information systems - images, actions and cultural norms. <ul> <li>Agency (ability to act) - At each state, an agent needs to have multiple choices to act. <em>If an agent has to select what tools to use but there’s always only one tool, is that agency?</em> The action space has to be well-defined to look for agency. Although there is a single tool call, different parameters for the tool call can probably be considered as different actions. Actions can be defined as something the agent does and changes the environment. The distinction between an agent and environment is not very clear in many cases. Although, our approximations mostly serve us well.</li> <li>Planning (Long horizon)</li> <li>Memory - <ul> <li>Short-term - What is the relevant information around the agent that it needs to use to act now</li> <li>Long term - What information has the agent already gathered that it can retrieve to take an action</li> </ul> </li> <li>Learning (from feedback) - Doesn’t necessarily always mean <em>backpropagation</em>.</li> </ul> </li> <li><strong>Additional</strong> - <ul> <li>Embodiment (physically acting in the real-world). <em>Embodied hypothesis</em> - embodiment is necessary for AGI.</li> <li>Communication - Can the agent communicate its intentions to other agents. Very necessary pre-requisite for multi-agent scenarios.</li> <li>World Modeling - Given the state of the world and an actions, predict the next state of the world. Is <a href="https://deepmind.google/technologies/veo/veo-2/">Veo</a>/<a href="https://sora.com">Sora</a> a world model? It is an attempt for world model since they have no verifiability. <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie</a> is another such attempt. So is <a href="https://genesis-embodied-ai.github.io">Genesis</a> - this is much better if it works.</li> <li>Multi-modality - The clean text on the internet is only a few terabytes, and our models have consumed it (took use 2 decades though). YouTube has 4.3 Petabytes of new videos a day. CERN generates 1 Petabyte a day (modalities outside vision and language). Some people believe this form of scaling is the way to go. There are more distinctions -</li> </ul> </li> </ul> <table> <thead> <tr> <th>Model</th> <th>AI System</th> <th>Agent</th> </tr> </thead> <tbody> <tr> <td>GPT-4</td> <td>ChatGPT</td> <td>ChatGPT computer use</td> </tr> <tr> <td>Forward passes of a neural net</td> <td>Mixing models together</td> <td>Has agency</td> </tr> </tbody> </table> <p>It is important to remember that not every use case needs an agent and most use cases just need models or AI systems. <em>Occam’s razor</em>.</p> <h1 id="simulated-environments-and-reality">Simulated Environments and Reality</h1> <p>Why do we need simulations? Most tasks have many ways of completing them. There is no notion of <em>global</em> optimal solutions ahead of time but usually known once the task is complete.</p> <p>The agent needs to explore to find many solutions to compare and see what is the most efficient. However, exploration in the read world is expensive - wear and tear of robots, excessive compute, danger to humans, etc.</p> <p>Simulations offer an easy solution to these problems. Assign a set of rules, and let a world emerge. One of the early examples of this is <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Conway’s Game of Life</a> which theorized that complicated behaviors can emerge by just a few rules.</p> <p>From an MDP perspective, a simulation contains \(&lt;S, A, T&gt;\) where</p> <ul> <li>\(S\) is the set of all states. It consists of propositions that are true/false. Example: You are in a house, door is open, knife in drawer</li> <li>\(A\) is the set of all actions. Example: Take knife from drawer, walk through door</li> <li>\(T\) is the transition matrix - (You are in the house, you walk out of the door) -&gt; You are outside the house.</li> </ul> <p>A simulation need not have an explicit reward.</p> <h2 id="sim2real-transfer">Sim2Real Transfer</h2> <p>The ability of an agent trained in simulation transfer to reality is dependent on how good the model extrapolates out of distribution. With the current stage of agents, the simulation is made as close to reality as possible to reduce the Sim2Real gap.</p> <p>How do we measure closeness to reality? The tasks in the real world have different types of complexities -</p> <ol> <li>Cognitive complexity - Problems that requires long chains of <em>reasoning</em> - puzzles, math problems or moral dilemmas</li> <li>Perceptive complexity - Requires high levels of vision and/or precise motor skills - bird watching, threading a needle, Where’s Waldo</li> </ol> <p>Examples of simulations -</p> <ol> <li>Grid world - low cognitive and almost zero perceptive. However, this idea can arbitrarily scale to test algorithms for their generalization potential in controllable settings.</li> <li>Atari - low perceptive, medium cognitive. Atari games became very popular in 2013, when Deepmind released their <a href="https://arxiv.org/pdf/1312.5602">Deep Q-Net</a> paper that achieved human level skills on these games.</li> <li><a href="https://en.wikipedia.org/wiki/Zork">Zork</a>, <a href="https://www.nethack.org">NetHack</a> - low perceptive, high cognitive. These are configurations or worlds that you purely interact with text. The worlds are actually so complex that there is no agent that is able to finish the challenge!</li> <li><a href="https://cs.stanford.edu/people/jcjohns/clevr/">Clevr simulation</a> - medium perceptive, low cognitive - This simulation generates images procedurally with certain set of objects and has reasoning questions for each image.</li> <li><a href="https://ai2thor.allenai.org">AI2 THOR</a> - medium perceptive, medium cognitive. Worlds with ego-centric views for robotics manipulation and navigation simulations</li> <li><a href="https://arxiv.org/pdf/2407.18901">AppWorld</a> - medium perceptive, medium cognitive. A bunch of different apps that you would generally use in daily life. The agents can access apps, and the simulation also has human simulators. This simulation is one that is closest to reality in the discussed so far!</li> <li><a href="https://www.minecraft.net/en-us">Minecraft</a> - medium perceptive, high cognitive. A voxel based open-world game that lets players take actions similar to early-age humans.</li> <li><a href="https://mujoco.org">Mujoco</a> - high perceptive, low cognitive. It is a free and open source physics engine to aid the development of robotics.</li> <li><a href="https://ai.meta.com/research/publications/habitat-a-platform-for-embodied-ai-research/">Habitat</a> - high perceptive, medium cognitive. A platform for research in embodied AI that contains indoor-world ego-centric views similar to AI2 THOR, but with much better graphics. They have recently added sound in the environment too!</li> <li> <p>High perceptive, high cognitive - Real world, and whoever gets this simulation right, wins the race to AGI. It requires people to sit down and enumerate all kinds of rules. Game Engines like Unreal and Unity are incredibly complex, and are the closest we’ve gotten.</p> <p>Some researchers try to “learn” the simulations from real-world demonstrations.</p> </li> </ol> <p>In each of these simulators, think of the complexity and reward sparsity in the environment. It is easy to build a simulator that gives rewards at a goal state than the one that gives a reward for each action. There are some open-lines of research in this domain -</p> <ol> <li>Which dimensions of complexity transfer more easily? Curriculum learning</li> <li>Can you train on lower complexity and switch to a higher complexity?</li> <li>Can we learn the world model holy grail?</li> </ol> <h2 id="how-to-make-simulations">How to make simulations?</h2> <p>As we’ve seen, simulations can range from games to real-world replications with physics involved. Most simulations are not designed keeping AI in mind. However, with the current state of AI, this is an important factor to keep in mind.</p> <p>Classical environments like in Zork/AI2 Thor/Mujoco have something known as <strong>PDDLs</strong>. Some simulations are built through AI, like <em>AI Dungeon</em> that spins up worlds for role-play games.</p> <h3 id="planning-domain-definition-language-pddl">Planning Domain Definition Language (PDDL)</h3> <p>Standard encoding for classic planning tasks. Many specific languages for creating simulations have similarities with PDDL.</p> <p>A PDDL Task consists of the following</p> <ul> <li>Objects - things in the world that interest us</li> <li>Predicates - Properties of objects that we are interested in, can be true or false</li> <li>Initial state - The state of the world that we start in</li> <li>Goal specification - Things that we want to be true</li> <li>Actions/Operators - Ways of changing the state of the world.</li> </ul> <p>These are split across two files - domain and problem <code class="language-plaintext highlighter-rouge">.pddl</code> files.</p> <p>Classic symbolic planners read PDDLs and give possible solutions. Checkout the <a href="https://planning.wiki/ref/planners/atoz">Planning.wiki</a>. In many cases these planners are used over reinforcement learning due to lack of algorithmic guarantees.</p> <p>There were other attempts</p> <h1 id="search-for-planning-in-simulations">Search for Planning in Simulations</h1> <h1 id="reinforcement-learning-abridged">Reinforcement Learning (Abridged)</h1> <p>We have been building chronologically, and next in line is basic RL.</p> <h2 id="terminology">Terminology</h2> <ul> <li> <p>A <strong>policy</strong> is a function that defines an agent’s behavior in the environment. Finding the optimal policy is known as the <em>control</em> problem.</p> <p>Formally, a policy is a distribution over actions for a given state.</p> \[\pi(a \vert s) = P(A_t = a \vert S_t = s)\] <p>Due to the Markov property, the policy depends only on the current state and not history. In cases where the history is needed, the state is modified to embed the history to evade this dependence.</p> <p>For a given MDP, an optimal policy always exists that achieves the maximum expected reward at every state. (This is proved using the compactness properties of the state-action space using the Banach’s fixed point theorem - refer to <a href="/blog/cse291g">these notes</a> for more details).</p> </li> <li> <p>The <strong>value function</strong> determines how good each state or actions is. Finding the optimal value functions is known as the <em>prediction</em> problem.</p> <p>There are two functions that capture the value of the current state/action of the agent</p> <ol> <li>\(V_\pi(s) = E_\pi[R_{t + 1} + \gamma V_\pi(S_{t + 1} \vert S_t = s]\) - The expected reward obtained by a policy \(\pi\) starting from a given state \(s\).</li> <li>\(Q_\pi(s, a) = E_\pi[R_{t + 1} + \gamma Q_\pi(S_{t + 1}, A_{t + 1}) \vert S_t = s, A_t = a]\) - The expected reward for a given state \(s\) upon taking a certain action \(a\). These two functions are closely related to each other, and knowing one determines the other. In general, these functions (matrices, in discrete spaces) do not have a closed form solution.</li> </ol> </li> <li> <p>A <strong>model</strong> is the agent’s representation of the environment</p> </li> </ul> <p>RL algorithms are classified under various categories</p> <ul> <li>Model free and Model-based</li> <li>Off-policy and on-policy</li> </ul> <h1 id="its-all-dynamic-programming">It’s all Dynamic Programming?</h1> <p>The core theory of RL, the properties of the Bellman equation (refer to these notes for more details) and the recursive nature of the value functions, ties to dynamic programming. This insight helps us design algorithms to solve the problems in RL (Prediction and Control).</p> <p>We ideally want the solution to the control problem since we want to define the optimal behavior of an agent in the environment. To do so, the prediction problem is a pre-cursor that needs to be solved.</p> <h2 id="policy-evaluation">Policy Evaluation</h2> <p>The prediction problem involves calculating the rewards obtained by a given policy \(\pi\). The expectation can be written as</p> \[\begin{align*} V_\pi(s) &amp;= \sum_{a \in A} \pi(a \vert s) Q_\pi(s, a) \\ &amp;= \sum_{a \in A} \pi(a \vert s) (R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’)V_\pi(s’)) \end{align*}\] <p>where \(T\) is the state-transition function of the MDP.</p> <p>Ideally, we want to find the optimal policy that reaches the maximum value at every state.</p> \[\begin{align*} V*(s) &amp;= \max_\pi V_\pi(s) \\ Q*(s, a) &amp;= \max_\pi Q_\pi(s, a) \\ \pi*(s) &amp;= \arg\max_\pi Q_\pi(s, a) \end{align*}\] <p>These can be determined (iteratively) from the Bellman’s Optimality equations -</p> \[\begin{align*} Q*(s, a) &amp;= R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’) V*(s’) &amp;= R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’) \max_{a’} Q*(s’, a’) \end{align*}\] <blockquote> <p>Note the subtlety here. Although Bellman’s optimality equations aren’t seemingly much different from the Bellman’s equations, there is a very strong claim the optimality equations make - they claim that the existence a policy that gets the maximum possible value at every state. The existence is not a trivial claim and it is the proof I referred to in the terminology. Furthermore, it turns out that a deterministic policy is just as good as a stochastic one.</p> </blockquote> <p>So how do we find these optimal values?</p> <h2 id="policy-iteration">Policy Iteration</h2> <p>Given a policy \(\pi\), we iteratively update its actions at each state to improve its value. Remember that we can <em>evaluate a policy</em> to get its value function.</p> <p>At each state, if there is an action \(a\) such that \(Q_\pi(s, a) &gt; Q_\pi(s, \pi(s))\), then the policy is <em>strictly improved</em> by updating \(\pi(s) \gets a\). In each iteration, we update the actions this way across all the states, and repeat this until the policy does not change.</p> <p>How many iterations should we repeat this for? Because the number of policies is finite (bounded by \(O(\vert A \vert^{\vert S\vert})\), we are guaranteed to reach the optimum. Each iteration costs \(O(\vert S\vert^2 \vert A\vert + \vert S\vert^3)\). Although these numbers seem big, in practice, this algorithm typically takes only a few iterations.</p> <h2 id="value-iteration">Value Iteration</h2> <p>It is similar to the policy iteration algorithm, but focuses on recursively improving the value function instead.</p> <p>We start out with a random value function, and at each state, we choose the action that gives the maximum value (with the currently set values across the states). Once the values are updated across all the states, the process is repeated until the improvement is below a threshold. At the end of the iterations, we can extract the policy from the value function deterministically (the algorithm itself is a hint to this).</p> <p>Although this seems very similar to the policy iteration algorithm, there are some key differences. We do need to reach the optimal value function to get the optimal policy - if it is close enough, we can get the optimal policy. Also, the iterations <em>asymptotically reach</em> the optimal policy and there is no upper bound to this.</p> <h2 id="limitations">Limitations</h2> <p>Although dynamic programming approaches have theoretical guarantees, they are not widely used in practice. Why?</p> <p>The curse of dimensionality. These algorithms are have very limited applicability in practice. Many environments have a very large set of states and actions. In some cases, these could be continuous as well. The iteration algorithms are computationally infeasible in such cases.</p> <h1 id="model-free-rl">Model-free RL</h1> <p>Since we cannot look at every state action combination, we resort to approximations. We explore the world (say, with Monte Carlo sampling) and build experiences to heuristically guide the policy.</p> <p>The goal is to optimize the value of an unknown MDP through experience based learning. Many real world problems are better suited to be solved by RL techniques over dynamic programming based approaches (the iterative algorithms).</p> <h2 id="monte-carlo-control">Monte Carlo Control</h2> <p>It suggests greedy policy improvements over \(V(s)\) requires a model of the MDP. However, improvement over \(Q(s, a)\) is a model-free method! (This was the importance of defining both \(V_\pi(s)\) and \(Q_\pi(s, a)\)).</p> <p>The \(Q\) function can be learned from experiences. These concepts are the foundation concepts for deep RL!</p> <p>This approach can be thought of as a hybrid approach between policy and value iteration. In these exploration/sampling based techniques, it is important to gather data about the model through exploration and not be greedy. This forms the basis of <strong>epsilon-greedy</strong> algorithms.</p> <h2 id="epsilon-greedy-exploration">\(\epsilon\)-greedy exploration</h2> <p>At each state, with a certain probability we choose to exploit (greedily take the action based on the optimal policy we developed so far) or explore (take a random action to sample more outcomes)</p> \[\pi(a \vert s) = \begin{cases} \epsilon/m + 1 - \epsilon &amp; a* = \arg\max_{a \in A} Q(s, a) \\ \epsilon/m \end{cases}\] <p>This class of algorithms also has some theory but it is limited. This core trade-off between exploration/exploitation is still a core element in the modern RL algorithms.</p> <h2 id="temporal-difference">Temporal Difference</h2> <p>In Monte Carlo methods, we update the value function from a complete episode, and so we use the actual accurate discounted return of the episode. However, with TD learning, we update the value function from a step, and we find an estimated return called <strong>TD target</strong> - a bootstrapping method similar to DP.</p> \[\begin{align*} \text{Monte Carlo }&amp;: V(S_t) \gets V(S_t) + \alpha[G_t - V(S_t)] \\ \text{TD Learning }&amp;: V(S_t) \gets V(S_t) + \alpha[R_{t + 1} + \gamma V(S_{t + 1}) - V(S_t)] \end{align*}\] <p>The high-level view of MCTS is <img src="/assets/img/AIAgents/17388852350762.jpg" alt=""/></p> <h2 id="alphago-a-case-study">AlphaGo: A case study</h2> <p>The game has a large number of states. The rewards we use are \(\plusminus 1\) based on the player won. We define policies for both the players and <em>train</em> the policies with self-play.</p> <p>Making use of the symmetry of the game, we can use the episodes of the opponent player seen before to train the policy.</p> <p>AlphaGo used these exact MC methods with neural networks (CNNs, which is super useful for Go) to learn the probabilities and outcome rewards. It was trained with a lot of human games to train initial value networks. The developers also hand-crafted features to represent knowledge in the game.</p> <p>AlphaZero, an extension of this, relaxed the constraint of requiring a lot of human data and scaled.</p> <p>All these algorithms have been model-free. That is, we cannot estimate the consequences of our actions in the environment, and are simply learning based off our experiences. We are not learning anything about the dynamics of the environment.</p> <p>On the flip side, if we know the model of the world, can we do better? So given a <em>world model</em>, how do we use it?</p> <h1 id="model-based-rl">Model-based RL</h1> <p>We learn the model of the world from experiences, and then plan a value function (and/or policy) from the model. What is a model? A representation of an MDP \((S,A,T,R, \gamma)\), and we try to approximate \(T, R\).</p> <p><em>Assumption.</em> A key assumption that developers make is that the state transitions and rewards are conditionally independent.</p> <p>We have the experience \(S_1, A_1, R_2, \dots, S_T\), and we just train a model in a supervised problem setting \(S_i, A_i \to R_{i + 1}, S_{i + 1}\). Learning \(R\) is a regression problem and learning \(P\) is</p> <p>How do we use the learned model? Since the learned model has errors and uncertainty, training a policy would take a long time. It is like we are learning the rules of Go whereas previously we knew the rules, and were just trying to win.</p> <p>The advantages of model-based RL is the it can use all the (self, un) supervised learning tricks to learn from large scale data and can reason about uncertainty. The disadvantage is that we need to first build a model, and then estimate a value from the estimated model - introduced two sources of error.</p> <h2 id="introduction-to-transformers-and-language">Introduction to Transformers and Language</h2> <p>A function approximator (neural networks) needs to be good for the task. For example, CNNs were great for Atari and Go but they did not work well for language.</p> <p>What are the right inductive biases for language then? The <em>attention mechanism</em> was again borrowed from some cognitive functions of the brain. An attention matrix tries to find the <em>alignment</em> of elements within a sequence. Before the scaled dot product attention, there were multiple variants of the formulation -</p> <p><img src="/assets/img/AIAgents/17393174376471.jpg" alt=""/> Self-attention was first proposed by Cheng et al. in 2016 (originally called intra-attention) that brought in a sense of understanding in models - finding relations between elements of the same sequence.</p> <p>In 2015, there was a concept of global vs local attention introduced in the form of windowed-attention. This concept is being used widely in vision and language based systems.</p> <p>Let us discuss the intuition for <em>scaled dot product attention</em> -</p> <ol> <li>Query: What are the things I am looking for?</li> <li>Key: What are the things I have?</li> <li>Value: What are the things I will communicate?</li> </ol> <p>So essentially, the queries attend to the keys to find the aligned ones, and the corresponding values are returned. The multi-head part of the architecture is simply the multiple ways of learning these alignments between the query and key sequences. The alternate way of thinking about it is, using an ensemble representation to provide robustness in our predictions. Furthermore, to add a notion of position in the sequence, a positional encoding is added to the sequence elements.</p> <p>To apply all these mechanisms to language, we need a way to represent language as sequences - this step is known as <em>tokenization</em>. Word-level is too discrete (causes issues for questions like “how many r’s are in ‘strawberry’?”) and character-level is too redundant (often causes issues for questions like “Is 9.9 greater than 9.11?”. The current standard is a sub-word (Tiktoken Byte Pair Encoding BPE) that is learned from a representative subset of data.</p> <p>The other ideas are to use <em>heuristics</em> for numerical values like new tokenizers and right-to-left processing, etc. The amount of research done in tokenization is underwhelming as compared to the rest of the transformer stack. People have also come up with <a href="https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/">byte-level transformers</a> and <a href="https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/">LCMs</a>. Since bytes form too long sequences, people tried hierarchical representations that kind of work. However, there are many training issues with this and byte-encoding issues (PNG and Unicode are weird encodings). They were only able to train a 7B model with the byte-level learning.</p> <p>Language modeling is another long-standing problem that is a fundamental cornerstone to make these networks learn. The two popular mechanisms used are</p> <ul> <li>Infill (like in BERT): The 47th US President is [??] Trump</li> <li>Next Token prediction (like in GPT): The sun rises in the ??</li> </ul> <p>RNNs had an encoder-decoder architecture wherein the initial sequence is <em>encoded</em> into a vector (latent space) and it is then <em>decoded</em> into another sequence. This terminology has been carried through and is still used in transformers. The original paper started out as an encoder-decoder architecture, but these models are not used much anymore.</p> <ol> <li>Encoder only models (auto encoding models) - They are used for Sentence classification, NER, extractive question-answering and masked language modeling. Examples are BERT, RoBERTa, distilBERT.</li> <li>Decoder only models (auto regressive models) - They are used for text generation and causal language modeling. Examples are GPT-2, GPT Nero, GPT-3</li> </ol> <p>Check <a href="https://github.com/karpathy/nanoGPT">this repository by Andrej Karpathy</a> for a clean implementation of GPT.</p> <h2 id="how-does-all-this-relate-to-rl">How does all this relate to RL?</h2> <p>We now have a neural net architecture that works well on language. How about using this for MDPs with language-based state-action spaces.</p> <h2 id="paper-discussion-2xdqn-network">[Paper Discussion] 2xDQN network</h2> <p>As we have seen the Q-learning update</p> \[Q(S_t, a_t) \gets Q(s_t, a_t) + \alpha[r(s_t, a_t) + \gamma \max_{a’} Q(s’, a’) - Q(s, a)]\] <p>The problems with Q-learning are</p> <ol> <li>Curse of dimensionality - High dimensions and continuous state spaces do not work</li> <li>Slow convergence</li> <li>Inefficient generalization</li> <li>Exploration-exploitation trade-off</li> </ol> <p>This led to <strong>Deep Q-learning</strong>. The idea is to replace Q0tables with Neural networks to approximate Q-functions.</p> <ul> <li><em>*Experience replay</em> stores the agent’s past experiences in a replay buffer. The algorithm randomly samples episodes for training, to break the correlation between consecutive experiences.</li> <li><strong>Target Networks</strong> - Uses a separate, slower updating networking to compute target Q-values. Reduces instability caused by changing Q-values too frequently during learning.</li> </ul> <p>The problems with this are</p> <ul> <li>Overestimation of Q-values - due to the mathematical formulation the Q-values are over-estimated (has been proved mathematically)</li> <li>Sample inefficient - uses large amount of e</li> <li>Catastrophic forgetting - may forget previously learned experiences when trained on new experiences</li> </ul> <p>This led to <strong>Double Deep Q-learning</strong>. They decouple the action evaluation and action selection networks. The online network is used for action selection whereas the action value function evaluates the action. Previously, to stabilize the learning in Q-learning, the updates to action selection network were less frequent leading to choosing sub-optimal actions in some cases.</p> <p>The problems with this approach are</p> <ol> <li>Might not completely eliminate the overestimation bias - Since action selection and value estimation are not completely decoupled.</li> <li>Leads to slower convergence in some environments - where overestimation is helpful (high exploration settings)</li> <li>Sensitive to hyper parameters</li> <li>Still susceptible to some core DQN issues - Sparse rewards, discrete action space, sample inefficient.</li> </ol> <p>After this, newer methods were proposed such as <em>Rainbow Deep Q-learning</em> - Combination of previous improvements - Prioritized experience replay, uses a dueling network (two explicit specialized networks) for evaluation and selection, Stabilized learning via distributional RL, and Multi-step updates.</p> <h1 id="rl-agents-for-llms">RL Agents for LLMs</h1> <p>The typical LLM training pipeline is as follows</p> <ol> <li>SfM fine-tuning - Depends heavily on human expert demo data, and requires a lot of effort. This step can be compared to behavior cloning in RL</li> <li>Some pipelines also talk about pre-training that can be thought of as weight initialization. This step is not used so much anymore.</li> <li> <p>After this stage, <em>learning based on feedback</em>, has become a standard step. It comes in many forms, RLHF, RLAIF, etc. The primary approach, Reinforcement Learning with Human Feedback essentially rates different texts generated by the AI, and uses this as a reward model and improves the model considering it as a policy.</p> <p>This step is cheaper than the other steps, so companies are pushing towards improving this. However, in practice it does not seem to work without pre-training.</p> <p>This step also involves training a human-proxy reward function. In whole, this is known as post-training development.</p> </li> </ol> <h2 id="rlhf">RLHF</h2> <p>As we mentioned, the goal is to collect preference feedback and train a reward model. It is a kind of a personalized learning to correct the text generated by the model. One thing to note is that the rewards are given towards the end and there are no intermediate rewards.</p> <p>For the policy training itself, we have studied value and policy based algorithms. If the value of the MDP is estimated, the policy can be determined implicitly (argmax or epsilon-greedy). However, values and rewards of partial sentences are more difficult to estimate.</p> <p>Due to these limitations, researchers have gravitated towards policy-based RL. It has better convergence properties and is effective in high-dimensional or continuous actions spaces. However, they can converge to a local optimal rather than a global optima.</p> <p>“”” I was super sleepy after this, please update after watching the recording “””</p> <p>Now, for each step, we make a decision within a one-step MDP</p> <ul> <li>Expectation equation <ul> <li>SFT step is important</li> </ul> </li> </ul> <p><strong>Policy Gradient Theorem</strong> - For any differential blue policy $$\pi_\theta (s, a)</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Everyone is talking about agents. But what is an agent? Is it just a buzzword being thrown around? This article talks deeply about this issue along with the technical ideas associated.]]></summary></entry><entry><title type="html">Brains and AI</title><link href="https://sudhansh6.github.io/blog/brains-and-ai/" rel="alternate" type="text/html" title="Brains and AI"/><published>2025-01-01T00:00:00+00:00</published><updated>2025-01-01T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/brains-and-ai</id><content type="html" xml:base="https://sudhansh6.github.io/blog/brains-and-ai/"><![CDATA[<h1 id="distributed-representations-of-words-and-phrases-and-their-compositionality"><a href="https://arxiv.org/abs/1310.4546">Distributed Representations of Words and Phrases and their Compositionality</a></h1> <p>Presented by <a href="https://www.linkedin.com/in/joycelyn-yiu">Joycelyn Yiu</a></p> <p>What are distribution representations of words? Since symbols have no internal structure, researchers tries to represent words with vectors such that words that are similar to each other are closer in the vector space and vice versa.</p> <p>The inspiration for this idea comes from 1980’s, to convey semantics of language to machines. These representations can capture patterns and relationships between words like synonyms and grammar rules. How do we capture these patterns in practice?</p> <h3 id="skip-gram-model">Skip-Gram model</h3> <p>The skip gram model is a method that teaches a computer to understand the meaningful of words by reading large pieces of text. It basically learns the word relationships by predicting the surrounding words for a given word.</p> <p>In essence, from the training text, we compute the probability of <em>context</em> words occurring after a <em>center</em> word. How do you learn a distribution over words?</p> <p>The authors of the paper replaced softmax with a simpler one called <strong>Noise Contrastive Estimation NCE)</strong> that improved training and quality of representations.</p> <p>This model typically does not work well for rare words.</p> <p>In addition to the vanilla model, they added extension to represent phrases. Let us delve into each of these contributions.</p> <h2 id="method">Method</h2> <h3 id="hierarchical-softmax">Hierarchical softmax</h3> <p>The authors introduced hierarchical softmax that brought down the number of evaluations to \(\log_2(W)\) nodes rather than the typical \(W\) output nodes (where \(w\) is the number of words in the vocabulary).</p> <blockquote> <p>Is this used in LLMs? LLMs have become ubiquitous because of their parallelizability, and this takes it away to some extent.</p> </blockquote> <h3 id="negative-sampling">Negative Sampling</h3> <p>Introduced by Guzman and Hyvarinen, NCE tries to differentiate data from noise using logistic regression.</p> <h3 id="subsampling-of-frequent-words">Subsampling of Frequent Words</h3> <p>Common words like “the”, “a”, etc. occur frequently in text and do not provide a lot of information as compared to rare words. To counter this imbalance, the authors introduced a frequency based discard probability for the words to subsample the words. This improves the training speed and the accuracy of rare words.</p> <h3 id="learning-phrases">Learning phrases</h3> <p>The simple addition the authors did was to create new tokens for phrases like “The New York Times” - increasing the vocabulary size potentially making it unscalable. However, iteratively training the model considering longer phrases seemed to obtain a decent performance according to the authors.</p> <p>These techniques can be added to any underlying neural network model.</p> <h2 id="experiments">Experiments</h2> <p>The authors noticed that the representations posses a linear structure that allowed vector arithmetic. This potentially is a consequence of the training objective - the word vectors are in a linear relationship with the inputs to softmax non-linearity.</p> <h1 id="attention-is-all-you-need"><a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a></h1> <p>Presented by <a href="https://www.linkedin.com/in/hansenlillemark">Hansen Jin Lillemark</a></p> <p>What is intelligence? We represented words with high-dimensional vectors, and we seemed to have cracked natural language? How does the brain understand and represent concepts?</p> <p>Language is one of the easiest avenues to test our algorithms of artificial intelligence. We have stored a lot of data in the past few decades, and this set up a good foundation to train models here.</p> <p>Transformers, inspired from some brain mechanisms, have become the go-to mechanism for most of the ML applications. It starts off with the language modeling task where the task is to predict the next word \(x_t\) give the previous words in the sentence. This very general task was first introduced by Shannon to define information and entropy.</p> <p>Domains like mathematics have deterministic distributions whereas reasoning usually has a flatter distribution. Another issue with language is that it keeps changing with time, and the models need to be dynamic enough to maintain this.</p> <h2 id="method-1">Method</h2> <p>Continuing from the Word2Vec paper, we represent words with vector embeddings.</p> <blockquote> <p>The embedding size is essentially like PCA - decomposing meanings into a low-dimensional space. Although there are over hundred thousand words, we are representing them in 500 dimensions?</p> </blockquote> <blockquote> <p>Can we create different similarity kernels to create different meanings? This effect is achieved through multi-head attention.</p> </blockquote> <blockquote> <p>Different language to language translation is possible with LLMs. Is there an inherent universal language?</p> </blockquote> <blockquote> <p>LLMs are able to reason well in some scenarios. Try asking it a novel puzzle and see how it does. However, it struggles with math. Is there something we are missing?</p> </blockquote> <blockquote> <p>Decoder inference is still sequential… Training seems more efficient but inference…</p> </blockquote> <h1 id="bias-variance-tradeoff">Bias-Variance Tradeoff</h1> <p>This topic seems like a thing of the past. We have built very large models around large amounts of data, and everything seems to work. So, why worry about bias or variance? Bias is solved by large models and variance is solved by large data. If all’s good in the world, then why don’t we have zero loss?</p> <blockquote> <p>Recap: The bias-variance trade-off. When low complexity models cannot inherently capture the complexity of the data, they have a <em>bias</em> or they <em>underfits</em> the training data. It can arise from a poor architecture or features engineered from the data. On the other end, complex models with large amount of parameters may fit the training data really well, but they tend to perform poorly on the test loss. This error, called <em>variance error</em>, occurs due to <em>overfitting</em>. in these cases, models are said to not generalize well across different data distributions. This trade-off between bias and variance is still present at the core of modern models.</p> </blockquote> <p>To counter these problems, early approaches involved <strong>regularizing</strong> models to prefer solutions with “simple” patterns - ones that have low-norm weights even in over-parameterized models. There are modern theoretical frameworks such as the <a href="https://arxiv.org/abs/1806.07572">Neural Tangent Kernels (NTK)</a> that show that over-parameterized networks behave like kernel machines and converge to smooth solutions due to regularization. That is, interpolating regularized solutions seems to generalize well.</p> <p>I also mentioned <em>poor model architecture</em> as a source of these errors. Over-parameterized models can often be replaced with models designed with a good <strong>inductive bias</strong> - models that leverage the structures in the data to generate structured solutions. For example, CNNs leveraged that images have spatial-features, and replaced fully-connected layers with convolutional kernels that greatly reduced the number of parameters.</p> <blockquote> <p><em>Model architectures</em> designed with inductive bias are the best kind of models. Attention is also a product of such design philosophies.</p> </blockquote> <p>So far, we have</p> <ul> <li>Maybe over-parameterization works but we need to have appropriate regularization tricks such as dropout and weight decay</li> <li>Adaptive optimizers, early stopping, large batch training all seem to make sense</li> </ul> <p>In 2018, <a href="https://arxiv.org/abs/1812.11118">Belkin et al.</a>, showed that test loss follows a “double descent” curve - it peaks at a critical model complexity, then decreases. This phenomenon has been seen to occur in CNNs, Transformers, and linear models with high dimensional features. The take away message is that more complexity does not mean worse generalization in the modern architectures.</p> <p><img src="/assets/img/BrainsAI/17377675747647.jpg" alt=""/></p> <p>How does data fit in all this? More the amount of data, the simpler the model becomes - we have seen that the inner layers of LLMs require sparse changes in the weights to fine-tune to different datasets (LoRA). It maybe seen as if large datasets prevent overfitting since larger models are able to absorb the noise in the data without harming the underlying signal.</p> <p>In 2019, <a href="https://arxiv.org/abs/1906.11300">Bartlett et al.</a>, showed that models can memorize noisy data but still generalize if noise is structured or data has low intrinsic dimensions. High-dimensional mode old can separate signal from noise via implicit regularization. At the core of some of the large models we have built, we made a rather huge assumption - the noise in the data is Gaussian. The MSE loss is nothing but a negative likelihood over Gaussian noise. These assumptions must be carefully considered while building models for different applications.</p> <p>So what do we make of all this? It’s new information that we didn’t have before while designing models. Maybe it’s because of this the model scaling laws are working.</p> <h1 id="benign-overfitting-in-linear-regression"><a href="https://arxiv.org/abs/1906.11300">Benign overfitting in linear regression</a></h1> <p>The ultimate goal of machine learning is how to train a model that fits to a given data. We are approaching this by reducing the empirical training risk/error through a loss function. There is a mis-match between what are doing and the goal we are trying to achieve - reducing the test loss or generalize well to new data. Let us understand this better.</p> <p>A model’s ability to fit a wide variety of functions or patterns in the data is a known as its <em>capacity</em>. As we have increased the models’ capacity, we seemed to have the cross the peak in the double descent curve - they are over-fitting but it seems to be benign. That is, they seem to have zero training risk and the test risk approaches the best possible value. Why do we think this is benign? As the model capacity increases, the test loss seems to be decreasing even more. So how do we reach this benign overfitting region?</p> <p>The authors tested this with linear regression and significant over-parameterization. For a linear regression model the minimum norm solution is given by</p> \[\hat \theta = X^T = (XX^T)^{-1}y \; X\theta = y\] <p>The authors define the excess risk of the estimator as</p> \[T(\theta) := \mathbb E_{x, t} [(y - x^T \theta)^2 - (y - X^T \theta^*)^2]\] <p>How do you over-parameterize a linear regression model? The authors consider the number of eigenvectors of the covariance of the data. They consider quantities from the PCA theory. They concluded that if the decay of the eigenvalues of the covariance is sharp, then we can reach the benign overfitting region.</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[The primary motivation for AI stems from our brains. How has our research in cognitive science shaped the modern AI systems?]]></summary></entry><entry><title type="html">Statistical Natural Language Processing</title><link href="https://sudhansh6.github.io/blog/Statistical-NLP/" rel="alternate" type="text/html" title="Statistical Natural Language Processing"/><published>2024-11-03T00:00:00+00:00</published><updated>2024-11-03T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Statistical-NLP</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Statistical-NLP/"><![CDATA[<h1 id="introduction">Introduction</h1> <p><strong>Natural Language Processing (NLP)</strong> is the study of computer systems that take as input or produce as output natural language - languages created by humans. The goal is to give machines the power to understand not just words, but entire sentences, paragraphs and documents.</p> <p>It it worth keeping in mind that the notion of “understanding” is contrived. There is no clear definition - when we claim Large Language Models (LLMs) understand our language, we really don’t know if it is understanding.</p> <p>NLP develops systems for</p> <ol> <li> <p>Analysis of language (NL to some useful output) - text classification, question answering, etc.</p> </li> <li> <p>Generation of language (NL to NL; Image to NL, etc) - summarization, image captioning, machine translation</p> </li> <li> <p>Representation of language (NL to some representation) - learning word embeddings</p> </li> </ol> <p>In the part, systems were task-specific, now we have more general purpose systems capable of all of the above and more.</p> <h3 id="origins">Origins</h3> <p>Back in 1949, Warren Weaver, a wartime US mathematician and scientist, brought the idea of the first computer based application related to natural language - machine translation (MT). He considered the problem of translation as a problem in cryptography. We still use the notation of encoder and decoder in the present techniques. He developed a rule-based program to convert Russian to English.</p> <p>Over time, it became obvious that human language are <strong>ambiguous</strong> (unlike programming and other formal languages) and they are <strong>rich</strong> - any meaning may be expressed in many ways. Human language interpretation depends on the real world, common sense and <strong>contextual knowledge</strong>. Furthermore, there is <strong>linguistic diversity</strong> across genres, styles, and so more.</p> <p>In 1957, Chomsky proposed a <em>generative grammar</em>, a rule based system of syntactic structures, brought insight into how linguistics can help MT. Since the results were not satisfactory, funding was cut-off and then came the winter of AI in 1966.</p> <p>In 1971, Terry Winograd’s MIT thesis has motivated the notion of <strong>grounded language understanding</strong>. In late 80’s, statistical techniques revolutionized NLP. They used early ML algorithms - decision trees with rule based systems.</p> <p>From 90’s to early 2000s, methods like logistic regression, Support Vector Machines (SVM), Hidden Markov Models (HMMs), Conditional Random Fields (CRFs), etc were introduced. Moreover, papers introduced feature engineering for specific tasks - POS tagging, Named Entity Recognition, Parsing, etc.</p> <p>The main language models during this time were n-grams with smoothing.</p> <p><img src="/assets/2024-11-03-Statistical-NLP/2024-11-09-15-15-16-image.png" alt=""/></p> <h3 id="dawn-of-deep-learning-era">Dawn of Deep Learning Era</h3> <p>Bengio et al. in 2003 proposed first neural language models with 1-hidden layer feed-forward neural network. It introduced the notion of <strong>word embeddings</strong> with a real-valued feature vector in \(\mathbb R^d\). In 2008, a new paper proposed training neural network along with a word embedding matrix jointly. There was no need of feature engineering anymore.</p> <p>In 2013, Mikolov et al. introduced arguably the most popular word embedding model - <strong>Word2Vec</strong> - they got rid of hidden layer in the model as well.</p> <p>From 2013 to 2018, Recurrent Neural Networks (RNNs; Elman 1990), Long-Short Term Memory Models (LSTMs), Convolution Neural Networks (CNNs), recursive neural networks (Socher et al.), etc were used for NLP. There were feats of Architectural engineering as well - combining RNNS with CRFs for sequence labeling, CNNs for text classification, summarization with pointer-generators RNNs (2017). <em>In present date, there are little to no changes in the model architecture.</em></p> <p>In 2014, Google introduced <strong>Sequence-to-sequence</strong> learning, a general end-to-end approach for mapping one sequence to another using a single neural network (encoder-decoder architecture). This proved very important for NLP tasks going forward. It was a fundamental shift in paradigm to perform tasks like translation with a single model instead of complicated designed models.</p> <p>Then, in 2015 came the notion of <strong>Attention</strong> - to reduce the bottleneck of sequence-to-sequence models that was compressing the entire content of source sequence into a fixed-size vector. This notion still required sequential processing with RNNs. Finally in 2017, <strong>Transformers</strong> were proposed which eschewed recurrence and relied entirely on attention mechanisms. The parallel nature of the model enabled fast computations.</p> <p>In 2020, people realized instead of just pre-training the word-embedding layer they could just pre-train the whole network and add a layer-head in the end if required for other specialized tasks. Pre-trained LMs then acted as an initialization for fine-tuning on downstream tasks - ELMo (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), GPT (Radford et al., 2018), and BERT (Devlin et al,. 2019). The impact of pre-training all the layers was significant.</p> <p><img src="/assets/2024-11-03-Statistical-NLP/2024-11-09-15-15-02-image.png" alt=""/></p> <h3 id="present-date">Present Date</h3> <p>NLP systems are increasing used in everyday life - in the form of chatbots and other AI assistants. Consider ChatGPT - fastest growing consumer computing applications in history.</p> <p>The key advantage os language models is that there is no need of annotation - nearly unlimited training data*. People also realized that using larger and larger models gives higher performance as data scales. The final ingredient to achieve all this is compute - GPU gave a huge advantage over CPU to train these networks. These three key ingredient - hardware scalability (GPUs), Model scalability (Transformer with many deep layers) and Data scalability (Large datasets with lots of text) enabled the success of GPT models.</p> <p><img src="/assets/2024-11-03-Statistical-NLP/2024-11-09-15-14-43-image.png" alt=""/></p> <p>Realizing the power of scale, GPT1 was trained with a few million parameters and now GPT4 has a few hundred billion parameters. In 2022, researchers at OpenAI realized some tasks were only possible at larger scales - scaling LMs leads to emergent abilities. Another paper (one of the best papers in NeurIPS) questioned this asking whether this finding is just an artifact of how we designed our metrics. The metrics used in the OpenAI paper did not allow continuous rewards which caused the sudden jump in performance after a certain point in scale. With a more continuous metric, the gains due to scale increase continuously without sudden jumps.</p> <p>Then came the question of prompting - how do we talk to these LMs? <strong>Prompt</strong> is a cue given to the pre-trained LM to allow it to better understand people’s questions (Best paper in NeurIPS 2020).</p> <p>GPT3.5 introduced the notion of <strong>Instruction Tuning</strong> - collect examples of (instruction, output) pairs across many tasks and then evaluate on unseen tasks. Furthermore, the output of LMs can be tuned with <strong>Reinforcement Learning with Human Feedback</strong> (RLHF) - explicitly attempt to satisfy human preferences using RL. This was implemented in an ingenious manner -</p> <p><img src="/assets/2024-11-03-Statistical-NLP/2024-11-09-15-13-21-image.png" alt=""/></p> <p>After adding some safety features, GPT3.5 was transformed into ChatGPT.</p> <p><strong>LLM as a reasoning engine</strong> - The knowledge base of LLMs is large but incomplete. To address this limitation, they need to retrieve information from elsewhere, add it to the prompt and then ask the LLM to process it to get an answer. This is the idea behind <strong>Retrieval Augmented Generation (RAG)</strong> for knowledge intensive NLP tasks. The pipeline is more sophisticated, and will be described later.</p> <h3 id="summary">Summary</h3> <p>We have the following big picture -</p> <ol> <li> <p>Feature Engineering in 1990s to 2000s</p> </li> <li> <p>Architecture Engineering - 2010 - 2018 (LSTMs, CNNs, Transformers)</p> </li> <li> <p>Objective Engineering - 2018 (ELMo, BERT, GPT)</p> </li> <li> <p>Prompt Engineering - 2020s - present (instruction-tuning, chain-of-thought, etc)</p> </li> </ol> <p><strong>NLP vs Humans.</strong> General language understanding is still a bit difficult for LLMs</p> <ul> <li> <p>Sample efficiency - LLMs require a lot of data</p> </li> <li> <p>Robustness - LLMs are brittle, can be easily fooled</p> </li> <li> <p>Originality - LLMs lack ability to create truly original content</p> </li> <li> <p>Causality and other forms of reasoning - LLMs have limited understanding of logic</p> </li> </ul> <p>In the following post, we will start from the basis such as text classification with simple neural networks and make our way to the modern sophisticated techniques being used. Although the initial part of the article may seem very straightforward, it is important to understand the motivations and ideas behind the approaches.</p> <h1 id="text-classification">Text Classification</h1> <p>The task is to assign a text document to one or more categories from a predefined set of labels. For example, Sentiment Analysis, Spam Detection, Topic Classification, Authorship Attribution and Language Identification fall under this domain of tasks.</p> <p>Unlike tasks like sorting numbers which have clear rules, text classification does not have a standard algorithm. The issues with rule-based algorithms are as follows -</p> <ol> <li> <p>Semantic Gap - Computer does not know what words mean - their relationships, sentiment, etc.</p> </li> <li> <p>Intra-class variations - There are many ways to be a particular label.</p> </li> <li> <p>Scalability - Have to write rules for every class label</p> </li> </ol> <p>The task rather requires a data-driven approach (in the form of machine learning) -</p> <ol> <li> <p>Collect a dataset of example text inputs and their labels - <strong>training data</strong></p> </li> <li> <p>Use Machine Learning algorithms to train a classifier on the training examples</p> </li> <li> <p>Evaluate the classifier of new text data - <strong>test data</strong> - a good classifier generalizes well.</p> </li> </ol> <p>This is the standard process (used to be) to work with machine learning models. Another important aspect with this approach is modeling of data - i.e., input representation. Machine Learning models require numerical input.</p> <blockquote> <p>At their core, machine learning models are optimization models relying on mathematical operations - cannot be done with text data.</p> </blockquote> <p>Therefore, we need to create a <em>feature vector</em> for the input text for text classification. By creating feature vectors, we are essentially creating another veil of abstraction to reduce the semantic and other complexities in text data.</p> <p><strong>Bag of words</strong> is one such early idea that represents text as an unordered collection of words, disregarding grammar and word order. The vector essentially contains the frequency of each word occurring in the training text. For unseen words in test data, we just treat them as a separate entity under the unknown tag.</p> <p>These feature vectors although being one of the simplest forms of representation have some limitations (owing to the simplicity)</p> <ol> <li> <p>Sparsity - high-dimensional, sparse vectors (size of vocabulary)</p> </li> <li> <p>Loss of sentence structure - This context is very important in some tasks</p> </li> <li> <p>Semantic Gap - Lacks the word meanings</p> </li> </ol> <h3 id="nearest-neighbor-classifier">Nearest Neighbor Classifier</h3> <p>The idea is to represent all training examples in a feature space. Given a test instance, we find the closest training example based on a <strong>distance metric</strong> and assign its label to the test instance. This algorithm does not have any training of sorts and takes constant time. However, the inference time is \(\mathcal O(n)\) since it has to be compared against every test instance. We do not want such behavior with ML models - even if the training is slow, we want the inference to be very fast.</p> <p>Furthermore, the decision boundaries created by this algorithm are not smooth - <em>from experience this is a bad outcome, and we ideally want a smooth decision boundary</em> without too many changes. Motivating from this idea, then came along \(k\)-Nearest Neighbor classification where the nearest \(k\) neighbors are chosen to decide the label. The parameter \(k\) is called as a <strong>hyperparameter</strong> that is tuned based on the dataset, model, etc. There is no clear answer as to which hyper-parameter gives the best performance; they have to be chosen empirically. The distance metric and word representation are other hyperparameters in this algorithm.</p> <p>Suppose we choose the hyperparameter that works best on the <em>test data</em>, it may not perform that well on other test/unseen data. To overcome this, a new split called <strong>validation</strong> is introduced. The hyperparameters are chosen with the validation data and the algorithm is tested on the test data. The golden rule is to run the model on the test set once after everything (training and hyper-parameter tuning) is completed.</p> <p>The advantages of these models are that they are non-parametric - they make no assumptions about the underlying data. Now, we see another form of classifier that does not have this property.</p> <h3 id="linear-classifier">Linear classifier</h3> <p>A linear classifier assumes a specific form of the decision boundary - predefined model complexity. These boundaries are defined by a set of fixed parameters - intercept \(b\) and slope \(m\). This form of a classifier is still used today in practice.</p> <p>The mathematical model can be represented as</p> \[y \text{ (label) } = \underbrace{f(x)}_{n \times 1} = \underbrace{W}_{n \times m} \underbrace{x}_{m \times 1} + b\] <p>Here, \(n\) is the number of classes and \(m\) is the feature dimension. The bias term \(b\) essentially incorporates prior knowledge into the model.</p> <p>Linear layers are the building block of <strong>neural networks</strong>. Neural networks consist of small functions that are stacked together to form a complex function, and linear layers are a common building block in these architectures.</p> <p>How do we find the best \(W, b\)? We define a <strong>loss function</strong> that quantifies our unhappiness with the scores across the training data. This loss function is minimized via <strong>optimization</strong> to get these best values. We typically average the loss function across all the training examples to get the total loss in a training dataset.</p> <p>Finally, the output from a classifier is unnormalized, and we generally want to interpret these raw scores as probabilities. To normalize the outputs, we use a softmax function -</p> \[P(Y = k, X = x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}}\] <p>where \(e\) is the raw output from the model. This interpretation allows us to now define a loss-function for (binary class) classifiers - <strong>negative log likelihood</strong>.</p> \[L_i = - \log P(y = y_i \vert X = x_i)\] <p>Such method of classification is termed as <strong>logistic regression</strong>.</p> <blockquote> <p>For logistic regression, with a small initialization of weights, the expected initial loss is \(\log (# classes)\). A good sanity check to look out for.</p> </blockquote> <p>The above loss function is essentially doing <strong>maximum likelihood estimation</strong> on the training data. Given training samples \((x_i, y_i)\), the maximum likelihood is given by \(W_{ML} = {\arg \max}_{W \in \mathbb R^{\gamma \times C}} \mathcal L(\bf W)\).</p> <p>The likelihood of the data is given as \(\mathcal L(\bf W) = \sum_{i = 1}^n \log p (Y_i \vert x_i; \bf W)\).</p> <h3 id="feed-forward-neural-networks">Feed-forward Neural Networks</h3> <p>Neural networks introduce a powerful regime of data-driven methods. These models are essentially linear layers stacked (without the softmax) against one another to form a huge network. Each linear layer is associated with an <strong>activation function</strong> that introduces non-linearity in the models. For example, the linear output \(Wx + b\) from a linear classifier layer is sent through a sigmoid activation function which essentially maps the input \(x\) to \((1 + e^{-x})^{-1}\). Each weight row in the network is referred to as a <strong>neuron</strong>.</p> <p>Note that we want our models to generalize well on unseen data. Since neural networks are powerful models, they can <em>overfit</em> on the training data to perform really well on the training set but poorly on the test set. Through <strong>regularization</strong>, we prevent overfitting by discouraging the model from fitting the training data too closely. This occurs frequently when in deep and complicated models. With regularization, we are no longer doing Maximum Likelihood Estimation (MLE) with our models. The loss function then becomes</p> \[L(W) = \frac{1}{N} \sum_{i = 1}^N L_i (F(x_i, W), y_i) + \lambda R(W)\] <p>where \(R(W)\) is the regularization function and \(\lambda\) is a <em>regularization parameter</em>, and it represents the regularization strength. There are more complex methods like <strong>dropout</strong> and <strong>batch normalization</strong> to prevent overfitting. Dropout refers to <em>randomly dropping</em> neurons in the network while training to simplify the model complexity during training time.</p> <p>With this model, we have described the basic recipe for supervised machine learning!</p> <p>How do we find the best \(W\) with these building blocks? Start with a random \(W\) and iteratively improve it to lower the loss function - <strong>gradient descent</strong>. The size of iterative improvement is given by the <strong>learning rate</strong> - another important hyperparameter. A small learning rate leads to slower convergence whereas a high learning rate can overshoot the minimum. Typically, the learning rate is changed across training epochs based on a <strong>learning rate schedules</strong> (ex. cosine learning).</p> <p>The gradient function is pre-computed to prevent recomputing the gradient for every example in practice. <strong>Backpropagation</strong> is a method to compute the gradient of the loss function with respect to the weights in the network - it is an application of the chain rule of calculus.</p> <p>The naïve version of gradient descent can be optimized much further using better convergence algorithms like ADAM and using stochasticity to descent over batches rather than the whole training dataset to increase speed.</p> <h1 id="word-embeddings-and-tokenization">Word Embeddings and Tokenization</h1> <p>How do we convert words to numerical values? A simple idea is to consider a <strong>one-hot vector</strong> - maps words into fixed length vectors and they contain only the identity information of the object without any semantic information. <strong>Bag-of-words</strong> is essentially the summation of one-hot vectors across the input text.</p> <p>An interesting result is that word meanings can also be captured through vectors of real numbers - a vector space where similar words (by meaning) have similar vectors (by some distance metric). How do we come up with such vectors that also have a reasonable size?</p> <h3 id="distributional-semantics">Distributional Semantics</h3> <p>Words that appear in similar contexts have similar meanings. The idea is to understand the context around the word and their relative ordering to understand the meaning of the word itself. To do so, we will build a dense vector for each word, chosen so that it is similar to vectors of words that appear in similar contexts, measuring similarity as the vector dot (scalar) product.</p> <h3 id="word2vec">Word2Vec</h3> <p>A simple and fast model to capture semantic meanings. They use two algorithms - skip-gram and Continuous Bag of Words (CBOW).</p> <p><strong>Skip-gram</strong> - Given a corpus of text as the input, the output is a set of embeddings which is a real valued vector. To generate these embeddings, we set up a fake prediction task - predict a word’s context from that word. For example, in the sentence “the dog bit the man”, for the <em>word</em> “bit”, the <em>context</em> can be “dog” or “the”.</p> <p>For each position $t = 1, \dots, T$ in the corpus, we predict context words within a window of fixed size \(m\) (<em>context window size</em>), given a center word \(w_j\). The joint probability expression is given by</p> \[\text{ Data Likelihood } = \prod_{t = 1}^{T} \prod_{-m \leq j \leq m; j \neq 0} P(w_{t + j} \vert w_t; \theta)\] <p>where \(\theta\) are all the parameters of the model. The loss function can be chosen as</p> \[L(\theta) = -\frac{1}{T} \sum_{t = 1}^T \sum_{-m \leq j \leq m; j \neq 0}\log P(W_{t + j} \vert w_t; \theta)\] <p>The ground truth probability is calculated by looking at all examples of the word occurring in the training corpus. The original paper predicts two vectors per word \(w\) - \(v_w\) when \(w\) is a center word, \(u_w\) when \(w\) is a context word. The probability is then given by</p> \[P(o \vert c) = \frac{\exp (u_o^T v_c)}{\sum_{w \in V} \exp(u^T_w v_c)}\] <p>The gradient of this loss function comes out to be expected context vector subtracted from the observed context vector. The center word is pulled towards words that are observed in its context and away from those that are not</p> \[v_c^{new } = v_c^{old} + \text{observed} - \text{expected}\] <h1 id="low-rank-adaptation-lora">Low-rank Adaptation (LoRA)</h1> \[h = W\] <h1 id="multi-layer-prompt-tuning">Multi-layer Prompt Tuning</h1> <p>Continuous prompts \(\phi_i\) are concatenated with the keys and values in the self-attention layer</p> <h1 id="adapters-in-transformer-models">Adapters in Transformer Models</h1> <p>An adapter in a Transformer layer</p> \[f_{\phi_i}(x) = W^U(\sigma(W^D x))\] <p>where \(W_^D \in \mathbb R^{d\times r},\)W^U$$</p> <h1 id="towards-a-unified-view-for-parameter-efficient-fine-tuning">Towards a Unified View for Parameter Efficient fine-tuning</h1> <p>This works shoes that LoRA, prefix-tuning, and adapters can be expressed with a similar functional form - all methods can be expressed as modifying a model’s hidden representation \(h\)</p> <h3 id="optimizer-state-comparison">Optimizer State comparison</h3> <h2 id="model-compression">Model compression</h2> <h3 id="knowledge-distillation">Knowledge Distillation</h3> <p>A classic approach from Hinton et. al</p> <h3 id="distilbert">DistilBERT</h3> <p>The idea is to use the classification model with output \(P_{\text{teacher}} (y \vert x)\) to minimize \(KL(P_{\text{teacher}}\vert\vert P_{\text{student}})\) to bring student distribution close to the teacher. Note that this approach does not require any labels since the student uses <em>pseudo-labels</em> that the teacher has.</p> <p>For example, we can choose BERT as the teacher model and create a small student model that has half the layers of BERT. The number of parameters reduce by half and so does the inference time. The performance difference is negligible and this is a huge gain for efficiency.</p> <h1 id="knowledge-representation-in-transformer-lms">Knowledge Representation in Transformer LMs</h1> <p>Factual knowledge is captured by the model during the training is stored in the form of model parameters. Which part of the transformer is this information stored? Token embeddings, feedforward laters or attention layers?</p> <h3 id="parameter-distribution-in-transformers">Parameter Distribution in Transformers</h3> <ul> <li> <p>Self-attention layers</p> <ul> <li> <p>Query, Key, Value matrices - \(W_q, W_k, W_v\) each of dimension \(d \times d\)</p> </li> <li> <p>Output matrix is \(W_o\) of dimension \(d \times d\)</p> </li> <li> <p>\(4d^2\) attention parameters per layer.</p> </li> </ul> </li> <li> <p>Feed-forward Network Layers</p> <ul> <li> <p>First linear transformation via \(W_1: 4d^s\) (input to \(4d\))</p> </li> <li> <p>Second linear transformation via \(W_2: 4d^2\) (\(4d\) to output)</p> </li> <li> <p>\(8d^2\) feedforward parameters per layer</p> </li> </ul> </li> <li> <p>The embedding parameters are not usually considered since the vocabulary across all the models is assumed to be the same</p> </li> </ul> <p>Note that feedforward layers have much higher number of parameters than the attention layers - they account for 2/3 of the total parameters.</p> <h2 id="transformer-feed-forward-layers-are-key-values-memories">Transformer Feed-Forward Layers Are Key-Values Memories</h2> <p>An interesting paper that explores the previous ideas. The feedforward layer is represented as \(y = W_2 \sigma(W_1 x)\).</p> <p>\(W_1\) corresponds to keys, and \(W_2\) to values - when the output from the first weight matrix is passed through ReLU - it is similar to <em>selecting</em> some entries of the vector (positive ones) which then choose the corresponding rows in \(W_2\).</p> <p>The authors tested this idea by considering which neurons are selected in the feedforward layers for different tokens - <strong>key trigger analysis</strong>. They analyzed the patterns in the activations to categorize them.</p> <p>Given a key \(k_i^l\) corresponding to \(i\)th row of the \(l\)th feed-forward layer \(W_1\) computer memory efficient for every prefix \(x_1, \dots, x_j\) of every sentence in the training data.</p> <p><strong>Memory coefficient calculation</strong> - calculate \(ReLU(x_j^l \cdot k)\) - incomplete</p> <p>They found the following results -</p> <ul> <li> <p>Shallow layers detect shallow patterns</p> </li> <li> <p>Middle FF layers store knowledge; Upper attention layers “aggregate” relevant knowledge for prediction</p> </li> </ul> <h2 id="editing-knowledge-in-transformer-lms">Editing Knowledge in Transformer LMs</h2> <p>Can we directly edit LM parameters to fix incorrect, obsolete facts? The edits must be deep -</p> <ul> <li> <p>Eiffel tower is located in the city ____ (change from Paris to Rome)</p> </li> <li> <p>Model should understand full implications - The tallest building in Rome is <em>__Eiffel Tower</em>__</p> </li> </ul> <p>The edit must be robust, should not edit all the facts in the model.</p> <p>Can we simply modify the columns of \(W_2\) to change the model’s behavior? This ends up breaking the model. Another work, Meg et al., suggested applying a rank-1 update \(W_2 \to W_1 + uv^T\) to maximize the probability of the edit output. This change minimizes the change in the behavior of \(W_2\) on other inputs.</p> <p>It works well in some scenarios and does not in some other - it is a new research direction!</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[Enter the world of Natural Language Processing.]]></summary></entry><entry><title type="html">Large Language Model Reasoning</title><link href="https://sudhansh6.github.io/blog/Large-Language-Models-Research/" rel="alternate" type="text/html" title="Large Language Model Reasoning"/><published>2024-10-14T00:00:00+00:00</published><updated>2024-10-14T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Large-Language-Models-Research</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Large-Language-Models-Research/"><![CDATA[<h1 id="introduction">Introduction</h1> <p>As a part of this article, we delve into the paradigm of Chain of Though reasoning in Large Language Models. The aim is to highlight the importance of this idea and summarize the main research in this area. The blog should provide enough context for the reader in the field of AI to understand the basic concepts and think about the potential research ideas addressing the limitations of the current models.</p> <h1 id="chain-of-thought-reasoning"><a href="https://arxiv.org/pdf/2201.11903">Chain of thought Reasoning</a></h1> <p>Chain of thought (CoT) refers to manifesting the human thought process in large language models by endowing language models with the ability to generate a chain of thought - a coherent series of intermediate reasoning steps.</p> <p>It is hypothesized that CoT prompting helps LLMs to tackle complex arithmetic, commonsense and symbolic reasoning tasks. The following demonstration highlights this improvement.</p> <p><img src="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&amp;w=1920&amp;q=75" alt="COT"/></p> <p>However, there are some limitations with this paradigm of reasoning with the current models.</p> <ul> <li>Small models are unable to improve with CoT prompting. LLMs with more than 100B parameters show performance gains with CoT.</li> <li>The performance improvements are larger with larger models. In other words, the benefits of CoT scale with the size of the models.</li> <li>Sometimes the models arrive at the correct answers with the wrong reasoning. The errors have been classified as <ul> <li><strong>Calculation error</strong> - LLMs are probabilistic models, predicting what token occurs next. So when an LLM tries to do \(3* 25* 8 =\), it does not really calculate the answer but probabilistically guesses the answer which is the next token. This highlights a fundamental limitation in the current architectures of LLMs.</li> <li><strong>Symbol mapping error</strong> - When there are too many variables involved, LLMs sometimes mix up the variables and arrive at the wrong answer. Again, the problem arises from the fundamental architecture flaw highlighted in the previous point.</li> <li>Other than these major errors, the models also have semnatic understanding problems, missing steps, incoherent chain of thought errors</li> </ul> </li> </ul> <h1 id="large-language-models-are-human-level-prompt-engineers"><a href="https://arxiv.org/abs/2211.01910">Large Language Models are Human-level prompt engineers</a></h1> <p>The motivation of this paper is as follows -</p> <ul> <li> <p><strong>Human effort in prompt engineering</strong> - Crafting effective prompts for LLMs is time-consuming and requires significant human expertise.</p> </li> <li> <p><strong>Optimization challenge</strong> - Primpts greatly influence LLM performance, but users often lack insight into how to optimize them for specific tasks.</p> </li> <li> <p><strong>Scalability</strong> - As LLMs grow in size and capabilities, manuallt designing prompts becomes less deasible for a wide range of applications.</p> </li> <li> <p><strong>Automating promtp design</strong> - There is a growing need to automate the prompt engineering process to enhance LLM usability and performance.</p> </li> <li> <p><strong>Real-world impact</strong> - Applications in diverse domains (e.g., AI chatbots, automated content generation) can benefit from optimized and automated prompts.</p> </li> </ul> <p>This work promposes an <strong>Automatic Prompt Engineer (APE)</strong> - asystem that automates prompt generationg and selection for Large Language Models. This task is treated as a program synthesis task wherein the input-output pairs (natural language questions and answers) are given to the APE, and it has to generate the instruction needed to generate these pairs.</p> <p>In essence, the APE is trying to learn the prompts generated by humans. The framework is as follows -</p> <ol> <li> <p>Instruction Generation. An LLM is used as an ingeerence model where the “instruction candidates” are generated based on a small set of input-output demonstrations</p> <p>Example: The input to APE is of the form -</p> <p><em>Input 1</em> - Forward generation technique</p> <p>”””</p> <p>I gave a friend an instruction and five inputs. The friend read the instruction and wrote an output for every one of the inputs. Here are the input-output pairs</p> <p>Input: [ ] Output: [ ]</p> <p>Input: [ ] Output: [ ]</p> <p>…</p> <p>The instruction was &lt;COMPLETE&gt;</p> <p>””””</p> <p><em>Input 2</em> - Reverse generation technique</p> <p>”””</p> <p>I instructed my friend to &lt;INSERT&gt;</p> <p>The friend read the instructions and wrote an output for every one of the inptus. Here are the input-output pairs:</p> <p>Input: [ ] Output: [ ]</p> <p>Input: [ ] Output: [ ]</p> <p>…</p> <p>”””</p> </li> <li> <p>Scoring Instructions. Evaluate each instruction by computing a score that reflects how well the instruction guides the target LLM for the task. This is simply the confidence score associated with the log likelihoods of token generation. The authors consider a <em>moving average</em> score considering the probabilities for a window of tokens.</p> <p>They also consider an <strong>execution accuracy</strong> - the success of an instruction by checking if the model produces the correct output (0-1 loss). However, this cannot. be used for all kinds of instructions.</p> <p>The top \(k\)-percentile prompts are selected and the rest are discarded.</p> </li> <li> <p>LLM as Resampling Model. They apply an Iterative Monte search method to resample more prompts. The LLM generates semnatically similar instructions variants to improve the top-performing candidates.</p> <p>Once the prompts are generated, the moving average scores are generated for each of the prompts and the better scoring prompts are selected again.</p> </li> </ol> <p>Can APE be used to guide LLMs?</p> <p><img src="/assets/img/LLMs/2024-10-23-10-41-54-image.png" alt=""/></p> <p>Although this is a very simple example, the work shows potential in taking such framework forward to work with more complex applications.</p> <p>Another interesting approach is to not generate the prompts from scratch, but to help humans design better prompts. Essentially, augment with context from humans to generate better prompts. On the flipside, RLHF can be used to improve these APE.</p> <h1 id="tree-of-thoughts"><a href="https://arxiv.org/abs/2305.10601">Tree of Thoughts</a></h1> <p>The early Language Models were limited by their token-level, left-to-right decision making. However, some tasks require exploration, stratefic lookahead, planning, and backtracking. The vanilla architecture does not support such mechanisms.</p> <h2 id="framework">Framework</h2> <p><img src="/assets/img/LLMs/2024-10-30-09-36-33-image.png" alt=""/></p> <p>Mathematically, these models are depicted as</p> <ul> <li> <p>Input output programming - \(y \sim p_\theta^{IO}(y \vert x)\)</p> </li> <li> <p>Chain</p> </li> </ul> <p>Theoretically, the model seems promising. However, there are some intricate details that need to be figured out -</p> <ul> <li> <p>How to decompose the porcess into steps</p> </li> <li> <p>How to generate the potential thoughts from each state</p> <ul> <li> <p>Need to be small enough so LMs can generate promising and diverse samples</p> </li> <li> <p>Big enough so LMs can evaluate the difference contributing to the results</p> </li> </ul> </li> <li> <p>How to heuristically evaluate each state</p> </li> <li> <p>How to navigate through the generated tree</p> </li> </ul> <h2 id="thought-decomposition">Thought decomposition</h2> <p><img src="/assets/img/LLMs/2024-10-30-09-41-30-image.png" alt=""/></p> <h3 id="method-1---direct-prompting">Method 1 - Direct Prompting</h3> <p>The prompts themselves can ask the LM to segment the problem into multiple problems. Due to the voting mechanism, LM generates multiple possibilities for an answer and chooses the best model. This works better when thought space is rich and i.i.d samples lead to diversity.</p> <h3 id="method-2---backtracking">Method 2 - Backtracking</h3> <p>Propose thoughts sequentially using a “propose prompt”. When the thought space is constrained, this works better - proposing different thoughts in the same context avoids duplication.</p> <h2 id="state-evaluator">State Evaluator</h2> <p>There are two strategies to evaluate each generated state</p> <ul> <li> <p><strong>Value</strong> each state independently, where a value prompt reasons about the state \(s\) to generate a scalar value \(v\). This value is very context dependent.</p> </li> <li> <p><strong>Vote</strong> across states by deliberately comparing different states in \(S\) in a vote prompt.</p> </li> </ul> <h2 id="search-algorithm">Search Algorithm</h2> <ul> <li> <p><strong>BFS</strong> is helpful when the tree depth is limited and the initial thought steps can be evaluated and pruned to a small set</p> </li> <li> <p><strong>DFS</strong> explores longer trees well - subtrees are pruned to trade exploration for exploitation.</p> </li> <li> <p>More advanced approaches such as \(A^*\) and MCTS are left to future work in the paper.</p> </li> </ul> <p>An interesting summary of all thought paradigms - <a href="https://arxiv.org/pdf/2401.14295">Demytifying Chains, Trees, and Graphs of Thoughts</a>.</p> <h1 id="on-second-thought-lets-not-think-step-by-step-bias-and-toxicity-in-zero-shot-reasoning"><a href="https://urldefense.com/v3/__https://arxiv.org/abs/2212.08061__;!!Mih3wA!FUSiREHKHqULp_GaFY0sSmJRsiVZqYBdk9nJf8WWrKLI4UoxKUzc3ir1rIQaWXw6bk6_UVVe0kXW$">On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning</a></h1> <p>We have seen how chain of thought improves problem solving capabilities. However, in some cases, CoT actually causes issues -</p> <p><img src="/assets/img/LLMs/2024-10-30-10-17-51-image.png" alt=""/></p> <p>The authors explore such effects in the paper. They consider</p> <h1 id="least-to-most-prompting-enables-complex-reasoning-in-large-language-models"><a href="https://arxiv.org/pdf/2205.10625">Least-to-most prompting enables complex reasoning in Large Language Models</a></h1> <p>The key motivators for the paper are as follows -</p> <ul> <li>Given a new task</li> </ul> <p>In the prior works, CoT reasoning has been effective for many tasks but struggled with “Easy-to-hard generalization”. Inspired from educational philosophies, the model is implemented by few-show ptompting in 2 stages - decomposition stage and subproblem solving stage.</p> <ul> <li> <p>Decomposition stage - The problem is divided into subtasks <em>once</em> before solving</p> </li> <li> <p>Subsequent solving stage - Solve the subsequent problems one by one.</p> </li> </ul> <p>The key difference from CoT prompting is that CoT starts each sub-problem from scratch and is unable to build from previous reasoning. This behaviour is depicted using the symbolic manipulation task in the paper - The performance of CoT progressively decreases as the length of the list increases.</p> <p>This method of prompting achieves significantly better results borrowing the context from the previous subproblems to arrive at the final answer.</p> <p>However, decomposition thoughts don’t generalize across domains. This limitation mainly shows up for math problems where the subproblems need to be correctly decomposed to solve the original problem.</p> <h1 id="chain-of-thoughtlessness"><a href="https://arxiv.org/pdf/2405.04776">Chain of Thoughtlessness</a></h1> <p>CoT prompting sounds too good to be true. The paper aims to test this paradigm rigorously to verify the claims. The paper also tries to identify the difference between complex reasoning and pattern matching - What seems like “complex reasoning” may just be a case of pattern matching?</p> <p>Consider the chain of thought reasoning -</p> <ul> <li> <p>How specific do the prompt examples have to be to the original problem?</p> </li> <li> <p>How generalizable are these prompts or how specific do they need ot be?</p> </li> <li> <p>How much human effort is needed to craft prompts for each problem subclass?</p> </li> </ul> <p>Furthermore, there are issues with the test domains as well. For example, GSM8K</p> <ul> <li> <p>They are non scalable - problem instances cannot be scaled</p> </li> <li> <p>The problems are static and be easily found in the training data</p> </li> </ul> <p>The main point the paper is trying to address the question - “Is it really possible to teach an LLM how to solve a generalizable problem?”. To test this claim, the authors choose “Blocks world” as the problem domain - given an initial and end configuration, output a series of steps to reach the end configuration from the initial configuration.</p> <p>They perform the following experiments</p> <ul> <li> <p><strong>Zero shot CoT</strong> - Simply append “Let’s think step by step” to the prompts.</p> </li> <li> <p><strong>Progression proof</strong> - Specific to planning problems. Each example’s steps describe the init state, action taken, reason of the action and the final step.</p> </li> </ul> <p>They see that zero-shot CoT achieves insignificant performance gains from zero-shot prompting. The progression proof CoT achieves a lower performance - this may be due to overfitting to the training examples. The LLM fails to learn the <em>universal block algorithm</em> (break the tower and put everything back) even with multiple version of CoT prompting. The authors chose a planning domain on purpose because these problems can be scaled up very well.</p> <p>The authors just wanted to highlight that there is a need for more rigorous testing. One might argue that planning problems are way out of domain of LLMs. So, the authors test the findings with commonly tested problems, and they find similar trends.</p> <h1 id="chain-of-thought-without-prompting"><a href="https://arxiv.org/abs/2402.10200">Chain of Thought without prompting</a></h1> <p>Prompting techniques, while effective, often encode task-specific human priors, thereby making it difficult to assess a language model’s intrinsic reasoning abilities. Ideally, a language model should be able to reason independently and provide the optimal response, without requiring humans to tweak the prompts or refine repeatedly if the initial response is unsatisfactory. Model-tuning can be expensive and requires a substantial amount of supervised data. In this work, we explore a different perspective and ask: Can LLMs reason effectively without prompting? And to what extent can they reason? We find that, perhaps surprisingly, there exists a task-agnostic way to elicit CoT reasoning from pre-trained LLMs by simply altering the decoding procedure. Figure 1 illustrates this phenomenon: given a reasoning question, the LLM generates a wrong answer via the standard greedy decoding path, yet alternative top-𝑘 token inspection unveiled inherent CoT paths (e.g., decoding paths 2 and 4), which accurately resolved the query. This decoding modification bypasses prompting and is entirely unsupervised without the need for model tuning.</p> <p><strong>Why can’t LLMs reason if we only consider greedy decoding path?</strong></p> <h1 id="large-language-models-are-zero-shot-reasoners"><a href="https://arxiv.org/abs/2205.11916">Large Language Models are Zero-shot reasoners</a></h1>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[A survey of papers to better understand the workings of Large Language Models.]]></summary></entry><entry><title type="html">Design and Analysis of Algorithms</title><link href="https://sudhansh6.github.io/blog/Design-and-Analysis-of-Algorithms/" rel="alternate" type="text/html" title="Design and Analysis of Algorithms"/><published>2024-09-27T00:00:00+00:00</published><updated>2024-09-27T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Design-and-Analysis-of-Algorithms</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Design-and-Analysis-of-Algorithms/"><![CDATA[<h1 id="greedy-algorithms">Greedy Algorithms</h1> <h2 id="minimum-spanning-tree">Minimum Spanning Tree</h2> <p>Consider a graph \(G\) describe by \(V\) and \(E\) (positive weights). A <strong>spanning tree</strong> of a graph is defined as an edge set \(T \subset E\) such that \((V, T)\) is a tree. A minimum spanning tree is such that \(\sum_{l \in T} l_e\) is minimized.</p> <p>For a graph with \(n\) vertices, there are \(n^{n - 2}\) spanning trees for a complete graph (<strong>Cayley’s formula</strong>).</p> <p>How do we calculate the number of spanning trees for a general graph? <strong>Kirchoff’s theorem</strong> states the following -</p> <ul> <li>Let \(M\) be the adjacency matrix of \(G\)</li> <li>Let \(L - M\) except \(L_{i, i} = -deg(i)\) - This is generally called as <strong>Graph Laplacian</strong></li> <li>Then, #spanning trees is the determinant of any \(m-1\) square sub-matrix (obtained by removing \(i\)th row and column) of \(L\).</li> </ul> <p>Notice how any sub-matrix yields the same value!</p> <h2 id="greedy-idea-1-kruskals-algorithm">Greedy Idea 1: Kruskal’s algorithm</h2> <ul> <li>Sort all the edges with their weights</li> <li>Pick edges as long as they don’t form a cycle</li> </ul> <p>Does this work? If it does, how do we prove it?</p> <p>Firstly, why is it a greedy idea? At each point of the algorithm, we select the current greedy edge (a local minimum) to obtain the minimum spanning tree (a global optimum).</p> <ul> <li> <p><strong>The cut property -</strong> Let \(S \subseteq V\) such that \(S\) and \(V - S\) are non-empty. If \(e\) is an edge across \(S\) and \(V - S\) with the minimum cost, then there always exists a minimum spanning tree with \(e\).</p> <p><strong>Proof.</strong> Exchange argument. If there an MST with \(e’\) across \(S\) and \(V - S\), then replace \(e\) with \(e’\) to obtain another MST.</p> <blockquote> <p>Shouldn’t this argument be more delicate? Why is there a single edge from S to V - S?</p> </blockquote> <p>Essentially, the exchange argument argues replacing a part of the solution improves the solution but does not worsen it.</p> </li> <li> <p>The time complexity of the algorithm comes out to be \(O(m \log m + m \alpha(n))\). The second term in the expression comes from union-find data structures.</p> </li> <li> <p><strong>Correctness of the algorithm</strong> - We shall prove this via induction.</p> <ul> <li>Induction hypothesis - The edges selected in the \(i\)th round of Kruskal’s algorithm can form an MST along with a subset of edges from the remaining edges.</li> <li>Base statement - True for \(i = 0\)</li> <li>Induction step - Cut property</li> </ul> </li> <li> <p><strong>Union-find data structure</strong> - A data structure that supports</p> <ul> <li>Merging elements of two sets into a single set - <code class="language-plaintext highlighter-rouge">union(x, y)</code></li> <li>Checking whether two elements are in the same set - <code class="language-plaintext highlighter-rouge">find(x)</code></li> </ul> <p>efficiently. The amortized time complexity for these operations is \(\alpha(n)\) where \(\alpha(n) \leq 4\) for \(n\) of the order \(2^{2^{2^{2^{16}}}}\). As a result, \(\alpha(n)\) can be regarded as a constant for practical purposes.</p> <p>In our case, the elements are edges and sets represent connected components.</p> </li> </ul> <h2 id="greedy-idea-2-prims-algorithm">Greedy Idea 2: Prim’s Algorithm</h2> <p>Start with any node and expand with the smallest edge connecting to the remaining set of edges. Note that this is different from Kruskal’s algorithm where we sort all the edges and create individual connected components that eventually merge together.</p> <p>The proof for Prim’s algorithm is very similar to that of Kruskal’s. The time complexity is \(O(n^2 + m)\) similar to Djikstra’s algorithm without a data structure. We can maintain a <strong>priority queue</strong> to maintain all edges that come from \(S\) to reduce the time-complexity to \(O((n + m) \log m)\) (without decrease-key). With decrease key and a binary heap, the complexity becomes \(O((n + m) \log n)\). Furthermore, with decrease key and a Fibonacci heap, the complexity reduces to \(O((n\log n + m)\).</p> <h2 id="other-algorithms">Other algorithms</h2> <ul> <li><strong>Reverse deletion</strong> - For every cycle in the original graph and the edge \(e\) with the maximum cost, there always exists an MST without \(e\). Until there are no cycles in the graph, find a cycle and delete the edge with a maximum cost. Note that this algorithm has a higher time complexity since we try and find a cycle for each iteration of the algorithm. How do we implement this?</li> </ul> <h2 id="union-find-data-structure">Union-Find data structure</h2> <p>The idea is to maintain trees with pointers to merge and find elements. The main complication comes while merging the individual sets.</p> <ul> <li> <p>Merging by size (consuming smaller ones by larger sets) - The complexity of merging sets of size \(n\), \(m\) times takes \(O(m \log n)\)</p> </li> <li> <p>To optimize this further, we merge by rank (generalizing the previous approach where rank was simply the size of the set). We add another trick to reduce the amortized time complexity.</p> <ul> <li>Path compression - When <code class="language-plaintext highlighter-rouge">find(x)</code> is called, attach the found elements along the path directly to the root to reduce the path size.</li> </ul> <p>The time complexity then becomes \(O(m \log^* n)\) where \(\log^* n\) is the minimum \(k\) such that \(\log^{(k)} n \leq 1\).</p> </li> </ul> <h1 id="more-greedy-problems-related-to-mst">More Greedy Problems related to MST</h1> <h3 id="k-clustering">\(k\)-clustering</h3> <p>A <strong>maximum spacing</strong> for \(k\)-clustering of \(G =(V, E)\) is defined as</p> <ul> <li>An edge set \(T \subset E\) such that \((V, T)\) has exactly \(k\) connected components</li> <li>The <strong>spacing</strong> is then \(\min d(u, v)\) for \(u, v\) in different connected components</li> <li>The goal is to maximize the spacing</li> </ul> <p>This problem can be solved again with Kruskal’s algotihm to find \(k\)-connected components - perform the <code class="language-plaintext highlighter-rouge">union</code> operation for \(n - k\) times. Why is this correct? WE can show this using a contradiction.</p> <ul> <li>Consider two nodes that lie in the same connected component in the result obtained by Kruskal’s (with spacing \(d’\)). Let them be in different connected components in the optimal solution (with spacing \(d\)). Then,</li> </ul> <h3 id="second-mst">Second MST</h3> <p>A second MST is essentially the spanning tree with the <em>second</em> lowest edge summation cost. How do we find this tree?</p> <ul> <li>Find an MST with weight \(w\)</li> <li>For every edge \(e\) not in the MST, if adding \(e\) yields a cycle in the graph; then remove the largest edge \(e’\) other than \(e\) in the cycle to obtain the second MST</li> <li>The cost of the tree would be \(w + l_e - l_{e’}\)</li> </ul> <p>The time complexity of this algorithm is \(\mathcal O(T_{MST} + mn)\) and can be improved to \(\mathcal O(T_{MST} + m \log n)\) with better data-structures and divide-and-conquer.</p> <p><strong>Lemma.</strong> The second MST only differs by one edge from the MST. Multiple MSTs? <strong>Proof.</strong> Can be shown using contradiction. The idea is that one can move from one spanning tree to another with local changes in the trees. The argument is that you can replace the edges in the second MST with the edges in the MST to obtain a tree with a lower cost. This process can be repeated until there is only one edge that is different from an MST and replacing that would cause the tree to become the MST.</p> <h2 id="more-greedy-problems">More Greedy Problems</h2> <h2 id="needle-in-haystack">Needle in Haystack</h2> <p>Given two strings \(s, t\), decide whether there is a subsequence (need not be contiguous) in \(s\) that matches with \(t\). A naive greedy algorithm is depicted as follows -</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="nf">len</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="k">break</span> 
    <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">t</span><span class="p">[</span><span class="n">j</span><span class="p">]:</span>
        <span class="n">j</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># s[i] is matched with t[j]
</span>    <span class="k">else</span><span class="p">:</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div> <p>On first glance, it looks as if this is a very intuitive algorithm. However, there are more intricate details and the proof makes these clearer. The proof relies on the Exchange argument. If there exists a subsequence \(i_1 i_2 \dots i_m\) in \(s\) matching \(t\) (\(\vert t \vert = m\)). If \(i^*_1 &lt; i_1\) is the first index that \(s[i_1] = t[1]\), then \(i^*_1i_2\dots i_m\) also matches \(t\). This way we can find a set of indices \(i^*_1i^*_2 \dots i^*_m\) through the greedy algorithm that gives the correct answer.</p> <p>In general, greedy algorithms can be proven in two general ways</p> <ul> <li>Consider any iteration of the algorithm and show that the decision made by the greedy algorithm is the best and conclude using induction</li> <li>Exchange argument: Pick an optimal solution and gradually change it to the solution produced by the greedy algorithm showing that the optimality is not affected.</li> </ul> <h2 id="matroids">Matroids</h2> <p>A finite matroid \(M\) consists of \((E, I)\) where</p> <ul> <li>\(E\) is a finite set (called the ground set). For example, \(E\) is the set of all edges in the graph \(G = (V, E)\)</li> <li>\(I\) is a collection of subsets of \(E\) (called the independent set). For example, \(I\) consists all subsets of \(S\) of \(E\) such that all the edges in \(s \in S\) form a forest.</li> </ul> <p>\((E, I)\) should satisfy the following properties -</p> <ul> <li>Null set should be in \(I\) - \(\phi \in I\)</li> <li>if \(A \subset B\) and \(B \in I\) then \(A \in I\)</li> <li>If \(A, B \in I\), \(\vert A\vert &gt; \vert B \vert\) then \(\exists e \in A - B\), such that \(B \cup \{e\} \in I\)</li> </ul> <p>Isn’t 2 inclusive of 1 and 3?</p> <p>How does this data structure help us? Suppose we have a graph \(G = (V, E)\) with weights \(c_e \geq 0\). Then, design an algorithm to find an independent set \(S\) that maximizes \(\sum_{e \in S} c_e\). Consider the following algorithm -</p> <ul> <li>Sort \(e\) in decreasing order of weights</li> <li>Let \(S = \phi\). Add \(e\) to \(S\) if \(e\) does not add cycles in \(S\)</li> </ul> <p>This algorithm is very similar to the reverse deletion algorithm and has a time complexity \(\mathcal O(\vert E \vert \log \vert E \vert + T_{check \text{ IS}})\).</p> <p><strong>Lemma.</strong> Let \(e \in E\) have the maximum cost \(c_e\)m then there always exists an IS \(A\) with maximum weight containing \(e\).</p> <p>The proof is very similar to what we have shown with MSTs. This example can be applied to MSTs, and it demonstrates how Matroids can be useful for greedy algorithms.</p> <h2 id="task-scheduling">Task Scheduling</h2> <p>Given \(n\) jobs each with \(t_i\) time to finish and deadlines \(d_i\). Consider that there is a single resource solving tasks sequentially from time \(0\), and a job has to completely finished before moving onto the next one.</p> <p>Suppose in a scheduling algorithm, job \(i\) finishes at time \(f_i\), then the lateness is defined as \(l_i = \max\{0, f_i - d_i\}\). The goal is find an algorithm that minimizes the maximum lateness \(minimize \max_i l_i\).</p> <p>Let us consider a simple case. Suppose there are two jobs with \(d_i \leq d_2\). Note that any scheduling algorithm should not have any idle time between jobs. Why so? If there is idle time, you can always do a job earlier to reduce the lateness. Furthermore, a scheduling algorithm should not have an <em>inversion.</em> That is, if job 1 has deadline earlier than job 2, then it is always optimal to perform job 1 before job 2. This claim can be easily proved using the exchange argument.</p> <h3 id="algorithm">Algorithm</h3> <p>Sort all jobs according to the increasing order of these deadlines \(d_i\), then complete each job without any idle time.</p> <p><strong>Proof.</strong> Generalize the previous two observations to \(n\) jobs.</p> <h2 id="huffman-codes">Huffman Codes</h2> <p>How do we encode an alphabet in binaries to have no ambiguities.</p> <h3 id="prefix-codes">Prefix codes</h3> <p>A prefix code for an alphabet \(T\) is a function \(f:T \to \{0, 1\}^*\), such that for distinct \(x, y \in T\), \(f(x)\) is not a prefix of \(f(y)\).</p> <p>It can be shown that a prefix code gives unique decoding.</p> <p>How do we design an encoding that is most efficient? Let us define efficiency. For every letter \(x\) in \(T\), let its frequency be \(p_x (\sum_{x \in T} p_x = 1)\). Let \(f\) be a prefix code and for every letter \(x \in T\), let \(\vert f(x)\vert\) is the number of bits. The goal is to find a prefix code \(f\) that minimizes the expected number of bits when encoding \(R\) under the frequency \(\{p_x\}\).</p> \[\text{minimize } \sum_{x \in T} p_x \cdot |f(x)|\] <p>It is beneficial to represent prefix codes as a binary tree. Each node has two children: 0 and 1. The paths from the root to other nodes in the tree represent the binary encodings.</p> <ul> <li>For prefix codes, no node of a symbol is an ancestor of node of another symbol (from the alphabet).</li> <li>Another observation is that any optimal prefix code is a full tree (every inner node has two children) - if anode has a single chide, the parent node can itself be used for the symbol deleting the leaf node making the encoding more efficient.</li> <li>There is an optimal tree (prefix code) such that two lower frequent letters are siblings, and are as deep as possible in the tree. This claim can be proved easily with the exchange argument.</li> </ul> <p>With these observations, consider the following algorithm</p> <ul> <li> <p>Initialize each letter \(x\) as a node and label it with \(p_x\)</p> </li> <li> <p>Put all nodes into a min-heap (according to the frequency)</p> </li> <li> <p>While min-heap has atleast two elements</p> <ul> <li> <p>Pop out the two smallest elements \(u, v\) (corresponds to two trees)</p> </li> <li> <p>Combine them to a single tree</p> </li> <li> <p>Push it into the heap, label with \(p_u + p_v\)</p> </li> </ul> </li> </ul> <p>This is the Huffman’s coding algorithm which has a time complexity of \(n \log n\).</p> <h2 id="shannons-source-coding-theorem">Shannon’s source coding theorem</h2> <p>Let \(T\) be an alphabet with frequency \(\{p_x\}\). The entropy of the alphabet is defined as</p> \[H := \sum_{x \in T} p_x \cdot \log \frac{1}{p_x}\] <p>The Shannon’s source coding theorem then states that you cannot send a letter from \(T\) with frequenct \(\{p_x\}\), with expected bits less than \(H\). Huffmman’s encoding gives a solution with expected bits at most \(H + 1\).</p> <p><strong>Important point</strong>. One can suggest to increase the alphabet size with dummy symbols to virtually reduce the value of \(H\) significantly. However, this introduces complexity for encoding algorithms. Therefore, there is a tradeoff with space occupied by encoding and the time for encoding.</p> <blockquote> <p>Even with augmented alphabet, the size of the encoding does not change for the original symbols in the alphabet?</p> </blockquote> <h1 id="binary-search-x-greedy-algorithms">Binary Search X Greedy Algorithms</h1> <p>The basic binary search takes advantage of the monotone structure in arrays to identify elements with certain properties. Any binary search problem can be converted to the following simpler version: For an array \(B\) that has binary elements with all zeros occurring before all ones, find the index of the first occuring \(1\).</p> <p>The binary search algorithm can be simply proved using induction.</p> <p>Let us consider an example - Ternary search. Given an array \(A[1\dots n]\) that is first strictly increasing and then strictly decreasing, dind the largest element. The array \(B[1\dots n - 1]\) is constructed as</p> <ul> <li> \[B[i] = 1 \iff A[i + 1] &gt; A[i]\] </li> <li> \[B[i] = 0 \iff A[i + 1] &lt; A[i]\] </li> </ul> <h2 id="split-array-largest-sum">Split-array largest sum</h2> <h2 id="minimum-fractional-st">Minimum fractional ST</h2> <p>Given an undirected graph \(G = (V, E)\) and each edge has two costs \(a_e, b_e\) both of which are positive, find a spanning tree \(T\) that minimizes</p> \[\frac{\sum_{e \in T} a_e}{\sum_{e \in T} b_e}\] <p>How is this related to binary search? Firstly, we will convert this problem to a decisional version - Given an undirected graph \(G = (V, E)\) and a real number \(U\), decide whether there exists a spanning tree \(T\) such that \(\frac{\sum_{e \in T} a_e}{\sum_{e \in T} b_e} \leq U\).</p> <p>This is equivalent to find a spanning tree such that \(\sum_{e \in T} a_e - U b_e \leq 0\). Construct a new graph with the weights \(a_e - Ub_e\). The reduction is easy to follow.</p> <p>How do we find the monotone structure for binary search? If the decision problem \((G, U)\) is satisfiable, then \((G, U')\) is also satisfiable for any \(U' &gt; U\). Conceptually, assume a function \(B\) (with continuous index) such that \(B[0, S] \to \{0, 1\}\) where \(S\) is an upper bound. \(B(U) = 1\) iuf and only if \((G, U)\) is satisifiable, and \(B\) is monotone.</p> <h1 id="divide-and-conquer">Divide and Conquer</h1> <h2 id="master-theorem">Master Theorem</h2> <p>Consider an algorithm that has the following relationship for running time complexity -</p> \[T(n) = 2T \left(\frac{n}{2}\right) + c n \log^k n \quad (k \geq 0)\] <p>then \(T(n) = \mathcal O(n \log^{k + 1} n)\).</p> <h2 id="closest-point">Closest Point</h2> <h2 id="fast-multiplication">Fast Multiplication</h2> <p>Suppose we have two integers in binary \(a = \sum_{0 \leq 1 \leq n} a_i \cdot 2^i, b = \sum_{0 \leq 1 \leq n} b_i \cdot 2^i\). The goal is to compute \(c = ab = \sum_{0 \leq j &lt; 2n} c_j \cdot 2^j\) where \(c_j = \sum_{0 \leq k \leq j} a_k b_{j - k}\). The naïve brute force approach takes \(\mathcal O(n^2)\) to compute the answer.</p> <p>This question is related to matrix multiplication as well. The naïve algorithm takes \(\mathcal O(n^3)\).</p> <h3 id="algorithm-1">Algorithm 1</h3> <p>We segment \(a, b\) as follows -</p> <ul> <li> <p>\(a = A_1 \cdot 2^{\frac{n}{2}} + A_0\) where \(A_0 = \sum_{0 \leq 1 &lt; n/2} a_i \cdot 2^i\) and \(A_1 = \sum_{n/2 \leq i &lt; n} a_i \cdot 2^{i - n/2}\)</p> </li> <li> <p>\(b = B_1 \cdot 2^{\frac{n}{2}} + B_0\) similarly.</p> </li> </ul> <p>Then, \(ab = (A_1 \cdot 2^{\frac{n}{2}} + A_0)(B_1 \cdot 2^{\frac{n}{2}} + B_0)\). The strategy then is to do a divide and conquer on these halves to get the final answer.</p> \[ab = A_1 B_1 2^n + (A_0 B_1 + A_1 B_0)2^{n/2} + A_0B_0\] <p>The time complexity is then \(T(n) = 4T(\frac{n}{2}) + \mathcal O(n)\). This is essentially \(\mathcal O(n^2)\) that does not give any improvement.</p> <p>This can be optimized further -</p> \[ab = A_1 B_1 2^n + ((A_0 + A_1)(B_0 + B_1) - A_0B_0 - A_1 B_1)2^{n/2} + A_0B_0\] <p>The number of multiplications reduced to 3 - \(T(n) = 3T(\frac{n}{2}) + \mathcal O(n)\). Deriving the final expression, \(T(n) = cn + cn\frac{3}{2} + \dots + cn\left(\frac{3}{2}\right)^{\log n} = \mathcal(3^{\log n})\).</p> <p>This algorithm can be extended to matrix multiplications as well.</p> \[C = AB = \begin{bmatrix} A_{00}B_{00} + A_{01}B_{10} &amp; A_{00}B_{01} + A_{01} B_{11} \\A_{10}B_{00} + A_{11}B_{10} &amp; A_{10}B_{01} + A_{11}B_{11}\end{bmatrix}\] <p>The naïve algorithm shown above is still \(O(n^3)\). Strassen’s algorithm reduces the number of multiplications to \(7\) providing an improvement over the \(\mathcal O(n^3)\) algorithm giving \(\approx \mathcal O(n^{2.81})\).</p> <p>The current state of the art algorithm for matrix multiplication achieves \(\mathcal O(n^{2.371552})\). We do not know if there is an algorithm that achieves \(\mathcal O (n^{2 + o(1)})\).</p> <h3 id="algorithm-2">Algorithm 2</h3> <p>Multiplication can be seen as a special case of convolution and we can use <strong>Fast Fourier Transform (FFT)</strong> to perform this in \(\mathcal O(n \log n)\). The details will be elaborated in the next section.</p> <h2 id="convolution">Convolution</h2> <p>Consider two vectors of the following form -</p> <ul> <li> \[a = (a_{n - 1}, a_{n - 2}, \dots, a_2, a_1, a_0)\] </li> <li> \[b = (b_{n - 1}, b_{n - 2}, \dots, b_2, b_1, b_0)\] </li> </ul> <p>The convolution operation \(\star\) is defined as</p> \[c = a\star b = (c_{n - 1}, \dots, c_0) \quad \text{ where } c_j = \sum_{0 \leq k &lt; n} a_j b_{(j - k)\mod n}\] <p>Convolution is a generalization of integer multiplication (padding + convolution = multiplication). Also, convolution is a central operation in signal processing - used for blurring images and also to learn features from spatial data.</p> <p>The naïve algorithm can be done in \(\mathcal O(n^2)\) time. We can perform convolution using \(\mathcal O(n\log n )\) using <strong>Fourier Transform</strong>.</p> <h1 id="fourier-transform">Fourier Transform</h1> <p>Consider the \(n\) dimensional vector \(a = (a_{n - 1}, a_{n - 2}, \dots, a_2, a_1, a_0)\) and \(b = (b_{n - 1}, b_{n - 2}, \dots, b_2, b_1, b_0)\). Let \(\{e_i\}_i\) form a unit basis of \(\mathbb R^n\) such that \(a = \sum_{0 \leq i &lt; n} a_i e_i, b = \sum_{0 \leq i &lt; n} b_i e_i\).</p> <p>Consider another basis \(\hat e_i(j) = \omega_n^{ij}\) where \(\omega_n = e^{\frac{1\pi \bf{i}}{n}}\) is the \(n\)-th root of unity. Therefore, \(\hat e_i = \frac{1}{\sqrt{n}} \omega_n^{(n - 1)i}, \dots, \omega_n^{2i}, \omega_n^{i}, 1)\).</p> <p>It is easy to check that this is a valid basis. So, again, \(a, b\) can be uniquely represented as</p> <ul> <li> <p>\(a = \sum_{0 \leq i &lt; n} \hat a_i \hat e_i\), \(\hat a_i = \langle a_i, \hat e_i\rangle = \frac{1}{\sqrt{n}} \sum_j a_j \omega_n^{-ij}\)</p> </li> <li> \[b = \sum_{0 \leq i &lt; n} \hat b_i \hat e_i\] </li> </ul> <p>A <strong>Fourier transform</strong> is then defined as - Given \(\{a_i\}_{i \in [n]}\), compute \(F(a) = \{\hat{a_i}\}_{i \in [n]}\).</p> <p>The <strong>inverse problem</strong> is to find \(F^{-1} (\hat a) = \{a_i \}_{i \in [n]}\). It essentially is a change of basis between \(\{e_i\} \iff \{\hat e_i\}\).</p> <h2 id="convolution-theorem">Convolution Theorem</h2> <p>Let \(a, b\) be two vectors in \(\mathbb R^n\); then,</p> \[a \star b = F^{-1} (F(a) \cdot F(b))\] <p>With this claim, convolution can be f=done in \(\mathcal O(2T_{FT} + T_{IFT} + n)\).</p> <h1 id="dynamic-programming">Dynamic Programming</h1> <h2 id="longest-path-on-a-dag">Longest path on a DAG</h2> <p>Given a DAF with \(n\) vertices, \(m\) edges, every edge \(e\) has a weight \(l_e\), compute the longest path on the DAG. The length of a path is defined as the weight sum over all edges in the path.</p> <p>Consider the following algorithm</p> <pre><code class="language-pseudocode">DFS(u):
    if marked[u] = true:
        return DP[u]
    cost &lt;- 0
    for all v that (v, u) in E:
        cost &lt;- max(cost, DFS(v) + l_{v, u})
    marked[u] &lt;- true
    DP[u] &lt;- cost
    return cost
</code></pre> <p>The time complexity of the algorithm is \(\mathcal O(n + m)\). The key point to notice is that instead of recomputing the cost of each path, we have essentially stored the costs in the array <code class="language-plaintext highlighter-rouge">DP</code> to reduce the redundant calculations. This step is known as <strong>memoization</strong>.</p> <h2 id="knapsack-problem">Knapsack Problem</h2> <p>Consider an integer \(U\) representing total capacity and a list of integers \(\{v_i, c_i\}\) that represents the volume and cost of each item respectively. The goal is to pick items such that their total volume is at most \(U\) and their value is maximized.</p> <p><strong>Idea 1</strong>. Sort everything by \(v_i/c_i\) and pick the items until value if \(U\). It is easy to see that this greedy algorithm will not work.</p> <h3 id="a-backtracking-algorithm">A Backtracking algorithm</h3> <p>Consider an iterative algorithm that at step \(i\) has \(C\) volume left and is considering whether to pick or skip the \(i\)-th item. Considering these two possibilities, we can implement a brute force algorithm with memoization for dynamic programming.</p> <p>We set a 2D matrix of size \((U, n)\) where each row \(i\) represents the set of items that need to be picked to maximize the cost within volume \(U\). The time complexity of this algorithm would be \(\mathcal O(2^n)\).</p> <p>The algorithm is as follows -</p> <pre><code class="language-pseudocodedfs(C,">   dfs(C, i):
       if i = 0: return 0
       Cost &lt;- dfs(C, i - 1)
       if C &gt;= c_i:
           Cost &lt;- max(Cost, dfs(C - c_i, i - 1) + v_i)
       return Cost
</code></pre> <h3 id="alternative-view">Alternative view</h3> <p>We can treat every possible \((C, i)\) as a vertex in a graph. Every vertex has at most two outcoming edges - \((C, i) \to (C - c_i, i - 1)\) with cost \(v_i\) and \((C, i) \to (C, i - 1)\) with cost \(0\). This constructed graph is a DAG and we essentially reduced Knapsack problem to longest path on a DAG.</p> <p>Based on the algorithm we have seen earlier, we modify the algorithm to include memoization</p> <pre><code class="language-psuedocode">   dfs(C, i):
       if marked[C][i] = true: return DP[C][i] // Modification
       if i = 0: return 0
       Cost &lt;- dfs(C, i - 1)
       if C &gt;= c_i:
           Cost &lt;- max(Cost, dfs(C - c_i, i - 1) + v_i)
       marked[C][i] &lt;- true // Modification
       DP[C][i] &lt;- cost // Modification
       return Cost
</code></pre> <p>The modified algorithm now has the time complexity \(\mathcal O(Un)\) since there are \(Un\) vertices in total with atmost 2 edges each.</p> <h2 id="general-observation">General observation</h2> <p>Dynamic Programming problems can be typically thought og as a decision-making processes. These decision problems can be converted to graphs where the states are vertices on a graph and the transitions are edges on a graph. Typically, the problem have a DP solution if the graph is a DAG and the number of states is not too large.</p> <p>The algorithm shown above can then be used as a general procedure to solve the problems. Sometimes, it is beneficial to implement the algorithms with a loop rather than recursion.</p> <h2 id="knapsack-with-unlimited-items">Knapsack with unlimited items</h2> <p>The algorithm remains pretty much the same except that teh recrusion call has <code class="language-plaintext highlighter-rouge">dfs(C - c_i, i)</code> instead of <code class="language-plaintext highlighter-rouge">dfs(C - c_i, i - 1)</code>.</p> <h2 id="knapsack-with-limited-items">Knapsack with limited items</h2> <p>We can consider another variant where item \(i\) can be used at most \(k_i\) times. Then, a similar algorithm would have the time complexity \(\mathcal (U \sum_i k_i)\).</p> <p>A better solution treats the \(i\)th item as \(\lceil \log k_i\rceil\) items. For example, if \(k_i = 8\), then divide the item as \((c_i, v_i), (2c_i, 2v_i), (4c_i, 4v_i), (c_i, v_i)\). Then the time complexity would be reduced to \(\mathcal O(U \sum_i \log k_i)\).</p> <p>However, it can be improved to \(\mathcal O(Un)\) using a <strong>monotonic queue</strong>.</p> <h2 id="2d-knapsack">2D Knapsack</h2> <p>In this variant, each item has value \(v_i\), volume integer \(c_i &gt; 0\) and a weight integer \(w_i &gt; 0\). The goal is to find a subset of items that has the total volume at most \(U\), total weight at most \(W\) and the total value is maximized. The dynamic programming algorithm has a runtime of \(\mathcal O(WUn)\).</p> <p>In the loop variant of the algorithm, it is better to iterate over the items first rather than the weights. Why is that? Furthermore, it is better to iterate decreasing the costs, because</p> <h2 id="summary">Summary</h2> <p>Many intractable problems can be efficiently solved on trees - combining with DFS enforces the computation ordering.</p> <h1 id="bellman-ford-algorithm">Bellman-Ford algorithm</h1> <h3 id="single-source-shortest-path-sssp">Single Source Shortest Path (SSSP)</h3> <p>Given a directed graph of \(n\) vertices and \(m\) edge, find the shortest paths for a vertex pair \((s, t)\).</p> <p>Recently, researchers showed that Dijkstra’s algorithm is the most optimal algorithm possible with a <em>specially</em> designed heap.</p> <p>Would dynamic programming work for SSSP? \(dp[u][i]\) represents a path from \(s\) to \(u\) using at most \(i - 1\) edges. This simply is the Bellman-Ford algorithm.</p> <h1 id="floyd-warshall-algorithm">Floyd-Warshall algorithm</h1> <p>All pair shortest path algorithm in \(O(n^3)\).</p> <h1 id="traveling-salesman-problem">Traveling Salesman Problem</h1> <p>Given.a complete graph of \(n\) vertices, every edge has a cost \(l_e\). The goal is to find a path that visits every node exactly once while minimizing the total cost. It is an NP-hard problem where the brute-force algorithm takes \(\\mathcal O(n!)\) - enumerating all possible permutations of nodes.</p> <p>There is a dynamic programming solutions of the order \(O(2^n n^2)\). This is computable with the modern computers upto \(n = 15\). We discuss the framework here -</p> <ul> <li> <p><strong>States</strong> - At node \(u\), set of all unvisited states - minimum cost to finish the rest of the task. The number of states is \(\mathcal O(n2^n)\).</p> </li> <li> <p><strong>Decision-making</strong></p> </li> <li> <p>What will be the next node to visit? If we choose \(v \in S\), then the next state will be $$(v, S - {v})$</p> </li> </ul> <p>This can be implemented using a bitmask for representing the state - saves space and is faster.</p>]]></content><author><name></name></author><category term="Notes"/><summary type="html"><![CDATA[A collection of ideas for design algorithms and analyzing them.]]></summary></entry><entry><title type="html">Tabletop Manipulation Algorithms</title><link href="https://sudhansh6.github.io/blog/Tabletop-Rearrangement/" rel="alternate" type="text/html" title="Tabletop Manipulation Algorithms"/><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Tabletop-Rearrangement</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Tabletop-Rearrangement/"><![CDATA[<h2 id="problem">Problem</h2> <p>Given a table top with various objects, their initial positions and final goal positions, we want to find the algorithm to rearrange the objects with the <strong>least number of actions</strong> and <strong>low buffer space</strong>.</p> <h3 id="buffers">Buffers</h3> <p>If one object is obstructing the goal position of the other, we would want to use <em>buffer space</em> to free-up the goal space. In a brute force manner, we can place everything on the side and put objects in their goals states. We want to optimize the number of actions, and doing this doesn’t do well in that scenario.</p> <p>The problem in the general case reduces to traveling salesman problem or the vertex cover problem, which are both NP-hard. We want to find a good approximation algorithm to get the least amount of actions.</p> <p>We also want real-time execution time because we do not want the algorithm to run for a long time.</p> <h3 id="variations">Variations</h3> <ol> <li> <p>Mobile and mobile base</p> </li> <li> <p>Disk objects and different shape objects</p> </li> <li> <p>stacking vs not stacking objects</p> </li> <li> <p>External buffer or not</p> </li> <li> <p>Single vs multiple tables</p> </li> <li> <p>Overlaps vs non-overlapping objects</p> </li> </ol> <h3 id="search-problem">Search Problem</h3> <p>The problem can be converted to a search problem where the search space is a tree representing all the possibilities of movements of objects (to goal positions) possible with the current configuration. The actions would be of the form “move object ‘i’ to buffer”, “move object ‘i’ to goal position”, etc. At each state, we have at most \(2n\) actions (if there are \(n\) objects on the table) - moving each object to buffer or goal.</p> <p>Obviously, this is a very high-dimensional space, and rudimentary search algorithms would not fare well if used directly.</p> <p>Let us now see some algorithms that try to tackle this.</p> <h2 id="trlb">TRLB</h2> <p><a href="https://arxiv.org/abs/2110.12325">Fast High-Quality Tabletop Rearrangement in Bounded Workspace</a> does the following -</p> <ol> <li> <p>Calculate a primitive plan by assuming there is always a feasible buffer space on the table (assume you have a second table to place objects)</p> </li> <li> <p>Try executing this plan, and assign buffer as we go along the path.</p> </li> <li> <p>If we fail to assign a buffer, we add a node to mark this search path.</p> </li> <li> <p>We repeat the procedure by finding a primitive plan from this stage until we find a feasible path.</p> </li> </ol> <p>The goal of this algorithm is to quickly calculate the solutions but it has a suboptimal traveling cost (example, robot mobile base needs to move a lot). Note that this is a non-deterministic algorithm, since we calculate random primitive plans.</p> <h2 id="orla">ORLA*</h2> <p>Aims to calculate the optimal path without considering the time as a heuristic. It essentially is \(A^*\) adapted to the table-top rearrangement problem - \(f(n) = g(n) + h(n)\) where \(g(n)\) is the travel cost from start to current node and \(h(n)\) expected travel cost from current node to goal.</p> <p>It results in an optimal plan but has a very long execution time. Another point to note is that ORLA* considers a mobile base which not many papers have considered previously.</p> <h2 id="our-research">Our Research</h2> <p>We aim to build an algorithm that gives the optimal solution and also executes fast. We assume mobile base manipulation robot with arbitrary shape objects and no external buffer.</p> <h3 id="approach-1">Approach 1</h3> <p>Perform search in TRLB using MCTS to find the most optimal search plan. Basically, execute TRLB for a fixed time to get multiple feasible paths. TRLB stops as soon as it finds a feasible path, but we execute it until we get a fixed number of possible paths.</p> <h3 id="bit">BIT*</h3> <p>We initially find a suboptimal path using TRLB* - this is fast.</p> <h2 id="future-directions">Future Directions</h2> <ol> <li> <p>Extend the algorithms for multi-agent scenarios where the problem becomes much harder.</p> </li> <li> <p>Learn conflict detection in TRLB or the modified algorithm to do better backtracking - one of the key ideas for combinatorial search algorithms. This can be done via SAT solving - learn conflicts as new constraints. This is particularly important when the number of objects on the table is high with a high density.</p> </li> </ol>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[Various methods for optimal mobile tabletop rearrangement - rearrange objects on the table with the least cost.]]></summary></entry><entry><title type="html">Evaluating interactions in music</title><link href="https://sudhansh6.github.io/blog/Evaluting-Music-with-RL/" rel="alternate" type="text/html" title="Evaluating interactions in music"/><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://sudhansh6.github.io/blog/Evaluting-Music-with-RL</id><content type="html" xml:base="https://sudhansh6.github.io/blog/Evaluting-Music-with-RL/"><![CDATA[<p>How do we identify how well two pieces of music complement each other? From notes, scales, rhythms, chords, time signatures, and many more abstract concepts associated with music, it is an interesting problem to capture the interactions between music pieces. This problem requires concepts from signal processing</p> <p>How is music represented? A Mel spectrogram is used to represent frequencies across time with amplitudes for audio files. A MIDI file is a tabular description of beats, positions, pitch, durations and instruments. These tabular instances can easily be converted to vector representations using one-hot encodings and feature normalization.</p> <p>Using such representations, the paper <a href="https://arxiv.org/abs/2207.06983">Multitrack Music Transformer</a> attempts to capture the interactions with attention mechanisms in a transformer. To understand this better, let us look at some theory.</p> <h3 id="total-information-flow">Total Information Flow</h3> <p>The total information flow is the sum of transfer entropies from \(X\) to \(Y\) and \(Y\) to \(X\).</p> \[T_{X \to Y} + T_{Y \to X} = H(X \vert \bar X) + H(Y \vert \bar Y) - H(XY \vert \bar{XY})\] <p>If the combined sequence \(XY\) does not make sense musically, then it will have a higher entropy, leading to lower total information flow. This concept is explored in depth in this context in this paper - <a href="https://arxiv.org/abs/2402.06810">Evaluating Co-Creativity using Total Information Flow</a>.</p> <h3 id="conditional-entropy">Conditional Entropy</h3> <p>Represented as \(H(Y \vert X)\) it measures how unpredicatble \(Y\) is, given that we have \(X\). How is this useful? A pre-trained music transformer is used to model \(p(X \vert \bar X)\) which represents the probability of a particular <em>token</em> (next part of music) after seeing a particular set of tokens (music till that point).</p> <h2 id="research">Research</h2> <p>Using the features from a MIDI file directly would not yield the best results. It composes of monotonically increasing data (beat), categorical features (instruments) and repeated values (position).</p> <p>This problem can also be posed as a Reinforcement Learning Problem - using total information flow as a reward. For example, <a href="https://arxiv.org/abs/2002.03082">RL-Duet: Online Music Accompaniment Generation Using Deep Reinforcement Learning</a> formulated a Deep RL algorithm for online accompaniment generation. The generation agent learns a policy to generate a musical note (action) based on previously generated context (state). RL has potential for real-time human-machine duel improvisation.</p> <h1 id="rl-duet">RL-Duet</h1> <p>What is the goal? Create an agent that can generate music <em>interactively</em> with a human. Again, this is done via Symbolic MIDI pitch + beat. The earlier approaches used GANs which have large data requirements and usntable training. Another approach, using Gibbs sampling, iteratively modifies the music fragments based on the previous context. However, this approach cannot be done in an online fashion.</p> <p>Typical approaches in reinforcement learning for sequence generation use maximum likelihood estimation. However, to improve the perceptual quality, global coherence and harmony, specific hand-crafted music rules-based rewards work much better.</p> <p>The work in RL-Duet captures the horizontal temporal consistency and vertical harmony relations for the reward function of the RL-agent. The current state is the previously generated context (by both human and the agent) and the action as mentioned before is symbolic MIDI (note and pitch). This involves horizontal view (like linear bi-directional language modeling in NLP), vertical part, joint modeling and hand-crafted rewards.</p>]]></content><author><name></name></author><category term="Research"/><summary type="html"><![CDATA[Capturing the quantifiers required to comment on how well two pieces of music complement each other.]]></summary></entry></feed>