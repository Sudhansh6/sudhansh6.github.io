# Lecture 22

> `03-03-22`

A large number of disks are connected by a high-speed network to a number of servers in a **Storage Area Network (SAN)**. The **Network Attached Storage (NAS)** provides a file system interface using a network file system protocol like FTP. SAN can be connected to multiple computers and gives a view of a local disk. It has fault tolerance and replication. A NAS pretends to be a file system unlike SAN.

## Magnetic Disks

The surface of the platter is divided into 50k-100k circular **tracks**. Each track is divided into 500-1000 (on inner tracks) or 1000-2000 (on outer tracks) **sectors**. A **cylinder** is a stack of platters.

The performance of a disk is measured via its **access time** which involves seek time and rotational latency,  **I/O operations per second (IOPS)**, **Mean/Annualized time to failure (MTTF)** and **data-transfer rate**. We can tune the performance by changing parameters such as **Disk block size** and **Sequential/Random access pattern**. MTTF decreases as disk ages.

***Note.*** Suppose the MTTF of a single disk is $$t$$. How do we calculate the average failing time of a disk in a set of $$n$$ disks? The probability that a disk fails in a given hour is $$1/t$$. The probability that one of the disks in $$n$$ fails is $$1 - (1 - 1/t)^n$$. However, if $$t$$ is large, it is simply $$n/t$$. That is, on an average a disk fails in every $$t/n$$ hours in a set of $$n$$ disks.

In a random access pattern, every request requires a **seek**. This method results in lower transfer rates. Current disks allow up to 50-200 IOPS.

## Flash storage

A **NAND flash** is widely used for storage in contrast to a **NOR flash**. A page can only be written once, and it must be erased to allow rewriting. Flash storage does page-at-a-time read. If we try for a byte read, then the control lines take up a lot of storage, and the capacity goes down.

A **solid state disk** uses standard block-oriented disk interfaces, but store data on multiple flash storage devices internally. We can use SSD using the SATA interface. An erase in flash storage happens in unit of **erase block**, and **remapping** of logical page addresses to physical page addresses avoids waiting for erase. The remapping is carried out by **flash translation layer**. After 100000 to 1000000 erases, the erase block becomes unreliable and cannot be use due to **wear leveling**. 

A SLC tolerates about $$10^6$$ erases. A QLC has 4 voltage levels (2 bits can be stored in 1 physical bit). These are much less tolerant to erases (about $$10^3$$ erases). **Wear leveling** normalises the erases in a region of the flash storage by storing cold data in the part where a lot of erases have been done. 

The performance of an SSD is measured through the data transfer rates. SSDs also support parallel reads. **Hybrid disks** combine small amount of flash cache with large magnetic disks.

Recently, Intel has come up with the 3D-XPoint memory technology which is shipped as Intel Optane. It allows lower latencies than flash SSDs.

### RAID

**Redundant Arrays of Independent Disks** is a set of disk organization techniques that manage a large number of disks <u>providing a view of a single disk.</u> The idea is that some disk out of a set of *N* disks will fails much higher than the chance that a specific single disk will fail. We expect <u>high capacity, high speed, and high reliability</u> from this system.

In a way, we improve the reliability of the storage system using redundancy. For example, the simplest way to do this is **mirroring** (or shadowing) where we just duplicate all disks. <u>The **mean time to data loss** depends on the mean time to failure and the mean time to repair</u>. For example, if the MTTF is 100000 hours and the mean time to repair is 10 hours, then we get the mean time to data loss as $$500\times 10^6$$ hours. How do we get this? The probability that one of the disk fails is $$2*10^{-5}$$. Now, what is the probability that the other disk fails within the repair time? It is $$2* 10^{-4}$$. Now, at this point we have data loss. Therefore, the mean time to data loss would be $$2.5 *10^8$$ for one disk. As we have two disks, we get $$5 * 10^8$$. Data loss occurs when both disks fail. 

The two main goals of parallelism in a disk system are to load balance multiple small accesses to increase throughput and parallelise large accesses to reduce the response time. We do this via bit-level stripping or **block-level striping**. In block level striping, with n disks, block $$i$$ of a file goes to disk to disk($$i\%n$$) + 1. Now, requests for the same file can run in parallel increasing the transfer rate.

- RAID level 0 : Block-striping; non-redundant. Used in high-performance applications where data loss is not critical.
- RAID level 1: Mirrored disks with block striping. Popular for best write performance and applications such as storing log files in a database system.

RAID also **parity blocks** that stores the XOR of bits from the block of each disk. Parity block $$j$$ stores XOR of bits from block $$j$$ of each disk. This helps in recovery of data in case of a single disk failure (XOR the parity bit with the remaining blocks on various disks). Parity blocks are often spread across various disks for obvious reasons.

- RAID level 5 - Block-interleaved Distributed Parity. This is nice but writes are slower. The cost of recovery is also high. *Think*.

- RAID level 6 - It has a P + Q redundancy scheme where 2 error correction blocks are stored instead of a single parity block.  Two parity blocks guard against multiple(2) disk failures. 

There are other RAID levels which are not used in practice.