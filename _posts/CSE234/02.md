So far, we have seen representations that express the forward computations using primitives. But, how do we represent backward computations?

# Autodiff (AD)
Derivative can be taken using the first order principles. However, this approach can be slow since we have to evaluate the function twice $$f(\theta + \epsilon) , f(\theta)$$ and it is also error prone $$\theta(\epsilon^2)$$. 

To optimize the derivative calculation, we pre store the gradients of primitives and map the derivative chain rules in the computational graph. There are two ways of doing this as well
1. Calculating the derivative from left (inside) to right (outside) in a network - from inputs to outputs
2. Calculating it from right to left - from outputs to inputs

Both are valid approaches and we will discuss them in detail.

## Forward Mode Autodiff
We start from the input nodes, and derive the gradients all the way to the output nodes. 
**Cons** - 
    - For $$f: R^n \to R^k$$, we need $$n$$ forward passes to get the gradients with respect to each input.
    - However, it is usually the case that $$k = 1$$ (loss) and $$n$$ is very large.

    
> If this is confusing, think of it this way - we want the gradient of output with respect to all parameters to update weights. However, forward mode calculates the gradient of inputs with respect to all parameters.

## Reverse Mode Autodiff
We define the quantity *adjoint* $$\bar v_i = \frac{\partial y}{\partial v_i}$$. We then compute each $$\bar v_i$$ in the reverse topological order of the graph. This way, we can simply do one backward pass to get the necessary gradients.

In some scientific scenarios, we can have $$k >> n$$ where the forward mode can be more efficient.

> What are the size bounds of the backward graph as compared to the neural network?

We construct backward graphs in a symbolic way to reuse it multiple times.

![](/assets/img/2025-01-06-data-systems-for-ml/17369099776396.jpg)

## Backpropagation vs. Reverse-mode AD
In old frameworks like Caffe/cuda-convnet, the backward computations were done through the forward graph itself. Newer frameworks like Tensorflow and PyTorch construct the backward graph explicitly. The reasons to do so are - 
1. Explicit graphs allow backward computation with any input values. They have flexibility to even calculate gradient of gradients.
2. Having an explicit backward graph can help optimization!
3. Gradient update rules can be efficiently implemented.

## Gradient update rules
Typically done via gradient descent, the weights are updated with the gradients with the following simplified rule

$$
    f(\theta, \nabla_l) = \theta - \eta \nabla_L
$$

![](/assets/img/2025-01-06-data-systems-for-ml/17369103443367.jpg)

# Architecture Overview of ML systems
The aim is to make the systems fast, scalable, memory-efficient, run on diverse hardware, energy efficient and easy to program/debug/deploy. Phew.

We have discussed dataflow and Autodiff graphs. However, there are numerous things that can be added to these - graph optimization, parallelization, runtime memory, operator optimizations and compilation.

## Graph Optimization
The goal is to rewrite the original graph $$G$$ as $$G’$$ that is faster.

Consider the following motivating example - Typically, convolution is followed by batch normalization. Instead of performing batch normalization, just update the weights in convolution to do everything in one step!

![](/assets/img/2025-01-06-data-systems-for-ml/17369110087495.jpg)

Note that some steps can become slower based on the hardware, but you get the general idea.

Similarly, in attention calculations, the code is typically written with a concatenated vector of queries, keys and values. This version is optimal - it can be understood with *Arithmetic Intensity* which the ratio of #operations and #bytes. For example, an addition operation has intensity of $$1/3$$ (2 loads and one store). However, fusing multiple arithmetic operations reduces the loads and stores by bringing all variables into memory once, improving the arithmetic intensity.

### So how do we optimize graphs?
We write rules or templates for opportunities to simplify graphs. There is also implementation of *auto-discovering* optimizations in the latest libraries, we shall study these.

## Parallelization
The goal is to parallelize the graph computation over multiple devices. Note that devices can be connected with fast (memory communication NVLink) and slow connections (across GPUs), with up to 10x performance difference. Ideally, we do not want to describe partitioning rules for every new model that comes up. Based on these communication patterns, distributing the tasks is not an easy problem. So, we shall discuss how we partition the computational graph on a device cluster.

## Runtime and Scheduling
How do we schedule the compute, communication and memory in a way that execution is as fast as possible, communication is overlapped with compute and is subject to memory constraints?

## Operator Implementations
The goal is this layer is to get the fastest possible implementation of `matmul`s, for different hardware, different precision and different shapes.

NVIDIA releases a GPU every 2 years, and they have rewrite all operations every time! Notably, previously, models were trained using 32-bit floating points, but now researchers are emphasizing on lower and lower precisions. 

Now, we shall delve into each of these architectures.

# Operator Optimization and Compilation
The goal is maximize arithmetic intensity. In general there are three ways to speed up operators
### Vectorization
![](/assets/img/2025-01-06-data-systems-for-ml/17369120762344.jpg)

The right version is faster because of the hardware - cache sizes, etc. Tensorflow and PyTorch have this built-in.

### Refactoring data layout
This is again related to how data is stored in memory. For example, C++ stores matrices in row-major order. Accessing columns of a matrix can be 10x slower! Remember this while writing code to lower cache misses and reduce pointer movements.

ML systems don’t store tensors in row or column major but in a new format called **strides format** - `A[i, j, …] = A.data[offset + i*A.strides[0] + j*A.strides[1] + …`. It is a generalization of row and column major storage, and it offers more flexibility - so based on the batch-sizes or other parameters in a neural network.

