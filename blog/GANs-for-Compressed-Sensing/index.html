<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Generative Adversarial Networks for Compressed Sensing | Sudhansh</title> <meta name="author" content="Sudhansh Peddabomma"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sudhansh6.github.io/blog/GANs-for-Compressed-Sensing/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?6185d15ea1982787ad7f435576553d64"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Sudhansh</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Articles</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/challenges/">Challenges</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/misc/">Miscellaneous</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Generative Adversarial Networks for Compressed Sensing</h1> <p class="post-meta">January 5, 0001</p> <p class="post-tags"> <a href="/blog/0001"> <i class="fa-solid fa-calendar fa-sm"></i> 0001 </a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> Research</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="generative-adversarial-networks">Generative Adversarial Networks</h1> <p>Take a look at <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29" rel="external nofollow noopener" target="_blank">this</a> blog for an introduction to GANs.</p> <h2 id="gans-for-inverse-problems">GANs for Inverse Problems</h2> <ul> <li>Traditionally, natural images are considered sparse in some fixed or learnable basis. Many algorithms use this property is used to recover images from compressive measurements.</li> <li>Typically, instead of a sparse prior, some algorithms also use a <strong>learned prior</strong>.</li> <li>Some approaches also use neural networks to solve inverse problems. These are an alternative to the classical regularization approach. These methods mainly fall into \(3\) categories. <ul> <li>Networks that learn prior and the regularization parameter</li> <li>Networks that learn the optimization solver</li> <li>Networks that perform the full reconstruction directly from the measurements. This method can be further classified as <ul> <li>Learned full reconstruction operators - This is computationally expensive, and the performance is highly dependent on the training data.</li> <li>Learned post-processing denoisers - The initial reconstruction is done via a primitive algorithm. For example, in tomography, the initial reconstruction is formulated via filtered back-projection. After this, the network constructs the final reconstruction using the initial reconstruction.</li> <li>Learned iterative reconstruction - These networks reconstruct the image through iterative steps. These are similar to the full reconstruction operators, but unlike them, these use multiple steps. Primal-Dual Hybrid Gradient is one such algorithm.</li> </ul> </li> </ul> </li> </ul> <p>The following approach uses GANs to model the distribution of the data. Generally, these algorithms involve the expensive computation of the Jacobian \(\nabla_z G\) of the differentiable generator \(G\) with respect to the latent input \(z\). Computing \(\nabla G_z\) involves backpropagation through \(G\) at every iteration.</p> <p>The <strong>Projected Gradient Descent (PGD)</strong> approach shows substantial improvements in performance and the recovery rate.</p> <h3 id="problem-formulation">Problem Formulation</h3> <p>Let \(x^* \in \mathbb R^n\) denote the ground truth image, \(A\) denote the measurement matrix, and \(y = Ax^* + v \in \mathbb R^m\) denote the noisy measurement with noise \(v \sim \mathcal N(0, \sigma^2I)\). We assume the ground truth images lie in a <em>non-convex</em> set \(S = R(G)\), the range of the generator \(G\). The maximum likelihood estimator of \(x^*\), \(\hat x _{MLE}\), can be formulated as:</p> <div style="text-align:centre"> $$ \begin{align} \hat x_{MLE} &amp;= \arg \min_ {x\in R(G)} -\log p(y|x) \\ &amp;= \arg \min_{x\in R(G)} \|y - Ax \|^2_2 \end{align} $$ </div> <p>One such approach suggested by Bora <em>et al</em> to solve the algorithm (which we shall refer to as CSGM) is as follows. They solve the optimization problem</p> <div style="text-align:centre"> $$ \hat z = \arg \min_{z \in \mathbb{R}^k}\|y - AG(z) \|^2 + \lambda\|z\|^2 $$ </div> <p>in the latent space(\(z\)), and set \(\hat x = G(\hat z)\). Formally, a generative model is given by a deterministic function \(G : \mathbb R^k \to \mathbb R^n\), and a distribution \(P_Z\) over \(z \in \mathbb R^k\). To generate a sample from the generator, we can draw \(z \sim P_Z\) and the sample is then \(G(z)\). Typically, we have \(k \ll n\), i.e. the generative model maps from a low dimensional representation space to a high dimensional sample space.</p> <p>In other words, we generate \(z\) from a known distribution, say a Gaussian. The GAN \(G\) takes \(z\) as input and produces an output \(x = G(z)\) in \(R(G)\). This \(x\) is supposed to model the distribution of the training data of images. Then, by using any optimization procedure, \(\hat z\) is estimated. We are simply optimizing in the representation space instead of the sample space. The results significantly outperform Lasso with relatively fewer measurements.</p> <p>The problem with this approach is that the gradient descent algorithm often gets stuck at local optima. To resolve this problem, Shah and Hegde proposed a projected gradient descent (PGD)- based method (we shall refer to this as PGD-GAN). They perform gradient descent in the ambient \((x)\)-space and project the updated term onto \(R(G)\). This projection involves solving another non-convex minimization problem using the <em>Adam optimizer</em>. Their algorithm can be summarised as:</p> <div align="center"> $$ x_k \rightarrow \fbox{$I - \eta\nabla_xf$} \\ x'_k \downarrow \\ \fbox{$\hat z_{k+1} \gets \arg\min_z \|G(z) - x'_k\|^2$} \\ \begin{align} \hat z_{k+1} &amp;\downarrow \\ &amp;\fbox{ $G$ } \rightarrow x_{k+1} \end{align} $$ </div> <p><strong>Note.</strong> <span style="color:red"> The GAN is trained separately. We are just trying to reconstruct the original image using the above algorithm </span></p> <p>Our aim is to replace this iterative scheme in the inner-loop (estimation of \(\hat z\)) with a learning-based approach. A network architecture can be carefully designed using a suitable training strategy that can project onto \(R(G)\). This does away with the inner loop in the above algorithm. The inner loop is the most expensive computational step in the algorithm. The new approach is summarised as:</p> <div style="text-align:centre"> $$ x_k \rightarrow \fbox{$I - \eta\nabla_x f$} \overset{x'_k}{\rightarrow} \fbox{$ P_G = GG^\dagger$} \rightarrow x_{k+1} $$ </div> <p>\(P_G\) is the network based projector. The following architecture is used to train a projector onto \(R(G)\).</p> <div style="text-align:centre"> $$ \begin{align} z \sim \mathcal N(0, I) \rightarrow \; &amp;\fbox{ $G$ } \\ &amp;\downarrow \text{Noise added} \\ \text{output} \leftarrow \fbox{ G } \leftarrow \; &amp;\fbox{$G^\dagger_\theta$} \end{align} $$ </div> <p>This approach has the following advantages:</p> <ol> <li>There is no inner loop which reduces the number of iterations and, hence, the time required.</li> <li>It does not require the computation of \(\nabla G_z\) which is a very expensive operation.</li> </ol> <h3 id="training-of-gan">Training of GAN</h3> <p>The goal is to train a network that projects an image \(x \in \mathbb R^n\) onto \(R(G)\). The projector, \(P_S\) onto a set \(S\) should satisfy two main properties:</p> <ol> <li> <em>Idempotence</em>, for any point \(x\), \(P_S(P_S(x)) = P_S(x)\)</li> <li> <em>Least distance</em>, for any point \(\tilde x\), \(P_S(\tilde x) = \arg\min_{x \in S}\|x - \tilde x\|^2\) .</li> </ol> <p>The multi-task loss to train the projector is</p> <div style="text-align:centre"> $$ \begin{equation} \mathcal L(\theta) = \mathbb E_{z, \nu} \left[ \| G\left(G^\dagger_\theta(G(z) + \nu)\right) - G(z)\|^2 \right] \\ + \mathbb E_{z, \nu} \left[\lambda \| G^\dagger_\theta(G(z) + \nu) - z \|^2 \right] \end{equation} $$ </div> <p>where \(G\) is a generator obtained from the GAN trained on a particular dataset. Operator \(G^\dagger_\theta : \mathbb R^n \to \mathbb R ^k\), parametrized by \(\theta\), approximated a non-linear least squares pseudo-inverse of \(G\) (Hence the usage of \(G^\dagger\) for representation) and \(\nu \sim \mathcal N(0, I_n)\) indicated noise added to the generator’s output for different \(z \sim \mathcal N(0, I_k)\) <span style="color:red"> so that the projector network denoted by $P_G = GG^\dagger_\theta$ is trained on points outside $R(G)$ and learns how to projects them onto $R(G)$ </span>.</p> <p>The objective function consists of two parts.</p> <ul> <li>The first is similar to the standard <em>Encoder-Decoder</em> framework, however the loss function is minimized over \(\theta\), while keeping the parameters of \(G\) fixed. This ensures that \(R(G)\) does not change and \(P_G\) is a mapping onto \(R(G)\).</li> <li>The second part is used to keep \(G^\dagger(G(z))\) close to true \(z\) used to generate training image \(G(z)\). This can be regarded as the regularizer.</li> </ul> <h2 id="theoretical-study">Theoretical Study</h2> <h3 id="convergence-analysis">Convergence Analysis</h3> <blockquote> <p><strong>Restricted Eigenvalue Constraint (REC)</strong> - Let \(S \subset \mathbb R^n\). For some parameters \(0 &lt; \alpha &lt; \beta\), matrix \(A \in \mathbb R^{m \times n}\) is said to satisfy the <em>REC(\(S, \alpha, \beta\))</em> if the following holds for all \(x_1, x_2 \in S\).</p> <div align="center"> $$ \alpha\|x_1 - x_2\|^2 \leq \|A(x_1 - x_2) \|^2 \leq \beta\|x_1 - x_2\|^2 $$ </div> <p><strong>Approximate Projection using GAN</strong> - A concatenated network \(G(G^\dagger(.)) : \mathbb R^n \rightarrow R(G)\) is a \(\delta\)-approximate projector, if the following holds for all \(x\in \mathbb R^n\).</p> <div align="center"> $$ \|x - G(G^\dagger(x))\|^2 \leq \min_{z \in \mathbb R^k} \|x - G(z)\|^2 + \delta $$ </div> </blockquote> <blockquote> <p><strong>Theorem</strong> Let matrix \(A \in \mathbb R^{m \times n}\) satisfy the \(REC(S, \alpha, \beta)\) with \(\beta/\alpha &lt; 2\), and let the concatenated network \(G(G^\dagger(.))\) be a \(\delta\)-approximate projects. Then for every \(x^* \in R(G)\) and measurement \(y = Ax^*\), executing the algorithm with step size \(\eta = 1/\beta\), will yield</p> <div align="center"> $$ f(x_n) \leq \left(\frac{\beta}{\alpha} - 1\right)^nf(x_0) + \frac{\beta\delta}{2 - \beta/\alpha} $$ </div> <p>Furthermore, the algorithm achieves \(\|x_n - x^* \|^2 \leq \left(C + \frac{1}{2\alpha/\beta - 1}\right)\delta\) after \(\frac{1}{2 - \beta/\alpha}\log\left(\frac{f(x_0)}{C\alpha\delta}\right)\) steps. When \(n \rightarrow \infty\), \(\|x^* - x_\infty\|^2 \leq \frac{\delta}{2\alpha/\beta - 1}\).</p> </blockquote> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Lattices-in-Cryptography/">Lattices in Cryptography and Quantum Computers</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Modulo-Compressed-Sensing/">Modulo Compressed Sensing</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Object-Detection/">Object Detection</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Machine-Learning-Cheatsheet/">Machine Learning Cheatsheet</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/Low-Rank-Tensor-Recovery/">Low Rank Tensor Recovery for Joint Probability Distribution</a> </li> </div> <script>document.querySelectorAll("#table-of-contents a").forEach(function(e){e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substring(1);document.querySelectorAll(".content-section").forEach(function(e){e.classList.add("hidden")});var n=document.getElementById(t);n&&n.classList.remove("hidden")})});</script> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 Sudhansh Peddabomma. Last updated: April 11, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?9b43d6e67ddc7c0855b1478ee4c48c2d" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0K9MLG0V24"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0K9MLG0V24");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>