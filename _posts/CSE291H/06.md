## Introduction to Transformers and Language

A function approximator (neural networks) needs to be good for the task. For example, CNNs were great for Atari and Go but they did not work well for language. 

What are the right inductive biases for language then? The *attention mechanism* was again borrowed from some cognitive functions of the brain. An attention matrix tries to find the *alignment* of elements within a sequence. Before the scaled dot product attention, there were multiple variants of the formulation -

![](/assets/img/AIAgents/17393174376471.jpg)
Self-attention was first proposed by Cheng et al. in 2016 (originally called intra-attention) that brought in a sense of understanding in models - finding relations between elements of the same sequence. 

In 2015, there was a concept of global vs local attention introduced in the form of windowed-attention. This concept is being used widely in vision and language based systems. 

Let us discuss the intuition for *scaled dot product attention* - 
1. Query: What are the things I am looking for?
2. Key: What are the things I have?
3. Value: What are the things I will communicate?

So essentially, the queries attend to the keys to find the aligned ones, and the corresponding values are returned. The multi-head part of the architecture is simply the multiple ways of learning these alignments between the query and key sequences. The alternate way of thinking about it is, using an ensemble representation to provide robustness in our predictions. Furthermore, to add a notion of position in the sequence, a positional encoding is added to the sequence elements. 

To apply all these mechanisms to language, we need a way to represent language as sequences - this step is known as *tokenization*. Word-level is too discrete (causes issues for questions like “how many r’s are in ‘strawberry’?”) and character-level is too redundant (often causes issues for questions like “Is 9.9 greater than 9.11?”. The current standard is a sub-word (Tiktoken Byte Pair Encoding BPE) that is learned from a representative subset of data.

The other ideas are to use *heuristics* for numerical values like new tokenizers and right-to-left processing, etc. The amount of research done in tokenization is underwhelming as compared to the rest of the transformer stack. People have also come up with [byte-level transformers](https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/) and [LCMs](https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/). Since bytes form too long sequences, people tried hierarchical representations that kind of work. However, there are many training issues with this and byte-encoding issues (PNG and Unicode are weird encodings). They were only able to train a 7B model with the byte-level learning. 

Language modeling is another long-standing problem that is a fundamental cornerstone to make these networks learn. The two popular mechanisms used are
- Infill (like in BERT): The 47th US President is [??] Trump
- Next Token prediction (like in GPT): The sun rises in the ??

RNNs had an encoder-decoder architecture wherein the initial sequence is *encoded* into a vector (latent space) and it is then *decoded* into another sequence. This terminology has been carried through and is still used in transformers. The original paper started out as an encoder-decoder architecture, but these models are not used much anymore. 
1. Encoder only models (auto encoding models) - They are used for Sentence classification, NER, extractive question-answering and masked language modeling. Examples are BERT, RoBERTa, distilBERT. 
2. Decoder only models (auto regressive models) - They are used for text generation and causal language modeling. Examples are GPT-2, GPT Nero, GPT-3

Check [this repository by Andrej Karpathy](https://github.com/karpathy/nanoGPT) for a clean implementation of GPT. 

## How does all this relate to RL?
We now have a neural net architecture that works well on language. How about using this for MDPs with language-based state-action spaces. 

## [Paper Discussion] 2xDQN network

As we have seen the Q-learning update

$$

Q(S_t, a_t) \gets Q(s_t, a_t) + \alpha[r(s_t, a_t) + \gamma \max_{a’} Q(s’, a’) - Q(s, a)] 

$$

The problems with Q-learning are
1. Curse of dimensionality - High dimensions and continuous state spaces do not work
2. Slow convergence
3. Inefficient generalization
4. Exploration-exploitation trade-off

This led to **Deep Q-learning**. The idea is to replace Q0tables with Neural networks to approximate Q-functions. 
- **Experience replay* stores the agent’s past experiences in a replay buffer. The algorithm randomly samples episodes for training, to break the correlation between consecutive experiences.
- **Target Networks** - Uses a separate, slower updating networking to compute target Q-values. Reduces instability caused by changing Q-values too frequently during learning.

The problems with this are 
- Overestimation of Q-values - due to the mathematical formulation the Q-values are over-estimated (has been proved mathematically)
- Sample inefficient - uses large amount of e
- Catastrophic forgetting - may forget previously learned experiences when trained on new experiences
 
This led to **Double Deep Q-learning**. They decouple the action evaluation and action selection networks. The online network is used for action selection whereas the action value function evaluates the action. Previously, to stabilize the learning in Q-learning, the updates to action selection network were less frequent leading to choosing sub-optimal actions in some cases. 

The problems with this approach are
1. Might not completely eliminate the overestimation bias - Since action selection and value estimation are not completely decoupled. 
2. Leads to slower convergence in some environments - where overestimation is helpful (high exploration settings)
3. Sensitive to hyper parameters
4. Still susceptible to some core DQN issues - Sparse rewards, discrete action space, sample inefficient.

After this, newer methods were proposed such as *Rainbow Deep Q-learning* - Combination of previous improvements - Prioritized experience replay, uses a dueling network (two explicit specialized networks) for evaluation and selection, Stabilized learning via distributional RL, and Multi-step updates.