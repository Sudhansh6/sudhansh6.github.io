# Reinforcement Learning (Abridged)
We have been building chronologically, and next in line is basic RL. 

## Terminology
- A **policy** is a function that defines an agent’s behavior in the environment. Finding the optimal policy is known as the *control* problem.
    
  Formally, a policy is a distribution over actions for a given state. 
  
  $$
    \pi(a \vert s) = P(A_t = a \vert S_t = s)
  $$ 
  
  Due to the Markov property, the policy depends only on the current state and not history. In cases where the history is needed, the state is modified to embed the history to evade this dependence. 
  
  For a given MDP, an optimal policy always exists that achieves the maximum expected reward at every state. (This is proved using the compactness properties of the state-action space using the Banach’s fixed point theorem - refer to [these notes](/blog/cse291g) for more details).
  
- The **value function** determines how good each state or actions is. Finding the optimal value functions is known as the *prediction* problem.
    
  There are two functions that capture the value of the current state/action of the agent
    1. $$V_\pi(s) = E_\pi[R_{t + 1} + \gamma V_\pi(S_{t + 1} \vert S_t = s]$$ - The expected reward obtained by a policy $$\pi$$ starting from a given state $$s$$.
    2. $$Q_\pi(s, a) = E_\pi[R_{t + 1} + \gamma Q_\pi(S_{t + 1}, A_{t + 1}) \vert S_t = s, A_t = a]$$ - The expected reward for a given state $$s$$ upon taking a certain action $$a$$.
    These two functions are closely related to each other, and knowing one determines the other. In general, these functions (matrices, in discrete spaces) do not have a closed form solution. 
- A **model** is the agent’s representation of the environment

RL algorithms are classified under various categories
- Model free and Model-based
- Off-policy and on-policy

# It’s all Dynamic Programming?
The core theory of RL, the properties of the Bellman equation (refer to these notes for more details) and the recursive nature of the value functions, ties to dynamic programming. This insight helps us design algorithms to solve the problems in RL (Prediction and Control).

We ideally want the solution to the control problem since we want to define the optimal behavior of an agent in the environment. To do so, the prediction problem is a pre-cursor that needs to be solved. 

## Policy Evaluation 
The prediction problem involves calculating the rewards obtained by a given policy $$\pi$$. The expectation can be written as 

$$
\begin{align*}
    V_\pi(s) &= \sum_{a \in A} \pi(a \vert s) Q_\pi(s, a) \\
    &= \sum_{a \in A} \pi(a \vert s) (R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’)V_\pi(s’))
\end{align*}
$$ 

where $$T$$ is the state-transition function of the MDP. 

Ideally, we want to find the optimal policy that reaches the maximum value at every state.

$$
\begin{align*}
    V*(s) &= \max_\pi V_\pi(s) \\
    Q*(s, a) &= \max_\pi Q_\pi(s, a) \\
    \pi*(s) &= \arg\max_\pi Q_\pi(s, a)
\end{align*}
$$

These can be determined (iteratively) from the Bellman’s Optimality equations -

$$
\begin{align*}
    Q*(s, a) &= R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’) V*(s’) 
    &= R(s, a) + \gamma \sum_{s’ \in S} T(s, a, s’) \max_{a’} Q*(s’, a’)
\end{align*}
$$

> Note the subtlety here. Although Bellman’s optimality equations aren’t seemingly much different from the Bellman’s equations, there is a very strong claim the optimality equations make - they claim that the existence a policy that gets the maximum possible value at every state. The existence is not a trivial claim and it is the proof I referred to in the terminology. Furthermore, it turns out that a deterministic policy is just as good as a stochastic one. 

So how do we find these optimal values? 

## Policy Iteration
Given a policy $$\pi$$, we iteratively update its actions at each state to improve its value. Remember that we can *evaluate a policy* to get its value function. 

At each state, if there is an action $$a$$ such that $$Q_\pi(s, a) > Q_\pi(s, \pi(s))$$, then the policy is *strictly improved* by updating $$\pi(s) \gets a$$. In each iteration, we update the actions this way across all the states, and repeat this until the policy does not change. 

How many iterations should we repeat this for? Because the number of policies is finite (bounded by $$O(\vert A \vert^{\vert S\vert})$$, we are guaranteed to reach the optimum. Each iteration costs $$O(\vert S\vert^2 \vert A\vert + \vert S\vert^3)$$. Although these numbers seem big, in practice, this algorithm typically takes only a few iterations.

## Value Iteration
It is similar to the policy iteration algorithm, but focuses on recursively improving the value function instead. 

We start out with a random value function, and at each state, we choose the action that gives the maximum value (with the currently set values across the states). Once the values are updated across all the states, the process is repeated until the improvement is below a threshold.  At the end of the iterations, we can extract the policy from the value function deterministically (the algorithm itself is a hint to this).

Although this seems very similar to the policy iteration algorithm, there are some key differences. We do need to reach the optimal value function to get the optimal policy - if it is close enough, we can get the optimal policy. Also, the iterations *asymptotically reach* the optimal policy and there is no upper bound to this. 

## Limitations
Although dynamic programming approaches have theoretical guarantees, they are not widely used in practice. Why?

The curse of dimensionality. These algorithms are have very limited applicability in practice. Many environments have a very large set of states and actions. In some cases, these could be continuous as well. The iteration algorithms are computationally infeasible in such cases. 

# Model-free RL 
Since we cannot look at every state action combination, we resort to approximations. We explore the world (say, with Monte Carlo sampling) and build experiences to heuristically guide the policy. 

The goal is to optimize the value of an unknown MDP through experience based learning. Many real world problems are better suited to be solved by RL techniques over dynamic programming based approaches (the iterative algorithms). 

## Monte Carlo Control
It suggests greedy policy improvements over $$V(s)$$ requires a model of the MDP. However, improvement over $$Q(s, a)$$ is a model-free method! (This was the importance of defining both $$V_\pi(s)$$ and $$Q_\pi(s, a)$$).

The $$Q$$ function can be learned from experiences. These concepts are the foundation concepts for deep RL!

This approach can be thought of as a hybrid approach between policy and value iteration. In these exploration/sampling based techniques, it is important to gather data about the model through exploration and not be greedy. This forms the basis of **epsilon-greedy** algorithms. 

## $$\epsilon$$-greedy exploration
At each state, with a certain probability we choose to exploit (greedily take the action based on the optimal policy we developed so far) or explore (take a random action to sample more outcomes)

$$
    \pi(a \vert s) = \begin{cases}
        \epsilon/m + 1 - \epsilon & a* = \arg\max_{a \in A} Q(s, a) \\
        \epsilon/m
    \end{cases}
$$

This class of algorithms also has some theory but it is limited. This core trade-off between exploration/exploitation is still a core element in the modern RL algorithms. 