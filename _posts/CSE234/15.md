Let us recall the memory usage in models
1. Model Weights - $$2 \times M$$ for FP16
2. Intermediate activation values
    1. If we checkpoint at transformers boundary, the activation memory is $$b \times s \times h \times n\_layers$$. It is same as what we need to communicate in inter-op parallelism. 
    2. The activation inside a transformer has the size $$(b, n, s, s)$$. It is the bottleneck for memory! We’ll see **Flash Attention** on how to avoid saving the complete attention matrix

    ![](/assets/img/2025-01-06-data-systems-for-ml/17411426493026.jpg)

    In summary, these are the activations inside the transformer block.
3. Optimizer State - $$12 \times M$$ for FP16-32 mixed-precision
4. Weight Gradients + Activation Gradients - $$2 \times M$$.

Based on this analysis, we have the following observations
1. All terms are at least linear with $$h$$ - Useful observation for scaling up
2. Dimensions of partitioning
    1. $$b$$ would be data parallelism
    2. $$d$$ - Megatron based architecture
    3. $$s$$ - Part of 4D parallelism

    
> Advanced Topics (Recently published research)
> Megatron-style parallelism has been well adapted. How do we partition along $$s$$? We can partition $$n$$ is attention (assign different heads to different GPUs) and $$s$$ in MLP. Deepspeed Ulysses sequence parallelism is a recently proposed technique that partitions on $$n$$ first and then partitions on $$s$$. Moreover, it often happens that $$n \ll \#GPUs$$ or is not a multiple of 8, then it is prudent to partition $$s$$ in both attention and MLP. This is implemented in the form of **ring attention**.

> Rule of thumb: In many computer systems and algorithms, anything more complex than quadratic is less likely to be adapted at large scale.

## Scaling Laws
Continuing on our observations
1. Computer is a function of $$h, i, b$$
2. $$\#$$parameters is a function of $$h, i$$
3. So the computer is a function $$\#$$ parameters. 

But as we know, the compute budget is limited. Then we have to decide between
1. Training models longer (on more data) vs training bigger models
2. Collect more data vs get more GPUs
3. How to choose exact $$h, i$$, etc. 

Our motivation for scaling laws is to understand how large a model should we train and how much data should we use to achieve a given performance subject to a compute budget.

For applications like statistical estimators, we can easily derive theoretical scaling laws based on mathematical axioms. However, that seems to be much difficult for language models. 

Then, how do we analyze these models? We perform empirical observations. Our analysis on machine learning has moved from mathematics based proofs to physics like experiments. Relying on observations. It became a strong trademark after the GPT-3 paper. 

Here is what we have got so far, observing based on empirical scaling laws:
- To conclude transformers are better than LSTMs, instead of spending tens of millions to train LSTM GPT-3, we extrapolated results and noticed that transformers outperform these models. 
- Similarly for depth vs width of networks - 1 vs 2 layers make s a huge difference but more layers have diminishing returns after a certain number of parameters.

Essentially, the scaling law way trains smaller models to establish a scaling law and select the optimal hyper-parameters based on the scaling law prediction. This allows us to get a heuristic on the effects of hyper-parameters in big LMs before training. 

Consider the question data vs compute trade-off - for a given compute is it better to train undertrained big models or well-trained small models? 

$$
    N_{opt} (C), D_{opt}(C) = \arg_{N, D}\min_{FLOPs(N, D) = C} L(N, D)
$$

where $$N$$ is the number of parameters, $$D$$ is the data and $$C$$ is the compute. We obtained the following after spending billions of dollars for next-prediction loss

![](/assets/img/2025-01-06-data-systems-for-ml/17411445543561.jpg)

$$
    L(N, D) = \frac{406.4}{N^{0.34}} \frac{410.7}{D^{0.29}} + 1.69
$$

and every LLM seems to follow this… 

## MoE LLMs
The key idea like we discussed before is to make each expert focus on predicting the right answer for a subset of cases.

Instead of choosing one expert each time, it is also possible to choose multiple experts for a token, and that becomes another hyper parameter. That is, we can choose to activate two experts each time and add the output of both of them. 

Let us consider the number of parameters and analyze MoE with scaling laws. 
- The number of parameters increase drastically - MLP params $$\times N/2$$ (if we choose to activate 2 experts each time) where $$N$$ is the number of experts. For example, Deepseek V3 has 256 experts!
- Memory for parameters increases by the same factor. However, activations don’t consume too much additional memory.
Even though the parameters increase drastically, the compute only increases mildly. This is why Deepseek blew up. For much higher number of experts, the attention parameters decrease largely (sparse activations in experts), and they were able to train dense model equivalent model with much lower compute. 

People also tried to get scaling laws for MoEs and noticed that MoE is a much more compute-efficient model (it has a better scaling law).

Let us analyze the architecture again. We stuck to a Megatron like architecture for transformers. However, it would not work with MoE since all the weights are replicated number of expert times. So we perform parallelism in the dimension of experts. 
![](/assets/img/2025-01-06-data-systems-for-ml/17411455774654.jpg)

This way, each GPU would have different workloads (bubbles!) - A hot expert problem. Deepseek implemented many mechanisms to reduce this as much as possible. 

## Inference
So far we’ve studied that LLMs are slow and expensive to serve. At least 10 A100-40GB GPUs are required to serve 175B GPT-3 in half precision and generating 256 tokens takes approximately 20 seconds. 

The key factor is that we perform autoregressive decoding - we require the current token to predict the next token. Note that we predict until the pre-defined maximum length or we reach the end of sequence. 

The inference stage can be divided into two stages
1. Prefilling - Process all input tokens at once (prompt)
2. Decoding phase - Process a single token generated from previous iteration (generating)

The decoding phase is optimized with **Key-value cache** by saving attention keys and values during the decoding phase to avoid recomputations. 

In a broader sense, serving an LLM has more considerations during inference. For a larger group, it would be prudent to emphasize on throughput and if it is an individual then latency is a concern. How do we optimize for these scenarios?

Different techniques have been proposed. For the single user case, a popular technique has been **speculative decoding**. 

The latency is essentially 
$$
\text{latency} = \text{ step latency} \times \#\text{steps}
$$

With speculative decoding, we try and reduce the number of steps!