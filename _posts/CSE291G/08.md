# Policy Gradient Methods

The algorithms we have seen are for discrete MDPs; that is, the state-action space is discrete. However, many applications in real—life deal with high-dimensional spaces or continuous representations as we discussed before. In such cases, we need a more general case of algorithms arrive at the optimal policies in a computationally feasible manner.

## Policy Representations
The generalized representation of a policy that seems to have worked for us is a parameterized distribution for $$V, Q$$ or $$\pi$$. For example, $$\pi$$ could be a Gaussian probability distribution with $$\mu, \sigma$$ as the parameters. With this, thus, we move on from the tabular representations to a more generalized one. 

However do we convert representations to probabilities? One simple idea is as follows. Consider a policy parameterized with $$\theta \in \mathbb R^d$$. Then, for every state (continuous), think of mapping the actions to a real-space. 

$$
    \phi_s : A \to \mathbb R^d
$$ 

The probabilities can be obtained from $$\text{softmax}(Q_s(a)^T \cdot \theta)$$. It is one of the simplest ideas, a linear approach, to arrive at the probabilities.

There are some things to keep in mind when working with this new paradigm. If the representation space is not chosen properly, then a small change in the representation space, resulting in two different policies, can have a large difference in terms of the value functions. It can cause stability issues and other problems while using policy gradient approaches. 

## Policy updates
So, with this representation, how do we obtain the optimal policy?

The loss function is of the form $$J(\theta) = \sum_s \mu(s) V_\pi(s)$$. You can imagine, $$\theta$$ being a neural network and how this approach scales up. 

$$
\begin{align*}
    \nabla J(\theta) &= \frac{\partial J(\theta)}{\partial \theta} \\
    &= \sum_s \mu(s) \frac{\partial V_\pi(s)}{\partial \theta}
\end{align*} \\
&= \sum_s d^{\pi}(s) \sum_a Q_\pi(s, a) \frac{\partial}{\partial \theta} \pi(s, a, \theta)
$$

where $$d^\pi(s) = \sum_{k \geq 0} \gamma^k Pr(S_k = s)$$ - how likely does the state $$s$$ appear in a Markov chain. Although it seems complicated, it can be computed easily. 

> Do we sum across all states and actions again? We are trading space for computation time. 

Let us simplify the equation a bit more - 
$$
\begin{align*}
\nabla J(\theta) = \sum_s d^\pi(s) \sum_a \pi(s, a, \theta) Q_\pi(s, a) \frac{\partial}{\partial \theta} \ln \pi(s, a, \theta)
\end{align*}
$$

This equation form helps us to think about it in terms of expectations of the policy. Let us derive these equations. We start with the definition of $$J(\theta)$$

$$
\begin{align*}
\frac{\partial}{\partial \theta} &= \sum_s Pr(S_o = s) \frac{\partial}{\partial \theta} V(s) \\
\frac{\partial}{\partial \theta} V(s) &= \sum_a Q(s, a) \frac{\partial}{\partial \theta} \pi(s, a, \theta) + \sum_a \pi(s, a, \theta) \frac{\partial}{\partial \theta} Q(s, a) \\
\frac{\partial}{\partial \theta} Q(s, a) \gamma &= \sum_{s’} P(s, a, a’) \frac{\partial}{\partial \theta} V(s’) \\
\frac{\partial}{\partial \theta} V(s) &= \sum_a Q(s, a) \frac{\partial}{\partial \theta} \pi(s, a, \theta) + \gamma  \sum_a \pi(s, a, theta) \sum_{s’} P(s, a, s’) \frac{\partial}{\partial \theta} V(s’) \\
&= \sum_a Q(s, a) \frac{\partial}{\partial \theta} \pi(s, a, \theta) + \gamma \sum_{s’} P[s_1 = s’ \vert s_0 = s]  \frac{\partial}{\partial \theta} V(s’) \\

 \frac{\partial}{\partial \theta}  J(\theta) &= \sum_s Pr(s_0 = s) (\sum_a Q(s, a) \frac{\partial}{\partial \theta} \pi(s, a, \theta) + \gamma \sum_{s’} P[s_1 = s’ \vert s_0 = s]  \frac{\partial}{\partial \theta} V(s’)) \\
 &= \sum_{s, a} Pr(s_0 = s)Q(s, a)\frac{\partial}{\partial \theta} \pi(s, a, \theta) + \gamma \sum_{s’} P(s_1 = s’)  \frac{\partial}{\partial \theta} V(s’)) \\ 
\end{align*}
$$

We have unrolled the recursion once, but unrolling (tail recursion) it infinite times, would lead us to the equation we had before

$$
     \frac{\partial}{\partial \theta}  J(\theta) &= \sum_{s} d^\pi(s) \sum_a Q(s, a)\frac{\partial}{\partial \theta} \pi(s, a, \theta)
$$






